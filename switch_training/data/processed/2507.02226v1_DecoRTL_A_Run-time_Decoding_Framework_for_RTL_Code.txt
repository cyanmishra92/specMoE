=== ORIGINAL PDF: 2507.02226v1_DecoRTL_A_Run-time_Decoding_Framework_for_RTL_Code.pdf ===\n\nRaw text length: 52769 characters\nCleaned text length: 51714 characters\nNumber of segments: 30\n\n=== CLEANED TEXT ===\n\narXiv:2507.02226v1 [cs.PL] 3 Jul 2025 DecoRTL: A Run-time Decoding Framework for RTL Code Generation with LLMs Mohammad Akyash, Kimia Azar, Hadi Kamali Department of Electrical and Computer Engineering (ECE), University of Central Florida, Orlando, FL 32816, USA {mohammad.akyash, azar, Abstract As one of their many applications, large language models (LLMs) have recently shown promise in automating register transfer level (RTL) code generation. However, conven- tional LLM decoding strategies, originally designed for natural language, often fail to meet the structural and semantic demands of RTL, leading to hallucinated, repetitive, or invalid code outputs. In this paper, we first investigate the root causes of these decoding failures through an empirical analysis of token- level entropy during RTL generation. Our findings reveal that LLMs exhibit low confidence in regions of structural ambi- guity or semantic complexity, showing that standard decoding strategies fail to differentiate between regions requiring deter- minism (syntax-critical regions) and those that benefit from creative exploratory variability (design-critical regions). Then, to overcome this, we introduce DecoRTL, a novel run-time decoding strategy, that is both syntax-aware and contrastive for RTL code generation. DecoRTL integrates two complemen- tary components: (i) self-consistency sampling, which generates multiple candidates and re-ranks them based on token-level agreement to promote correctness while maintaining diversity; and (ii) syntax-aware temperature adaptation, which classifies tokens by their syntactical and functional roles and adjusts the sampling temperature accordingly, enforcing low temperature for syntax-critical tokens and higher temperature for exploratory ones. Our approach operates entirely at inference time without requiring any additional model fine-tuning. Through evaluations on multiple open-source LLMs using the VerilogEval benchmark, we demonstrate significant improvements in syntactic validity, functional correctness, and output diversity, while the execution overhead (performance overhead) is imperceptible1. Index Terms LLMs, RTL Code Generation, Decoding Strat- egy, Self-Consistency Sampling, Temperature Adaptation I. INTRODUCTION As hardware design becomes increasingly complex, ma- chine learning (ML) offers a tempting path forward, enabling faster register transfer level (RTL) code generation, reducing manual design effort, and opening the field to a broader community of designers [2]. With the rapid progress of large language models (LLMs), there is growing interest in leveraging them for hardware code synthesis from abstract design specifications [3], [4]. However, these models still face significant challenges in producing syntactically sound and semantically coherent RTL, particularly for structurally constrained and functionally complicated design scenarios. Current approaches to RTL code generation with LLM fall into two main categories [5]: (i) prompt engineering and (ii) fine-tuning. Prompt-based methods guide pre-trained 1Code is available at [1] models toward more accurate RTL generation through tailored instructions, task-specific context, or exemplars [3], [4]. In contrast, fine-tuning approaches focus on adapting LLMs using curated RTL datasets, either collected from open-source repositories [6], manually curated [7], or synthesized from high-level specifications [8], to expose models to a broader range of hardware design patterns. While both strategies are beneficial, they face critical limitations. Prompt engineering alone often lacks the robustness needed to consistently produce the needed (correct) RTL. On the other hand, fine-tuning demands large quantities of diverse, high-quality RTL data, which is difficult to collect and verify at scale. Despite these challenges, relatively little attention has been given to the decoding strategies used at inference time, which directly influences the quality, correctness, and diversity of LLMs outputs, i.e., the generated RTL code. All existing LLM- based RTL generation methods rely on conventional decoding strategies, which is developed for natural language generation. While standard decoding strategies have shown success in natural languages, they often fall short in code generation tasks [9], and RTL code is no exception. Unlike natural lan- guage, RTL code demands strict syntax semantic correctness, structural precision with cycle-accurate concurrency, and an appropriate level of design diversity creativity [10]. These limitations stems from two key issues: (i) The LLM with standard decoding often produces tokens with low confidence (i.e., high entropy), particularly in uncommon or complex regions of code, leading to unstable or incorrect outputs; (ii) TABLE I: Comparison of RTL Code Generation Strategies with LLMs. Feature Prompt Fine Decoding Engineering Tuning (Ours) Training Required No Yes No Dataset Requirement Low High None Compute Cost Low High Low HW Engineer Knowledge Required Medium High Low Adaptability to New Tasks Medium Low High Effectiveness on Structural Constraints Low Medium High Semantic Consistency Low Medium High Output Diversity Low Medium High Risk of Hallucination High Medium Low Implementation Complexity Low High Medium Reusability Across Models High Low High In standard decoding with fixed temperature, the model fails to account for the varying needs of different token types in the code (syntax-critical tokens such as delimiters and keywords require high determinism to ensure validity, while semantically rich or design-critical tokens benefit from higher variability to support meaningful exploration of the design space). To address these limitations, the proposed framework, DecoRTL, takes a first-of-its-kind step in shifting the focus of LLM-based RTL code generation from prompt engineering and fine-tuning to the unexplored space of decoding strategies, operating entirely at inference time. As summarized in Table I, and as shown in Fig. 1 in an illustrative example, decoding can offer a unique set of advantages, such as zero training cost, high adaptability, and improved control over structural and semantic correctness, yet has received little attention in prior LLM-based RTL generation work. DecoRTL lever- ages contrastive plus temperature-adaptive (C TA) decoding, providing a lightweight and model-agnostic alternative that enhances output quality without modifying the underlying LLM. Specifically, our contributions are as follows: (1) Empirical Analysis of Uncertainty and Context: We ana- lyze the softmax distributions over logits during RTL decoding and observe that tokens appearing in ambiguous contexts often exhibit higher entropy, indicating model uncertainty and aligning with regions prone to structural or semantic errors. To further investigate, we categorize RTL tokens into high-impact and structural classes and analyze their context across open- source Verilog code. Our analysis shows that the class of a token is often predictable from its preceding token, revealing consistent local syntactic patterns in RTL. (2) Self-Consistency via Contrastive Top-K Re-ranking: We propose an inference-time decoding method that improves self-consistency by selecting the top-K candidate tokens at each decoding step and re-ranking them via a contrastive mechanism, penalizing those overly similar to the mean em- bedding to improve diversity while preserving confidence and distinctiveness in token selection. (3) Syntax-Aware Temperature Adaptation: Building on our entropy analysis, we introduce a dynamic temperature adapta- tion mechanism that predicts the class of the next token based on the most recently generated token. Using this prediction, we adjust the sampling temperature, applying lower temperatures for syntax-critical structural tokens to ensure determinism, and higher temperatures for high-impact tokens for diversity. Through extensive experiments, we demonstrate the effec- tiveness of our domain-specific C TA decoding strategies. We evaluated our approach on multiple open-source LLMs using the VerilogEval benchmark [11] and show that it consistently improves both the synthesizability and functional correctness of generated RTL code across all models. In particular, these improvements come with minimal computational overhead, as DecoRTL operates purely at the decoding level. Our approach achieves these gains without any fine-tuning, additional data gathering, or expensive retraining which denotes the prac- ticality and generalizability of our method and make it a lightweight yet powerful solution for enhancing RTL code Write a Verilog module that computes the bitwise AND of two 8-bit inputs a and b, and outputs the result as out. Greedy Top-K Self-consistency Temperature Adaptation Instruction: module and_gate( input [7:0] a, input [7:0] b, output [7:0] out ); assign out a b; endmodule module and_gate( input [7:0] a, input [7:0] b, output [7:0] out ); wire [7:0] temp a b; assign out a b temp; endmodule module and_gate( input [7:0] a, input [7:0] b, output [7:0] out ); assign out a a a a a a; endmodule Fig. 1: RTL codes generated using different decoding strategies. Greedy and sampling-based methods often produce repetitive invalid code, while our ap- proach (Contrastive TA decoding) generates more diverse and syntactically correct RTL (Examples are abstracted due to space constrained)2. generation with existing models. II. RELATED WORKS A. LLM for Code Generation LLMs have achieved remarkable success in code generation tasks across a wide range of high-level (software) program- ming languages. Models, such as Codex [12], CodeGen [13], and AlphaCode [14], leverage massive pretraining on diverse code corpora and have demonstrated the ability to translate natural language instructions into syntactically correct and functionally coherent source code. Building on these capabilities, LLMs have recently been to a variety of hardware design and verification related tasks, including hardware debugging [17] design optimization [18] (e.g., pipelining or parallelization), and detection of hardware- oriented security vulnerabilities [19], [20]. In parallel, several efforts have explored the use of LLMs for RTL code genera- tion [3], [4], [21] [23]. Early approaches, such as ChatEDA [3] and ChipGPT [4], employed prompt engineering techniques to guide general-purpose models (such as GPT-3.5 or GPT-4) by embedding design specifications, toolchain feedback, and format constraints directly into the prompts. These prompt- based strategies aim to elicit accurate Verilog outputs and capture design intent or tool responses, but often require manual intervention, prompt iteration, or post-processing to ensure valid and synthesizable outputs. Beyond prompt engineering, more recent efforts have turned to fine-tuning open-source language models on RTL-specific datasets. VeriGen [24], which utilized Verilog data from public repositories for supervised training, and RTLCoder [8], which addressed dataset limitations by synthesizing instruction-code pairs uaing GPT3.5 to enhance diversity. Advancing this line of research, OriGen [7] introduced mechanisms like code augmentation and self-reflection to iteratively refine model outputs, while BetterV [25] focused on design optimization by aligning generation objectives with Power, Performance, 2Greedy decoding is deterministic but often yields short, repetitive, and structurally flawed RTL code. Beam search [15] improves coverage but tends to generate generic and redundant designs. Sampling methods like top-k [16] add diversity through randomness, but in RTL generation, they can introduce syntax errors, semantic inconsistencies, and hallucinated hardware logic. and Area (PPA) metrics. Additionally, CraftRTL [22] enriched model understanding by integrating auxiliary design artifacts, e.g., state diagrams and waveforms into the training process. While these methods have significantly advanced LLM- based RTL code generation, they share a common underlying limitation: they inherit natural language-oriented decoding strategies, which are not suited to the strict syntactic, struc- tural, and semantic constraints of RTL code. Specifically, they may produce hallucinated logic, incomplete modules, or structurally invalid outputs, especially when the model encounters complex or uncommon design scenarios. Moreover, even in fine-tuned models, decoding is typically performed using greedy search, beam search, or top-k sampling, which is not optimized for the unique demands of RTL. To date, no prior study has explored decoding-time adaptations as a means of improving RTL generation with LLMs. This is a critical gap in the field: while prompt engineering and fine-tuning rely on extensive human effort or large-scale dataset curation, decoding-time strategies offer a lightweight, generalizable, and model-agnostic alternative capable of significantly enhancing code quality without additional data collection or training. B. Decoding Strategies in LLMs LLMs generate text through an autoregressive decoding process, producing one token at a time based on the prob- ability distribution conditioned on the previously generated context [26]. While the model computes probabilities over the vocabulary, the actual output depends on the decoding strategy, which governs how tokens are selected. These strategies are broadly divided into the following categories: (1) Deterministic Decoding: Deterministic methods prioritize coherence and confidence by always selecting the most proba- ble tokens. For instance, greedy decoding chooses the highest- probability token at each step, making it simple and fast but of- ten resulting in repetitive or generic outputs [27]. Beam search [15] improves upon this by maintaining multiple candidate sequences to optimize the overall sequence-level likelihood. However, it still tends to favor low-diversity completions due to its emphasis on probability maximization. (2) Stochastic Probabilistic Decoding: Stochastic methods introduces controlled randomness to increase diversity and re- duce repetition. Temperature sampling [28] adjusts the sharp- ness of the probability distribution: lower temperatures make the model more confident (favoring top tokens), while higher values flatten the distribution to allow more exploration. Top-k sampling [16] restricts sampling to the k most likely tokens, while top-p (nucleus) sampling [29] dynamically selects from the smallest set of tokens whose cumulative probability ex- ceeds a threshold p, enabling adaptive diversity. (2) Constrastive Decoding: Contrastive methods [30] aims to improve generation quality by penalizing undesirable patterns rather than relying solely on probabilities. SimCTG [31] implements this by discouraging tokens that are semantically similar to recent context embeddings, thereby reducing degen- eration in natural language generation. Another variant [32] uses two models, an expert and an amateur , to prefer 0.38 0.23 0.20 0.11 0.04 0.38 0.23 0.20 0.11 0.04 0.38 0.23 0.20 0.11 0.04 0.38 0.23 0.20 0.11 0.04 0.32 0.28 0.11 0.13 0.09 1 2 3 4 5 Mean 1 2 3 4 5 Contrastive Re-Ranking Embedding Space Greedy Top-3 Nucleus (P 0.95) Selects Token With Highest Probability Select Top-3 and Chooses Randomly Select tokens above a cumulative probability 0.95 then chooses randomly selects top tokens, penalizes those similar to the context, and picks the one with the highest adjusted score. b 1'b0 c temp a assign sum a Code to be completed: b Fig. 2: The Overall Comparison of Greedy, Top-k, Nucleus, and Our Contrastive Decoding Strategies for RTL Token Generation. tokens endorsed by the expert but disfavored by the amateur, enhancing reasoning and reducing hallucination [33]. As shown in Figure 2, decoding strategies, whether deter- ministic, stochastic, or contrastive, vary in their implemen- tation, provide trade-offs between diversity, confidence, and syntactic control. While contrastive approaches have recently gained attention for their ability to improve generation diver- sity and consistency, they often rely on access to full hidden states, intermediate activations, or additional external models. These requirements limit their practicality and compatibility with structured, syntax-sensitive domains like RTL. In con- trast, our approach introduces a lightweight, decoding-time contrastive mechanism that is fully model-agnostic and easily deployable across pre-trained LLMs. By applying token-level re-ranking within the top-k candidates and penalizing options that are overly similar to the mean embedding, our method promotes output diversity without compromising syntactic fidelity or introducing architectural complexity. III. UNCERTAINTY IN RTL CODE GENERATION W LLMS This section presents a set of key empirical observations that motivate our decoding strategy for RTL code generation with LLMs. Through comparative analysis with natural language (NL) generation and a corpus-level examination of RTL syntax patterns, we identify characteristic entropy dynamics and local context regularities that proves the need for a syntax-aware, temperature-adaptive decoding approach. module MUX_4to1( input wire [3:0] data, input wire [1:0] sel, Output out ); assign out (sel 2'b00) ? data[0] : (sel 2'b01) ? data[1] : (sel 2'b10) ? data[2] : data[3]; endmodule Natural Language RTL Code 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0 Entropy 0 10 20 30 40 50 60 Token Index (sequence of appearance) Fig. 3: Token-wise Entropy and their Comparison between Prompts for Natural Language vs. that of RTL Generation. A. Entropy Patterns in Natural Language vs. RTL Generation To quantify the model s confidence (or the level of un- certainty) during generation, we conducted a comparative experiment between the RTL code and the NL outputs. We prompted a quantized Qwen2.5 model with two tasks: (1) a free-form NL request, and (2) a prompt for generating a simple RTL (Verilog) code. At each task, and during the decoding step, we recorded the softmax distribution over the vocabulary, computed its entropy, and plotted the token-wise entropy curves, as demonstrated in Fig. 3. The analysis mentioned above reveals key differences: (1) Higher Overall Uncertainty in NL: As shown in Fig. 3, the NL generation shows significantly higher mean entropy and greater variance (vs. RTL code generation), which reflects its inherently open-ended structure and the large number of plausible continuations at each step. (2) Localized Entropy Spikes in RTL Generation: While the RTL code shows overall lower entropy due to its rigid syntax, sharp entropy peaks occur at specific points, especially around control construsts (e.g., if, always, etc.) or block closures (e.g., endif, endmodule, etc.). These spikes reflect the potential ambiguity or uncommon contexts and correspond to regions with errors (e.g., missing or misplaced tokens). (3) Entropy Decay Across the RTL Sequence: At the be- ginning of the RTL code generation, e.g., modules headers, port declarations, signaling names, etc., entorpy is relatively high as the model explores naming and configuration options. As the code structure solidifies, entropy gradually decreases, showing growing model confidence. Such observations show the fragility of RTL code gen- eration, where small errors in high-entropy regions (e.g., missing syntactical requirements) can make a module non- synthesizable or functionally incorrect. As such, identifying and mitigating these uncertainty peaks is critical to improving generation quality for the RTL code. B. Structural Regularities in Token Contexts To further understand the structured nature of RTL code, we performed a corpus-level analysis on approximately 200K open-source Verilog modules. Our goal was to evaluate whether the local context preceding a token can predict its functional class (serves as an insight that informs our syntax- aware temperature adaptation mechanism. We focused on two classes of tokens in the RTL code (see Table II): Structural Tokens: (e.g., endmodule, begin, punctua- tions) that define code boundaries and syntax. High-impact Tokens: (e.g., operators, logic keywords) that carry significant semantic weight in hardware behavior. For each token in these categories (listed in Table II), we computed the most frequent preceding tokens to assess context predictability. As shown in Fig. 4: Structural tokens are typically preceded by control keywords or delimiters, indicating deterministic, syntax-sensitive re- gions where precise decoding is essential. TABLE II: Token classes used for syntax-aware temperature adaptation. Token Class Tokens High-impact , -, , , , , , , !, , , ! , , , , , ?, :, , , , Structural module, endmodule, input, output, inout, wire, reg, logic, parameter, assign, always, begin, end, if, else, case, default, for, while, ;, ,, ., [, ], (, ), {, }, posedge, negedge 1.8 1.6 1.4 1.0 0.8 0.6 0.4 0.2 0 Frequency ( 106) Preceding Token Top Preceding Tokens for Structural Tokens 1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0 Frequency ( 106) Preceding Token Top Preceding Tokens for High-impact Tokens (what is the token generated before high-impact tokens? (what is the token generated before structural tokens? ; , : ] ) ( input end wire [ reg output if begin module always else posedge { . } - ; [ ( begin : ] ) assign end - parameter , default else } ! Fig. 4: Distribution of Preceding Tokens for Structural and High-Impact Categories (For code generation purposes, only syntactic tokens such as keywords, operators, and punctuation are considered; variable names and identifiers are excluded from parsing.). High-impact tokens more often follow expressions, operands, or operators, reflecting semantically rich regions where exploration and diversity may yield better designs. These patterns confirm that the context of the most recently token provides a strong signal for adjusting decoding behavior. Structural tokens benefit from low-temperature (deterministic) decoding to maintain validity, while high-impact tokens can tolerate or even benefit from increased sampling temperature to support design variability. Together, these observations pro- vide the foundation for our decoding-time framework, which dynamically adapts sampling temperature and contrastive se- lection based on local context and token uncertainty. IV. PROPOSED METHOD: DECORTL Fig. 5 depicts an overview of the DecoRTL framework, illustrated through an example in which an LLM is tasked ... typedef enum logic [2:0] { IDLE 3'b000, INIT 3'b001, ABSORB_AD 3'b010, PROC_MSG 3'b011, FINAL_PROC 3'b100 } state_t; ... case (state) S1: state Instruction Verilog Code Top 5 next token , -, , , , , , , !, , , ! , , , , , ?, :, , , Re-ranking Write a Verilog module implementing an FSM controller for the Acorn lightweight AEAD cipher . module, endmodule, input, output, inout, wire, reg, logic, parameter, assign, else, case, etc. 0.38 IDLE 0.30 WAIT 0.10 S0 0.08 S1 0.08 READY ... 0.42 WAIT 0.28 IDLE 0.20 READY 0.05 S0 0.04 S1 ... Temperature Adjustment Pre-trained or Fine-tuned Higher Temperature Lower Temperature Fig. 5: Overview of DecoRTL Framework for RTL Generation. Given a design prompt (here the encryption controller FSM), the LLM generates RTL code (token by token) with two decoding-time enhancements: syntax-aware temperature adaptation adjusts sampling based on token type (the middle part), and contrastive re-ranking selects the next token by penalizing semantic redundancy among top candidates (the right part), improving RTL quality w o retraining. with generating the controller logic of a lightweight authen- ticated encryption with associated data (AEAD) module. Our proposed domain-specific decoding strategy to improve RTL code generation consists of two parts: (i) self-consistency re- ranking, which promotes stability and output diversity by re- evaluating top-k token candidates based on embedding simi- larity, and (ii) syntax-aware temperature adaptation, which dynamically modulates the sampling temperature according to the predicted class of the next token. As illustrated in Fig. 5, these components operate entirely at inference time and are designed to enhance both the syntactic validity and functional correctness of generated RTL code. A. Contrastive Self-Consistency Decoding In standard autoregressive decoding, a language model generates a sequence X (x1, x2, . . . , xt) one token at a time. At each time step t, the model outputs a vector of logits z(t) R V over the vocabulary V . After applying temperature scaling with parameter T, the probability of token x given the generated context x1:t is computed using the softmax function: p(x x1:t) exp(z(t) x T) P y V exp(z(t) y T) While this formula prioritizes high-likelihood tokens, it may lead to repetitive or semantically clustered outputs, which is error-prone in structured domains like RTL code. To address this, we introduce a contrastive re-ranking mechanism that pro- motes self-consistency and diversity among high-probability candidates. Our method refines token selection by using embedding-level similarity among the top-K candidates. First, we extract the top-K tokens from the probability distribution: {x(1), x(2), . . . , x(K)}, with corresponding log-probabilities: Li log p(x(i) x1:t), i 1, . . . , K. Each candidate x(i) is mapped to its embedding ei via the model s embedding matrix E as ei E(x(i)), and normalized so that ei 1. The mean embedding of the top-K candidates is computed as: e 1 K K X i 1 ei. This mean serves as a semantic reference point for evaluat- ing diversity. Then we calculate the cosine similarity between each candidate embedding and the mean is given by: sim(ei, e) ei e, where a higher similarity indicates a candidate is closer to the average, and thus more redundant. To promote diversity, we penalize candidates that are too close to this mean. The adjusted score is defined as: score(x(i)) Li 位 sim(ei, e), where 位 is a tunable hyperparameter controlling the trade- off between confidence and diversity. he next token is then selected as the candidate with the highest adjusted score: xt 1 arg max i {1,...,K} score(x(i)). This re-ranking strategy ensures that the selected token is both probabilistically sound and semantically distinct from the average of the high-probability set. From a probabilistic perspective, this modification can be viewed as adjusting the original distribution with a diversity-aware term. The original probability for a top-K candidate is: p(x(i) x1:t) exp(Li) Z , where Z is the partition function. After applying the con- trastive penalty, the unnormalized modified probability is: pmodified(x(i) x1:t) p(x(i) x1:t) exp( 位 sim(ei, e)). This re-weighting flattens overconfident, high-density re- gions in the distribution, where semantically redundant can- didates dominate, and redistributes probability mass toward diverse, yet still plausible tokens. As a result, it reduces the dominance of clustered tokens and produces outputs that are not only high-quality and coherent but also less prone to repetition and syntactic collapse. In LLMs, tokens with similar semantic and syntactic roles naturally cluster together in the embedding space. Thus, the top-K candidates often represent minor variations on a com- mon theme. By penalizing candidates with high cosine similar- ity to the mean embedding, our method discourages the model from repeatedly choosing near-duplicate tokens. This helps to produce a sharper, more definitive distribution, lowering the overall entropy, and promotes diversity by encouraging the selection of tokens that are distinct yet still plausible. The resulting output is both more stable and less susceptible to errors, which is critical in synthesizable RTL code where even a small mistake may render a design non-functional. B. Syntax-aware Temperature Adaptation In standard decoding, the temperature parameter T controls the entropy of the output distribution. Lower temperatures make the output more deterministic, while higher temperatures introduce more randomness and diversity. However, a fixed temperature throughout decoding fails to reflect the varying demands of different token types in RTL code where certain tokens require precise syntax, while others benefit from ex- ploratory flexibility. In DecoRTL, we introduce a syntax-aware temperature adaptation strategy that dynamically adjusts the sampling temperature at each decoding step based on the syntactic role of the next token. Rather than predicting token type directly, we infer it from the most recently generated token by leveraging structural patterns common in RTL. This approach is motivated by our earlier analysis (refer to Section III), which demonstrated that certain token categories are frequently preceded by consistent syntactic patterns. Based on the observation, we define two token classes: (1) Structural Tokens: Tokens that define the syntactic frame- work of Verilog code and must be generated with high deter- minism to maintain correctness. These include keywords that establish module structure (module, endmodule, input, output, wire, etc.), control flow constructs (if, else, case, for, while), and delimiters or punctuation symbols (;, ,, ., [, ], (, ), {, }). (1) High-impact Tokens: These tokens contribute directly to the functional and logical semantics of a design and benefit from moderate sampling diversity to allow expression of valid alternatives. This category includes arithmetic and bitwise operators ( , -, , , , , , ), logical and comparison operators ( , ! , , , , , , ), and conditional tokens (?, :, , , ). All remaining tokens do not require explicit control, in- cluding filler tokens in comments, identifiers in unambiguous contexts, or tokens in low-impact sections where variation does not significantly affect syntax or semantics. This adaptive decoding strategy dynamically modulates the model s sampling temperature based on the anticipated syntac- tic role of the next token. Given a token xt generated at time step t, we estimate the likely category of the upcoming token xt 1 as C(xt 1), and adjust the temperature accordingly: Tt 1 Tbase 0.1, if C(xt 1) is structural, Tbase 0.1, if C(xt 1) is high-impact, Tbase, otherwise. Algorithm 1 Token Generation via Contrastive Re-Ranking and Syntax-Aware Temperature Adaptation 1: Input: Instruction I, decoder d, initial temperature 0, penalty 位; 2: Initialize sequence T , temperature  0; 3: while not end-of-sequence do Step 1: Top-K Candidate Generation 4: Get logits from decoder d; 5: Apply temperature ; 6: Extract top-K tokens {x(1), . . . , x(K)}; 7: Extract log-probabilities {L1, . . . , LK}; Step 2: Contrastive Re-Ranking 8: Normalize embeddings: ei E(x(i)) E(x(i)) ; 9: Compute mean: e 1 K P ei; 10: Score: scorei Li 位 (ei e); 11: Select: x arg maxi scorei; Append x to T; Step 3: Syntax-Aware Temperature Adaptation 12: if next token likely structural (based on x ) then 13:  0 0.1; 14: else if next token likely high-impact then 15:  0 0.1; 16: else 17:  0; 18: end if 19: end while 20: Output: Generated token sequence T; This dynamic temperature is then applied to the softmax distribution when generating the token at time t 1: p(x x1:t; Tt 1) exp(zx Tt 1) P y V exp(zy Tt 1). Lower temperatures sharpen the distribution, yielding more deterministic outputs, while higher temperatures flatten it, promoting diversity. As a result, the decoder becomes more conservative in syntax-critical contexts and more exploratory in regions where semantic flexibility is beneficial. Algorithm 1 shows the detailed decoding procedure of the DecoRTL framework, which integrates both contrastive re- ranking and syntax-aware temperature adaptation to enhance RTL code generation. Starting with initial temperature 0, and a contrastive penalty parameter 位, the temperature variable  is dynamically adjusted throughout the decoding process based on the local context of the generated tokens. Each de- coding iteration consists of three main steps: (1) The decoder generates logits at the current temperature , and the top- K candidate tokens {x(1), . . . , x(K)} are selected along with their corresponding log-probabilities {L1, . . . , LK}; (2) Each token s embedding is compared to the mean embedding of the top-K candidates using cosine similarity, sim(ei, e), which quantifies semantic redundancy. Tokens more similar to the mean are penalized, and the token with the highest adjusted score is selected; (3) The temperature  is updated based on the class of the most recently generated token. If the token is structural, the temperature is decreased ( 0 0.1) to enforce deterministic sampling. If the token is high-impact, the temperature is increased ( 0 0.1) to encourage explo- ration. For all other token types (neither structural nor high- impact), the temperature is reset to its base value ( 0). This decoding procedure enables DecoRTL to dynamically TABLE III: Mean and Variance of Token-level Entropy during RTL Code Generation across Decoding Strategies. Strategy QwenCoder-2.5-14B CodeLlama-7B Mean Variance Mean Variance Top-k (k 10) 0.106 0.065 0.219 0.139 Nucleus (p 0.9) 0.176 0.144 0.275 0.179 Contrastive (Ours) 0.071 0.061 0.134 0.097 balance syntactic precision and semantic diversity, improving quality without requiring fine-tuning or more training. V. EXPERIMENTAL RESULTS To assess the effectiveness of our proposed decoding strate- gies for RTL generation, we conduct a series of quantitative experiments across multiple dimensions: generation stability based on the decoding strategy, functional correctness (and synthesizability) w.r.t. the decoding, and decoding efficiency. We leverage the VerilogEval benchmark (Human) [11] to evaluate models on realistic RTL design prompts. A diverse set of LLMs, either pre-trained or fine-tuned for RTL generation, varying in parameter size and architectural design, is used to ensure broad applicability. This includes QwenCoder-2.5- 14B [34], CodeLlama-7B [35], and CodeV [23], and the following demonstrates the consistent improvements enabled by our decoding framework over these LLMs. A. Entropy-Based Evaluation of Decoding Methods To evaluate our decoding method w.r.t. generation stability and model confidence, we conducted an experiment analyzing how different decoding strategies affect token-level entropy during Verilog code generation. Entropy reflects the uncer- tainty of the model s prediction, higher values show indecision, while lower entropy suggests confident and consistent output. We began by sampling 40 instruction-to-code prompts from the VerilogEval [11] and used them to generate Verilog mod- ules with two LLMs: QwenCoder-2.5 (14B) and CodeLlama (7B). Each prompt was decoded using three strategies: (1) top- k sampling with k 10, (2) nucleus sampling with p 0.9, and (3) our contrastive decoding method with k 5 and a penalty coefficient 位 0.5. For each method, we computed the entropy of the softmax distribution at each generation step and aggregated the mean and variance across all prompts. As shown in Table III, contrastive decoding consistently achieves a lower mean and variance in entropy compared to both top-k and nucleus sampling. A lower mean entropy indicates that the model is making more confident token selections, while a reduced entropy variance suggests that the model s confidence remains stable throughout the sequence. By flattening entropy spikes and narrowing the distribution of uncertainty, our decoding method improves the consistency and reliability of generated Verilog code, making it more aligned with the deterministic nature of RTL codes. B. Comparison with Fixed Temperature Decoding To evaluate the effectiveness of our syntax-aware adap- tive temperature mechanism, we compare it against fixed- temperature decoding strategies using the CodeLlama 7B TABLE IV: Functionality Correctness Comparison of Fixed vs. Adaptive Temperature Decoding (The Model is CodeLlama 7B). Decoding Strategy Functional Pass Rate ( ) Fixed Temperature (Temperature set to T 0.5) 18.5 Fixed Temperature (Temperature set to T 0.7) 18.5 Fixed Temperature (Temperature set to T 0.9) 19.2 Adaptive Temperature (Ours) 25.6 model. Specifically, we test three static temperature settings: T 0.5, T 0.7, and T 0.9, representing conservative, balanced, and exploratory decoding behaviors, respectively. For each temperature, we generate RTL code from a shared set of design instructions and evaluate the functionality of the output based on a pass fail criterion. As shown in Table IV, in fixed-temperature decoding, the model applies a uniform degree of randomness throughout generation, regardless of token context. Lower temperatures (e.g., T 0.5) bias the model toward high-confidence predictions, yielding deterministic but often repetitive and rigid code. In contrast, higher temperatures (e.g., T 0.9) promote diversity and exploration, but increase the likelihood of producing structurally invalid or semantically inconsistent outputs. Note that from a modeling perspective, adaptive temperature control better aligns with the internal behavior of transformer-based LLMs. During generation, different layers and attention heads specialize in capturing hierarchical and positional dependencies, especially important in structured domains like code. A fixed-temperature setting fails to leverage this internal structure, applying uniform sampling across both high-confidence and uncertain regions. C. Reduction of Hallucination and Repetition in RTL Code To evaluate the impact of our full decoding framework on RTL code quality, we analyze the outputs of both LLMs, CodeLlama 7B and QwenCoder-2.5 14B, before and after applying our combined contrastive temperature-adaptive decoding method. As shown in Table V, the baseline top- k decoding strategy exhibits a considerable number of both hallucinated3 and repetitive4 outputs. This is while applying our proposed decoding framework reduces these incidents. This highlights the complementary strengths of contrastive decoding and temperature adaptation: the former penalizes semantically redundant token choices, encouraging meaningful variation, while the latter enforces deterministic behavior in 3Hallucination in RTL code refers to the generation of syntactically valid but semantically incorrect or meaningless hardware constructs that do not correspond to the design intent, including illogical control flows. 4Repetition refers to the unintended duplication of RTL code segments, such as repeated logic, redundant declarations, or loops of identical expressions. TABLE V: Number of Hallucinated and Repetitive RTL Code Outputs before and after Applying Contrastive TA Decoding. Model Baseline (top-k) Contrastive TA Hallucinated Repetitive Hallucinated Repetitive CodeLlama 7B 18 9 3 None QwenCoder-2.5 14B 11 5 None None TABLE VI: RTL Generation Synthesizability Success Rate across Models. Model CodeLlama 7B QwenCoder 14B CodeV [23] (Pre-trained) (Pre-trained) (RTL Fine-tuned) Syn Syn Syn Syn Syn Syn Syn Syn Syn 1 5 10 1 5 10 1 5 10 Base 35.2 38.4 41.6 64.1 70.5 76.2 75.6 82.6 85.9 TA 41.6 44.2 47.4 75.0 79.4 83.9 83.3 87.1 90.3 C 42.3 46.1 49.3 74.3 78.8 84.6 82.6 87.8 89.1 C TA 46.1 49.3 52.5 80.4 82.0 85.9 88.4 91.6 93.5 : Base: Baseline decoding using top-k, TA: Temperature Adaptive Only, C: Contrastive Only, and C TA: Contrastive with Temperature Adaptive. : means synthesizability in i runs. TABLE VII: RTL Generation Functional Correctness Rate across Models. Model CodeLlama 7B QwenCoder 14B CodeV [23] (Pre-trained) (Pre-trained) (Fine-tuned for RTL) Pass Pass Pass Pass Pass Pass Pass Pass Pass 1 5 10 1 5 10 1 5 10 Base 18.2 22.7 24.3 37.1 44.8 50.6 53.2 65.1 68.5 TA 25.6 28.8 31.4 46.1 48.7 52.5 64.7 75.6 79.4 C 27.5 28.8 33.3 47.4 51.9 55.1 66.3 75.6 80.1 C TA 32.0 35.2 39.1 54.4 58.3 62.1 69.8 79.4 82.0 : Base: Baseline decoding using top-k, TA: Temperature Adaptive Only, C: Contrastive Only, and C TA: Contrastive with Temperature Adaptive. : means functional correctness in i runs. structurally sensitive regions and allows controlled exploration in semantically rich segments. D. Comparison with Base Decoding Techniques To show the overall effectiveness of our decoding approach, we evaluate it on three LLMs: (i) CodeLlama 7B (pre-trained), QwenCoder-2.5 14B (pre-trained), and CodeV (fine-tuned using RTL codes based on CodeQwen-2.5 7B). Our analysis focuses on two main metrics for RTL generation quality: Functional Correctness: If the generated RTL behaves ac- cording to its specification, tested via logic simulation. Synthesizability: If the RTL code can be successfully parsed, elaborated, and synthesized by a Verilog toolchain (Xilinx Vivado in our case), showing syntactic soundness. In all experiments, we use a contrastive candidate set size of k 5, a contrastive penalty coefficient of 位 0.5, and a base decoding temperature of 0.7. As summarized in Table VII and Table VI , our method outperforms standard decoding strategies, e.g., top-k sampling, showing that our proposed decoding framework significantly reduces functionality and synthesizability issues, consistently leading to higher success rate in both perspectives. E. Computational Efficiency of Contrastive Decoding We evaluated the computational efficiency of our contrastive decoding strategy on the QwenCoder-2.5-14B model. To en- sure a fair comparison of computational efficiency across de- coding strategies, we enabled Key-Value (KV) caching during inference. KV caching significantly accelerates autoregressive decoding by storing the intermediate key and value tensors computed at each step of generation. This allows the model to avoid recomputing the entire attention mechanism over the full sequence at each step and instead perform attention only over the new token, thereby reducing inference time from quadratic to linear complexity with respect to sequence length. As shown in Table VIII, the average decoding time per token increased marginally from 0.1383 to 0.1413 seconds when using contrastive re-ranking. Peak GPU memory usage remained nearly unchanged, with only a 0.5 increase. In our proposed contrastive decoding strategy, at each decoding step, we sample a small set of candidate tokens (e.g., top-3) and compute their modified scores by combining the model s predicted log-probabilities with a contrastive penalty based on semantic similarity. This re-ranking is efficient for two reasons, (i) it operates on a small subset of tokens, not the full vocabulary, limiting the cost of additional computation, (ii) the similarity computations are lightweight vector opera- tions (i.e., cosine similarity) between the precomputed token embeddings and the current context representation, avoiding any need for extra forward passes through the model. Because the token embeddings are already available at decoding time, the re-ranking step integrates seamlessly without disrupting the KV cache or triggering redundant computations. TABLE VIII: Decoding Efficiency Comparison on QwenCoder-2.5-14B. Method Avg. Time per Token (s) Peak Memory (MB) Baseline 0.1383 15012.32 Contrastive 0.1413 15089.43 VI. CONCLUSION In this paper, we investigated a critical yet underexamined dimension of RTL code generation with LLMs: the role of decoding strategies. While prior efforts have primarily focused on prompt engineering and fine-tuning, we demon- strate that decoding-time adaptations offer a lightweight and effective alternative for improving generation quality, without requiring additional data, model retraining, or architectural modifications. Through empirical analysis of entropy patterns and contextual token structures, we identify key challenges in RTL generation, including localized uncertainty and rigid syntactic dependencies. In response, we propose a domain- specific decoding framework that operates entirely at infer- ence time, combining two complementary techniques: (1) a contrastive self-consistency mechanism that re-ranks top-k candidates based on embedding-level diversity, encouraging confident yet diverse token selection; and (2) a syntax-aware temperature adaptation strategy that modulates sampling tem- perature based on the syntactic class of each token, effectively balancing determinism in structural regions with exploration in semantically rich contexts. Extensive experiments across three competitive LLMs, i.e., CodeLlama, QwenCoder, and CodeV, show that our method significantly improves both functional correctness and synthesizability, while also reducing hallucinations and repetition in the generated RTL code. REFERENCES [1] DecoRTL Repository - A Run-time Decoding Framework for RTL Code Generation with LLMs, 2025. [2] G. Huang, J. Hu, Y. He, J. Liu, M. Ma, Z. Shen, J. Wu, Y. Xu, H. Zhang, K. Zhong et al., Machine learning for electronic design automation: A survey, ACM Transactions on Design Automation of Electronic Systems (TODAES), vol. 26, no. 5, pp. 1 46, 2021. [3] H. Wu, Z. He, X. Zhang, X. Yao, S. Zheng, H. Zheng, and B. Yu, Chateda: A large language model powered autonomous agent for eda, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2024. [4] K. Chang, Y. Wang, H. Ren, M. Wang, S. Liang, Y. Han, H. Li, and X. Li, Chipgpt: How far are we from natural language hardware design, arXiv preprint arXiv:2305.14019, 2023. [5] M. Akyash and H. M Kamali, Evolutionary large language models for hardware security: A comparative survey, in Proceedings of the great lakes symposium on VLSI 2024, 2024, pp. 496 501. [6] M. Akyash, K. Azar, and H. Kamali, Rtl : Graph-enhanced llm for rtl code generation, arXiv preprint arXiv:2505.13479, 2025. [7] F. Cui, C. Yin, K. Zhou, Y. Xiao, G. Sun, Q. Xu, Q. Guo, D. Song, D. Lin, X. Zhang et al., Origen: Enhancing rtl code generation with code-to-code augmentation and self-reflection, arXiv preprint arXiv:2407.16237, 2024. [8] S. Liu, W. Fang, Y. Lu, J. Wang, Q. Zhang, H. Zhang, and Z. Xie, Rtl- coder: Fully open-source and efficient llm-assisted rtl code generation technique, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2024. [9] Y. Zhu, J. Li, G. Li, Y. Zhao, Z. Jin, and H. Mei, Hot or cold? adaptive temperature sampling for code generation with large language models, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 1, 2024, pp. 437 445. [10] M. Akyash and H. Mardani Kamali, Simeval: Investigating the similarity obstacle in llm-based hardware code generation, in Proceedings of the 30th Asia and South Pacific Design Automation Conference, ser. ASPDAC 25. New York, NY, USA: Association for Computing Machinery, 2025, p. 1002 1007. [Online]. Available: [11] M. Liu, N. Pinckney, B. Khailany, and H. Ren, Verilogeval: Evaluating large language models for verilog code generation, in 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD). IEEE, 2023, pp. 1 8. [12] M. Chen et al., Evaluating large language models trained on code, 2021. [Online]. Available: [13] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, Codegen: An open large language model for code with multi-turn program synthesis, arXiv preprint arXiv:2203.13474, 2022. [14] Y. Li et al., Competition-level code generation with alphacode, Science, vol. 378, no. 6624, p. 1092 1097, Dec. 2022. [Online]. Available: [15] A. Vijayakumar, M. Cogswell, R. Selvaraju, Q. Sun, S. Lee, D. Crandall, and D. Batra, Diverse beam search for improved description of complex scenes, Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, Apr. 2018. [Online]. Available: [16] A. Fan, M. Lewis, and Y. Dauphin, Hierarchical neural story generation, in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), I. Gurevych and Y. Miyao, Eds. Melbourne, Australia: Association for Computational Linguistics, Jul. 2018, pp. 889 898. [Online]. Available: [17] H. Huang, Z. Lin, Z. Wang, X. Chen, K. Ding, and J. Zhao, Towards llm-powered verilog rtl assistant: Self-verification and self-correction, arXiv preprint arXiv:2406.00115, 2024. [18] X. Yao, Y. Wang, X. Li, Y. Lian, R. Chen, L. Chen, M. Yuan, H. Xu, and B. Yu, Rtlrewriter: Methodologies for large models aided rtl code optimization, arXiv preprint arXiv:2409.11414, 2024. [19] M. Akyash and H. M. Kamali, Self-hwdebug: Automation of llm self- instructing for hardware security verification, in 2024 IEEE Computer Society Annual Symposium on VLSI (ISVLSI), 2024, pp. 391 396. [20] N. Mashnoor, M. Akyash, H. Kamali, and K. Azar, Llm-ift: Llm- powered information flow tracking for secure hardware, in 2025 IEEE 43rd VLSI Test Symposium (VTS), 2025, pp. 1 5. [21] Y. Zhang, Z. Yu, Y. Fu, C. Wan, and Y. C. Lin, Mg-verilog: Multi- grained dataset towards enhanced llm-assisted verilog generation, in 2024 IEEE LLM Aided Design Workshop (LAD). IEEE, 2024, pp. 1 5. [22] M. Liu, Y.-D. Tsai, W. Zhou, and H. Ren, Craftrtl: High-quality synthetic data generation for verilog code models with correct-by- construction non-textual representations and targeted code repair, arXiv preprint arXiv:2409.12993, 2024. [23] Y. Zhao, D. Huang, C. Li, P. Jin, Z. Nan, T. Ma, L. Qi, Y. Pan, Z. Zhang, R. Zhang et al., Codev: Empowering llms for verilog generation through multi-level summarization, arXiv preprint arXiv:2407.10424, 2024. [24] S. Thakur, B. Ahmad, H. Pearce, B. Tan, B. Dolan-Gavitt, R. Karri, and S. Garg, Verigen: A large language model for verilog code generation, ACM Transactions on Design Automation of Electronic Systems, vol. 29, no. 3, pp. 1 31, 2024. [25] Z. Pei, H.-L. Zhen, M. Yuan, Y. Huang, and B. Yu, Betterv: Con- trolled verilog generation with discriminative guidance, arXiv preprint arXiv:2402.03375, 2024. [26] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., Language mod- els are few-shot learners, Advances in neural information processing systems, vol. 33, pp. 1877 1901, 2020. [27] Y. Song, G. Wang, S. Li, and B. Y. Lin, The good, the bad, and the greedy: Evaluation of llms should not ignore non-determinism, arXiv preprint arXiv:2407.10457, 2024. [28] M. Renze, The effect of sampling temperature on problem solving in large language models, in Findings of the Association for Computa- tional Linguistics: EMNLP 2024, 2024, pp. 7346 7356. [29] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, The curious case of neural text degeneration, 2020. [Online]. Available: [30] Y.-S. Chuang, Y. Xie, H. Luo, Y. Kim, J. Glass, and P. He, Dola: Decoding by contrasting layers improves factuality in large language models, arXiv preprint arXiv:2309.03883, 2023. [31] Y. Su, T. Lan, Y. Wang, D. Yogatama, L. Kong, and N. Collier, A contrastive framework for neural text generation, 2022. [Online]. Available: [32] X. L. Li, A. Holtzman, D. Fried, P. Liang, J. Eisner, T. Hashimoto, L. Zettlemoyer, and M. Lewis, Contrastive decoding: Open- ended text generation as optimization, 2023. [Online]. Available: [33] S. O Brien and M. Lewis, Contrastive decoding improves reasoning in large language models, arXiv preprint arXiv:2309.09117, 2023. [34] B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J. Zhang, B. Yu, K. Lu et al., Qwen2. 5-coder technical report, arXiv preprint arXiv:2409.12186, 2024. [35] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, R. Sauvestre, T. Remez et al., Code llama: Open foundation models for code, arXiv preprint arXiv:2308.12950, 2023.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2507.02226v1 [cs.PL] 3 Jul 2025 DecoRTL: A Run-time Decoding Framework for RTL Code Generation with LLMs Mohammad Akyash, Kimia Azar, Hadi Kamali Department of Electrical and Computer Engineering (ECE), University of Central Florida, Orlando, FL 32816, USA {mohammad.akyash, azar, Abstract As one of their many applications, large language models (LLMs) have recently shown promise in automating register transfer level (RTL) code generation. However, conven- tional LLM decoding strategies, originally designed for natural language, often fail to meet the structural and semantic demands of RTL, leading to hallucinated, repetitive, or invalid code outputs. In this paper, we first investigate the root causes of these decoding failures through an empirical analysis of token- level entropy during RTL generation. Our findings reveal that LLMs exhibit low confidence in regions of structural ambi- guity or semantic complexity, showing that standard decoding strategies fail to differentiate between regions requiring deter- minism (syntax-critical regions) and those that benefit from creative exploratory variability (design-critical regions). Then, to overcome this, we introduce DecoRTL, a novel run-time decoding strategy, that is both syntax-aware and contrastive for RTL code generation. DecoRTL integrates two complemen- tary components: (i) self-consistency sampling, which generates multiple candidates and re-ranks them based on token-level agreement to promote correctness while maintaining diversity; and (ii) syntax-aware temperature adaptation, which classifies tokens by their syntactical and functional roles and adjusts the sampling temperature accordingly, enforcing low temperature for syntax-critical tokens and higher temperature for exploratory ones. Our approach operates entirely at inference time without requiring any additional model fine-tuning. Through evaluations on multiple open-source LLMs using the VerilogEval benchmark, we demonstrate significant improvements in syntactic validity, functional correctness, and output diversity, while the execution overhead (performance overhead) is imperceptible1. Index Terms LLMs, RTL Code Generation, Decoding Strat- egy, Self-Consistency Sampling, Temperature Adaptation I.\n\n--- Segment 2 ---\nThrough evaluations on multiple open-source LLMs using the VerilogEval benchmark, we demonstrate significant improvements in syntactic validity, functional correctness, and output diversity, while the execution overhead (performance overhead) is imperceptible1. Index Terms LLMs, RTL Code Generation, Decoding Strat- egy, Self-Consistency Sampling, Temperature Adaptation I. INTRODUCTION As hardware design becomes increasingly complex, ma- chine learning (ML) offers a tempting path forward, enabling faster register transfer level (RTL) code generation, reducing manual design effort, and opening the field to a broader community of designers [2]. With the rapid progress of large language models (LLMs), there is growing interest in leveraging them for hardware code synthesis from abstract design specifications [3], [4]. However, these models still face significant challenges in producing syntactically sound and semantically coherent RTL, particularly for structurally constrained and functionally complicated design scenarios. Current approaches to RTL code generation with LLM fall into two main categories [5]: (i) prompt engineering and (ii) fine-tuning. Prompt-based methods guide pre-trained 1Code is available at [1] models toward more accurate RTL generation through tailored instructions, task-specific context, or exemplars [3], [4]. In contrast, fine-tuning approaches focus on adapting LLMs using curated RTL datasets, either collected from open-source repositories [6], manually curated [7], or synthesized from high-level specifications [8], to expose models to a broader range of hardware design patterns. While both strategies are beneficial, they face critical limitations. Prompt engineering alone often lacks the robustness needed to consistently produce the needed (correct) RTL. On the other hand, fine-tuning demands large quantities of diverse, high-quality RTL data, which is difficult to collect and verify at scale. Despite these challenges, relatively little attention has been given to the decoding strategies used at inference time, which directly influences the quality, correctness, and diversity of LLMs outputs, i.e., the generated RTL code. All existing LLM- based RTL generation methods rely on conventional decoding strategies, which is developed for natural language generation. While standard decoding strategies have shown success in natural languages, they often fall short in code generation tasks [9], and RTL code is no exception.\n\n--- Segment 3 ---\nAll existing LLM- based RTL generation methods rely on conventional decoding strategies, which is developed for natural language generation. While standard decoding strategies have shown success in natural languages, they often fall short in code generation tasks [9], and RTL code is no exception. Unlike natural lan- guage, RTL code demands strict syntax semantic correctness, structural precision with cycle-accurate concurrency, and an appropriate level of design diversity creativity [10]. These limitations stems from two key issues: (i) The LLM with standard decoding often produces tokens with low confidence (i.e., high entropy), particularly in uncommon or complex regions of code, leading to unstable or incorrect outputs; (ii) TABLE I: Comparison of RTL Code Generation Strategies with LLMs. Feature Prompt Fine Decoding Engineering Tuning (Ours) Training Required No Yes No Dataset Requirement Low High None Compute Cost Low High Low HW Engineer Knowledge Required Medium High Low Adaptability to New Tasks Medium Low High Effectiveness on Structural Constraints Low Medium High Semantic Consistency Low Medium High Output Diversity Low Medium High Risk of Hallucination High Medium Low Implementation Complexity Low High Medium Reusability Across Models High Low High In standard decoding with fixed temperature, the model fails to account for the varying needs of different token types in the code (syntax-critical tokens such as delimiters and keywords require high determinism to ensure validity, while semantically rich or design-critical tokens benefit from higher variability to support meaningful exploration of the design space). To address these limitations, the proposed framework, DecoRTL, takes a first-of-its-kind step in shifting the focus of LLM-based RTL code generation from prompt engineering and fine-tuning to the unexplored space of decoding strategies, operating entirely at inference time. As summarized in Table I, and as shown in Fig. 1 in an illustrative example, decoding can offer a unique set of advantages, such as zero training cost, high adaptability, and improved control over structural and semantic correctness, yet has received little attention in prior LLM-based RTL generation work. DecoRTL lever- ages contrastive plus temperature-adaptive (C TA) decoding, providing a lightweight and model-agnostic alternative that enhances output quality without modifying the underlying LLM.\n\n--- Segment 4 ---\n1 in an illustrative example, decoding can offer a unique set of advantages, such as zero training cost, high adaptability, and improved control over structural and semantic correctness, yet has received little attention in prior LLM-based RTL generation work. DecoRTL lever- ages contrastive plus temperature-adaptive (C TA) decoding, providing a lightweight and model-agnostic alternative that enhances output quality without modifying the underlying LLM. Specifically, our contributions are as follows: (1) Empirical Analysis of Uncertainty and Context: We ana- lyze the softmax distributions over logits during RTL decoding and observe that tokens appearing in ambiguous contexts often exhibit higher entropy, indicating model uncertainty and aligning with regions prone to structural or semantic errors. To further investigate, we categorize RTL tokens into high-impact and structural classes and analyze their context across open- source Verilog code. Our analysis shows that the class of a token is often predictable from its preceding token, revealing consistent local syntactic patterns in RTL. (2) Self-Consistency via Contrastive Top-K Re-ranking: We propose an inference-time decoding method that improves self-consistency by selecting the top-K candidate tokens at each decoding step and re-ranking them via a contrastive mechanism, penalizing those overly similar to the mean em- bedding to improve diversity while preserving confidence and distinctiveness in token selection. (3) Syntax-Aware Temperature Adaptation: Building on our entropy analysis, we introduce a dynamic temperature adapta- tion mechanism that predicts the class of the next token based on the most recently generated token. Using this prediction, we adjust the sampling temperature, applying lower temperatures for syntax-critical structural tokens to ensure determinism, and higher temperatures for high-impact tokens for diversity. Through extensive experiments, we demonstrate the effec- tiveness of our domain-specific C TA decoding strategies. We evaluated our approach on multiple open-source LLMs using the VerilogEval benchmark [11] and show that it consistently improves both the synthesizability and functional correctness of generated RTL code across all models. In particular, these improvements come with minimal computational overhead, as DecoRTL operates purely at the decoding level.\n\n--- Segment 5 ---\nWe evaluated our approach on multiple open-source LLMs using the VerilogEval benchmark [11] and show that it consistently improves both the synthesizability and functional correctness of generated RTL code across all models. In particular, these improvements come with minimal computational overhead, as DecoRTL operates purely at the decoding level. Our approach achieves these gains without any fine-tuning, additional data gathering, or expensive retraining which denotes the prac- ticality and generalizability of our method and make it a lightweight yet powerful solution for enhancing RTL code Write a Verilog module that computes the bitwise AND of two 8-bit inputs a and b, and outputs the result as out. Greedy Top-K Self-consistency Temperature Adaptation Instruction: module and_gate( input [7:0] a, input [7:0] b, output [7:0] out ); assign out a b; endmodule module and_gate( input [7:0] a, input [7:0] b, output [7:0] out ); wire [7:0] temp a b; assign out a b temp; endmodule module and_gate( input [7:0] a, input [7:0] b, output [7:0] out ); assign out a a a a a a; endmodule Fig. 1: RTL codes generated using different decoding strategies. Greedy and sampling-based methods often produce repetitive invalid code, while our ap- proach (Contrastive TA decoding) generates more diverse and syntactically correct RTL (Examples are abstracted due to space constrained)2. generation with existing models. II. RELATED WORKS A. LLM for Code Generation LLMs have achieved remarkable success in code generation tasks across a wide range of high-level (software) program- ming languages. Models, such as Codex [12], CodeGen [13], and AlphaCode [14], leverage massive pretraining on diverse code corpora and have demonstrated the ability to translate natural language instructions into syntactically correct and functionally coherent source code. Building on these capabilities, LLMs have recently been to a variety of hardware design and verification related tasks, including hardware debugging [17] design optimization [18] (e.g., pipelining or parallelization), and detection of hardware- oriented security vulnerabilities [19], [20].\n\n--- Segment 6 ---\nModels, such as Codex [12], CodeGen [13], and AlphaCode [14], leverage massive pretraining on diverse code corpora and have demonstrated the ability to translate natural language instructions into syntactically correct and functionally coherent source code. Building on these capabilities, LLMs have recently been to a variety of hardware design and verification related tasks, including hardware debugging [17] design optimization [18] (e.g., pipelining or parallelization), and detection of hardware- oriented security vulnerabilities [19], [20]. In parallel, several efforts have explored the use of LLMs for RTL code genera- tion [3], [4], [21] [23]. Early approaches, such as ChatEDA [3] and ChipGPT [4], employed prompt engineering techniques to guide general-purpose models (such as GPT-3.5 or GPT-4) by embedding design specifications, toolchain feedback, and format constraints directly into the prompts. These prompt- based strategies aim to elicit accurate Verilog outputs and capture design intent or tool responses, but often require manual intervention, prompt iteration, or post-processing to ensure valid and synthesizable outputs. Beyond prompt engineering, more recent efforts have turned to fine-tuning open-source language models on RTL-specific datasets. VeriGen [24], which utilized Verilog data from public repositories for supervised training, and RTLCoder [8], which addressed dataset limitations by synthesizing instruction-code pairs uaing GPT3.5 to enhance diversity. Advancing this line of research, OriGen [7] introduced mechanisms like code augmentation and self-reflection to iteratively refine model outputs, while BetterV [25] focused on design optimization by aligning generation objectives with Power, Performance, 2Greedy decoding is deterministic but often yields short, repetitive, and structurally flawed RTL code. Beam search [15] improves coverage but tends to generate generic and redundant designs. Sampling methods like top-k [16] add diversity through randomness, but in RTL generation, they can introduce syntax errors, semantic inconsistencies, and hallucinated hardware logic. and Area (PPA) metrics. Additionally, CraftRTL [22] enriched model understanding by integrating auxiliary design artifacts, e.g., state diagrams and waveforms into the training process.\n\n--- Segment 7 ---\nand Area (PPA) metrics. Additionally, CraftRTL [22] enriched model understanding by integrating auxiliary design artifacts, e.g., state diagrams and waveforms into the training process. While these methods have significantly advanced LLM- based RTL code generation, they share a common underlying limitation: they inherit natural language-oriented decoding strategies, which are not suited to the strict syntactic, struc- tural, and semantic constraints of RTL code. Specifically, they may produce hallucinated logic, incomplete modules, or structurally invalid outputs, especially when the model encounters complex or uncommon design scenarios. Moreover, even in fine-tuned models, decoding is typically performed using greedy search, beam search, or top-k sampling, which is not optimized for the unique demands of RTL. To date, no prior study has explored decoding-time adaptations as a means of improving RTL generation with LLMs. This is a critical gap in the field: while prompt engineering and fine-tuning rely on extensive human effort or large-scale dataset curation, decoding-time strategies offer a lightweight, generalizable, and model-agnostic alternative capable of significantly enhancing code quality without additional data collection or training. B. Decoding Strategies in LLMs LLMs generate text through an autoregressive decoding process, producing one token at a time based on the prob- ability distribution conditioned on the previously generated context [26]. While the model computes probabilities over the vocabulary, the actual output depends on the decoding strategy, which governs how tokens are selected. These strategies are broadly divided into the following categories: (1) Deterministic Decoding: Deterministic methods prioritize coherence and confidence by always selecting the most proba- ble tokens. For instance, greedy decoding chooses the highest- probability token at each step, making it simple and fast but of- ten resulting in repetitive or generic outputs [27]. Beam search [15] improves upon this by maintaining multiple candidate sequences to optimize the overall sequence-level likelihood. However, it still tends to favor low-diversity completions due to its emphasis on probability maximization. (2) Stochastic Probabilistic Decoding: Stochastic methods introduces controlled randomness to increase diversity and re- duce repetition. Temperature sampling [28] adjusts the sharp- ness of the probability distribution: lower temperatures make the model more confident (favoring top tokens), while higher values flatten the distribution to allow more exploration.\n\n--- Segment 8 ---\n(2) Stochastic Probabilistic Decoding: Stochastic methods introduces controlled randomness to increase diversity and re- duce repetition. Temperature sampling [28] adjusts the sharp- ness of the probability distribution: lower temperatures make the model more confident (favoring top tokens), while higher values flatten the distribution to allow more exploration. Top-k sampling [16] restricts sampling to the k most likely tokens, while top-p (nucleus) sampling [29] dynamically selects from the smallest set of tokens whose cumulative probability ex- ceeds a threshold p, enabling adaptive diversity. (2) Constrastive Decoding: Contrastive methods [30] aims to improve generation quality by penalizing undesirable patterns rather than relying solely on probabilities. SimCTG [31] implements this by discouraging tokens that are semantically similar to recent context embeddings, thereby reducing degen- eration in natural language generation. Another variant [32] uses two models, an expert and an amateur , to prefer 0.38 0.23 0.20 0.11 0.04 0.38 0.23 0.20 0.11 0.04 0.38 0.23 0.20 0.11 0.04 0.38 0.23 0.20 0.11 0.04 0.32 0.28 0.11 0.13 0.09 1 2 3 4 5 Mean 1 2 3 4 5 Contrastive Re-Ranking Embedding Space Greedy Top-3 Nucleus (P 0.95) Selects Token With Highest Probability Select Top-3 and Chooses Randomly Select tokens above a cumulative probability 0.95 then chooses randomly selects top tokens, penalizes those similar to the context, and picks the one with the highest adjusted score. b 1'b0 c temp a assign sum a Code to be completed: b Fig. 2: The Overall Comparison of Greedy, Top-k, Nucleus, and Our Contrastive Decoding Strategies for RTL Token Generation. tokens endorsed by the expert but disfavored by the amateur, enhancing reasoning and reducing hallucination [33].\n\n--- Segment 9 ---\n2: The Overall Comparison of Greedy, Top-k, Nucleus, and Our Contrastive Decoding Strategies for RTL Token Generation. tokens endorsed by the expert but disfavored by the amateur, enhancing reasoning and reducing hallucination [33]. As shown in Figure 2, decoding strategies, whether deter- ministic, stochastic, or contrastive, vary in their implemen- tation, provide trade-offs between diversity, confidence, and syntactic control. While contrastive approaches have recently gained attention for their ability to improve generation diver- sity and consistency, they often rely on access to full hidden states, intermediate activations, or additional external models. These requirements limit their practicality and compatibility with structured, syntax-sensitive domains like RTL. In con- trast, our approach introduces a lightweight, decoding-time contrastive mechanism that is fully model-agnostic and easily deployable across pre-trained LLMs. By applying token-level re-ranking within the top-k candidates and penalizing options that are overly similar to the mean embedding, our method promotes output diversity without compromising syntactic fidelity or introducing architectural complexity. III. UNCERTAINTY IN RTL CODE GENERATION W LLMS This section presents a set of key empirical observations that motivate our decoding strategy for RTL code generation with LLMs. Through comparative analysis with natural language (NL) generation and a corpus-level examination of RTL syntax patterns, we identify characteristic entropy dynamics and local context regularities that proves the need for a syntax-aware, temperature-adaptive decoding approach. module MUX_4to1( input wire [3:0] data, input wire [1:0] sel, Output out ); assign out (sel 2'b00) ? data[0] : (sel 2'b01) ? data[1] : (sel 2'b10) ? data[2] : data[3]; endmodule Natural Language RTL Code 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0 Entropy 0 10 20 30 40 50 60 Token Index (sequence of appearance) Fig. 3: Token-wise Entropy and their Comparison between Prompts for Natural Language vs. that of RTL Generation.\n\n--- Segment 10 ---\ndata[2] : data[3]; endmodule Natural Language RTL Code 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0 Entropy 0 10 20 30 40 50 60 Token Index (sequence of appearance) Fig. 3: Token-wise Entropy and their Comparison between Prompts for Natural Language vs. that of RTL Generation. A. Entropy Patterns in Natural Language vs. RTL Generation To quantify the model s confidence (or the level of un- certainty) during generation, we conducted a comparative experiment between the RTL code and the NL outputs. We prompted a quantized Qwen2.5 model with two tasks: (1) a free-form NL request, and (2) a prompt for generating a simple RTL (Verilog) code. At each task, and during the decoding step, we recorded the softmax distribution over the vocabulary, computed its entropy, and plotted the token-wise entropy curves, as demonstrated in Fig. 3. The analysis mentioned above reveals key differences: (1) Higher Overall Uncertainty in NL: As shown in Fig. 3, the NL generation shows significantly higher mean entropy and greater variance (vs. RTL code generation), which reflects its inherently open-ended structure and the large number of plausible continuations at each step. (2) Localized Entropy Spikes in RTL Generation: While the RTL code shows overall lower entropy due to its rigid syntax, sharp entropy peaks occur at specific points, especially around control construsts (e.g., if, always, etc.) or block closures (e.g., endif, endmodule, etc.). These spikes reflect the potential ambiguity or uncommon contexts and correspond to regions with errors (e.g., missing or misplaced tokens). (3) Entropy Decay Across the RTL Sequence: At the be- ginning of the RTL code generation, e.g., modules headers, port declarations, signaling names, etc., entorpy is relatively high as the model explores naming and configuration options. As the code structure solidifies, entropy gradually decreases, showing growing model confidence. Such observations show the fragility of RTL code gen- eration, where small errors in high-entropy regions (e.g., missing syntactical requirements) can make a module non- synthesizable or functionally incorrect.\n\n--- Segment 11 ---\nAs the code structure solidifies, entropy gradually decreases, showing growing model confidence. Such observations show the fragility of RTL code gen- eration, where small errors in high-entropy regions (e.g., missing syntactical requirements) can make a module non- synthesizable or functionally incorrect. As such, identifying and mitigating these uncertainty peaks is critical to improving generation quality for the RTL code. B. Structural Regularities in Token Contexts To further understand the structured nature of RTL code, we performed a corpus-level analysis on approximately 200K open-source Verilog modules. Our goal was to evaluate whether the local context preceding a token can predict its functional class (serves as an insight that informs our syntax- aware temperature adaptation mechanism. We focused on two classes of tokens in the RTL code (see Table II): Structural Tokens: (e.g., endmodule, begin, punctua- tions) that define code boundaries and syntax. High-impact Tokens: (e.g., operators, logic keywords) that carry significant semantic weight in hardware behavior. For each token in these categories (listed in Table II), we computed the most frequent preceding tokens to assess context predictability. As shown in Fig. 4: Structural tokens are typically preceded by control keywords or delimiters, indicating deterministic, syntax-sensitive re- gions where precise decoding is essential. TABLE II: Token classes used for syntax-aware temperature adaptation. Token Class Tokens High-impact , -, , , , , , , !, , , ! , , , , , ?, :, , , , Structural module, endmodule, input, output, inout, wire, reg, logic, parameter, assign, always, begin, end, if, else, case, default, for, while, ;, ,, ., [, ], (, ), {, }, posedge, negedge 1.8 1.6 1.4 1.0 0.8 0.6 0.4 0.2 0 Frequency ( 106) Preceding Token Top Preceding Tokens for Structural Tokens 1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0 Frequency ( 106) Preceding Token Top Preceding Tokens for High-impact Tokens (what is the token generated before high-impact tokens?\n\n--- Segment 12 ---\nToken Class Tokens High-impact , -, , , , , , , !, , , ! , , , , , ?, :, , , , Structural module, endmodule, input, output, inout, wire, reg, logic, parameter, assign, always, begin, end, if, else, case, default, for, while, ;, ,, ., [, ], (, ), {, }, posedge, negedge 1.8 1.6 1.4 1.0 0.8 0.6 0.4 0.2 0 Frequency ( 106) Preceding Token Top Preceding Tokens for Structural Tokens 1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0 Frequency ( 106) Preceding Token Top Preceding Tokens for High-impact Tokens (what is the token generated before high-impact tokens? (what is the token generated before structural tokens? ; , : ] ) ( input end wire [ reg output if begin module always else posedge { . } - ; [ ( begin : ] ) assign end - parameter , default else } ! Fig. 4: Distribution of Preceding Tokens for Structural and High-Impact Categories (For code generation purposes, only syntactic tokens such as keywords, operators, and punctuation are considered; variable names and identifiers are excluded from parsing.). High-impact tokens more often follow expressions, operands, or operators, reflecting semantically rich regions where exploration and diversity may yield better designs. These patterns confirm that the context of the most recently token provides a strong signal for adjusting decoding behavior. Structural tokens benefit from low-temperature (deterministic) decoding to maintain validity, while high-impact tokens can tolerate or even benefit from increased sampling temperature to support design variability. Together, these observations pro- vide the foundation for our decoding-time framework, which dynamically adapts sampling temperature and contrastive se- lection based on local context and token uncertainty. IV. PROPOSED METHOD: DECORTL Fig.\n\n--- Segment 13 ---\nIV. PROPOSED METHOD: DECORTL Fig. 5 depicts an overview of the DecoRTL framework, illustrated through an example in which an LLM is tasked ... typedef enum logic [2:0] { IDLE 3'b000, INIT 3'b001, ABSORB_AD 3'b010, PROC_MSG 3'b011, FINAL_PROC 3'b100 } state_t; ... case (state) S1: state Instruction Verilog Code Top 5 next token , -, , , , , , , !, , , ! , , , , , ?, :, , , Re-ranking Write a Verilog module implementing an FSM controller for the Acorn lightweight AEAD cipher . module, endmodule, input, output, inout, wire, reg, logic, parameter, assign, else, case, etc. 0.38 IDLE 0.30 WAIT 0.10 S0 0.08 S1 0.08 READY ... 0.42 WAIT 0.28 IDLE 0.20 READY 0.05 S0 0.04 S1 ... Temperature Adjustment Pre-trained or Fine-tuned Higher Temperature Lower Temperature Fig. 5: Overview of DecoRTL Framework for RTL Generation. Given a design prompt (here the encryption controller FSM), the LLM generates RTL code (token by token) with two decoding-time enhancements: syntax-aware temperature adaptation adjusts sampling based on token type (the middle part), and contrastive re-ranking selects the next token by penalizing semantic redundancy among top candidates (the right part), improving RTL quality w o retraining. with generating the controller logic of a lightweight authen- ticated encryption with associated data (AEAD) module. Our proposed domain-specific decoding strategy to improve RTL code generation consists of two parts: (i) self-consistency re- ranking, which promotes stability and output diversity by re- evaluating top-k token candidates based on embedding simi- larity, and (ii) syntax-aware temperature adaptation, which dynamically modulates the sampling temperature according to the predicted class of the next token. As illustrated in Fig. 5, these components operate entirely at inference time and are designed to enhance both the syntactic validity and functional correctness of generated RTL code.\n\n--- Segment 14 ---\nAs illustrated in Fig. 5, these components operate entirely at inference time and are designed to enhance both the syntactic validity and functional correctness of generated RTL code. A. Contrastive Self-Consistency Decoding In standard autoregressive decoding, a language model generates a sequence X (x1, x2, . . . , xt) one token at a time. At each time step t, the model outputs a vector of logits z(t) R V over the vocabulary V . After applying temperature scaling with parameter T, the probability of token x given the generated context x1:t is computed using the softmax function: p(x x1:t) exp(z(t) x T) P y V exp(z(t) y T) While this formula prioritizes high-likelihood tokens, it may lead to repetitive or semantically clustered outputs, which is error-prone in structured domains like RTL code. To address this, we introduce a contrastive re-ranking mechanism that pro- motes self-consistency and diversity among high-probability candidates. Our method refines token selection by using embedding-level similarity among the top-K candidates. First, we extract the top-K tokens from the probability distribution: {x(1), x(2), . . . , x(K)}, with corresponding log-probabilities: Li log p(x(i) x1:t), i 1, . . . , K. Each candidate x(i) is mapped to its embedding ei via the model s embedding matrix E as ei E(x(i)), and normalized so that ei 1. The mean embedding of the top-K candidates is computed as: e 1 K K X i 1 ei. This mean serves as a semantic reference point for evaluat- ing diversity. Then we calculate the cosine similarity between each candidate embedding and the mean is given by: sim(ei, e) ei e, where a higher similarity indicates a candidate is closer to the average, and thus more redundant. To promote diversity, we penalize candidates that are too close to this mean. The adjusted score is defined as: score(x(i)) Li 位 sim(ei, e), where 位 is a tunable hyperparameter controlling the trade- off between confidence and diversity.\n\n--- Segment 15 ---\nTo promote diversity, we penalize candidates that are too close to this mean. The adjusted score is defined as: score(x(i)) Li 位 sim(ei, e), where 位 is a tunable hyperparameter controlling the trade- off between confidence and diversity. he next token is then selected as the candidate with the highest adjusted score: xt 1 arg max i {1,...,K} score(x(i)). This re-ranking strategy ensures that the selected token is both probabilistically sound and semantically distinct from the average of the high-probability set. From a probabilistic perspective, this modification can be viewed as adjusting the original distribution with a diversity-aware term. The original probability for a top-K candidate is: p(x(i) x1:t) exp(Li) Z , where Z is the partition function. After applying the con- trastive penalty, the unnormalized modified probability is: pmodified(x(i) x1:t) p(x(i) x1:t) exp( 位 sim(ei, e)). This re-weighting flattens overconfident, high-density re- gions in the distribution, where semantically redundant can- didates dominate, and redistributes probability mass toward diverse, yet still plausible tokens. As a result, it reduces the dominance of clustered tokens and produces outputs that are not only high-quality and coherent but also less prone to repetition and syntactic collapse. In LLMs, tokens with similar semantic and syntactic roles naturally cluster together in the embedding space. Thus, the top-K candidates often represent minor variations on a com- mon theme. By penalizing candidates with high cosine similar- ity to the mean embedding, our method discourages the model from repeatedly choosing near-duplicate tokens. This helps to produce a sharper, more definitive distribution, lowering the overall entropy, and promotes diversity by encouraging the selection of tokens that are distinct yet still plausible. The resulting output is both more stable and less susceptible to errors, which is critical in synthesizable RTL code where even a small mistake may render a design non-functional. B. Syntax-aware Temperature Adaptation In standard decoding, the temperature parameter T controls the entropy of the output distribution. Lower temperatures make the output more deterministic, while higher temperatures introduce more randomness and diversity.\n\n--- Segment 16 ---\nB. Syntax-aware Temperature Adaptation In standard decoding, the temperature parameter T controls the entropy of the output distribution. Lower temperatures make the output more deterministic, while higher temperatures introduce more randomness and diversity. However, a fixed temperature throughout decoding fails to reflect the varying demands of different token types in RTL code where certain tokens require precise syntax, while others benefit from ex- ploratory flexibility. In DecoRTL, we introduce a syntax-aware temperature adaptation strategy that dynamically adjusts the sampling temperature at each decoding step based on the syntactic role of the next token. Rather than predicting token type directly, we infer it from the most recently generated token by leveraging structural patterns common in RTL. This approach is motivated by our earlier analysis (refer to Section III), which demonstrated that certain token categories are frequently preceded by consistent syntactic patterns. Based on the observation, we define two token classes: (1) Structural Tokens: Tokens that define the syntactic frame- work of Verilog code and must be generated with high deter- minism to maintain correctness. These include keywords that establish module structure (module, endmodule, input, output, wire, etc. ), control flow constructs (if, else, case, for, while), and delimiters or punctuation symbols (;, ,, ., [, ], (, ), {, }). (1) High-impact Tokens: These tokens contribute directly to the functional and logical semantics of a design and benefit from moderate sampling diversity to allow expression of valid alternatives. This category includes arithmetic and bitwise operators ( , -, , , , , , ), logical and comparison operators ( , ! , , , , , , ), and conditional tokens (?, :, , , ). All remaining tokens do not require explicit control, in- cluding filler tokens in comments, identifiers in unambiguous contexts, or tokens in low-impact sections where variation does not significantly affect syntax or semantics. This adaptive decoding strategy dynamically modulates the model s sampling temperature based on the anticipated syntac- tic role of the next token. Given a token xt generated at time step t, we estimate the likely category of the upcoming token xt 1 as C(xt 1), and adjust the temperature accordingly: Tt 1 Tbase 0.1, if C(xt 1) is structural, Tbase 0.1, if C(xt 1) is high-impact, Tbase, otherwise.\n\n--- Segment 17 ---\nThis adaptive decoding strategy dynamically modulates the model s sampling temperature based on the anticipated syntac- tic role of the next token. Given a token xt generated at time step t, we estimate the likely category of the upcoming token xt 1 as C(xt 1), and adjust the temperature accordingly: Tt 1 Tbase 0.1, if C(xt 1) is structural, Tbase 0.1, if C(xt 1) is high-impact, Tbase, otherwise. Algorithm 1 Token Generation via Contrastive Re-Ranking and Syntax-Aware Temperature Adaptation 1: Input: Instruction I, decoder d, initial temperature 0, penalty 位; 2: Initialize sequence T , temperature  0; 3: while not end-of-sequence do Step 1: Top-K Candidate Generation 4: Get logits from decoder d; 5: Apply temperature ; 6: Extract top-K tokens {x(1), . . . , x(K)}; 7: Extract log-probabilities {L1, . . . , LK}; Step 2: Contrastive Re-Ranking 8: Normalize embeddings: ei E(x(i)) E(x(i)) ; 9: Compute mean: e 1 K P ei; 10: Score: scorei Li 位 (ei e); 11: Select: x arg maxi scorei; Append x to T; Step 3: Syntax-Aware Temperature Adaptation 12: if next token likely structural (based on x ) then 13:  0 0.1; 14: else if next token likely high-impact then 15:  0 0.1; 16: else 17:  0; 18: end if 19: end while 20: Output: Generated token sequence T; This dynamic temperature is then applied to the softmax distribution when generating the token at time t 1: p(x x1:t; Tt 1) exp(zx Tt 1) P y V exp(zy Tt 1). Lower temperatures sharpen the distribution, yielding more deterministic outputs, while higher temperatures flatten it, promoting diversity. As a result, the decoder becomes more conservative in syntax-critical contexts and more exploratory in regions where semantic flexibility is beneficial.\n\n--- Segment 18 ---\nLower temperatures sharpen the distribution, yielding more deterministic outputs, while higher temperatures flatten it, promoting diversity. As a result, the decoder becomes more conservative in syntax-critical contexts and more exploratory in regions where semantic flexibility is beneficial. Algorithm 1 shows the detailed decoding procedure of the DecoRTL framework, which integrates both contrastive re- ranking and syntax-aware temperature adaptation to enhance RTL code generation. Starting with initial temperature 0, and a contrastive penalty parameter 位, the temperature variable  is dynamically adjusted throughout the decoding process based on the local context of the generated tokens. Each de- coding iteration consists of three main steps: (1) The decoder generates logits at the current temperature , and the top- K candidate tokens {x(1), . . . , x(K)} are selected along with their corresponding log-probabilities {L1, . . . , LK}; (2) Each token s embedding is compared to the mean embedding of the top-K candidates using cosine similarity, sim(ei, e), which quantifies semantic redundancy. Tokens more similar to the mean are penalized, and the token with the highest adjusted score is selected; (3) The temperature  is updated based on the class of the most recently generated token. If the token is structural, the temperature is decreased ( 0 0.1) to enforce deterministic sampling. If the token is high-impact, the temperature is increased ( 0 0.1) to encourage explo- ration. For all other token types (neither structural nor high- impact), the temperature is reset to its base value ( 0). This decoding procedure enables DecoRTL to dynamically TABLE III: Mean and Variance of Token-level Entropy during RTL Code Generation across Decoding Strategies. Strategy QwenCoder-2.5-14B CodeLlama-7B Mean Variance Mean Variance Top-k (k 10) 0.106 0.065 0.219 0.139 Nucleus (p 0.9) 0.176 0.144 0.275 0.179 Contrastive (Ours) 0.071 0.061 0.134 0.097 balance syntactic precision and semantic diversity, improving quality without requiring fine-tuning or more training.\n\n--- Segment 19 ---\nThis decoding procedure enables DecoRTL to dynamically TABLE III: Mean and Variance of Token-level Entropy during RTL Code Generation across Decoding Strategies. Strategy QwenCoder-2.5-14B CodeLlama-7B Mean Variance Mean Variance Top-k (k 10) 0.106 0.065 0.219 0.139 Nucleus (p 0.9) 0.176 0.144 0.275 0.179 Contrastive (Ours) 0.071 0.061 0.134 0.097 balance syntactic precision and semantic diversity, improving quality without requiring fine-tuning or more training. V. EXPERIMENTAL RESULTS To assess the effectiveness of our proposed decoding strate- gies for RTL generation, we conduct a series of quantitative experiments across multiple dimensions: generation stability based on the decoding strategy, functional correctness (and synthesizability) w.r.t. the decoding, and decoding efficiency. We leverage the VerilogEval benchmark (Human) [11] to evaluate models on realistic RTL design prompts. A diverse set of LLMs, either pre-trained or fine-tuned for RTL generation, varying in parameter size and architectural design, is used to ensure broad applicability. This includes QwenCoder-2.5- 14B [34], CodeLlama-7B [35], and CodeV [23], and the following demonstrates the consistent improvements enabled by our decoding framework over these LLMs. A. Entropy-Based Evaluation of Decoding Methods To evaluate our decoding method w.r.t. generation stability and model confidence, we conducted an experiment analyzing how different decoding strategies affect token-level entropy during Verilog code generation. Entropy reflects the uncer- tainty of the model s prediction, higher values show indecision, while lower entropy suggests confident and consistent output. We began by sampling 40 instruction-to-code prompts from the VerilogEval [11] and used them to generate Verilog mod- ules with two LLMs: QwenCoder-2.5 (14B) and CodeLlama (7B).\n\n--- Segment 20 ---\nEntropy reflects the uncer- tainty of the model s prediction, higher values show indecision, while lower entropy suggests confident and consistent output. We began by sampling 40 instruction-to-code prompts from the VerilogEval [11] and used them to generate Verilog mod- ules with two LLMs: QwenCoder-2.5 (14B) and CodeLlama (7B). Each prompt was decoded using three strategies: (1) top- k sampling with k 10, (2) nucleus sampling with p 0.9, and (3) our contrastive decoding method with k 5 and a penalty coefficient 位 0.5. For each method, we computed the entropy of the softmax distribution at each generation step and aggregated the mean and variance across all prompts. As shown in Table III, contrastive decoding consistently achieves a lower mean and variance in entropy compared to both top-k and nucleus sampling. A lower mean entropy indicates that the model is making more confident token selections, while a reduced entropy variance suggests that the model s confidence remains stable throughout the sequence. By flattening entropy spikes and narrowing the distribution of uncertainty, our decoding method improves the consistency and reliability of generated Verilog code, making it more aligned with the deterministic nature of RTL codes. B. Comparison with Fixed Temperature Decoding To evaluate the effectiveness of our syntax-aware adap- tive temperature mechanism, we compare it against fixed- temperature decoding strategies using the CodeLlama 7B TABLE IV: Functionality Correctness Comparison of Fixed vs. Adaptive Temperature Decoding (The Model is CodeLlama 7B). Decoding Strategy Functional Pass Rate ( ) Fixed Temperature (Temperature set to T 0.5) 18.5 Fixed Temperature (Temperature set to T 0.7) 18.5 Fixed Temperature (Temperature set to T 0.9) 19.2 Adaptive Temperature (Ours) 25.6 model. Specifically, we test three static temperature settings: T 0.5, T 0.7, and T 0.9, representing conservative, balanced, and exploratory decoding behaviors, respectively. For each temperature, we generate RTL code from a shared set of design instructions and evaluate the functionality of the output based on a pass fail criterion.\n\n--- Segment 21 ---\nSpecifically, we test three static temperature settings: T 0.5, T 0.7, and T 0.9, representing conservative, balanced, and exploratory decoding behaviors, respectively. For each temperature, we generate RTL code from a shared set of design instructions and evaluate the functionality of the output based on a pass fail criterion. As shown in Table IV, in fixed-temperature decoding, the model applies a uniform degree of randomness throughout generation, regardless of token context. Lower temperatures (e.g., T 0.5) bias the model toward high-confidence predictions, yielding deterministic but often repetitive and rigid code. In contrast, higher temperatures (e.g., T 0.9) promote diversity and exploration, but increase the likelihood of producing structurally invalid or semantically inconsistent outputs. Note that from a modeling perspective, adaptive temperature control better aligns with the internal behavior of transformer-based LLMs. During generation, different layers and attention heads specialize in capturing hierarchical and positional dependencies, especially important in structured domains like code. A fixed-temperature setting fails to leverage this internal structure, applying uniform sampling across both high-confidence and uncertain regions. C. Reduction of Hallucination and Repetition in RTL Code To evaluate the impact of our full decoding framework on RTL code quality, we analyze the outputs of both LLMs, CodeLlama 7B and QwenCoder-2.5 14B, before and after applying our combined contrastive temperature-adaptive decoding method. As shown in Table V, the baseline top- k decoding strategy exhibits a considerable number of both hallucinated3 and repetitive4 outputs. This is while applying our proposed decoding framework reduces these incidents. This highlights the complementary strengths of contrastive decoding and temperature adaptation: the former penalizes semantically redundant token choices, encouraging meaningful variation, while the latter enforces deterministic behavior in 3Hallucination in RTL code refers to the generation of syntactically valid but semantically incorrect or meaningless hardware constructs that do not correspond to the design intent, including illogical control flows. 4Repetition refers to the unintended duplication of RTL code segments, such as repeated logic, redundant declarations, or loops of identical expressions. TABLE V: Number of Hallucinated and Repetitive RTL Code Outputs before and after Applying Contrastive TA Decoding.\n\n--- Segment 22 ---\n4Repetition refers to the unintended duplication of RTL code segments, such as repeated logic, redundant declarations, or loops of identical expressions. TABLE V: Number of Hallucinated and Repetitive RTL Code Outputs before and after Applying Contrastive TA Decoding. Model Baseline (top-k) Contrastive TA Hallucinated Repetitive Hallucinated Repetitive CodeLlama 7B 18 9 3 None QwenCoder-2.5 14B 11 5 None None TABLE VI: RTL Generation Synthesizability Success Rate across Models. Model CodeLlama 7B QwenCoder 14B CodeV [23] (Pre-trained) (Pre-trained) (RTL Fine-tuned) Syn Syn Syn Syn Syn Syn Syn Syn Syn 1 5 10 1 5 10 1 5 10 Base 35.2 38.4 41.6 64.1 70.5 76.2 75.6 82.6 85.9 TA 41.6 44.2 47.4 75.0 79.4 83.9 83.3 87.1 90.3 C 42.3 46.1 49.3 74.3 78.8 84.6 82.6 87.8 89.1 C TA 46.1 49.3 52.5 80.4 82.0 85.9 88.4 91.6 93.5 : Base: Baseline decoding using top-k, TA: Temperature Adaptive Only, C: Contrastive Only, and C TA: Contrastive with Temperature Adaptive. : means synthesizability in i runs. TABLE VII: RTL Generation Functional Correctness Rate across Models.\n\n--- Segment 23 ---\n: means synthesizability in i runs. TABLE VII: RTL Generation Functional Correctness Rate across Models. Model CodeLlama 7B QwenCoder 14B CodeV [23] (Pre-trained) (Pre-trained) (Fine-tuned for RTL) Pass Pass Pass Pass Pass Pass Pass Pass Pass 1 5 10 1 5 10 1 5 10 Base 18.2 22.7 24.3 37.1 44.8 50.6 53.2 65.1 68.5 TA 25.6 28.8 31.4 46.1 48.7 52.5 64.7 75.6 79.4 C 27.5 28.8 33.3 47.4 51.9 55.1 66.3 75.6 80.1 C TA 32.0 35.2 39.1 54.4 58.3 62.1 69.8 79.4 82.0 : Base: Baseline decoding using top-k, TA: Temperature Adaptive Only, C: Contrastive Only, and C TA: Contrastive with Temperature Adaptive. : means functional correctness in i runs. structurally sensitive regions and allows controlled exploration in semantically rich segments. D. Comparison with Base Decoding Techniques To show the overall effectiveness of our decoding approach, we evaluate it on three LLMs: (i) CodeLlama 7B (pre-trained), QwenCoder-2.5 14B (pre-trained), and CodeV (fine-tuned using RTL codes based on CodeQwen-2.5 7B). Our analysis focuses on two main metrics for RTL generation quality: Functional Correctness: If the generated RTL behaves ac- cording to its specification, tested via logic simulation. Synthesizability: If the RTL code can be successfully parsed, elaborated, and synthesized by a Verilog toolchain (Xilinx Vivado in our case), showing syntactic soundness. In all experiments, we use a contrastive candidate set size of k 5, a contrastive penalty coefficient of 位 0.5, and a base decoding temperature of 0.7. As summarized in Table VII and Table VI , our method outperforms standard decoding strategies, e.g., top-k sampling, showing that our proposed decoding framework significantly reduces functionality and synthesizability issues, consistently leading to higher success rate in both perspectives.\n\n--- Segment 24 ---\nIn all experiments, we use a contrastive candidate set size of k 5, a contrastive penalty coefficient of 位 0.5, and a base decoding temperature of 0.7. As summarized in Table VII and Table VI , our method outperforms standard decoding strategies, e.g., top-k sampling, showing that our proposed decoding framework significantly reduces functionality and synthesizability issues, consistently leading to higher success rate in both perspectives. E. Computational Efficiency of Contrastive Decoding We evaluated the computational efficiency of our contrastive decoding strategy on the QwenCoder-2.5-14B model. To en- sure a fair comparison of computational efficiency across de- coding strategies, we enabled Key-Value (KV) caching during inference. KV caching significantly accelerates autoregressive decoding by storing the intermediate key and value tensors computed at each step of generation. This allows the model to avoid recomputing the entire attention mechanism over the full sequence at each step and instead perform attention only over the new token, thereby reducing inference time from quadratic to linear complexity with respect to sequence length. As shown in Table VIII, the average decoding time per token increased marginally from 0.1383 to 0.1413 seconds when using contrastive re-ranking. Peak GPU memory usage remained nearly unchanged, with only a 0.5 increase. In our proposed contrastive decoding strategy, at each decoding step, we sample a small set of candidate tokens (e.g., top-3) and compute their modified scores by combining the model s predicted log-probabilities with a contrastive penalty based on semantic similarity. This re-ranking is efficient for two reasons, (i) it operates on a small subset of tokens, not the full vocabulary, limiting the cost of additional computation, (ii) the similarity computations are lightweight vector opera- tions (i.e., cosine similarity) between the precomputed token embeddings and the current context representation, avoiding any need for extra forward passes through the model. Because the token embeddings are already available at decoding time, the re-ranking step integrates seamlessly without disrupting the KV cache or triggering redundant computations. TABLE VIII: Decoding Efficiency Comparison on QwenCoder-2.5-14B. Method Avg.\n\n--- Segment 25 ---\nTABLE VIII: Decoding Efficiency Comparison on QwenCoder-2.5-14B. Method Avg. Time per Token (s) Peak Memory (MB) Baseline 0.1383 15012.32 Contrastive 0.1413 15089.43 VI. CONCLUSION In this paper, we investigated a critical yet underexamined dimension of RTL code generation with LLMs: the role of decoding strategies. While prior efforts have primarily focused on prompt engineering and fine-tuning, we demon- strate that decoding-time adaptations offer a lightweight and effective alternative for improving generation quality, without requiring additional data, model retraining, or architectural modifications. Through empirical analysis of entropy patterns and contextual token structures, we identify key challenges in RTL generation, including localized uncertainty and rigid syntactic dependencies. In response, we propose a domain- specific decoding framework that operates entirely at infer- ence time, combining two complementary techniques: (1) a contrastive self-consistency mechanism that re-ranks top-k candidates based on embedding-level diversity, encouraging confident yet diverse token selection; and (2) a syntax-aware temperature adaptation strategy that modulates sampling tem- perature based on the syntactic class of each token, effectively balancing determinism in structural regions with exploration in semantically rich contexts. Extensive experiments across three competitive LLMs, i.e., CodeLlama, QwenCoder, and CodeV, show that our method significantly improves both functional correctness and synthesizability, while also reducing hallucinations and repetition in the generated RTL code. REFERENCES [1] DecoRTL Repository - A Run-time Decoding Framework for RTL Code Generation with LLMs, 2025. [2] G. Huang, J. Hu, Y. He, J. Liu, M. Ma, Z. Shen, J. Wu, Y. Xu, H. Zhang, K. Zhong et al., Machine learning for electronic design automation: A survey, ACM Transactions on Design Automation of Electronic Systems (TODAES), vol. 26, no. 5, pp. 1 46, 2021. [3] H. Wu, Z.\n\n--- Segment 26 ---\n1 46, 2021. [3] H. Wu, Z. He, X. Zhang, X. Yao, S. Zheng, H. Zheng, and B. Yu, Chateda: A large language model powered autonomous agent for eda, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2024. [4] K. Chang, Y. Wang, H. Ren, M. Wang, S. Liang, Y. Han, H. Li, and X. Li, Chipgpt: How far are we from natural language hardware design, arXiv preprint arXiv:2305.14019, 2023. [5] M. Akyash and H. M Kamali, Evolutionary large language models for hardware security: A comparative survey, in Proceedings of the great lakes symposium on VLSI 2024, 2024, pp. 496 501. [6] M. Akyash, K. Azar, and H. Kamali, Rtl : Graph-enhanced llm for rtl code generation, arXiv preprint arXiv:2505.13479, 2025. [7] F. Cui, C. Yin, K. Zhou, Y. Xiao, G. Sun, Q. Xu, Q. Guo, D. Song, D. Lin, X. Zhang et al., Origen: Enhancing rtl code generation with code-to-code augmentation and self-reflection, arXiv preprint arXiv:2407.16237, 2024. [8] S. Liu, W. Fang, Y. Lu, J. Wang, Q. Zhang, H. Zhang, and Z. Xie, Rtl- coder: Fully open-source and efficient llm-assisted rtl code generation technique, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2024. [9] Y. Zhu, J. Li, G. Li, Y. Zhao, Z. Jin, and H. Mei, Hot or cold? adaptive temperature sampling for code generation with large language models, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 1, 2024, pp. 437 445.\n\n--- Segment 27 ---\n1, 2024, pp. 437 445. [10] M. Akyash and H. Mardani Kamali, Simeval: Investigating the similarity obstacle in llm-based hardware code generation, in Proceedings of the 30th Asia and South Pacific Design Automation Conference, ser. ASPDAC 25. New York, NY, USA: Association for Computing Machinery, 2025, p. 1002 1007. [Online]. Available: [11] M. Liu, N. Pinckney, B. Khailany, and H. Ren, Verilogeval: Evaluating large language models for verilog code generation, in 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD). IEEE, 2023, pp. 1 8. [12] M. Chen et al., Evaluating large language models trained on code, 2021. [Online]. Available: [13] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, Codegen: An open large language model for code with multi-turn program synthesis, arXiv preprint arXiv:2203.13474, 2022. [14] Y. Li et al., Competition-level code generation with alphacode, Science, vol. 378, no. 6624, p. 1092 1097, Dec. 2022. [Online]. Available: [15] A. Vijayakumar, M. Cogswell, R. Selvaraju, Q. Sun, S. Lee, D. Crandall, and D. Batra, Diverse beam search for improved description of complex scenes, Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, Apr. 2018. [Online]. Available: [16] A. Fan, M. Lewis, and Y. Dauphin, Hierarchical neural story generation, in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), I. Gurevych and Y. Miyao, Eds. Melbourne, Australia: Association for Computational Linguistics, Jul. 2018, pp. 889 898. [Online].\n\n--- Segment 28 ---\n889 898. [Online]. Available: [17] H. Huang, Z. Lin, Z. Wang, X. Chen, K. Ding, and J. Zhao, Towards llm-powered verilog rtl assistant: Self-verification and self-correction, arXiv preprint arXiv:2406.00115, 2024. [18] X. Yao, Y. Wang, X. Li, Y. Lian, R. Chen, L. Chen, M. Yuan, H. Xu, and B. Yu, Rtlrewriter: Methodologies for large models aided rtl code optimization, arXiv preprint arXiv:2409.11414, 2024. [19] M. Akyash and H. M. Kamali, Self-hwdebug: Automation of llm self- instructing for hardware security verification, in 2024 IEEE Computer Society Annual Symposium on VLSI (ISVLSI), 2024, pp. 391 396. [20] N. Mashnoor, M. Akyash, H. Kamali, and K. Azar, Llm-ift: Llm- powered information flow tracking for secure hardware, in 2025 IEEE 43rd VLSI Test Symposium (VTS), 2025, pp. 1 5. [21] Y. Zhang, Z. Yu, Y. Fu, C. Wan, and Y. C. Lin, Mg-verilog: Multi- grained dataset towards enhanced llm-assisted verilog generation, in 2024 IEEE LLM Aided Design Workshop (LAD). IEEE, 2024, pp. 1 5. [22] M. Liu, Y.-D. Tsai, W. Zhou, and H. Ren, Craftrtl: High-quality synthetic data generation for verilog code models with correct-by- construction non-textual representations and targeted code repair, arXiv preprint arXiv:2409.12993, 2024. [23] Y. Zhao, D. Huang, C. Li, P. Jin, Z. Nan, T. Ma, L. Qi, Y. Pan, Z. Zhang, R. Zhang et al., Codev: Empowering llms for verilog generation through multi-level summarization, arXiv preprint arXiv:2407.10424, 2024.\n\n--- Segment 29 ---\n[22] M. Liu, Y.-D. Tsai, W. Zhou, and H. Ren, Craftrtl: High-quality synthetic data generation for verilog code models with correct-by- construction non-textual representations and targeted code repair, arXiv preprint arXiv:2409.12993, 2024. [23] Y. Zhao, D. Huang, C. Li, P. Jin, Z. Nan, T. Ma, L. Qi, Y. Pan, Z. Zhang, R. Zhang et al., Codev: Empowering llms for verilog generation through multi-level summarization, arXiv preprint arXiv:2407.10424, 2024. [24] S. Thakur, B. Ahmad, H. Pearce, B. Tan, B. Dolan-Gavitt, R. Karri, and S. Garg, Verigen: A large language model for verilog code generation, ACM Transactions on Design Automation of Electronic Systems, vol. 29, no. 3, pp. 1 31, 2024. [25] Z. Pei, H.-L. Zhen, M. Yuan, Y. Huang, and B. Yu, Betterv: Con- trolled verilog generation with discriminative guidance, arXiv preprint arXiv:2402.03375, 2024. [26] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., Language mod- els are few-shot learners, Advances in neural information processing systems, vol. 33, pp. 1877 1901, 2020. [27] Y. Song, G. Wang, S. Li, and B. Y. Lin, The good, the bad, and the greedy: Evaluation of llms should not ignore non-determinism, arXiv preprint arXiv:2407.10457, 2024. [28] M. Renze, The effect of sampling temperature on problem solving in large language models, in Findings of the Association for Computa- tional Linguistics: EMNLP 2024, 2024, pp. 7346 7356.\n\n--- Segment 30 ---\n[28] M. Renze, The effect of sampling temperature on problem solving in large language models, in Findings of the Association for Computa- tional Linguistics: EMNLP 2024, 2024, pp. 7346 7356. [29] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, The curious case of neural text degeneration, 2020. [Online]. Available: [30] Y.-S. Chuang, Y. Xie, H. Luo, Y. Kim, J. Glass, and P. He, Dola: Decoding by contrasting layers improves factuality in large language models, arXiv preprint arXiv:2309.03883, 2023. [31] Y. Su, T. Lan, Y. Wang, D. Yogatama, L. Kong, and N. Collier, A contrastive framework for neural text generation, 2022. [Online]. Available: [32] X. L. Li, A. Holtzman, D. Fried, P. Liang, J. Eisner, T. Hashimoto, L. Zettlemoyer, and M. Lewis, Contrastive decoding: Open- ended text generation as optimization, 2023. [Online]. Available: [33] S. O Brien and M. Lewis, Contrastive decoding improves reasoning in large language models, arXiv preprint arXiv:2309.09117, 2023. [34] B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J. Zhang, B. Yu, K. Lu et al., Qwen2. 5-coder technical report, arXiv preprint arXiv:2409.12186, 2024. [35] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, R. Sauvestre, T. Remez et al., Code llama: Open foundation models for code, arXiv preprint arXiv:2308.12950, 2023.\n\n