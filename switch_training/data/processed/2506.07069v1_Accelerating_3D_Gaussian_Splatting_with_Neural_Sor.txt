=== ORIGINAL PDF: 2506.07069v1_Accelerating_3D_Gaussian_Splatting_with_Neural_Sor.pdf ===\n\nRaw text length: 75286 characters\nCleaned text length: 74506 characters\nNumber of segments: 47\n\n=== CLEANED TEXT ===\n\narXiv:2506.07069v1 [cs.GR] 8 Jun 2025 Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization Zhican Wang Shanghai Jiao Tong University Guanghui He Shanghai Jiao Tong University Dantong Liu University of Cambridge Lingjun Gao Imperial College London Shell Xu Hu Samsung AI Cambridge Chen Zhang Shanghai Jiao Tong University Zhuoran Song Shanghai Jiao Tong University Nicholas Lane University of Cambridge Wayne Luk Imperial College London Hongxiang Fan Imperial College London Abstract 3D Gaussian Splatting (3DGS) has recently gained significant attention for high-quality and efficient view synthesis, making it widely adopted in fields such as AR VR, robotics, and autonomous driving. Despite its impressive algorithmic performance, real-time rendering on resource-constrained devices remains a major chal- lenge due to tight power and area budgets. Although existing accel- erators have attempted to improve the hardware performance of 3DGS, several key inefficiency, including rasterization redundancy and cross-stage pipeline imbalance, have been overlooked in prior work. This paper presents an architecture-algorithm co-design to address these inefficiencies. First, we reveal substantial redundancy caused by repeated computation of common terms expressions during the conventional rasterization. To resolve this, we propose axis-oriented rasterization, which pre-computes and reuses shared terms along both the X and Y axes through a dedicated hardware design, effectively reducing multiply-and-add (MAC) operations by up to 63 . Second, by identifying the resource and performance inefficiency of the sorting process, we introduce a novel neural sorting approach that predicts order-independent blending weights using an efficient neural network, eliminating the need for costly hardware sorters. A dedicated training framework is also proposed to improve its algorithmic stability. Third, to uniformly support rasterization and neural network inference, we design an efficient reconfigurable processing array that maximizes hardware utiliza- tion and throughput. Furthermore, we introduce a ğœ‹-trajectory tile schedule, inspired by Morton encoding and Hilbert curve, to optimize Gaussian reuse and reduce memory access overhead. Com- prehensive experiments demonstrate that the proposed design pre- serves rendering quality while achieving a speedup of 23.4 27.8 and energy savings of 28.8 51.4 compared to edge GPUs for real-world scenes. We plan to open-source our design to foster further development in this field. Keywords 3D Gaussian Splatting, Hardware Accelerator, Algorithm-hardware Co-design, Reconfigurability 1 Introduction 3D Gaussian Splatting (3DGS) [18] has emerged as a promising computer graphics technique for novel view synthesis owing to its superior synthesis quality and efficient rendering speed. It has been widely adopted in diverse fields, such as robotics [42], augmented and virtual reality (AR VR) [34, 41], and autonomous driving [19]. In contrast to the neural radiance fields (NeRF) technique [25] that implicitly represents a three-dimensional scene via a neural net- work, 3DGS explicitly encodes the scene using a large set of 3D Gaussians with learnable positions, sizes, shapes, colors, and opaci- ties. Due to its lower algorithmic complexity, 3DGS achieves faster rendering performance than NeRF, making it well-suited for inter- active applications. However, achieving real-time 3DGS rendering on resource-constrained devices such as edge GPUs remains a chal- lenge. For instance, the NVIDIA Jetson Xavier NX [1] only achieves less than 10 frame per second on the MipNeRF-360 dataset [2]. The stringent power and area constraints of AR VR edge devices further exacerbate the difficulty of deploying 3DGS in such settings. Rasterization and sorting are two core components that con- tribute significantly to overall latency. Although prior work [22] has explored accelerating 3DGS to improve hardware performance, this work identifies unique challenges and optimization opportuni- ties within rasterization and sorting that have been overlooked in previous research. Challenge-1: Computational redundancy in rasterization. As shown in Fig. 1(a) (left), most existing implementations simply map the rasterization of each pixel into a separate processing element (PE) or compute core (such as [ğ‘†ğ‘¥ 2 ğ‘†ğ‘¦ 0 ğ‘(2,0)] at the core with index [2, 0]), without considering compute efficiency within each pixel. However, as analysed in Sec. 2.2, this pixel-wise parallelization leads to redundant computation of intermediate common terms expressions (ğ‘†ğ‘¦ 2 in the provided example) that are repeatedly computed across the pixels belonging to the same row (Y-axis) or column (X-axis). Such redundancy increases the num- ber of multiply-accumulate (MAC) operations and contributes to high overall latency during rasterization. Based on this observa- tion, we identify significant computational redundancy as a key challenge in achieving high-performance 3D Gaussian splatting. Challenge-2: Resource overhead and pipeline imbalance as- sociated with sorting. As shown in Fig. 1(b) (top), due to the heterogeneous nature of sorting and subsequent rasterization operations, previous solutions have adopted a cascaded pipeline composed of a sorting module and a rasterization module to reduce latency [22]. However, this design exhibits several draw- backs: First, it incurs a high hardware resource overhead. The sorting module is commonly implemented using a bitonic sorting unit [17], whose area scales proportionally to ğ‘˜log2(ğ‘˜), where ğ‘˜ represents the input parallelism, and as ğ‘˜increases, the overhead 1 RPE Challenge 1: High rasterization redundancy due to independent parallel processing. 1.High Parallelism 2.High Redundancy 3.High Latency GPU GSCore Rasterization: X 1.High Parallelism 2.Low Redundancy 3.Low Latency Y X Broadcast Combine Axis-oriented Rasterization: Challenge 2: Heterogeneous operations and pipeline imbalance caused by sorting. Solution Sorting Module Rasterization Module Conventional Pipeline: Operational complexity: O(Nlog2N) O(N) N N N: GS Time Schedule: Sorting Module Rasterization Module Idle! T N Increases Idle! T 1.High Overhead N varies across tiles! 2.Low Utilization High Latency Solution Algorithm: Neural Sorting Hardware Opt. RPE Reconfigurable PE Array Co-design RPE Array T 1.Low Overhead 2.High Utilization Low Latency RPE Time Schedule: â‘  â‘¡ â‘¢ Compute Axes Compute Axes â‘  Sy 2 is shared. CUDA Core PE Unit RPE RPE RPE RPE RPE RPE (Sec. 3) 1. 2. (Sec. 4) (Sec. 5) 2. 1. (a) (b) Interleaved Pipeline Î -Trajectory Schedule Sx 0 Sy N(0,2) Sx Shared Term N(x,y) Non-shared Term Sy 2 ... ... Y Sx 1 N(1,2) Sy 2 Sx 2 N(2,2) Sy 2 Sx L N(L,2) Sy 2 Sx 2 N(2,L) Sy L Sx 2 N(2,1) Sy 1 Sx 2 N(2,0) Sy 0 Array size L L Notations Sx 2 is shared. Sx 2 N(2,1) Broadcast â‘¡ Sy 1 Figure 1: Challenges of 3DGS acceleration and organization of the paper. increases dramatically. Second, sorting and rasterization exhibit different computational complexities, which can lead to severe pipeline imbalance. Specifically, given ğ‘Gaussians, sorting has a complexity of ğ‘‚(ğ‘log2(ğ‘)), whereas rasterization s complex- ity is ğ‘‚(ğ‘). Our profiling further reveals that the number of Gaussians per tile can vary by up to two orders of magnitude, ex- acerbating the imbalance. As a result, when the sorting hardware module is designed with fixed parallelism, the rasterization mod- ule often suffers from significant underutilization due to pipeline imbalance, especially as the number of Gaussians increases. To address Challenge-1, we propose an axis-oriented rasteri- zation technique, as depicted in Fig. 1(a) (right), which follows a three-step process designed to eliminate computational redundancy efficiently. First, the common intermediate terms along the X and Y axes are computed once per axis. Second, these precomputed terms are broadcast to the corresponding PEs within the tile. Third, each PE simply combines the broadcasted terms to complete the ğ›¼computation with minized MAC overhead. This axis-oriented rasterization is implemented using a dedicated hardware design, effectively reducing the MAC counts for the ğ›¼computation in the rasterization process from 8 additions (ADD) and 4 multiplications (MUL) to 2 ADD and 2 MUL. To overcome Challenge-2, this work proposes a novel neural sorting method via algorithm-hardware co-design. As shown in Fig. 1(b) (bottom), we re-examine the role of sorting in Gaussian splitting at the algorithmic level. Sorting is traditionally required to ensure the correct front-to-back depth order for ğ›¼blending, as transparency depends on the sequential visibility of Gaussians. However, our key insight is that the pri- mary purpose of sorting is to produce a decay factor for blending. This observation leads us to explore the learning-based approach for sorting. To this end, we propose neural sorting that predicts the decay factor with a tiny neural network, thereby avoiding the need for explicit depth sorting. At the hardware level, we reuse the highly parallel rasterization array for efficient neural sorting by en- hancing these PEs with runtime reconfigurability. This co-designed approach efficiently eliminates the need for a high-overhead sorting engine, effectively enhancing PE utilization and reducing latency. Our fine-grained interleaved pipeline further mitigates the substan- tial operational intensity gap between sorting and rasterization. Furthermore, beyond the single-tile optimization, this work also explores cross-tile rendering trajectory optimization. Inspired by Morton encoding [26] and Hilbert curve [14], we propose a gener- alized ğœ‹ trajectory tile schedule that minimizes external memory access by improving data locality across tiles. In conclusion, we make the following contributions: We identify unique challenges and opportunities associated with accelerating 3D Gaussian splatting, including high computational redundancy in the rasterization process and heterogeneous op- erations and pipeline imbalance incurred by sorting (Sec. 2.2). We propose axis-oriented rasterization with a dedicated hard- ware architecture, effectively eliminating computational redun- dancy and significantly reducing the MAC count (Sec. 3). We introduce a novel neural sorting that optimizes the traditional sorting process, achieving efficient decay factor prediction with negligible quality degradation (Sec. 4). We propose multiple hardware optimizations, such as runtime reconfigurability and a generalized ğœ‹ trajectory tile schedule, to enhance compute efficiency and reduce external memory access overhead. (Sec. 5). 2 Background and Motivation 2.1 3D Guassian Splatting 3DGS parameters. 3DGS is designed to represent a 3D scene using a set of Gaussians. For any given view, it can render an image by splatting these Gaussians into 2D space according to the rendering steps. Each Gaussian is modeled by Equation (1), where ğ‘ denotes the 3D coordinates. It uses a total of 59 parameters to describe: i) the mean (position) ğœ‡ (3 parameters); ii) the size and shape, represented by Î£ and determined by the scale ğ‘ (3 parameters) and rotation ğ‘(4 parameters); iii) the opacity factor ğ‘œ(1 parameter); and iv) the view-dependent color, described by spherical harmonics (SH) coefficients (16 3 48 parameters). Rendering steps. 3DGS rendering mainly comprises three steps: projection, sorting, and rasterization. For projection, according to the view (camera pose), 3D Gaussians are projected into 2D Gaus- sians: the 3D mean (ğœ‡ ) and covariance (Î£ ) are transformed into the 2D mean ğœ‡( 2 1 vector) and 2D covariance Î£ ( 2 2 matrix), and depth (d) of each 3D Gaussian relative to the camera is ac- quired. In addition to the spatial information, the color information, represented by an RGB vector ( 3 1 vector), is computed based on the SH coefficients and the camera pose. A 16 16 pixel tile is 2 Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization used as the granularity for rendering, and after projection, each 2D Gaussian is assigned to the tiles it intersects. For sorting, because the relative order of 3D Gaussians reflects occlusion and influences the ğ›¼blending in the subsequent step, each tile sorts the Gaussians it intersects in order of increasing depth (i.e., from near to far from the camera). For rasterization, it computes the color contribution from each Gaussian to each pixel within the tile, a process that mainly consists of ğ›¼computation followed by ğ›¼blending. The ğ›¼ computation is described by Equation (2), where ğ‘( 2 1 vector) rep- resents the pixel position. Using the computed ğ›¼values, ğ›¼blending determines the final color ğ¶for each pixel according to Equation (3) (left), where ğ‘‡represents the transmittance, ğ‘–denotes the index of Gaussians in sorted order, and ğ‘ğ‘–represents the RGB color of each Gaussian. The transmittanceğ‘‡ğ‘–is determined by the ğ‘– 1 Gaussians in front of it, as described by Equation (3) (right). ğº(p) ğ‘’ 1 2 (p ğ )Tğšº 1(p ğ ), (1) ğ›¼ ğ‘œ ğ‘’ 1 2 (p ğ)Tğšº 1(p ğ), (2) ğ¶ ğ‘ ğ‘– 1 ğ‘‡ğ‘–ğ›¼ğ‘–ğ‘ğ‘–, ğ‘‡ğ‘– ğ‘– 1 Ã– ğ‘— 1 (1 ğ›¼ğ‘–) (3) Profiling and analysis. We profile the breakdown of these three steps on the NVIDIA Jetson Xavier NX GPU using the MipNeRF- 360 dataset [2] and average the results across scenes. The results show that projection, sorting, and rasterization account for 12.6 , 25.5 , and 61.9 of the total latency, respectively. Since sorting and rasterization together constitute nearly 90 of the latency, this paper focuses on accelerating these steps. Although both projection and rasterization involve MAC operations, there exists a significant disparity between them. For each tile, rasterization computes the contribution of each Gaussian to 256 pixels, i.e., Equation (2) is repeated 256 times per Gaussian, whereas projection computes the relevant information only once per Gaussian. Moreover, the total number of 2D Gaussians (i.e., the instances of a 3D Gaussian intersecting tiles) is significantly higher than the original count of 3D Gaussians, as a single Gaussian can intersect multiple tiles. Analyzing the MAC count per Gaussian within the rasterization step, the ğ›¼computation involves 8 MUL, 4 ADD, and 1 exponential operation, whereasğ›¼blending requires 5 MUL and 4 ADD, revealing that ğ›¼computation is the most MAC-intensive. 2.2 Challenge Analysis and Motivation 2.2.1 Computational Redundancy. As demonstrated in profil- ing, rasterization is the most time-consuming component in 3DGS. Within the rasterization pipeline, ğ›¼-blending is the most MAC- intensive process, involving cascaded matrix-vector multiplications. The formula expansion for ğ›¼computing is shown in Fig. 2 (top), where ğœ‡ğ‘¥ ğ‘–and ğœ‡ğ‘¦ ğ‘–denote the center of the ğ‘–-th Gaussian, ğ‘¥and ğ‘¦ denote the coordinates of a pixel. The conic matrix (Î£ 1), which is defined as the inverse of 2D Gaussian s covariance matrix, is param- eterized by ğ‘ğ‘–, ğ‘ğ‘–, and ğ‘ğ‘–. According to the formulation, it requires 8 multiplications (MUL), 4 additions (ADD), and 1 exponential op- eration (EXP). Conventionally, prior work [22] designs a PE array for rasterization, where the computation for each pixel is mapped to one PE. In these designs, the PE structure reflects the theoretical MAC count derived from the ğ›¼computation formulation, as shown in Fig. 2 (bottom), with registers omitted for simplicity. Î±i exp(-1 2(p-Î¼i)T -1(p-Î¼i)) oi p x y Î¼ Î¼i x Î¼i y i -1 ai ci ci bi Î±i exp(-1 2 (ai (x-Î¼i x)2 bi (y-Î¼i y)2) ci (x-Î¼i x) (y-Î¼i y)) oi cross term x Î¼i x y Î¼i y ai ci bi -1 2 ex oi 8 MUL 4 ADD 1 EXP 8 MUL 4 ADD 1 EXP GSCore PE are shared across PEs are shared across PEs are shared across rows are shared across rows Î± Blending Rasterization Array X-axis quadratic term Y-axis quadratic term Y-axis quadratic term X-axis quadratic term X-axis quadratic term Y-axis quadratic term Figure 2: The source of redundant computing in rasterization. To illustrate the source of redundancy, we categorize the expo- nent in the ğ›¼computation into three components: (i) the X-axis quadratic term, representing the squared distance between the pixel and the Gaussian center along the X-axis; (ii) the Y-axis quadratic term, representing the squared distance along the Y-axis; and (iii) the cross term, capturing the interaction between the X and Y co- ordinates. As shown in Fig. 2 (right), these terms exhibit spatial redundancy: for any row of pixels, the Y-axis quadratic terms are constant across PEs, and for any column, the X-axis quadratic terms are shared. This analysis reveals that significant redundant compu- tation exists in both the X- and Y-axis quadratic terms. However, conventional PE designs compute these terms repeatedly for each pixel, failing to exploit the redundency for computational savings. To avoid redundant computation, our key idea is to redesign the entire data flow by precomputing the axis-shared terms. Each PE then performs only a simple combination of these precomputed values, thereby significantly reducing complexity while maintaining parallelism. This axis-oriented rasterization is implemented through a dedicated hardware architecture (Sec. 3) to improve the compute efficiency. 5903.5 11016.1 4745.5 3935.5 2947.1 4393.1 125.1 106.4 75.9 109.9 85.3 78.6 1080.7 1378.9 740.9 778.2 613 672.4 1E 0 1E 1 1E 2 1E 3 1E 4 1E 5 Garden Bicycle Counter Kitchen Room Stump Gaussians Scene max min mean Two Magnitude! Varies Across Scenes Figure 3: The number of Gaussians varies across tiles and scenes. 3 2.2.2 Heterogeneous Operation and Pipeline Imbalance. As illustrated in Fig. 1(b), sorting incurs high hardware overhead and degrades PE utilization. Once the challenges in rasterization are addressed, the relative impact of sorting on overall performance becomes more significant based on Amdahl s Law [31]. To quanti- tatively analyze the impact of sorting on each tile to be rendered, we conducted profiling using the MipNeRF-360 dataset [2]. We em- ploy a trained Gaussian checkpoint at 7k iterations and utilize the validation set to measure the difference in the number of Gaussians across tiles and scenes. Since different camera poses yield varying rendering results, the reported Gaussian count is averaged across the entire validation set. As shown in Fig. 3, the maximum Gaussian count can reach up to 10, 000, while the minimum is only around 80, spanning two orders of magnitude. This difference exists both within the tiles of a single scene and across different scenes, posing a significant challenge to hardware design. Let ğ‘represent the number of Gaussians assigned to a pixel tile. The typical sorting complexity can be ğ‘‚(ğ‘log(ğ‘)), ğ‘‚(ğ‘ğ‘™ğ‘œğ‘”2(ğ‘)) such as quick sort and bitonic sort [15, 17], whereas rasterization has a complexity of ğ‘‚(ğ‘). As ğ‘increases, sorting becomes the bottleneck, degrading the PE utilization of the rasterization mod- ule and increasing overall latency. A straightforward solution is to increase the parallelism of the sorting module, such as the Bitonic network, however, this approach has two deficiencies. First, the area overhead of sorting increases dramatically, as it is proportional to ğ‘˜log2(ğ‘˜), where ğ‘˜denotes the degree of parallelism. Second, because the number of Gaussians varies dramatically across tiles and scenes, only increasing the sorting parallelism would degrade area and energy efficiency for tiles with fewer Gaussians. Due to this variation, it is challenging to achieve an optimal design that performs well across different scenes and tiles. Prior work [22] pro- poses an alternative solution: hierarchical sorting. This approach first iteratively establishes a series of pivots as boundaries to divide the Gaussians into subsets for coarse sorting and then performs precise sorting using a sorting module with fixed parallelism. How- ever, determining suitable pivots for different tiles and scenes is a non-trivial task, as the depth range can vary significantly. Further- more, it requires multiple data passes during sorting, which remain time- and energy-consuming. To comprehensively address this challenge, our key motivation is to adopt an algorithm-hardware co-design approach. At the al- gorithmic level, we reexamine the role of sorting and propose an alternative solution using neural sorting as described in Sec. 4. This solution fully leverages the high throughput of the PE array via reconfigurability (Sec. 5), uniformly supporting both sorting and rasterization. 3 Axis-Oriented Rasterization Inspiration and overview. We propose axis-oriented rasteri- zation to address Challenge 1 described in Sec. 2.2. This approach consists of three steps: 1) computing shared terms along the X and Y axes, 2) broadcasting the derived axis terms to each PE, and 3) combining the received terms in each PE to produce the final result. To efficiently implement axis-oriented rasterization, our key con- siderations involve i) simplifying the control logic of the three-step process, and ii) minimizing or eliminating the additional storage required. Given a tile of size ğ¿ ğ¿, we observe that computing the shared terms has ğ‘‚(ğ¿) complexity, whereas the combination step has ğ‘‚(ğ¿2) complexity with each shared term reused by ğ¿times. These observations inspire the design of an ğ¿ ğ¿array for the combination step, equipped with pre-processing modules of size ğ¿to compute the shared terms. Following these design principles, an overview of our is illustrated in Fig. 4 (top left). For a tile size of 16 16, the design includes (i) a rasterization array of size 16 16, (ii) an X-PE line of length 16, and (ii) a Y-PE line of size 16. The X-PE line generates the X-axis shared terms to the rasterization array, whereas the Y-PE line generates the Y-axis shared terms. Each X-PE and Y-PE directly broadcast the intermediate results to the corresponding rasterization PE within the same column or row, repeating 16 times. This broadcast overhead is small enough to implement and meets timing closure requirements. X-PE Y-PE x-term y-term x2-term y2-term ex 2 MUL 2 ADD 1 EXP Axis-oriented Rasterization PE Compute Axes â‘  Broadcast â‘¡ Broadcast â‘¡ Combine â‘¢ Axis-oriented Rasterization PE Î± Blending X-PE X-PE X-PE Y-PE Y-PE Y-PE (iii) Y-PE Line (ii) X-PE Line Î¼i x, -1 2ai, ci Î¼i y, -1 2bi (i) Rasterization Array ... ... One GS per Cycle ci(x-Î¼i x) x-term x2-term -1 2ai(x-Î¼i x)2 y-term (y-Î¼i y) y2-term -1 2bi(y-Î¼i y)2 oi x data y data GS Feature Y-PE y Î¼i y -1 2bi y-term y2-term y Î¼i y -1 2bi y-term y2-term Y-PE y Î¼i y -1 2bi y-term y2-term x Î¼i x ci -1 2ai X-PE x-term x2-term Compute Axes â‘  T Notations Figure 4: Hardware and computation flow of axis-oriented rasterization array. Computation flow and PE structure. The rasterization array renders each Gaussian and computes its contribution to a 16 16 tile of pixels continuously in each cycle. The Gaussian parameters are provided as input to the X-PE line and Y-PE line, which perform the axes computation, and the outputs from these PE lines are then fed to the rasterization PE array. To eliminate redundant processing of the Gaussian conic matrix parameters (e.g., computing the factor 1 2), we directly store the parameter set { 1 2ğ‘ğ‘–, 1 2ğ‘ğ‘–,ğ‘ğ‘–}, where ğ‘– denotes the index of each Gaussian. The X-PE line receives the Gaussian parameters ğ‘ğ‘–and 1 2ğ‘ğ‘–, which are broadcast to all X-PEs. Internally, each X-PE line gen- erates 16 x-coordinates (ğ‘¥0 ğ‘¥15), which are automatically in- cremented by 16 when shifting to the next tile on the right. The detailed X-PE structure is illustrated in Fig. 4 (right). For simplicity, the x index is omitted (similarly for y in the following descrip- tion). Each X-PE consists of two adders and two multipliers. One computation branch calculates the term from X-axis (ğ‘¥-term) by computing (ğ‘¥ ğœ‡ğ‘¥ ğ‘–) and multiplying it by ğ‘ğ‘–, while the other branch squares (ğ‘¥ ğœ‡ğ‘¥ ğ‘–) and multiplies it by 1 2ğ‘ğ‘–to obtain the X-axis quadratic term (ğ‘¥2-term). The Y-PE line receives the Gaussian pa- rameter 1 2ğ‘ğ‘–, which is broadcast to each Y-PE. The y-coordinates (ğ‘¦0 ğ‘¦15) are similarly incremented by 16 when moving vertically between tiles. As shown in Fig. 4 (right), each Y-PE contains one 4 Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization adder and two multipliers. One computation branch produces the Y-axis term (ğ‘¦-term) by computing (ğ‘¦ ğœ‡ğ‘¦ ğ‘–) directly, while the other squares (ğ‘¦ ğœ‡ğ‘¦ ğ‘–) and multiplies it by 1 2ğ‘ğ‘–to generate the Y-axis quadratic term (ğ‘¦2-term). For the rasterization PE array, each PE receives vertical inputs from the corresponding X-PE and horizontal inputs from the corre- sponding Y-PE based on the pixel coordinates. As shown in Fig. 4 (bottom), each rasterization PE comprises only two adders and two multipliers. One multiplier is dedicated to multiplying the opacity factor ğ‘œğ‘–at the final stage, while the remaining MAC unit combines the inputs from the PE lines. Specifically, the multiplier multiplies the ğ‘¥term with the ğ‘¦term, and the two adders subsequently add this result to the ğ‘¥2-term and ğ‘¦2-term, respectively. Considering synchronization, the X-PE line initiates one cycle earlier than the Y-PE line, ensuring that the ğ‘¥term andğ‘¦term arrive simultaneously at each rasterization PE during the first cycle, with the ğ‘¥2-term and ğ‘¦2-term arriving in the second and third cycles, respectively. After executing the exponential operation and multiplying by ğ‘œğ‘–, the computation of ğ›¼ğ‘–is completed, and the result is then fed into the ğ›¼blending unit. Due to the continuous processing of Gaussians, the computation of ğ›¼ğ‘– 1 is completed in the subsequent cycle. Overhead analysis. Compared to the GScore implementation, our axis-oriented rasterization requires an additional X-PE line and Y-PE line while significantly simplifying the design of the rasterization PE. To ensure a fair comparison, the MAC cost of the extra PE lines is amortized across the rasterization PE array. The total number of multipliers is calculated as 2 (2 16) 64 for the X-PE and Y-PE lines and 16 16 2 512 for the rasterization PE array, resulting in a total of 576. Similarly, the total number of adders is calculated as 2 16 1 16 48 for the extra PE lines and 16 16 2 512 for the rasterization PE array, resulting in a total of 560. When averaged over a 16 16 rasterization PE array, our axis-oriented approach requires only 2.25 multipliers and 2.19 adders per PE. Compared with 8 multipliers and 4 adders in GSCore implementation, the count of them is significantly reduced by 63 . 4 Neural Sorting Challenge-2 in Sec. 2.2 highlights the performance bottleneck brought by sorting and the demand of sorting-free rendering. In this section, we provide three insights for our neural sorting algorithm in Sec. 4.1, and then the neural network structure and training framework are provided in Sec. 4.2. 4.1 Motivation Insight-1: Sorting is designed to obtain the decay factor. The sort- ing process is indispensable for the original 3DGS due to ğ›¼blending, which heavily relies on the relative depth order determined by the camera pose. However, reexamining the blending equation (3), we note that the ultimate goal of sorting is to obtain the correct trans- mittanceğ‘‡ğ‘–. The Gaussians in front of the ğ‘–th Gaussian have smaller depth, and each contributes a factor of 1 ğ›¼ğ‘—(for ğ‘— 0, 1, . . . ,ğ‘– 1) in the cascade product. Since each ğ›¼lies in the interval (0, 1), as the depth of the ğ‘–th Gaussian increases, ğ‘‡ğ‘–becomes smaller, effectively serving as a decay factor. This naturally raises the question: Can we directly derive the decay factor based on depth? Insight-2: The ğ›¼-blending for 3DGS is analogous to image compo- sition. The original 3DGS paper [18] highlights that its ğ›¼-blending follows the same image formation model as NeRF-style volumet- ric rendering, whereas reusing the term ğ›¼-blending from classic computer graphics [30]. The term ğ›¼ originates from the linear interpolation formula ğ›¼ğ´ (1 ğ›¼)ğµ[32], also interpreted as the over operation of image ğ´over ğµ. Inspired by this connection, we ob- serve that 3DGS ğ›¼-blending is similar to image composition. Fig. 5 illustrates this using three Gaussians: 3DGS blends front-to-back (left), while standard image compositing is back-to-front (right). Here, ğ¶3, ğ¶2, and ğ¶1 represent accumulated colors, computed via successive over operations involving image colors and their opaci- ties (ğ‘, ğ›¼), yielding the same final result as 3DGS. This equivalence generalizes to any number of Gaussians. Î±1 c1 Î±2 c2 Î±3 c3 3DGS Compute : Front-to-Back C T1Î±1c1 T2Î±2c2 T3Î±3c3 Î±1c1 (1-Î±1)Î±2c2 (1-Î±1)(1-Î±2)Î±3c3 Î±1 c1 Î±3 c3 Î±2 c2 Image Composition : Back-to-Front C3 C2 C1 C3 Î±3c3 C2 (1-Î±2) C3 Î±2c2 C C1 (1-Î±1)C2 Î±1c1 Î±1c1 (1-Î±1)Î±2c2 (1-Î±1)(1-Î±2)Î±3c3 Identical! Figure 5: 3DGS ğ›¼blending is similar to image composition. Apparently, the over operation is non-commutative, thus requir- ing costly depth sorting. Fortunately, several order-independent transparency (OIT) techniques have been proposed in computer graphics [3, 5, 6, 24, 27]. Given the similarity between 3DGS and image composition, these methods offer inspiration for optimizing 3DGS sorting in our work. Among these methods, weighted OIT aligns well with Insight-1. By omitting the background contribu- tion, equation (4) is derived, where ğ¶is the computed color, and ğ¹(ğ‘‘ğ‘–) is a monotonic decreasing function with respect to depth ğ‘‘ğ‘–. [24] proposes several general-purpose functions for ğ¹(ğ‘‘ğ‘–), however, they often require scene-specific tuning to perform well. ğ¶ Ãğ‘› ğ‘– 1 ğ¹(ğ‘‘ğ‘–)ğ›¼ğ‘–ğ‘ğ‘– Ãğ‘› ğ‘– 1 ğ¹(ğ‘‘ğ‘–)ğ›¼ğ‘– (4) Insight-3: A tiny multilayer perception (MLP) is a suitable replace- ment for sorting. First, weighted OIT requires a manual selection and tuning process for an appropriate ğ¹(ğ‘‘ğ‘–), and it still suffers from some performance loss. Second, NeRF implicitly encodes the scene using a large MLP (Multilayer Perceptron) that maps 3D spatial coordinates and viewing directions into volume density and color. Inspired by this, it is intuitive to adopt an MLP in 3DGS to encode the impact of depth information. Third, due to the rapid 3DGS training speed on desktop or server GPUs, co-training a tiny MLP as a pipeline component is both cost-effective and practical. 4.2 MLP Design and Training Framework MLP Design. The MLP design should balance the expressiveness with hardware deployment efficiency. From a hardware perspective, since the rasterization array provides highly parallel MAC units, 5 w2 w1 w3 w4 w5 w6 Leaky ReLU Exp b1 b2 b3 b4 Initialization Pretrained Checkpoint 3D Gaussians Camera Projection Tile Rasterization Rendered Image Ground Truth Image High learning rate Low learning rate Operational Flow Gradient Flow Neural Network Structure di di 1. 2. 3. 4. 5. 6. Figure 6: Neural network structure and the training framework. our key motivation is to reuse this array to accelerate MLP infer- ence. Our tiny MLP, as illustrated in Fig. 6 (left), comprises only 2 layers, 10 parameters, and 6 MACs. In addition to the network architecture, initialization methods and activation functions are critical for MLP expressiveness. He initialization [13] is applied to the first layer, while Xavier initialization [21] is adopted for the output layer. Leaky ReLU [37] is used as the activation function in the first layer, and an exponential function is employed in the output layer. The Leaky ReLU expression is given in equation (5), with a coefficient of 1 8 chosen for hardware simplicity. Although the standard ReLU function is simpler, we observed dead ReLU issues during training, indicating that its inputs remain constantly negative and prevent weight updates. The exponential function is chosen for the output layer due to its simplicity and wide dy- namic range, as well as it can reuse of the exponential unit for rasterization. Other activation functions, such as sigmoid, are less suitable because they involve extra division and addition operations. A more detailed exploration of the neural network architecture and activation functions is provided in Sec. 6. Leaky ReLU(ğ‘¥) ( ğ‘¥, if ğ‘¥ 0 1 8ğ‘¥, otherwise (5) Training Framework. Our training framework is illustrated in Fig. 6 (right), where the black arrow denotes the forward (opera- tional) flow and the blue arrow denotes the gradient flow during backpropagation. Firstly, the 3D Gaussians are initialized using a pre-trained checkpoint, which is obtained from the original algo- rithm that incorporates sorting (denoted by 1. in the figure). This step is crucial, as our experiments with co-training the 3D Gaus- sians and the MLP from scratch resulted in convergence difficulties. Moreover, the original 3DGS training flow begins from SFM points [18], further reaffirming the significance of proper initialization. After initialization, the Gaussians are projected into 2D space according to the camera pose (denoted by 2.), and the computed depth of each Gaussian is extracted as the input ğ‘‘ğ‘–to the MLP. Since the sorting process is optimized away, with the MLP outputs ğ¹(ğ‘‘ğ‘–), the projected Gaussians are directly used for tile rasterization (denoted by 3.). the rendered image is then obtained from equation (4). Subsequently, the rendered image is compared with the ground truth image to compute the loss (denoted by 4.), and the loss follows the original setting [18]. As shown on the rightmost side, initially, the rendered image does not handle occlusion well, resulting in a somewhat transparent object. As training progresses, the rendered image gradually improves and increasingly resembles the ground truth. The backward process follows the blue arrow, and the gradients are used to optimize both the 3D Gaussians and the MLP. However, their learning rates differ significantly. We multiply a frozen factor, such as 0.01, to the original 3DGS learning rate while assigning a relatively larger learning rate to the MLP. This causes the loss to optimize the MLP rapidly (denoted by 5.) and adjust the 3D Gaussians more gradually (denoted by 6.). Interestingly, we found that freezing the optimization of the 3D Gaussians and training only the MLP does not work, implying that slight adjustments to the Gaussians to accommodate the new paradigm are crucial. Another important setting is that the Gaussian clone and split are disabled because they can be regarded as an abrupt change that undermines training stability. Disabling this step keeps the number of Gaussians constant and yields better rendering results. 5 Hardware Architecture and Optimizations In this section, we present the detailed hardware architecture for unified rasterization and neural sorting in Sec. 5.1. Since rasteriza- tion is a typically compute-bound workload, whereas computation of the tiny MLP is heavily memory-bound, we observe that the naive pipeline suffers from PE underutilization. To mitigate this dis- parity and enhance overall performance, we propose a fine-grained interleaved pipeline in Sec. 5.2. Finally, our analysis reveals that tile scheduling patterns have a significant impact on off-chip memory access. Hence, we introduce a generalized ğœ‹-trajectory tile schedul- ing method in Sec. 5.3. 5.1 Reconfigurable Hardware Design Overview. Fig. 7 (left) illustrates the unified hardware archi- tecture for both rasterization and neural sorting, with the arrow notation at the top left. The architecture comprises dedicated com- putation and storage modules. For computation, the design incor- porates a 16 16 reconfigurable PE array equipped with a broadcast register per row, as well as X-PE and Y-PE lines (introduced in Sec. 3). Both PE lines receive either x or y coordinates from the coordinate generator, as depicted in Fig. 7 (left). The operation of addition or subtraction is determined by the trajectory of the tiles to be rendered, thereby realizing the ğœ‹-trajectory tile scheduling 6 Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization M-4-1 M-4-2 M-4-3 di M-3-r w1 Ri w2 Gi w3 Bi A-4-1 A-4-2 A-4-3 b1 b2 b3 M-1 x-term y-term A-4-1-r w4 A-1 x2-term M-2-r M-2 E-r A-4-2-r A-2 y2-term A-3-r M-3 F(d) w6 A-4-3-r M-2-r M-2-r M-2-r A-3 b4 ex E Reconfigurable PE (RPE) RPE X-PE RPE RPE RPE RPE RPE RPE RPE RPE RPE RPE RPE RPE RPE RPE RPE ... ... 16 16 Y-PE Î¼x , Î¼y -1 2a,-1 2b,c R, G, B, o Projected GS Feature Cache Y-PE Y-PE Y-PE X-PE X-PE X-PE Y-PE Line X-PE Line Pixel Output Buffer Div. Array B-Reg Ctrl. Î¼i x, -1 2ai, ci Î¼i y, -1 2bi B-Reg B-Reg B-Reg w1 Ri w1 Ri w2 Gi w2 Gi w3 Bi w3 Bi w6 F(di 1) w6 F(di 1) w5 oi 2 w5 oi 2 w4 w4 b1 b1 b2 b2 b3 b3 b4 b4 oi w5 Broadcast Reg (B-reg) 4 Depth Buffer 88KB 4KB x0 y0 x1 y1 x2 y2 x15 y15 16 -16 Coordinate Generator (CG) ... x CG y CG 16 -16 16 -16 16 -16 x data y data F(d) d (depth) RPE output PE line input B-reg data 4KB Figure 7: Unified hardware architecture of rasterization and neural sorting (left), and the structure of modules (right). method (see Sec. 5.3). For storage, the projected Gaussian (GS) fea- tures are buffered in a four-way set-associative cache, while the depth information, along with the computed decay factor ğ¹(ğ‘‘ğ‘–), is stored separately in the depth buffer due to different on-chip bandwidth requirements. Furthermore, the computed denominator and numerator from equation (4) are transferred to the pixel output buffer, where they are subsequently normalized by the division array to yield the final pixels. Reconfigurable PE. Fig. 7 (left) illustrates the reconfigurable PE, which comprises 6 multipliers, 6 adders, and an exponential unit, all operating in FP16.The multipliers and adders (MACs) are divided into two groups, each comprising three MACs. In one group, each MAC operates independently, denoted as ğ‘€-{1 3} and ğ´-{1 3}, whereas the three MACs in the other group operate cooperatively, denoted as ğ‘€-4-{1 3} and ğ´-4-{1 3}. Each multiplier or adder is equipped with a register to ensure timing closure at high frequen- cies. Certain connections are implicitly indicated by the notation, where the suffix ğ‘Ÿdenotes the wire output from the corresponding register. For instance, ğ´-4-1-ğ‘Ÿ(the fourth wire on the left, denoted by a blue circle) indicates that this wire is connected with the regis- ter output of the ğ´-4-1 adder (the second wire on the right, denoted by a blue circle). The control of the multiplexers determines the datapath configuration between the rasterization mode (upper) and the sorting mode (lower). The workflow and the configured PE structures are described as follows: Rasterization mode. The rasterization mode PE is configured as shown in Fig. 8. In this mode, the PE array continuously processes one GS per cycle for the 16 16 tile pixels. The GS-feature cache provides the GS position, ğœ‡ğ‘¥ ğ‘–and ğœ‡ğ‘¦ ğ‘–, and the conic parameters 1 2ğ‘ğ‘–, 1 2ğ‘ğ‘–, and ğ‘ğ‘–to X-and Y- PE lines, where ğ‘–denotes the GS index. ğ‘œğ‘–is unique, loaded into the broadcast register, and broadcast 16 times to the PEs on the same line. In the configured PE, ğ‘€-{1 2}, ğ´-{1 2}, and the exponential unit are used for rasterization, cor- responding to the computation required in Fig. 4. The structure of X-PE, Y-PE, and the workflow for ğ›¼computation have been intro- duced in Sec. 3. The additional component is ğ›¼blending, which is defined by equation (4). The corresponding GS color features, ğ‘…ğ‘–, ğºğ‘–, and ğµğ‘–, are loaded into the broadcast register. The broadcast reg- ister, as shown in Fig. 7 (right), is equipped with 10 register units, of which only 5 are utilized in the rasterization mode. The decay factor ğ¹(ğ‘‘ğ‘–) is also loaded into the broadcast register, which is sourced from the depth buffer. After ğ‘…ğ‘–, ğºğ‘–, ğµğ‘–, and ğ¹(ğ‘‘ğ‘–) are broadcast 16 times to the PEs on the same line, the PE computes Equation 4. ğ‘€-3 multiplies ğ¹(ğ‘‘ğ‘–) with ğ›¼ğ‘–, and ğ´-3 accumulates ğ¹(ğ‘‘ğ‘–)ğ›¼ğ‘–across Gaussians, corresponding to the denominator. ğ‘€-4-{1 3} multi- ply ğ¹(ğ‘‘ğ‘–)ğ›¼ğ‘–with ğ‘…ğ‘–, ğµğ‘–, and ğºğ‘–, and ğ´-4-{1 3} accumulates the result across GSs for the RGB channels. ex M-4-1 M-4-2 M-4-3 A-4-1 A-4-2 A-4-3 M-1 M-2 M-3 A-1 A-2 A-3 E x-term y-term x2-term y2-term oi F(di) Ri Gi Bi Rasterization Mode PE Denominator Î± computation Î± blending Î±i Figure 8: Rasterization mode configured PE. ex M-4-1 M-4-2 M-4-3 A-4-1 E A-4-2 A-4-3 M-1 A-1 M-2 M-3 A-3 A-2 di w1 w2 w3 b1 b2 b3 w4 w5 w6 b4 Sorting Mode PE w4 w5 w6 b4 ex b1 b2 b3 Layer-1 Layer-2 w1 w2 w3 Figure 9: Sorting mode configured PE. Sorting mode. The configured sorting mode PE is shown in Fig. 9. In this mode, the PE array processes 16 16 256 depths per cycle and outputs 256 ğ¹(ğ‘‘) values. The inputs come from the depth buffer with high on-chip bandwidth, and the outputs are also written back to the depth buffer. The MLP weights are stored in the broadcast register, fully utilizing the ten register units. As shown in the figure, ğ‘€-4-{1 3} and ğ´-4-{1 3} perform the computations for the first layer, including the bias addition. ğ‘€-{1 3} imple- ments the second layer, while ğ´-{1 3} carries out the accumula- tion and bias addition. For the activation function, the exponential unit implements the exponential function. Leaky ReLU is omitted from the figure for simplicity. It is implemented through the sign detection and a 5-bit integer adder, realizing the subtraction of 3 from the FP16 exponent part if it is a normal negative. For negative subnormal or positive values, the value remains unchanged. 5.2 Fine-grained Interleaved Pipeline Memory-bound issue. Since our MLP-based sorting only relies on the depth of each GS, it is straightforward to compute the ğ¹(ğ‘‘) 7 for the entire GS, followed by tile-by-tile rasterization, as shown in the naive pipeline of Fig. 10 (top right). This pipeline theoretically eliminates replicated sorting computations across tiles. However, we observe that PE utilization is extremely low during the sorting process. The reason is that the operational intensity of sorting and rasterization differs significantly. For rasterization, each projected Gaussian comprises 9 parameters and performs 256 6 MAC oper- ations. In contrast, for sorting, each depth parameter only incurs 6 MAC operations, indicating a nearly 30 fold difference. Given a typical configuration for our architecture, as shown in Fig. 10 (left), rasterization is compute-bound, whereas sorting is heavily memory- bound [35]. This memory-bound issue also makes deploying our optimized algorithm directly to GPUs not an optimal choice. Optimization. To address this issue, we propose a fine-grained interleaved pipeline that further divides each tile into subtiles, as shown in Fig. 10 (right). The core idea is to overlap the rasterization computation of the current subtile with the memory access required for sorting the next subtile. This fine-grained processing of subtiles is essential because each tile contains a variable number of GS, but the depth buffer size is limited. Fig. 10 (right bottom) illustrates how the proposed pipeline is mapped onto the hardware. For the first subtile, DRAM transfers the depths of GS to the depth buffer, which then outputs the depths to the PE array for sorting. The resulting ğ¹(ğ‘‘) values are written back to the depth buffer. Subsequently, the PE array performs sorting using the ğ¹(ğ‘‘) provided by the depth buffer, while the depth buffer simultaneously receives the depths for the next subtile, thereby overlapping rasterization with memory access. This process repeats, with the latency of depth access being completely hidden except for the first subtile. Throughput (GOPS) Intensity (MACs Byte) Memory- Bound Compute- Bound Sorting Rasterization Fine-grained Interleaved 38.4GB s 1536 Entire Sorting Tile1 Tile2 Tilen ... Low PE Utilization Naive Pipeline High PE Utilization Roofline Model Analysis Fine-grain Interleaved Pipeline ... PE Array T Buffer Access d Access GS d Rasterization Sorting Time Schedule Rasterization Sorting High PE Utilization DRAM Output d Input F(d) Input dnext Output F(d) Figure 10: Roofline model analysis and pipeline comparison. 5.3 ğœ‹ Trajectory Tile Schedule Our architecture adopts a GS-feature cache, where each cache line is tagged using a 28-bit GS ID. An additional 4 bits indicate the number of tiles intersected by each GS. The cache prioritizes the replacement of less important GS. These 4 bits, together with the 28-bit ID, form a 32-bit aligned storage. The cache design effectively reduces off-chip accesses due to the spatial locality of GSs. However, different tile scheduling trajectories affect the cache hit rate. As shown in Fig. 11(a), the baseline implementation scans tiles line by line, exploiting horizontal locality but lacking vertical and hierarchical locality. For instance, it may reuse GSs intersecting horizontally aligned tiles such as 1 2 or 1 3, but not those aligned vertically, such as 2 1 or 3 1. A slight improvement is achieved by using the S-trajectory, which reverses direction at the end of each row. Although they lack hierarchical locality, the row-by- row scheduling makes them flexible and applicable to any tile size. Hence, achieving 2D spatial locality through 1D tile scheduling is a non-trivial problem. Inspired by Morton encoding [26], which interleaves the ğ‘¦- and ğ‘¥-axes to encode each tile, for a 2-bit example, it interleaves ğ‘¦1ğ‘¦0 with ğ‘¥1ğ‘¥0 to obtain the 4-bit code ğ‘¦1ğ‘¥1ğ‘¦0ğ‘¥0, and schedules tiles in increasing order of this code, as shown in Fig. 11(b). This encoding forms a Z-trajectory curve and effectively preserves 2D locality in a 1D scheduling order. However, we find that the diagonal segments of the Z-trajectory often span large distances, resulting in discon- tinuities. To address this issue, and inspired by the continuity of Gray code [11], the Z-trajectory is modified into a "ğœ‹" trajectory that schedules tiles more continuously, as shown in Fig. 11(c). Both the Z-trajectory and the ğœ‹trajectory exhibit hierarchical locality, as indicated by the blue arrow (trajectory of 2 2 tiles), making them scalable. This curve, also known as the Hilbert curve [14], was originally proposed for fractal geometry for squares in 1891. We further generalize the design to better fit various 3DGS applications with different image sizes, as shown Fig. 11(d). First, the ğœ‹trajec- tory is applied only within each 8 8 tile block, while block-level scheduling follows the S-trajectory. Second, for images where the number of tiles is not divisible by 8, a row-by-row S-trajectory is used for the remaining tiles. 6 Evaluation 6.1 Experiment Methodology Algorithm Setup. Dataset and baseline: Our algorithm imple- mentation and GPU-based inference rely on gsplat, a popular and efficient library for 3DGS [38]. Following the original paper [18], we conduct experiments on real-world scenes from the MipNeRF- 360 dataset [2], including garden, bicycle, stump, bonsai, counter kitchen, and room. We compare our neural sorting algorithm with the SOTA sort-free algorithm for 3DGS [16]. As this work is not open-source, we carefully reproduce its results using our training framework. Evaluation metric: We employ several widely used met- rics to quantitatively assess rendered image quality, including Peak Signal-to-Noise Ratio (PSNR), with higher values indicating better quality, Structural Similarity Index (SSIM), where higher scores suggest better perceptual similarity, and Learned Perceptual Image Patch Similarity (LPIPS), with lower values corresponding to better perceptual quality. Implementation details: Training is conducted on an NVIDIA 3090 GPU, and the checkpoint for all scenes is obtained at 7000 epochs, following the original paper [18]. The neural sort- ing algorithm is subsequently trained for 15000 epochs, initialized from these checkpoints, and we save the model with the best PSNR across these training epochs. The learning rate is set to 0.005 for the MLP, while the original GS learning rates are scaled by a factor of 0.01 and carefully tuned. Hardware Setup. Our architecture is implemented using Sys- temVerilog and synthesized with Design Compiler under the TSMC 28nm CMOS library. The design performs rendering using FP16 arithmetic, implemented via DesignWare IP [33]. The design is fully pipelined, operating at a frequency of 1GHz. A DDR5 4800 DRAM with 38.4GB s bandwidth, together with Ramulator [20], is employed to model off-chip memory, while on-chip SRAM energy 8 Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization 00 01 10 11 00 01 10 11 0000 0001 0100 0101 0010 0011 0110 0111 1000 1001 1100 1101 1010 1011 1110 1111 Vertical Locality Horizontal Locality Hierarchical Locality Vertical Locality Horizontal Locality Hierarchical Locality Vertical Locality Horizontal Locality Hierarchical Locality (a) Baseline Trajectory (b) Z-Trajectory (c) Î -Trajectory Block of 8 8 Tiles Î -Trajectory within the Block (d) Generalized Î -Trajectory Figure 11: Comparison of different tile schedule trajectories with 4 4 size as the example. and area are estimated using CACTI [28], and the energy consump- tion includes on-chip and off-chip memory access estimated by DRAMPower [4]. A cycle-accurate simulator is implemented to estimate latency and model memory traffic, cross-validated against RTL simulation results. We compare our solution with the SOTA 3DGS accelerator GSCore [22] and follow it to compare the design with the NVIDIA Jetson Xavier NX edge GPU [1]. 6.2 Algorithm Evaluation Render quality comparison. Table 1 presents the rendering quality of neural sorting and compares it with the baseline and the SOTA sort-free algorithm for 3DGS [16]. [16] propose several simple functions related to depth to achieve nearly sort-free rendering, we choose the best function named LC-WSR (w o view dependent opacity) for comparison. As shown in the table, our algorithm achieves a close PNSR result of 26.50 compared with the baseline. The gap of SSIM is smaller and our algorithm even outperforms the baseline for LPIPS in certain scenes. Moreover, our algorithm achieves better results than the SOTA sort-free solution across scenes for three metrics. Table 1: Image quality comparison with baseline and the SOTA solution. Scene Bicycle Bonsai Counter Garden Kitchen Room Stump Avg. Base. PSNR 24.02 29.77 27.27 26.67 29.04 29.52 25.86 27.45 Sort-free [16] 23.05 26.65 24.96 24.67 26.24 28.11 24.37 25.43 Ours 23.76 28.22 26.44 26.05 27.79 28.09 25.15 26.50 Base. SSIM 0.6787 0.9278 0.8863 0.8389 0.9074 0.9034 0.7300 0.8389 Sort-free [16] 0.6604 0.8772 0.8240 0.62510 0.8692 0.8811 0.6809 0.7740 Ours 0.6780 0.9034 0.8711 0.8230 0.8892 0.8916 0.6936 0.8214 Base. LPIPS 0.2456 0.1473 0.1935 0.1172 0.1182 0.2021 0.2426 0.1809 Sort-free [16] 0.2667 0.2037 0.2321 0.2930 0.1490 0.1964 0.2504 0.2273 Ours 0.2491 0.1641 0.1813 0.1025 0.1281 0.1974 0.2393 0.1803 Fig. 12 visualizes our training process using examples from an indoor and an outdoor scene. After initialization with a pre- trained checkpoint, indoor renderings show clear contours and object boundaries but appear blurred and hazy due to the cluttered environment causing texture distortion. Texture details begin to recover after 1, 000 epochs, though minor artifacts (e.g., television screen color) remain. By 5, 000 epochs, the reconstruction closely matches the real scene. Similarly, initial outdoor renderings display clear outlines but suffer from occlusion and transparency distor- tions due to scene sparsity. Most artifacts are corrected after 1, 000 epochs, with residual issues (e.g., table leg occlusion) resolved by 5, 000 epochs. Overall, our method achieves high-quality rendering with negligible degradation. Neural network design exploration. We conduct a compre- hensive neural network design exploration to select suitable activa- tion functions while balancing the trade-off between image quality and network size. Fig. 13 provides a concrete example for the gar- den scene. For activation functions, without altering the network structure, we compare ReLU with Leaky ReLU (LReLU) for the first layer and sigmoid (sig.) with the exponential function (Exp) for the output layer. As shown in the figure (top), Leaky ReLU outperforms ReLU across three metrics by avoiding the dead ReLU phenomenon, where weights fail to be optimized. Although sigmoid exhibits a similar effect to Exp, it requires additional operations (addition and division); hence, Leaky ReLU combined with Exp is chosen, as denoted in green. For network structure, with the activation function choice fixed, we vary the number of network layers (l) and neurons (n) per layer. For instance, 2ğ‘™ 2ğ‘›represents two layers with two neurons per layer. The four choices shown in the figure (bottom) have MAC counts of 4, 8, 6, and 15, respectively. Our configura- tion with 6 MACs (denoted in green) achieves the best trade-off between image quality and computational efficiency. Increasing the network configuration from 2ğ‘™ 3ğ‘›to 3ğ‘™ 3ğ‘›yields a slight quality improvement but incurs a 2.5 increase in computational cost. In addition, the use of 6 MACs is consistent with the rasterization design, rendering the PE reconfigurable, and the Exp activation ingeniously reuses the exponential unit. 6.3 Hardware Evaluation Table 2 presents the details of our design, which occupies a total area of 3.85 mm2 and consumes 1.64 W. We further perform an ablation study to evaluate our axis-oriented rasterization and the overhead incurred by reconfiguration. The experimental settings include: baseline PE (13 MUL, 8 ADD, 1 EXP), axis-oriented rasteri- zation PE (6 MUL, 6 ADD, 1 EXP), and reconfigurable PE (6 MUL, 6 ADD, 1 EXP with multiplexers). Our experiments demonstrate that the axis-oriented rasterization PE reduces area by 38 and power by 39 , while reconfiguration incurs only a 5 increase in area and a 6 increase in power. Overall, our axis-oriented method effi- ciently reduces the MAC count and holds the potential to become the mainstream choice for future rasterization hardware design. 6.4 Ablation Study We conduct an ablation study to investigate the performance improvement of different optimizations. We evaluate four design variants: (i) Baseline (BS), which comprises a conventional 16 16 rasterization array integrated with a 32-parallel bitonic sorting 9 Initialization PSNR: 20.68 1k epochs PSNR: 24.54 5k epochs PSNR: 27.46 Initialization PSNR: 20.01 1k epochs PSNR: 22.82 5k epochs PSNR: 25.16 Figure 12: Visualization of our neural sorting training process with an indoor (left) and an outdoor (right) scene. 25.1 25.3 25.5 25.7 25.9 26.1 2l-2n 3l-2n 2l-3n 3l-3n PNSR 0.8 0.805 0.81 0.815 0.82 0.825 2l-2n 3l-2n 2l-3n 3l-3n SSIM 0.095 0.1 0.105 0.11 0.115 0.12 2l-2n 3l-2n 2l-3n 3l-3n LPIPS 25.1 25.3 25.5 25.7 25.9 26.1 ReLU-Sig. LReLU-Sig. ReLU-Exp LReLU-Exp PNSR 0.795 0.8 0.805 0.81 0.815 0.82 0.825 ReLU-Sig. LReLU-Sig. ReLU-Exp LReLU-Exp SSIM 0.1 0.102 0.104 0.106 0.108 0.11 0.112 ReLU-Sig. LReLU-Sig. ReLU-Exp LReLU-Exp LPIPS Figure 13: Design exploration of activation function (top) and network structure (bottom). Green marks denote our choice. Table 2: Area and power of our design. Coponent Configuration Area [mm2] Power [W] Reconfigurable PE Array 16 16 Reconfigurable PE 2.958 1.48 Support Modules X-PE Line Y-PE line Coord. Gen. Div. Array (4 Div.) 0.064 0.02 On-chip Buffer GS Feature(88KB) Output (4KB) Depth(4KB) 0.826 0.14 Total 3.85 1.64 0 0.5 1 1.5 2 2.5 Garden Bicycle Counter Kitchen Room Stump BS BS AR BS NS AR BS NS AR IP Figure 14: Speedup of variants isolating each optimization. network [17]. (ii) BS AR, in which the conventional rasterization array in the baseline is replaced by an axis-oriented rasterization (AR) array. (iii) BS AR NS further applies neural sorting (NS) to eliminate the sorting network, supporting its execution on our re- configurable array. (iv) BS AR NS IP further incorporates our in- terleaved pipeline (IP). Since both arrays share the same dimensions, the axis-oriented rasterization array achieves equivalent through- put to the conventional design while occupying a smaller area. Thus, the latency of the conventional array scales up according to the area difference for fairness. Fig. 14 demonstrates that the axis-oriented rasterization achieves a geometric mean speedup of 1.37 . When neural sorting is integrated, the speedup increases to 2.16 , and ultimately, our complete optimization scheme achieves a geometric mean speedup of 2.27 . Tile schedule trajectory study. Fig. 15 (left) presents the aver- age cache hit rate across scenes for three tile scheduling methods: 0 0.2 0.4 0.6 0.8 1 Î - Trajectory Z-Trajectory Baseline Cache Hit Rate 0.0 0.2 0.4 0.6 0.8 1.0 w o Cache Baseline Z-Trajectory Î -Trajectory Off-chip Energy Figure 15: Cache hit rate and energy comparison. the baseline trajectory, the Z-trajectory, and our generalized ğœ‹- trajectory tile schedule. The baseline achieves a hit rate of 43 , the Z-trajectory 55 , and our method improves the hit rate to 62 . The corresponding off-chip access energy is shown on the right, with the configuration without cache normalized to 1. Thanks to the hor- izontal, vertical, and hierarchical locality utilized, our ğœ‹-trajectory tile schedule achieves a 2.56 , 1.51 , and 1.23 energy saving over no-cache setting, baseline trajectory, and Z-trajectory. 6.5 Comparison with Other Implementations Comparison with Edge GPU. As shown in Fig. 16 (left), our de- sign achieves a rasterization speedup of 16.9 20.4 over the edge GPU, with a throughput exceeding 150 FPS (frames per second) as indicated on the secondary axis. This speedup is attributed to the axis-oriented rasterization, which effectively avoids redundant computations and reduces the MAC count. Our dedicated hard- ware architecture provides high parallelism and excellent PE uti- lization. Fig. 16 (right) illustrates the remarkable speedup achieved by neural sorting. Even the naive pipeline achieves a speedup of 102 525 over the edge GPU, as neural sorting converts the orig- inally expensive sorting process into a low-cost MAC operation, which is efficiently accelerated by our PE array. Interestingly, in- door scenes specifically counter, kitchen, and room exhibit higher speedups. This is because these scenes contain fewer Gaussians but exhibit more intersections with tiles, and the naive pipeline elim- inates replicated computations by performing the entire sorting at the beginning. However, memory-bound issues significantly de- grade PE utilization. Thanks to our fine-grained interleaved pipeline, the speedup nearly reaches 2000 . Although the fine-grained inter- leaved pipeline incurs some replicated computation for Gaussians across tiles, the small size of the MLP and high PE utilization out- weigh this effect, rendering the sorting latency negligible. Overall comparison: Our rasterization and sorting optimizations are combined for further analysis. To simulate real applications, we also report the end-to-end performance by using the edge GPU to execute the Gaussian projection, from which we extract latency and energy consumption information and incorporate it into our design. Fig. 17 (left) shows that our sorting and rasterization optimizations collectively achieve a speedup of 23.4 27.8 over the GPU imple- mentation, while the end-to-end speedup is reduced to 7.8 11.5 , 10 Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization 18.2 16.9 18.4 17.8 17.0 20.4 0 50 100 150 200 250 300 350 400 0 5 10 15 20 25 Garden Bicycle Counter Kitchen Room Stump FPS Ras. Speedup Our Rasterization FPS 139 141 525 340 385 102 1,972 1,687 1,915 1,806 1,885 2,416 1E 0 1E 1 1E 2 1E 3 1E 4 Garden Bicycle Counter Kitchen Room Stump Naive Neural Sort. Speedup Pipeline Interleaved Sort. Speedup Figure 16: Speedup of rasterization and sorting over GPU. 34.9 36.6 51.4 42.4 44.5 28.8 5.1 6.0 8.8 8.3 7.4 4.1 1E 0 1E 1 1E 2 Garden Bicycle Counter Kitchen Room Stump Ras. Sort. Energy Saving End-to-end Energy Saving 25.8 23.4 25.8 24.7 24.3 27.8 8.5 8.3 11.5 11.1 10.2 7.8 0 20 40 60 80 100 0 5 10 15 20 25 30 Garden Bicycle Counter Kitchen Room Stump FPS Ras. Sort. Speedup End-to-end Speedup End-to-end FPS 50 Figure 17: Overall speedup and energy saving over GPU. with the Gaussian projection being the major source of latency. The secondary axis indicates our end-to-end throughput, with our design consistently achieving over 50 FPS, thereby meeting the real-time rendering requirements on edge devices. Fig. 17 (right) illustrates the energy savings, with our combined rasterization and sorting optimizations achieving between 28.8 and 51.4 energy savings. These savings result from our dedicated architecture, which features low on-chip power consumption, uniformly supporting efficient neural sorting and rasterization, and from the cache with a ğœ‹-trajectory tile scheduling scheme that exploits spatial locality to enhance data reuse, thereby further reducing off-chip power consumption. Even with the energy overhead from the Gaussian projection which dominates overall energy consumption, the end- to-end savings still amount to between 4.1 and 8.8 . 0 0.5 1 1.5 2 2.5 3 Garden Bicycle Counter Kitchen Room Stump Area Efficiency Gain Over GSCore 0.0 0.4 0.8 1.2 1.6 2.0 2.4 Garden Bicycle Counter Kitchen Room Stump Energy Efficiency Over GSCore Figure 18: Comparison with SOTA accelerator [22]. Comparison with SOTA Accelerators. We select GSCore [22] for comparison because it also focuses on the original 3DGS infer- ence and accelerates the sorting and rasterization through dedi- cated hardware design, while others focus on specific applications [12, 23, 36]. Based on the details provided in [22], we implement a cycle-accurate simulator for GSCore to estimate performance. GScore is also designed using a 28 nm process and operates at 1GHz. We extract the power and area metrics for GSCore s sort- ing and rasterization modules for comparison. Since GScore has a different scale from our design, for fairness we evaluate area effi- ciency defined as the throughput of sorting rasterization divided by area, and energy efficiency, defined as energy consumption of sorting and rasterization per rendered image1. As shown in Fig. 18, our design achieves an area efficiency improvement of 1.94 2.39 . On the one hand, our rasterization effectively reduces the MAC count to only 6 per PE nearly a 50 reduction; on the other hand, the sorting and rasterization functions are uniformly supported via the reconfiguration of the PE array, incurring the cost of a single engine. For energy efficiency, our design achieves an improvement of 1.36 1.89 . Firstly, our design nearly eliminates the sorting latency, which is significant in GSCore. Secondly, the unified engine reduces both static and dynamic power consumption. Thirdly, our ğœ‹- trajectory tile scheduling, combined with an efficient cache design, effectively reduces the substantial energy consumption associated with off-chip access. 7 Related Work Efficient 3D Gaussian Splatting algorithm. Many studies have been proposed to accelerate rendering or reduce the mem- ory overhead in 3DGS by pruning the number of Gaussians [7 10, 29, 40], while accelerating the sorting process remains a rela- tively unexplored direction. Recently, [16] propose several simple, monotonically decreasing functions of depth to achieve nearly sort- free rendering; however, these fixed-form functions are inadequate for representing the wide range of real-world scenes. To overcome this, our neural sorting algorithm employs an expressive, tiny MLP, optimized by our training framework to enhance rendering quality, without compromising speed due to the co-designed hardware. 3D Gaussian Splatting accelerators. To address the real-time rendering requirements on edge devices, several specialized hard- ware accelerators have been proposed. GScore [22] streamlines the Gaussian splatting pipeline by reducing the number of Gaussians assigned to tiles via a shape-aware intersection test, thereby ef- fectively accelerating both sorting and rasterization. Recently, the Gaussian Blending Unit [39] proposed an edge GPU plug-in module for rasterization that renders each row of pixels sequentially from left to right, by leveraging intermediate values shared between adjacent pixels. This approach significantly reduces computational cost; however, it sacrifices parallelism because the row dimension is processed sequentially. Several 3DGS accelerators are designed to target segmented regions. For example, MetaSapiens [23] accel- erates foveated rendering using efficiency-aware pruning, GauSPU [36] proposes a co-processor for SLAM, and GsArch [12] accelerates the training process by enhancing memory access efficiency. Our approach diverges from these methods in two key aspects. First, we uniquely identify the inherent redundancy in the rasterization step and propose an axis-oriented computation flow, supported by a highly parallel rasterization array, which effectively reduces the MAC count without compromising generality. Second, we focus on optimizing the sorting process through our algorithm-hardware co-design, wherein our reconfigurable array with an interleaved pipeline dramatically reduces sorting latency to a negligible level. 1Since GSCore proposes the shape-aware intersection test, and this technique is com- patible with our design, to guarantee fairness we assume that neither design employs this technique. 11 8 Conclusion This work proposes an architecture-algorithm co-design ap- proach to overcome the key challenges of enabling real-time 3D Gaussian Splatting on edge devices. By introducing axis-oriented rasterization, we effectively reduce redundant MAC operations by effectively reusing the common terms expressions. Furthermore, a tiny neural network is employed to replace the conventional sorting process, mitigating pipeline bottlenecks and resource overheads. To support both rasterization and inference efficiently, we develop a reconfigurable PE array, together with an interleaved pipeline and ğœ‹-trajectory tile schedule to enhance PE utilization and mem- ory efficiency. Experimental results demonstrate that our approach achieves up to 27.8 speedup and 51.4 energy savings on real- world scenes with minimal quality loss. We plan to open-source our design to foster further development in this field. References [1] ADLINK Technology Inc. 2022. NVIDIA Jetson Xavier NX-based AI Vision System. vision-system [2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. 2022. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE CVF conference on computer vision and pattern recognition. 5470 5479. [3] Loren Carpenter. 1984. The A-buffer, an antialiased hidden surface method. In Proceedings of the 11th annual conference on Computer graphics and interactive techniques. 103 108. [4] Karthik Chandrasekar, Christian Weis, Yonghui Li, Benny Akesson, Norbert Wehn, and Kees Goossens. 2012. DRAMPower: Open-source DRAM power energy estimation tool. URL: drampower. info 22 (2012). [5] Eric Enderton, Erik Sintorn, Peter Shirley, and David Luebke. 2010. Stochastic transparency. In Proceedings of the 2010 ACM SIGGRAPH symposium on Interactive 3D Graphics and Games. 157 164. [6] Cass Everitt. 2001. Interactive order-independent transparency. White paper, nVIDIA 2, 6 (2001), 7. [7] Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, Zhangyang Wang, et al. 2024. Lightgaussian: Unbounded 3d gaussian compression with 15x reduc- tion and 200 fps. Advances in neural information processing systems 37 (2024), 140138 140158. [8] Guangchi Fang and Bing Wang. 2024. Mini-splatting: Representing scenes with a constrained number of gaussians. In European Conference on Computer Vision. Springer, 165 181. [9] Guangchi Fang and Bing Wang. 2024. Mini-Splatting2: Building 360 Scenes within Minutes via Aggressive Gaussian Densification. arXiv preprint arXiv:2411.12788 (2024). [10] Sharath Girish, Kamal Gupta, and Abhinav Shrivastava. 2024. Eagles: Efficient accelerated 3d gaussians with lightweight encodings. In European Conference on Computer Vision. Springer, 54 71. [11] Frank Gray. 1953. Pulse code communication. United States Patent Number 2632058 (1953). [12] Houshu He, Gang Li, Fangxin Liu, Li Jiang, Xiaoyao Liang, and Zhuoran Song. 2025. GSArch: Breaking Memory Barriers in 3D Gaussian Splatting Training via Architectural Support. In Proceedings of the 2025 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE. [13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision. 1026 1034. [14] David Hilbert and David Hilbert. 1935. Ãœber die stetige Abbildung einer Linie auf ein FlÃ¤chenstÃ¼ck. Dritter Band: Analysis Grundlagen der Mathematik Physik Verschiedenes: Nebst Einer Lebensgeschichte (1935), 1 2. [15] Charles Antony Richard Hoare. 1961. Algorithm 64: quicksort. Commun. ACM 4, 7 (1961), 321. [16] Qiqi Hou, Randall Rauwendaal, Zifeng Li, Hoang Le, Farzad Farhadzadeh, Fatih Porikli, Alexei Bourd, and Amir Said. 2024. Sort-free Gaussian Splatting via Weighted Sum Rendering. arXiv preprint arXiv:2410.18931 (2024). [17] Mihai F Ionescu and Klaus E Schauser. 1997. Optimizing parallel bitonic sort. In Proceedings 11th International Parallel Processing Symposium. IEEE, 303 309. [18] Bernhard Kerbl, Georgios Kopanas, Thomas LeimkÃ¼hler, and George Drettakis. 2023. 3D Gaussian splatting for real-time radiance field rendering. ACM Trans. Graph. 42, 4 (2023), 139 1. [19] Mustafa Khan, Hamidreza Fazlali, Dhruv Sharma, Tongtong Cao, Dongfeng Bai, Yuan Ren, and Bingbing Liu. 2024. Autosplat: Constrained gaussian splatting for autonomous driving scene reconstruction. arXiv preprint arXiv:2407.02598 (2024). [20] Yoongu Kim, Weikun Yang, and Onur Mutlu. 2015. Ramulator: A fast and extensible DRAM simulator. IEEE Computer architecture letters 15, 1 (2015), 45 49. [21] Siddharth Krishna Kumar. 2017. On weight initialization in deep neural networks. arXiv preprint arXiv:1704.08863 (2017). [22] Junseo Lee, Seokwon Lee, Jungi Lee, Junyong Park, and Jaewoong Sim. 2024. Gscore: Efficient radiance field rendering via architectural support for 3d gaussian splatting. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3. 497 511. [23] Weikai Lin, Yu Feng, and Yuhao Zhu. 2025. MetaSapiens: Real-Time Neural Rendering with Efficiency-Aware Pruning and Accelerated Foveated Rendering. In Proceedings of the 30th ACM International Conference on Architectural Sup- port for Programming Languages and Operating Systems, Volume 1 (Rotterdam, Netherlands) (ASPLOS 25). Association for Computing Machinery, New York, NY, USA, 669 682. [24] Morgan McGuire and Louis Bavoil. 2013. Weighted blended order-independent transparency. Journal of Computer Graphics Techniques 2, 4 (2013). [25] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. 2021. Nerf: Representing scenes as neural radiance fields for view synthesis. Commun. ACM 65, 1 (2021), 99 106. [26] Guy M Morton. 1966. A computer oriented geodetic data base and a new tech- nique in file sequencing. (1966). [27] Cedrick MÃ¼nstermann, Stefan Krumpen, Reinhard Klein, and Christoph Peters. 2018. Moment-based order-independent transparency. Proceedings of the ACM on Computer Graphics and Interactive Techniques 1, 1 (2018), 1 20. [28] Naveen Muralimanohar, Rajeev Balasubramonian, and Norman P Jouppi. 2009. CACTI 6.0: A tool to model large caches. HP laboratories 27 (2009), 28. [29] Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Daniel Duckworth, Rama Gosula, Keisuke Tateno, John Bates, Dominik Kaeser, and Federico Tombari. 2024. Radsplat: Radiance field-informed gaussian splatting for robust real-time rendering with 900 fps. arXiv preprint arXiv:2403.13806 (2024). [30] Thomas Porter and Tom Duff. 1984. Compositing digital images. In Proceedings of the 11th annual conference on Computer graphics and interactive techniques. 253 259. [31] David P Rodgers. 1985. Improvements in multiprocessor system design. ACM SIGARCH Computer Architecture News 13, 3 (1985), 225 231. [32] Alvy Ray Smith. 1995. Alpha and the history of digital compositing. Technical Report. Citeseer. [33] Synopsys, Inc. [n. d.]. DesignWare Library. designware-ip soc-infrastructure-ip designware-library.html. [34] Xuechang Tu, Bernhard Kerbl, and Fernando de la Torre. 2024. Fast and robust 3D Gaussian splatting for virtual reality. In SIGGRAPH Asia 2024 Posters. 1 3. [35] Samuel Williams, Andrew Waterman, and David Patterson. 2009. Roofline: an insightful visual performance model for multicore architectures. Commun. ACM 52, 4 (2009), 65 76. [36] Lizhou Wu, Haozhe Zhu, Siqi He, Jiapei Zheng, Chixiao Chen, and Xiaoyang Zeng. 2024. GauSPU: 3D Gaussian Splatting Processor for Real-Time SLAM Systems. In 2024 57th IEEE ACM International Symposium on Microarchitecture (MICRO). IEEE, 1562 1573. [37] Jin Xu, Zishan Li, Bowen Du, Miaomiao Zhang, and Jing Liu. 2020. Reluplex made more practical: Leaky ReLU. In 2020 IEEE Symposium on Computers and communications (ISCC). IEEE, 1 7. [38] Vickie Ye, Ruilong Li, Justin Kerr, Matias Turkulainen, Brent Yi, Zhuoyang Pan, Otto Seiskari, Jianbo Ye, Jeffrey Hu, Matthew Tancik, et al. 2025. gsplat: An open-source library for Gaussian splatting. Journal of Machine Learning Research 26, 34 (2025), 1 17. [39] Zhifan Ye, Yonggan Fu, Jingqun Zhang, Leshu Li, Yongan Zhang, Sixu Li, Cheng Wan, Chenxi Wan, Chaojian Li, Sreemanth Prathipati, and Yingyan (Celine) Lin. 2025. Gaussian Blending Unit: An Edge GPU Plug-in for Real-Time Gaussian- Based Rendering in AR VR. In Proceedings of the 2025 IEEE International Sympo- sium on High Performance Computer Architecture (HPCA). IEEE. [40] Zhifan Ye, Chenxi Wan, Chaojian Li, Jihoon Hong, Sixu Li, Leshu Li, Yongan Zhang, and Yingyan Celine Lin. 2024. 3D Gaussian Rendering Can Be Sparser: Ef- ficient Rendering via Learned Fragment Pruning. Advances in Neural Information Processing Systems 37 (2024), 5850 5869. [41] Hongjia Zhai, Xiyu Zhang, Boming Zhao, Hai Li, Yijia He, Zhaopeng Cui, Hujun Bao, and Guofeng Zhang. 2025. Splatloc: 3D Gaussian splatting-based visual lo- calization for augmented reality. IEEE Transactions on Visualization and Computer Graphics (2025). [42] Siting Zhu, Guangming Wang, Xin Kong, Dezhi Kong, and Hesheng Wang. 2024. 3D Gaussian splatting in robotics: A survey. arXiv preprint arXiv:2410.12262 (2024). 12\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2506.07069v1 [cs.GR] 8 Jun 2025 Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization Zhican Wang Shanghai Jiao Tong University Guanghui He Shanghai Jiao Tong University Dantong Liu University of Cambridge Lingjun Gao Imperial College London Shell Xu Hu Samsung AI Cambridge Chen Zhang Shanghai Jiao Tong University Zhuoran Song Shanghai Jiao Tong University Nicholas Lane University of Cambridge Wayne Luk Imperial College London Hongxiang Fan Imperial College London Abstract 3D Gaussian Splatting (3DGS) has recently gained significant attention for high-quality and efficient view synthesis, making it widely adopted in fields such as AR VR, robotics, and autonomous driving. Despite its impressive algorithmic performance, real-time rendering on resource-constrained devices remains a major chal- lenge due to tight power and area budgets. Although existing accel- erators have attempted to improve the hardware performance of 3DGS, several key inefficiency, including rasterization redundancy and cross-stage pipeline imbalance, have been overlooked in prior work. This paper presents an architecture-algorithm co-design to address these inefficiencies. First, we reveal substantial redundancy caused by repeated computation of common terms expressions during the conventional rasterization. To resolve this, we propose axis-oriented rasterization, which pre-computes and reuses shared terms along both the X and Y axes through a dedicated hardware design, effectively reducing multiply-and-add (MAC) operations by up to 63 . Second, by identifying the resource and performance inefficiency of the sorting process, we introduce a novel neural sorting approach that predicts order-independent blending weights using an efficient neural network, eliminating the need for costly hardware sorters. A dedicated training framework is also proposed to improve its algorithmic stability. Third, to uniformly support rasterization and neural network inference, we design an efficient reconfigurable processing array that maximizes hardware utiliza- tion and throughput. Furthermore, we introduce a ğœ‹-trajectory tile schedule, inspired by Morton encoding and Hilbert curve, to optimize Gaussian reuse and reduce memory access overhead. Com- prehensive experiments demonstrate that the proposed design pre- serves rendering quality while achieving a speedup of 23.4 27.8 and energy savings of 28.8 51.4 compared to edge GPUs for real-world scenes.\n\n--- Segment 2 ---\nFurthermore, we introduce a ğœ‹-trajectory tile schedule, inspired by Morton encoding and Hilbert curve, to optimize Gaussian reuse and reduce memory access overhead. Com- prehensive experiments demonstrate that the proposed design pre- serves rendering quality while achieving a speedup of 23.4 27.8 and energy savings of 28.8 51.4 compared to edge GPUs for real-world scenes. We plan to open-source our design to foster further development in this field. Keywords 3D Gaussian Splatting, Hardware Accelerator, Algorithm-hardware Co-design, Reconfigurability 1 Introduction 3D Gaussian Splatting (3DGS) [18] has emerged as a promising computer graphics technique for novel view synthesis owing to its superior synthesis quality and efficient rendering speed. It has been widely adopted in diverse fields, such as robotics [42], augmented and virtual reality (AR VR) [34, 41], and autonomous driving [19]. In contrast to the neural radiance fields (NeRF) technique [25] that implicitly represents a three-dimensional scene via a neural net- work, 3DGS explicitly encodes the scene using a large set of 3D Gaussians with learnable positions, sizes, shapes, colors, and opaci- ties. Due to its lower algorithmic complexity, 3DGS achieves faster rendering performance than NeRF, making it well-suited for inter- active applications. However, achieving real-time 3DGS rendering on resource-constrained devices such as edge GPUs remains a chal- lenge. For instance, the NVIDIA Jetson Xavier NX [1] only achieves less than 10 frame per second on the MipNeRF-360 dataset [2]. The stringent power and area constraints of AR VR edge devices further exacerbate the difficulty of deploying 3DGS in such settings. Rasterization and sorting are two core components that con- tribute significantly to overall latency. Although prior work [22] has explored accelerating 3DGS to improve hardware performance, this work identifies unique challenges and optimization opportuni- ties within rasterization and sorting that have been overlooked in previous research. Challenge-1: Computational redundancy in rasterization. As shown in Fig.\n\n--- Segment 3 ---\nChallenge-1: Computational redundancy in rasterization. As shown in Fig. 1(a) (left), most existing implementations simply map the rasterization of each pixel into a separate processing element (PE) or compute core (such as [ğ‘†ğ‘¥ 2 ğ‘†ğ‘¦ 0 ğ‘(2,0)] at the core with index [2, 0]), without considering compute efficiency within each pixel. However, as analysed in Sec. 2.2, this pixel-wise parallelization leads to redundant computation of intermediate common terms expressions (ğ‘†ğ‘¦ 2 in the provided example) that are repeatedly computed across the pixels belonging to the same row (Y-axis) or column (X-axis). Such redundancy increases the num- ber of multiply-accumulate (MAC) operations and contributes to high overall latency during rasterization. Based on this observa- tion, we identify significant computational redundancy as a key challenge in achieving high-performance 3D Gaussian splatting. Challenge-2: Resource overhead and pipeline imbalance as- sociated with sorting. As shown in Fig. 1(b) (top), due to the heterogeneous nature of sorting and subsequent rasterization operations, previous solutions have adopted a cascaded pipeline composed of a sorting module and a rasterization module to reduce latency [22]. However, this design exhibits several draw- backs: First, it incurs a high hardware resource overhead. The sorting module is commonly implemented using a bitonic sorting unit [17], whose area scales proportionally to ğ‘˜log2(ğ‘˜), where ğ‘˜ represents the input parallelism, and as ğ‘˜increases, the overhead 1 RPE Challenge 1: High rasterization redundancy due to independent parallel processing. 1.High Parallelism 2.High Redundancy 3.High Latency GPU GSCore Rasterization: X 1.High Parallelism 2.Low Redundancy 3.Low Latency Y X Broadcast Combine Axis-oriented Rasterization: Challenge 2: Heterogeneous operations and pipeline imbalance caused by sorting. Solution Sorting Module Rasterization Module Conventional Pipeline: Operational complexity: O(Nlog2N) O(N) N N N: GS Time Schedule: Sorting Module Rasterization Module Idle! T N Increases Idle! T 1.High Overhead N varies across tiles!\n\n--- Segment 4 ---\nT N Increases Idle! T 1.High Overhead N varies across tiles! 2.Low Utilization High Latency Solution Algorithm: Neural Sorting Hardware Opt. RPE Reconfigurable PE Array Co-design RPE Array T 1.Low Overhead 2.High Utilization Low Latency RPE Time Schedule: â‘  â‘¡ â‘¢ Compute Axes Compute Axes â‘  Sy 2 is shared. CUDA Core PE Unit RPE RPE RPE RPE RPE RPE (Sec. 3) 1. 2. (Sec. 4) (Sec. 5) 2. 1. (a) (b) Interleaved Pipeline Î -Trajectory Schedule Sx 0 Sy N(0,2) Sx Shared Term N(x,y) Non-shared Term Sy 2 ... ... Y Sx 1 N(1,2) Sy 2 Sx 2 N(2,2) Sy 2 Sx L N(L,2) Sy 2 Sx 2 N(2,L) Sy L Sx 2 N(2,1) Sy 1 Sx 2 N(2,0) Sy 0 Array size L L Notations Sx 2 is shared. Sx 2 N(2,1) Broadcast â‘¡ Sy 1 Figure 1: Challenges of 3DGS acceleration and organization of the paper. increases dramatically. Second, sorting and rasterization exhibit different computational complexities, which can lead to severe pipeline imbalance. Specifically, given ğ‘Gaussians, sorting has a complexity of ğ‘‚(ğ‘log2(ğ‘)), whereas rasterization s complex- ity is ğ‘‚(ğ‘). Our profiling further reveals that the number of Gaussians per tile can vary by up to two orders of magnitude, ex- acerbating the imbalance. As a result, when the sorting hardware module is designed with fixed parallelism, the rasterization mod- ule often suffers from significant underutilization due to pipeline imbalance, especially as the number of Gaussians increases. To address Challenge-1, we propose an axis-oriented rasteri- zation technique, as depicted in Fig. 1(a) (right), which follows a three-step process designed to eliminate computational redundancy efficiently. First, the common intermediate terms along the X and Y axes are computed once per axis.\n\n--- Segment 5 ---\n1(a) (right), which follows a three-step process designed to eliminate computational redundancy efficiently. First, the common intermediate terms along the X and Y axes are computed once per axis. Second, these precomputed terms are broadcast to the corresponding PEs within the tile. Third, each PE simply combines the broadcasted terms to complete the ğ›¼computation with minized MAC overhead. This axis-oriented rasterization is implemented using a dedicated hardware design, effectively reducing the MAC counts for the ğ›¼computation in the rasterization process from 8 additions (ADD) and 4 multiplications (MUL) to 2 ADD and 2 MUL. To overcome Challenge-2, this work proposes a novel neural sorting method via algorithm-hardware co-design. As shown in Fig. 1(b) (bottom), we re-examine the role of sorting in Gaussian splitting at the algorithmic level. Sorting is traditionally required to ensure the correct front-to-back depth order for ğ›¼blending, as transparency depends on the sequential visibility of Gaussians. However, our key insight is that the pri- mary purpose of sorting is to produce a decay factor for blending. This observation leads us to explore the learning-based approach for sorting. To this end, we propose neural sorting that predicts the decay factor with a tiny neural network, thereby avoiding the need for explicit depth sorting. At the hardware level, we reuse the highly parallel rasterization array for efficient neural sorting by en- hancing these PEs with runtime reconfigurability. This co-designed approach efficiently eliminates the need for a high-overhead sorting engine, effectively enhancing PE utilization and reducing latency. Our fine-grained interleaved pipeline further mitigates the substan- tial operational intensity gap between sorting and rasterization. Furthermore, beyond the single-tile optimization, this work also explores cross-tile rendering trajectory optimization. Inspired by Morton encoding [26] and Hilbert curve [14], we propose a gener- alized ğœ‹ trajectory tile schedule that minimizes external memory access by improving data locality across tiles. In conclusion, we make the following contributions: We identify unique challenges and opportunities associated with accelerating 3D Gaussian splatting, including high computational redundancy in the rasterization process and heterogeneous op- erations and pipeline imbalance incurred by sorting (Sec. 2.2).\n\n--- Segment 6 ---\nIn conclusion, we make the following contributions: We identify unique challenges and opportunities associated with accelerating 3D Gaussian splatting, including high computational redundancy in the rasterization process and heterogeneous op- erations and pipeline imbalance incurred by sorting (Sec. 2.2). We propose axis-oriented rasterization with a dedicated hard- ware architecture, effectively eliminating computational redun- dancy and significantly reducing the MAC count (Sec. 3). We introduce a novel neural sorting that optimizes the traditional sorting process, achieving efficient decay factor prediction with negligible quality degradation (Sec. 4). We propose multiple hardware optimizations, such as runtime reconfigurability and a generalized ğœ‹ trajectory tile schedule, to enhance compute efficiency and reduce external memory access overhead. (Sec. 5). 2 Background and Motivation 2.1 3D Guassian Splatting 3DGS parameters. 3DGS is designed to represent a 3D scene using a set of Gaussians. For any given view, it can render an image by splatting these Gaussians into 2D space according to the rendering steps. Each Gaussian is modeled by Equation (1), where ğ‘ denotes the 3D coordinates. It uses a total of 59 parameters to describe: i) the mean (position) ğœ‡ (3 parameters); ii) the size and shape, represented by Î£ and determined by the scale ğ‘ (3 parameters) and rotation ğ‘(4 parameters); iii) the opacity factor ğ‘œ(1 parameter); and iv) the view-dependent color, described by spherical harmonics (SH) coefficients (16 3 48 parameters). Rendering steps. 3DGS rendering mainly comprises three steps: projection, sorting, and rasterization. For projection, according to the view (camera pose), 3D Gaussians are projected into 2D Gaus- sians: the 3D mean (ğœ‡ ) and covariance (Î£ ) are transformed into the 2D mean ğœ‡( 2 1 vector) and 2D covariance Î£ ( 2 2 matrix), and depth (d) of each 3D Gaussian relative to the camera is ac- quired. In addition to the spatial information, the color information, represented by an RGB vector ( 3 1 vector), is computed based on the SH coefficients and the camera pose.\n\n--- Segment 7 ---\nFor projection, according to the view (camera pose), 3D Gaussians are projected into 2D Gaus- sians: the 3D mean (ğœ‡ ) and covariance (Î£ ) are transformed into the 2D mean ğœ‡( 2 1 vector) and 2D covariance Î£ ( 2 2 matrix), and depth (d) of each 3D Gaussian relative to the camera is ac- quired. In addition to the spatial information, the color information, represented by an RGB vector ( 3 1 vector), is computed based on the SH coefficients and the camera pose. A 16 16 pixel tile is 2 Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization used as the granularity for rendering, and after projection, each 2D Gaussian is assigned to the tiles it intersects. For sorting, because the relative order of 3D Gaussians reflects occlusion and influences the ğ›¼blending in the subsequent step, each tile sorts the Gaussians it intersects in order of increasing depth (i.e., from near to far from the camera). For rasterization, it computes the color contribution from each Gaussian to each pixel within the tile, a process that mainly consists of ğ›¼computation followed by ğ›¼blending. The ğ›¼ computation is described by Equation (2), where ğ‘( 2 1 vector) rep- resents the pixel position. Using the computed ğ›¼values, ğ›¼blending determines the final color ğ¶for each pixel according to Equation (3) (left), where ğ‘‡represents the transmittance, ğ‘–denotes the index of Gaussians in sorted order, and ğ‘ğ‘–represents the RGB color of each Gaussian. The transmittanceğ‘‡ğ‘–is determined by the ğ‘– 1 Gaussians in front of it, as described by Equation (3) (right).\n\n--- Segment 8 ---\nUsing the computed ğ›¼values, ğ›¼blending determines the final color ğ¶for each pixel according to Equation (3) (left), where ğ‘‡represents the transmittance, ğ‘–denotes the index of Gaussians in sorted order, and ğ‘ğ‘–represents the RGB color of each Gaussian. The transmittanceğ‘‡ğ‘–is determined by the ğ‘– 1 Gaussians in front of it, as described by Equation (3) (right). ğº(p) ğ‘’ 1 2 (p ğ )Tğšº 1(p ğ ), (1) ğ›¼ ğ‘œ ğ‘’ 1 2 (p ğ)Tğšº 1(p ğ), (2) ğ¶ ğ‘ ğ‘– 1 ğ‘‡ğ‘–ğ›¼ğ‘–ğ‘ğ‘–, ğ‘‡ğ‘– ğ‘– 1 Ã– ğ‘— 1 (1 ğ›¼ğ‘–) (3) Profiling and analysis. We profile the breakdown of these three steps on the NVIDIA Jetson Xavier NX GPU using the MipNeRF- 360 dataset [2] and average the results across scenes. The results show that projection, sorting, and rasterization account for 12.6 , 25.5 , and 61.9 of the total latency, respectively. Since sorting and rasterization together constitute nearly 90 of the latency, this paper focuses on accelerating these steps. Although both projection and rasterization involve MAC operations, there exists a significant disparity between them. For each tile, rasterization computes the contribution of each Gaussian to 256 pixels, i.e., Equation (2) is repeated 256 times per Gaussian, whereas projection computes the relevant information only once per Gaussian. Moreover, the total number of 2D Gaussians (i.e., the instances of a 3D Gaussian intersecting tiles) is significantly higher than the original count of 3D Gaussians, as a single Gaussian can intersect multiple tiles.\n\n--- Segment 9 ---\nFor each tile, rasterization computes the contribution of each Gaussian to 256 pixels, i.e., Equation (2) is repeated 256 times per Gaussian, whereas projection computes the relevant information only once per Gaussian. Moreover, the total number of 2D Gaussians (i.e., the instances of a 3D Gaussian intersecting tiles) is significantly higher than the original count of 3D Gaussians, as a single Gaussian can intersect multiple tiles. Analyzing the MAC count per Gaussian within the rasterization step, the ğ›¼computation involves 8 MUL, 4 ADD, and 1 exponential operation, whereasğ›¼blending requires 5 MUL and 4 ADD, revealing that ğ›¼computation is the most MAC-intensive. 2.2 Challenge Analysis and Motivation 2.2.1 Computational Redundancy. As demonstrated in profil- ing, rasterization is the most time-consuming component in 3DGS. Within the rasterization pipeline, ğ›¼-blending is the most MAC- intensive process, involving cascaded matrix-vector multiplications. The formula expansion for ğ›¼computing is shown in Fig. 2 (top), where ğœ‡ğ‘¥ ğ‘–and ğœ‡ğ‘¦ ğ‘–denote the center of the ğ‘–-th Gaussian, ğ‘¥and ğ‘¦ denote the coordinates of a pixel. The conic matrix (Î£ 1), which is defined as the inverse of 2D Gaussian s covariance matrix, is param- eterized by ğ‘ğ‘–, ğ‘ğ‘–, and ğ‘ğ‘–. According to the formulation, it requires 8 multiplications (MUL), 4 additions (ADD), and 1 exponential op- eration (EXP). Conventionally, prior work [22] designs a PE array for rasterization, where the computation for each pixel is mapped to one PE. In these designs, the PE structure reflects the theoretical MAC count derived from the ğ›¼computation formulation, as shown in Fig. 2 (bottom), with registers omitted for simplicity.\n\n--- Segment 10 ---\nIn these designs, the PE structure reflects the theoretical MAC count derived from the ğ›¼computation formulation, as shown in Fig. 2 (bottom), with registers omitted for simplicity. Î±i exp(-1 2(p-Î¼i)T -1(p-Î¼i)) oi p x y Î¼ Î¼i x Î¼i y i -1 ai ci ci bi Î±i exp(-1 2 (ai (x-Î¼i x)2 bi (y-Î¼i y)2) ci (x-Î¼i x) (y-Î¼i y)) oi cross term x Î¼i x y Î¼i y ai ci bi -1 2 ex oi 8 MUL 4 ADD 1 EXP 8 MUL 4 ADD 1 EXP GSCore PE are shared across PEs are shared across PEs are shared across rows are shared across rows Î± Blending Rasterization Array X-axis quadratic term Y-axis quadratic term Y-axis quadratic term X-axis quadratic term X-axis quadratic term Y-axis quadratic term Figure 2: The source of redundant computing in rasterization. To illustrate the source of redundancy, we categorize the expo- nent in the ğ›¼computation into three components: (i) the X-axis quadratic term, representing the squared distance between the pixel and the Gaussian center along the X-axis; (ii) the Y-axis quadratic term, representing the squared distance along the Y-axis; and (iii) the cross term, capturing the interaction between the X and Y co- ordinates. As shown in Fig. 2 (right), these terms exhibit spatial redundancy: for any row of pixels, the Y-axis quadratic terms are constant across PEs, and for any column, the X-axis quadratic terms are shared. This analysis reveals that significant redundant compu- tation exists in both the X- and Y-axis quadratic terms. However, conventional PE designs compute these terms repeatedly for each pixel, failing to exploit the redundency for computational savings. To avoid redundant computation, our key idea is to redesign the entire data flow by precomputing the axis-shared terms. Each PE then performs only a simple combination of these precomputed values, thereby significantly reducing complexity while maintaining parallelism.\n\n--- Segment 11 ---\nTo avoid redundant computation, our key idea is to redesign the entire data flow by precomputing the axis-shared terms. Each PE then performs only a simple combination of these precomputed values, thereby significantly reducing complexity while maintaining parallelism. This axis-oriented rasterization is implemented through a dedicated hardware architecture (Sec. 3) to improve the compute efficiency. 5903.5 11016.1 4745.5 3935.5 2947.1 4393.1 125.1 106.4 75.9 109.9 85.3 78.6 1080.7 1378.9 740.9 778.2 613 672.4 1E 0 1E 1 1E 2 1E 3 1E 4 1E 5 Garden Bicycle Counter Kitchen Room Stump Gaussians Scene max min mean Two Magnitude! Varies Across Scenes Figure 3: The number of Gaussians varies across tiles and scenes. 3 2.2.2 Heterogeneous Operation and Pipeline Imbalance. As illustrated in Fig. 1(b), sorting incurs high hardware overhead and degrades PE utilization. Once the challenges in rasterization are addressed, the relative impact of sorting on overall performance becomes more significant based on Amdahl s Law [31]. To quanti- tatively analyze the impact of sorting on each tile to be rendered, we conducted profiling using the MipNeRF-360 dataset [2]. We em- ploy a trained Gaussian checkpoint at 7k iterations and utilize the validation set to measure the difference in the number of Gaussians across tiles and scenes. Since different camera poses yield varying rendering results, the reported Gaussian count is averaged across the entire validation set. As shown in Fig. 3, the maximum Gaussian count can reach up to 10, 000, while the minimum is only around 80, spanning two orders of magnitude. This difference exists both within the tiles of a single scene and across different scenes, posing a significant challenge to hardware design. Let ğ‘represent the number of Gaussians assigned to a pixel tile. The typical sorting complexity can be ğ‘‚(ğ‘log(ğ‘)), ğ‘‚(ğ‘ğ‘™ğ‘œğ‘”2(ğ‘)) such as quick sort and bitonic sort [15, 17], whereas rasterization has a complexity of ğ‘‚(ğ‘).\n\n--- Segment 12 ---\nLet ğ‘represent the number of Gaussians assigned to a pixel tile. The typical sorting complexity can be ğ‘‚(ğ‘log(ğ‘)), ğ‘‚(ğ‘ğ‘™ğ‘œğ‘”2(ğ‘)) such as quick sort and bitonic sort [15, 17], whereas rasterization has a complexity of ğ‘‚(ğ‘). As ğ‘increases, sorting becomes the bottleneck, degrading the PE utilization of the rasterization mod- ule and increasing overall latency. A straightforward solution is to increase the parallelism of the sorting module, such as the Bitonic network, however, this approach has two deficiencies. First, the area overhead of sorting increases dramatically, as it is proportional to ğ‘˜log2(ğ‘˜), where ğ‘˜denotes the degree of parallelism. Second, because the number of Gaussians varies dramatically across tiles and scenes, only increasing the sorting parallelism would degrade area and energy efficiency for tiles with fewer Gaussians. Due to this variation, it is challenging to achieve an optimal design that performs well across different scenes and tiles. Prior work [22] pro- poses an alternative solution: hierarchical sorting. This approach first iteratively establishes a series of pivots as boundaries to divide the Gaussians into subsets for coarse sorting and then performs precise sorting using a sorting module with fixed parallelism. How- ever, determining suitable pivots for different tiles and scenes is a non-trivial task, as the depth range can vary significantly. Further- more, it requires multiple data passes during sorting, which remain time- and energy-consuming. To comprehensively address this challenge, our key motivation is to adopt an algorithm-hardware co-design approach. At the al- gorithmic level, we reexamine the role of sorting and propose an alternative solution using neural sorting as described in Sec. 4. This solution fully leverages the high throughput of the PE array via reconfigurability (Sec. 5), uniformly supporting both sorting and rasterization. 3 Axis-Oriented Rasterization Inspiration and overview. We propose axis-oriented rasteri- zation to address Challenge 1 described in Sec. 2.2.\n\n--- Segment 13 ---\nWe propose axis-oriented rasteri- zation to address Challenge 1 described in Sec. 2.2. This approach consists of three steps: 1) computing shared terms along the X and Y axes, 2) broadcasting the derived axis terms to each PE, and 3) combining the received terms in each PE to produce the final result. To efficiently implement axis-oriented rasterization, our key con- siderations involve i) simplifying the control logic of the three-step process, and ii) minimizing or eliminating the additional storage required. Given a tile of size ğ¿ ğ¿, we observe that computing the shared terms has ğ‘‚(ğ¿) complexity, whereas the combination step has ğ‘‚(ğ¿2) complexity with each shared term reused by ğ¿times. These observations inspire the design of an ğ¿ ğ¿array for the combination step, equipped with pre-processing modules of size ğ¿to compute the shared terms. Following these design principles, an overview of our is illustrated in Fig. 4 (top left). For a tile size of 16 16, the design includes (i) a rasterization array of size 16 16, (ii) an X-PE line of length 16, and (ii) a Y-PE line of size 16. The X-PE line generates the X-axis shared terms to the rasterization array, whereas the Y-PE line generates the Y-axis shared terms. Each X-PE and Y-PE directly broadcast the intermediate results to the corresponding rasterization PE within the same column or row, repeating 16 times. This broadcast overhead is small enough to implement and meets timing closure requirements.\n\n--- Segment 14 ---\nEach X-PE and Y-PE directly broadcast the intermediate results to the corresponding rasterization PE within the same column or row, repeating 16 times. This broadcast overhead is small enough to implement and meets timing closure requirements. X-PE Y-PE x-term y-term x2-term y2-term ex 2 MUL 2 ADD 1 EXP Axis-oriented Rasterization PE Compute Axes â‘  Broadcast â‘¡ Broadcast â‘¡ Combine â‘¢ Axis-oriented Rasterization PE Î± Blending X-PE X-PE X-PE Y-PE Y-PE Y-PE (iii) Y-PE Line (ii) X-PE Line Î¼i x, -1 2ai, ci Î¼i y, -1 2bi (i) Rasterization Array ... ... One GS per Cycle ci(x-Î¼i x) x-term x2-term -1 2ai(x-Î¼i x)2 y-term (y-Î¼i y) y2-term -1 2bi(y-Î¼i y)2 oi x data y data GS Feature Y-PE y Î¼i y -1 2bi y-term y2-term y Î¼i y -1 2bi y-term y2-term Y-PE y Î¼i y -1 2bi y-term y2-term x Î¼i x ci -1 2ai X-PE x-term x2-term Compute Axes â‘  T Notations Figure 4: Hardware and computation flow of axis-oriented rasterization array. Computation flow and PE structure. The rasterization array renders each Gaussian and computes its contribution to a 16 16 tile of pixels continuously in each cycle. The Gaussian parameters are provided as input to the X-PE line and Y-PE line, which perform the axes computation, and the outputs from these PE lines are then fed to the rasterization PE array. To eliminate redundant processing of the Gaussian conic matrix parameters (e.g., computing the factor 1 2), we directly store the parameter set { 1 2ğ‘ğ‘–, 1 2ğ‘ğ‘–,ğ‘ğ‘–}, where ğ‘– denotes the index of each Gaussian.\n\n--- Segment 15 ---\nThe Gaussian parameters are provided as input to the X-PE line and Y-PE line, which perform the axes computation, and the outputs from these PE lines are then fed to the rasterization PE array. To eliminate redundant processing of the Gaussian conic matrix parameters (e.g., computing the factor 1 2), we directly store the parameter set { 1 2ğ‘ğ‘–, 1 2ğ‘ğ‘–,ğ‘ğ‘–}, where ğ‘– denotes the index of each Gaussian. The X-PE line receives the Gaussian parameters ğ‘ğ‘–and 1 2ğ‘ğ‘–, which are broadcast to all X-PEs. Internally, each X-PE line gen- erates 16 x-coordinates (ğ‘¥0 ğ‘¥15), which are automatically in- cremented by 16 when shifting to the next tile on the right. The detailed X-PE structure is illustrated in Fig. 4 (right). For simplicity, the x index is omitted (similarly for y in the following descrip- tion). Each X-PE consists of two adders and two multipliers. One computation branch calculates the term from X-axis (ğ‘¥-term) by computing (ğ‘¥ ğœ‡ğ‘¥ ğ‘–) and multiplying it by ğ‘ğ‘–, while the other branch squares (ğ‘¥ ğœ‡ğ‘¥ ğ‘–) and multiplies it by 1 2ğ‘ğ‘–to obtain the X-axis quadratic term (ğ‘¥2-term). The Y-PE line receives the Gaussian pa- rameter 1 2ğ‘ğ‘–, which is broadcast to each Y-PE. The y-coordinates (ğ‘¦0 ğ‘¦15) are similarly incremented by 16 when moving vertically between tiles. As shown in Fig. 4 (right), each Y-PE contains one 4 Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization adder and two multipliers.\n\n--- Segment 16 ---\nAs shown in Fig. 4 (right), each Y-PE contains one 4 Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization adder and two multipliers. One computation branch produces the Y-axis term (ğ‘¦-term) by computing (ğ‘¦ ğœ‡ğ‘¦ ğ‘–) directly, while the other squares (ğ‘¦ ğœ‡ğ‘¦ ğ‘–) and multiplies it by 1 2ğ‘ğ‘–to generate the Y-axis quadratic term (ğ‘¦2-term). For the rasterization PE array, each PE receives vertical inputs from the corresponding X-PE and horizontal inputs from the corre- sponding Y-PE based on the pixel coordinates. As shown in Fig. 4 (bottom), each rasterization PE comprises only two adders and two multipliers. One multiplier is dedicated to multiplying the opacity factor ğ‘œğ‘–at the final stage, while the remaining MAC unit combines the inputs from the PE lines. Specifically, the multiplier multiplies the ğ‘¥term with the ğ‘¦term, and the two adders subsequently add this result to the ğ‘¥2-term and ğ‘¦2-term, respectively. Considering synchronization, the X-PE line initiates one cycle earlier than the Y-PE line, ensuring that the ğ‘¥term andğ‘¦term arrive simultaneously at each rasterization PE during the first cycle, with the ğ‘¥2-term and ğ‘¦2-term arriving in the second and third cycles, respectively. After executing the exponential operation and multiplying by ğ‘œğ‘–, the computation of ğ›¼ğ‘–is completed, and the result is then fed into the ğ›¼blending unit. Due to the continuous processing of Gaussians, the computation of ğ›¼ğ‘– 1 is completed in the subsequent cycle. Overhead analysis. Compared to the GScore implementation, our axis-oriented rasterization requires an additional X-PE line and Y-PE line while significantly simplifying the design of the rasterization PE. To ensure a fair comparison, the MAC cost of the extra PE lines is amortized across the rasterization PE array.\n\n--- Segment 17 ---\nCompared to the GScore implementation, our axis-oriented rasterization requires an additional X-PE line and Y-PE line while significantly simplifying the design of the rasterization PE. To ensure a fair comparison, the MAC cost of the extra PE lines is amortized across the rasterization PE array. The total number of multipliers is calculated as 2 (2 16) 64 for the X-PE and Y-PE lines and 16 16 2 512 for the rasterization PE array, resulting in a total of 576. Similarly, the total number of adders is calculated as 2 16 1 16 48 for the extra PE lines and 16 16 2 512 for the rasterization PE array, resulting in a total of 560. When averaged over a 16 16 rasterization PE array, our axis-oriented approach requires only 2.25 multipliers and 2.19 adders per PE. Compared with 8 multipliers and 4 adders in GSCore implementation, the count of them is significantly reduced by 63 . 4 Neural Sorting Challenge-2 in Sec. 2.2 highlights the performance bottleneck brought by sorting and the demand of sorting-free rendering. In this section, we provide three insights for our neural sorting algorithm in Sec. 4.1, and then the neural network structure and training framework are provided in Sec. 4.2. 4.1 Motivation Insight-1: Sorting is designed to obtain the decay factor. The sort- ing process is indispensable for the original 3DGS due to ğ›¼blending, which heavily relies on the relative depth order determined by the camera pose. However, reexamining the blending equation (3), we note that the ultimate goal of sorting is to obtain the correct trans- mittanceğ‘‡ğ‘–. The Gaussians in front of the ğ‘–th Gaussian have smaller depth, and each contributes a factor of 1 ğ›¼ğ‘—(for ğ‘— 0, 1, . . . ,ğ‘– 1) in the cascade product. Since each ğ›¼lies in the interval (0, 1), as the depth of the ğ‘–th Gaussian increases, ğ‘‡ğ‘–becomes smaller, effectively serving as a decay factor. This naturally raises the question: Can we directly derive the decay factor based on depth?\n\n--- Segment 18 ---\nSince each ğ›¼lies in the interval (0, 1), as the depth of the ğ‘–th Gaussian increases, ğ‘‡ğ‘–becomes smaller, effectively serving as a decay factor. This naturally raises the question: Can we directly derive the decay factor based on depth? Insight-2: The ğ›¼-blending for 3DGS is analogous to image compo- sition. The original 3DGS paper [18] highlights that its ğ›¼-blending follows the same image formation model as NeRF-style volumet- ric rendering, whereas reusing the term ğ›¼-blending from classic computer graphics [30]. The term ğ›¼ originates from the linear interpolation formula ğ›¼ğ´ (1 ğ›¼)ğµ[32], also interpreted as the over operation of image ğ´over ğµ. Inspired by this connection, we ob- serve that 3DGS ğ›¼-blending is similar to image composition. Fig. 5 illustrates this using three Gaussians: 3DGS blends front-to-back (left), while standard image compositing is back-to-front (right). Here, ğ¶3, ğ¶2, and ğ¶1 represent accumulated colors, computed via successive over operations involving image colors and their opaci- ties (ğ‘, ğ›¼), yielding the same final result as 3DGS. This equivalence generalizes to any number of Gaussians. Î±1 c1 Î±2 c2 Î±3 c3 3DGS Compute : Front-to-Back C T1Î±1c1 T2Î±2c2 T3Î±3c3 Î±1c1 (1-Î±1)Î±2c2 (1-Î±1)(1-Î±2)Î±3c3 Î±1 c1 Î±3 c3 Î±2 c2 Image Composition : Back-to-Front C3 C2 C1 C3 Î±3c3 C2 (1-Î±2) C3 Î±2c2 C C1 (1-Î±1)C2 Î±1c1 Î±1c1 (1-Î±1)Î±2c2 (1-Î±1)(1-Î±2)Î±3c3 Identical!\n\n--- Segment 19 ---\nThis equivalence generalizes to any number of Gaussians. Î±1 c1 Î±2 c2 Î±3 c3 3DGS Compute : Front-to-Back C T1Î±1c1 T2Î±2c2 T3Î±3c3 Î±1c1 (1-Î±1)Î±2c2 (1-Î±1)(1-Î±2)Î±3c3 Î±1 c1 Î±3 c3 Î±2 c2 Image Composition : Back-to-Front C3 C2 C1 C3 Î±3c3 C2 (1-Î±2) C3 Î±2c2 C C1 (1-Î±1)C2 Î±1c1 Î±1c1 (1-Î±1)Î±2c2 (1-Î±1)(1-Î±2)Î±3c3 Identical! Figure 5: 3DGS ğ›¼blending is similar to image composition. Apparently, the over operation is non-commutative, thus requir- ing costly depth sorting. Fortunately, several order-independent transparency (OIT) techniques have been proposed in computer graphics [3, 5, 6, 24, 27]. Given the similarity between 3DGS and image composition, these methods offer inspiration for optimizing 3DGS sorting in our work. Among these methods, weighted OIT aligns well with Insight-1. By omitting the background contribu- tion, equation (4) is derived, where ğ¶is the computed color, and ğ¹(ğ‘‘ğ‘–) is a monotonic decreasing function with respect to depth ğ‘‘ğ‘–. [24] proposes several general-purpose functions for ğ¹(ğ‘‘ğ‘–), however, they often require scene-specific tuning to perform well. ğ¶ Ãğ‘› ğ‘– 1 ğ¹(ğ‘‘ğ‘–)ğ›¼ğ‘–ğ‘ğ‘– Ãğ‘› ğ‘– 1 ğ¹(ğ‘‘ğ‘–)ğ›¼ğ‘– (4) Insight-3: A tiny multilayer perception (MLP) is a suitable replace- ment for sorting.\n\n--- Segment 20 ---\n[24] proposes several general-purpose functions for ğ¹(ğ‘‘ğ‘–), however, they often require scene-specific tuning to perform well. ğ¶ Ãğ‘› ğ‘– 1 ğ¹(ğ‘‘ğ‘–)ğ›¼ğ‘–ğ‘ğ‘– Ãğ‘› ğ‘– 1 ğ¹(ğ‘‘ğ‘–)ğ›¼ğ‘– (4) Insight-3: A tiny multilayer perception (MLP) is a suitable replace- ment for sorting. First, weighted OIT requires a manual selection and tuning process for an appropriate ğ¹(ğ‘‘ğ‘–), and it still suffers from some performance loss. Second, NeRF implicitly encodes the scene using a large MLP (Multilayer Perceptron) that maps 3D spatial coordinates and viewing directions into volume density and color. Inspired by this, it is intuitive to adopt an MLP in 3DGS to encode the impact of depth information. Third, due to the rapid 3DGS training speed on desktop or server GPUs, co-training a tiny MLP as a pipeline component is both cost-effective and practical. 4.2 MLP Design and Training Framework MLP Design. The MLP design should balance the expressiveness with hardware deployment efficiency. From a hardware perspective, since the rasterization array provides highly parallel MAC units, 5 w2 w1 w3 w4 w5 w6 Leaky ReLU Exp b1 b2 b3 b4 Initialization Pretrained Checkpoint 3D Gaussians Camera Projection Tile Rasterization Rendered Image Ground Truth Image High learning rate Low learning rate Operational Flow Gradient Flow Neural Network Structure di di 1. 2. 3. 4. 5. 6. Figure 6: Neural network structure and the training framework. our key motivation is to reuse this array to accelerate MLP infer- ence. Our tiny MLP, as illustrated in Fig. 6 (left), comprises only 2 layers, 10 parameters, and 6 MACs. In addition to the network architecture, initialization methods and activation functions are critical for MLP expressiveness. He initialization [13] is applied to the first layer, while Xavier initialization [21] is adopted for the output layer.\n\n--- Segment 21 ---\nIn addition to the network architecture, initialization methods and activation functions are critical for MLP expressiveness. He initialization [13] is applied to the first layer, while Xavier initialization [21] is adopted for the output layer. Leaky ReLU [37] is used as the activation function in the first layer, and an exponential function is employed in the output layer. The Leaky ReLU expression is given in equation (5), with a coefficient of 1 8 chosen for hardware simplicity. Although the standard ReLU function is simpler, we observed dead ReLU issues during training, indicating that its inputs remain constantly negative and prevent weight updates. The exponential function is chosen for the output layer due to its simplicity and wide dy- namic range, as well as it can reuse of the exponential unit for rasterization. Other activation functions, such as sigmoid, are less suitable because they involve extra division and addition operations. A more detailed exploration of the neural network architecture and activation functions is provided in Sec. 6. Leaky ReLU(ğ‘¥) ( ğ‘¥, if ğ‘¥ 0 1 8ğ‘¥, otherwise (5) Training Framework. Our training framework is illustrated in Fig. 6 (right), where the black arrow denotes the forward (opera- tional) flow and the blue arrow denotes the gradient flow during backpropagation. Firstly, the 3D Gaussians are initialized using a pre-trained checkpoint, which is obtained from the original algo- rithm that incorporates sorting (denoted by 1. in the figure). This step is crucial, as our experiments with co-training the 3D Gaus- sians and the MLP from scratch resulted in convergence difficulties. Moreover, the original 3DGS training flow begins from SFM points [18], further reaffirming the significance of proper initialization. After initialization, the Gaussians are projected into 2D space according to the camera pose (denoted by 2. ), and the computed depth of each Gaussian is extracted as the input ğ‘‘ğ‘–to the MLP. Since the sorting process is optimized away, with the MLP outputs ğ¹(ğ‘‘ğ‘–), the projected Gaussians are directly used for tile rasterization (denoted by 3.). the rendered image is then obtained from equation (4).\n\n--- Segment 22 ---\nSince the sorting process is optimized away, with the MLP outputs ğ¹(ğ‘‘ğ‘–), the projected Gaussians are directly used for tile rasterization (denoted by 3.). the rendered image is then obtained from equation (4). Subsequently, the rendered image is compared with the ground truth image to compute the loss (denoted by 4. ), and the loss follows the original setting [18]. As shown on the rightmost side, initially, the rendered image does not handle occlusion well, resulting in a somewhat transparent object. As training progresses, the rendered image gradually improves and increasingly resembles the ground truth. The backward process follows the blue arrow, and the gradients are used to optimize both the 3D Gaussians and the MLP. However, their learning rates differ significantly. We multiply a frozen factor, such as 0.01, to the original 3DGS learning rate while assigning a relatively larger learning rate to the MLP. This causes the loss to optimize the MLP rapidly (denoted by 5.) and adjust the 3D Gaussians more gradually (denoted by 6.). Interestingly, we found that freezing the optimization of the 3D Gaussians and training only the MLP does not work, implying that slight adjustments to the Gaussians to accommodate the new paradigm are crucial. Another important setting is that the Gaussian clone and split are disabled because they can be regarded as an abrupt change that undermines training stability. Disabling this step keeps the number of Gaussians constant and yields better rendering results. 5 Hardware Architecture and Optimizations In this section, we present the detailed hardware architecture for unified rasterization and neural sorting in Sec. 5.1. Since rasteriza- tion is a typically compute-bound workload, whereas computation of the tiny MLP is heavily memory-bound, we observe that the naive pipeline suffers from PE underutilization. To mitigate this dis- parity and enhance overall performance, we propose a fine-grained interleaved pipeline in Sec. 5.2. Finally, our analysis reveals that tile scheduling patterns have a significant impact on off-chip memory access. Hence, we introduce a generalized ğœ‹-trajectory tile schedul- ing method in Sec. 5.3. 5.1 Reconfigurable Hardware Design Overview. Fig.\n\n--- Segment 23 ---\n5.1 Reconfigurable Hardware Design Overview. Fig. 7 (left) illustrates the unified hardware archi- tecture for both rasterization and neural sorting, with the arrow notation at the top left. The architecture comprises dedicated com- putation and storage modules. For computation, the design incor- porates a 16 16 reconfigurable PE array equipped with a broadcast register per row, as well as X-PE and Y-PE lines (introduced in Sec. 3). Both PE lines receive either x or y coordinates from the coordinate generator, as depicted in Fig. 7 (left). The operation of addition or subtraction is determined by the trajectory of the tiles to be rendered, thereby realizing the ğœ‹-trajectory tile scheduling 6 Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization M-4-1 M-4-2 M-4-3 di M-3-r w1 Ri w2 Gi w3 Bi A-4-1 A-4-2 A-4-3 b1 b2 b3 M-1 x-term y-term A-4-1-r w4 A-1 x2-term M-2-r M-2 E-r A-4-2-r A-2 y2-term A-3-r M-3 F(d) w6 A-4-3-r M-2-r M-2-r M-2-r A-3 b4 ex E Reconfigurable PE (RPE) RPE X-PE RPE RPE RPE RPE RPE RPE RPE RPE RPE RPE RPE RPE RPE RPE RPE ... ... 16 16 Y-PE Î¼x , Î¼y -1 2a,-1 2b,c R, G, B, o Projected GS Feature Cache Y-PE Y-PE Y-PE X-PE X-PE X-PE Y-PE Line X-PE Line Pixel Output Buffer Div. Array B-Reg Ctrl.\n\n--- Segment 24 ---\nThe operation of addition or subtraction is determined by the trajectory of the tiles to be rendered, thereby realizing the ğœ‹-trajectory tile scheduling 6 Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization M-4-1 M-4-2 M-4-3 di M-3-r w1 Ri w2 Gi w3 Bi A-4-1 A-4-2 A-4-3 b1 b2 b3 M-1 x-term y-term A-4-1-r w4 A-1 x2-term M-2-r M-2 E-r A-4-2-r A-2 y2-term A-3-r M-3 F(d) w6 A-4-3-r M-2-r M-2-r M-2-r A-3 b4 ex E Reconfigurable PE (RPE) RPE X-PE RPE RPE RPE RPE RPE RPE RPE RPE RPE RPE RPE RPE RPE RPE RPE ... ... 16 16 Y-PE Î¼x , Î¼y -1 2a,-1 2b,c R, G, B, o Projected GS Feature Cache Y-PE Y-PE Y-PE X-PE X-PE X-PE Y-PE Line X-PE Line Pixel Output Buffer Div. Array B-Reg Ctrl. Î¼i x, -1 2ai, ci Î¼i y, -1 2bi B-Reg B-Reg B-Reg w1 Ri w1 Ri w2 Gi w2 Gi w3 Bi w3 Bi w6 F(di 1) w6 F(di 1) w5 oi 2 w5 oi 2 w4 w4 b1 b1 b2 b2 b3 b3 b4 b4 oi w5 Broadcast Reg (B-reg) 4 Depth Buffer 88KB 4KB x0 y0 x1 y1 x2 y2 x15 y15 16 -16 Coordinate Generator (CG) ... x CG y CG 16 -16 16 -16 16 -16 x data y data F(d) d (depth) RPE output PE line input B-reg data 4KB Figure 7: Unified hardware architecture of rasterization and neural sorting (left), and the structure of modules (right).\n\n--- Segment 25 ---\nArray B-Reg Ctrl. Î¼i x, -1 2ai, ci Î¼i y, -1 2bi B-Reg B-Reg B-Reg w1 Ri w1 Ri w2 Gi w2 Gi w3 Bi w3 Bi w6 F(di 1) w6 F(di 1) w5 oi 2 w5 oi 2 w4 w4 b1 b1 b2 b2 b3 b3 b4 b4 oi w5 Broadcast Reg (B-reg) 4 Depth Buffer 88KB 4KB x0 y0 x1 y1 x2 y2 x15 y15 16 -16 Coordinate Generator (CG) ... x CG y CG 16 -16 16 -16 16 -16 x data y data F(d) d (depth) RPE output PE line input B-reg data 4KB Figure 7: Unified hardware architecture of rasterization and neural sorting (left), and the structure of modules (right). method (see Sec. 5.3). For storage, the projected Gaussian (GS) fea- tures are buffered in a four-way set-associative cache, while the depth information, along with the computed decay factor ğ¹(ğ‘‘ğ‘–), is stored separately in the depth buffer due to different on-chip bandwidth requirements. Furthermore, the computed denominator and numerator from equation (4) are transferred to the pixel output buffer, where they are subsequently normalized by the division array to yield the final pixels. Reconfigurable PE. Fig. 7 (left) illustrates the reconfigurable PE, which comprises 6 multipliers, 6 adders, and an exponential unit, all operating in FP16.The multipliers and adders (MACs) are divided into two groups, each comprising three MACs. In one group, each MAC operates independently, denoted as ğ‘€-{1 3} and ğ´-{1 3}, whereas the three MACs in the other group operate cooperatively, denoted as ğ‘€-4-{1 3} and ğ´-4-{1 3}. Each multiplier or adder is equipped with a register to ensure timing closure at high frequen- cies. Certain connections are implicitly indicated by the notation, where the suffix ğ‘Ÿdenotes the wire output from the corresponding register.\n\n--- Segment 26 ---\nEach multiplier or adder is equipped with a register to ensure timing closure at high frequen- cies. Certain connections are implicitly indicated by the notation, where the suffix ğ‘Ÿdenotes the wire output from the corresponding register. For instance, ğ´-4-1-ğ‘Ÿ(the fourth wire on the left, denoted by a blue circle) indicates that this wire is connected with the regis- ter output of the ğ´-4-1 adder (the second wire on the right, denoted by a blue circle). The control of the multiplexers determines the datapath configuration between the rasterization mode (upper) and the sorting mode (lower). The workflow and the configured PE structures are described as follows: Rasterization mode. The rasterization mode PE is configured as shown in Fig. 8. In this mode, the PE array continuously processes one GS per cycle for the 16 16 tile pixels. The GS-feature cache provides the GS position, ğœ‡ğ‘¥ ğ‘–and ğœ‡ğ‘¦ ğ‘–, and the conic parameters 1 2ğ‘ğ‘–, 1 2ğ‘ğ‘–, and ğ‘ğ‘–to X-and Y- PE lines, where ğ‘–denotes the GS index. ğ‘œğ‘–is unique, loaded into the broadcast register, and broadcast 16 times to the PEs on the same line. In the configured PE, ğ‘€-{1 2}, ğ´-{1 2}, and the exponential unit are used for rasterization, cor- responding to the computation required in Fig. 4. The structure of X-PE, Y-PE, and the workflow for ğ›¼computation have been intro- duced in Sec. 3. The additional component is ğ›¼blending, which is defined by equation (4). The corresponding GS color features, ğ‘…ğ‘–, ğºğ‘–, and ğµğ‘–, are loaded into the broadcast register. The broadcast reg- ister, as shown in Fig. 7 (right), is equipped with 10 register units, of which only 5 are utilized in the rasterization mode.\n\n--- Segment 27 ---\nThe broadcast reg- ister, as shown in Fig. 7 (right), is equipped with 10 register units, of which only 5 are utilized in the rasterization mode. The decay factor ğ¹(ğ‘‘ğ‘–) is also loaded into the broadcast register, which is sourced from the depth buffer. After ğ‘…ğ‘–, ğºğ‘–, ğµğ‘–, and ğ¹(ğ‘‘ğ‘–) are broadcast 16 times to the PEs on the same line, the PE computes Equation 4. ğ‘€-3 multiplies ğ¹(ğ‘‘ğ‘–) with ğ›¼ğ‘–, and ğ´-3 accumulates ğ¹(ğ‘‘ğ‘–)ğ›¼ğ‘–across Gaussians, corresponding to the denominator. ğ‘€-4-{1 3} multi- ply ğ¹(ğ‘‘ğ‘–)ğ›¼ğ‘–with ğ‘…ğ‘–, ğµğ‘–, and ğºğ‘–, and ğ´-4-{1 3} accumulates the result across GSs for the RGB channels. ex M-4-1 M-4-2 M-4-3 A-4-1 A-4-2 A-4-3 M-1 M-2 M-3 A-1 A-2 A-3 E x-term y-term x2-term y2-term oi F(di) Ri Gi Bi Rasterization Mode PE Denominator Î± computation Î± blending Î±i Figure 8: Rasterization mode configured PE. ex M-4-1 M-4-2 M-4-3 A-4-1 E A-4-2 A-4-3 M-1 A-1 M-2 M-3 A-3 A-2 di w1 w2 w3 b1 b2 b3 w4 w5 w6 b4 Sorting Mode PE w4 w5 w6 b4 ex b1 b2 b3 Layer-1 Layer-2 w1 w2 w3 Figure 9: Sorting mode configured PE. Sorting mode. The configured sorting mode PE is shown in Fig. 9.\n\n--- Segment 28 ---\nThe configured sorting mode PE is shown in Fig. 9. In this mode, the PE array processes 16 16 256 depths per cycle and outputs 256 ğ¹(ğ‘‘) values. The inputs come from the depth buffer with high on-chip bandwidth, and the outputs are also written back to the depth buffer. The MLP weights are stored in the broadcast register, fully utilizing the ten register units. As shown in the figure, ğ‘€-4-{1 3} and ğ´-4-{1 3} perform the computations for the first layer, including the bias addition. ğ‘€-{1 3} imple- ments the second layer, while ğ´-{1 3} carries out the accumula- tion and bias addition. For the activation function, the exponential unit implements the exponential function. Leaky ReLU is omitted from the figure for simplicity. It is implemented through the sign detection and a 5-bit integer adder, realizing the subtraction of 3 from the FP16 exponent part if it is a normal negative. For negative subnormal or positive values, the value remains unchanged. 5.2 Fine-grained Interleaved Pipeline Memory-bound issue. Since our MLP-based sorting only relies on the depth of each GS, it is straightforward to compute the ğ¹(ğ‘‘) 7 for the entire GS, followed by tile-by-tile rasterization, as shown in the naive pipeline of Fig. 10 (top right). This pipeline theoretically eliminates replicated sorting computations across tiles. However, we observe that PE utilization is extremely low during the sorting process. The reason is that the operational intensity of sorting and rasterization differs significantly. For rasterization, each projected Gaussian comprises 9 parameters and performs 256 6 MAC oper- ations. In contrast, for sorting, each depth parameter only incurs 6 MAC operations, indicating a nearly 30 fold difference. Given a typical configuration for our architecture, as shown in Fig. 10 (left), rasterization is compute-bound, whereas sorting is heavily memory- bound [35]. This memory-bound issue also makes deploying our optimized algorithm directly to GPUs not an optimal choice. Optimization. To address this issue, we propose a fine-grained interleaved pipeline that further divides each tile into subtiles, as shown in Fig. 10 (right).\n\n--- Segment 29 ---\nTo address this issue, we propose a fine-grained interleaved pipeline that further divides each tile into subtiles, as shown in Fig. 10 (right). The core idea is to overlap the rasterization computation of the current subtile with the memory access required for sorting the next subtile. This fine-grained processing of subtiles is essential because each tile contains a variable number of GS, but the depth buffer size is limited. Fig. 10 (right bottom) illustrates how the proposed pipeline is mapped onto the hardware. For the first subtile, DRAM transfers the depths of GS to the depth buffer, which then outputs the depths to the PE array for sorting. The resulting ğ¹(ğ‘‘) values are written back to the depth buffer. Subsequently, the PE array performs sorting using the ğ¹(ğ‘‘) provided by the depth buffer, while the depth buffer simultaneously receives the depths for the next subtile, thereby overlapping rasterization with memory access. This process repeats, with the latency of depth access being completely hidden except for the first subtile. Throughput (GOPS) Intensity (MACs Byte) Memory- Bound Compute- Bound Sorting Rasterization Fine-grained Interleaved 38.4GB s 1536 Entire Sorting Tile1 Tile2 Tilen ... Low PE Utilization Naive Pipeline High PE Utilization Roofline Model Analysis Fine-grain Interleaved Pipeline ... PE Array T Buffer Access d Access GS d Rasterization Sorting Time Schedule Rasterization Sorting High PE Utilization DRAM Output d Input F(d) Input dnext Output F(d) Figure 10: Roofline model analysis and pipeline comparison. 5.3 ğœ‹ Trajectory Tile Schedule Our architecture adopts a GS-feature cache, where each cache line is tagged using a 28-bit GS ID. An additional 4 bits indicate the number of tiles intersected by each GS. The cache prioritizes the replacement of less important GS. These 4 bits, together with the 28-bit ID, form a 32-bit aligned storage. The cache design effectively reduces off-chip accesses due to the spatial locality of GSs. However, different tile scheduling trajectories affect the cache hit rate. As shown in Fig. 11(a), the baseline implementation scans tiles line by line, exploiting horizontal locality but lacking vertical and hierarchical locality.\n\n--- Segment 30 ---\nAs shown in Fig. 11(a), the baseline implementation scans tiles line by line, exploiting horizontal locality but lacking vertical and hierarchical locality. For instance, it may reuse GSs intersecting horizontally aligned tiles such as 1 2 or 1 3, but not those aligned vertically, such as 2 1 or 3 1. A slight improvement is achieved by using the S-trajectory, which reverses direction at the end of each row. Although they lack hierarchical locality, the row-by- row scheduling makes them flexible and applicable to any tile size. Hence, achieving 2D spatial locality through 1D tile scheduling is a non-trivial problem. Inspired by Morton encoding [26], which interleaves the ğ‘¦- and ğ‘¥-axes to encode each tile, for a 2-bit example, it interleaves ğ‘¦1ğ‘¦0 with ğ‘¥1ğ‘¥0 to obtain the 4-bit code ğ‘¦1ğ‘¥1ğ‘¦0ğ‘¥0, and schedules tiles in increasing order of this code, as shown in Fig. 11(b). This encoding forms a Z-trajectory curve and effectively preserves 2D locality in a 1D scheduling order. However, we find that the diagonal segments of the Z-trajectory often span large distances, resulting in discon- tinuities. To address this issue, and inspired by the continuity of Gray code [11], the Z-trajectory is modified into a "ğœ‹" trajectory that schedules tiles more continuously, as shown in Fig. 11(c). Both the Z-trajectory and the ğœ‹trajectory exhibit hierarchical locality, as indicated by the blue arrow (trajectory of 2 2 tiles), making them scalable. This curve, also known as the Hilbert curve [14], was originally proposed for fractal geometry for squares in 1891. We further generalize the design to better fit various 3DGS applications with different image sizes, as shown Fig. 11(d). First, the ğœ‹trajec- tory is applied only within each 8 8 tile block, while block-level scheduling follows the S-trajectory. Second, for images where the number of tiles is not divisible by 8, a row-by-row S-trajectory is used for the remaining tiles.\n\n--- Segment 31 ---\nFirst, the ğœ‹trajec- tory is applied only within each 8 8 tile block, while block-level scheduling follows the S-trajectory. Second, for images where the number of tiles is not divisible by 8, a row-by-row S-trajectory is used for the remaining tiles. 6 Evaluation 6.1 Experiment Methodology Algorithm Setup. Dataset and baseline: Our algorithm imple- mentation and GPU-based inference rely on gsplat, a popular and efficient library for 3DGS [38]. Following the original paper [18], we conduct experiments on real-world scenes from the MipNeRF- 360 dataset [2], including garden, bicycle, stump, bonsai, counter kitchen, and room. We compare our neural sorting algorithm with the SOTA sort-free algorithm for 3DGS [16]. As this work is not open-source, we carefully reproduce its results using our training framework. Evaluation metric: We employ several widely used met- rics to quantitatively assess rendered image quality, including Peak Signal-to-Noise Ratio (PSNR), with higher values indicating better quality, Structural Similarity Index (SSIM), where higher scores suggest better perceptual similarity, and Learned Perceptual Image Patch Similarity (LPIPS), with lower values corresponding to better perceptual quality. Implementation details: Training is conducted on an NVIDIA 3090 GPU, and the checkpoint for all scenes is obtained at 7000 epochs, following the original paper [18]. The neural sort- ing algorithm is subsequently trained for 15000 epochs, initialized from these checkpoints, and we save the model with the best PSNR across these training epochs. The learning rate is set to 0.005 for the MLP, while the original GS learning rates are scaled by a factor of 0.01 and carefully tuned. Hardware Setup. Our architecture is implemented using Sys- temVerilog and synthesized with Design Compiler under the TSMC 28nm CMOS library. The design performs rendering using FP16 arithmetic, implemented via DesignWare IP [33]. The design is fully pipelined, operating at a frequency of 1GHz.\n\n--- Segment 32 ---\nThe design performs rendering using FP16 arithmetic, implemented via DesignWare IP [33]. The design is fully pipelined, operating at a frequency of 1GHz. A DDR5 4800 DRAM with 38.4GB s bandwidth, together with Ramulator [20], is employed to model off-chip memory, while on-chip SRAM energy 8 Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization 00 01 10 11 00 01 10 11 0000 0001 0100 0101 0010 0011 0110 0111 1000 1001 1100 1101 1010 1011 1110 1111 Vertical Locality Horizontal Locality Hierarchical Locality Vertical Locality Horizontal Locality Hierarchical Locality Vertical Locality Horizontal Locality Hierarchical Locality (a) Baseline Trajectory (b) Z-Trajectory (c) Î -Trajectory Block of 8 8 Tiles Î -Trajectory within the Block (d) Generalized Î -Trajectory Figure 11: Comparison of different tile schedule trajectories with 4 4 size as the example. and area are estimated using CACTI [28], and the energy consump- tion includes on-chip and off-chip memory access estimated by DRAMPower [4]. A cycle-accurate simulator is implemented to estimate latency and model memory traffic, cross-validated against RTL simulation results. We compare our solution with the SOTA 3DGS accelerator GSCore [22] and follow it to compare the design with the NVIDIA Jetson Xavier NX edge GPU [1]. 6.2 Algorithm Evaluation Render quality comparison. Table 1 presents the rendering quality of neural sorting and compares it with the baseline and the SOTA sort-free algorithm for 3DGS [16]. [16] propose several simple functions related to depth to achieve nearly sort-free rendering, we choose the best function named LC-WSR (w o view dependent opacity) for comparison. As shown in the table, our algorithm achieves a close PNSR result of 26.50 compared with the baseline. The gap of SSIM is smaller and our algorithm even outperforms the baseline for LPIPS in certain scenes. Moreover, our algorithm achieves better results than the SOTA sort-free solution across scenes for three metrics. Table 1: Image quality comparison with baseline and the SOTA solution.\n\n--- Segment 33 ---\nMoreover, our algorithm achieves better results than the SOTA sort-free solution across scenes for three metrics. Table 1: Image quality comparison with baseline and the SOTA solution. Scene Bicycle Bonsai Counter Garden Kitchen Room Stump Avg. Base. PSNR 24.02 29.77 27.27 26.67 29.04 29.52 25.86 27.45 Sort-free [16] 23.05 26.65 24.96 24.67 26.24 28.11 24.37 25.43 Ours 23.76 28.22 26.44 26.05 27.79 28.09 25.15 26.50 Base. SSIM 0.6787 0.9278 0.8863 0.8389 0.9074 0.9034 0.7300 0.8389 Sort-free [16] 0.6604 0.8772 0.8240 0.62510 0.8692 0.8811 0.6809 0.7740 Ours 0.6780 0.9034 0.8711 0.8230 0.8892 0.8916 0.6936 0.8214 Base. LPIPS 0.2456 0.1473 0.1935 0.1172 0.1182 0.2021 0.2426 0.1809 Sort-free [16] 0.2667 0.2037 0.2321 0.2930 0.1490 0.1964 0.2504 0.2273 Ours 0.2491 0.1641 0.1813 0.1025 0.1281 0.1974 0.2393 0.1803 Fig. 12 visualizes our training process using examples from an indoor and an outdoor scene. After initialization with a pre- trained checkpoint, indoor renderings show clear contours and object boundaries but appear blurred and hazy due to the cluttered environment causing texture distortion. Texture details begin to recover after 1, 000 epochs, though minor artifacts (e.g., television screen color) remain. By 5, 000 epochs, the reconstruction closely matches the real scene. Similarly, initial outdoor renderings display clear outlines but suffer from occlusion and transparency distor- tions due to scene sparsity.\n\n--- Segment 34 ---\nBy 5, 000 epochs, the reconstruction closely matches the real scene. Similarly, initial outdoor renderings display clear outlines but suffer from occlusion and transparency distor- tions due to scene sparsity. Most artifacts are corrected after 1, 000 epochs, with residual issues (e.g., table leg occlusion) resolved by 5, 000 epochs. Overall, our method achieves high-quality rendering with negligible degradation. Neural network design exploration. We conduct a compre- hensive neural network design exploration to select suitable activa- tion functions while balancing the trade-off between image quality and network size. Fig. 13 provides a concrete example for the gar- den scene. For activation functions, without altering the network structure, we compare ReLU with Leaky ReLU (LReLU) for the first layer and sigmoid (sig.) with the exponential function (Exp) for the output layer. As shown in the figure (top), Leaky ReLU outperforms ReLU across three metrics by avoiding the dead ReLU phenomenon, where weights fail to be optimized. Although sigmoid exhibits a similar effect to Exp, it requires additional operations (addition and division); hence, Leaky ReLU combined with Exp is chosen, as denoted in green. For network structure, with the activation function choice fixed, we vary the number of network layers (l) and neurons (n) per layer. For instance, 2ğ‘™ 2ğ‘›represents two layers with two neurons per layer. The four choices shown in the figure (bottom) have MAC counts of 4, 8, 6, and 15, respectively. Our configura- tion with 6 MACs (denoted in green) achieves the best trade-off between image quality and computational efficiency. Increasing the network configuration from 2ğ‘™ 3ğ‘›to 3ğ‘™ 3ğ‘›yields a slight quality improvement but incurs a 2.5 increase in computational cost. In addition, the use of 6 MACs is consistent with the rasterization design, rendering the PE reconfigurable, and the Exp activation ingeniously reuses the exponential unit.\n\n--- Segment 35 ---\nIncreasing the network configuration from 2ğ‘™ 3ğ‘›to 3ğ‘™ 3ğ‘›yields a slight quality improvement but incurs a 2.5 increase in computational cost. In addition, the use of 6 MACs is consistent with the rasterization design, rendering the PE reconfigurable, and the Exp activation ingeniously reuses the exponential unit. 6.3 Hardware Evaluation Table 2 presents the details of our design, which occupies a total area of 3.85 mm2 and consumes 1.64 W. We further perform an ablation study to evaluate our axis-oriented rasterization and the overhead incurred by reconfiguration. The experimental settings include: baseline PE (13 MUL, 8 ADD, 1 EXP), axis-oriented rasteri- zation PE (6 MUL, 6 ADD, 1 EXP), and reconfigurable PE (6 MUL, 6 ADD, 1 EXP with multiplexers). Our experiments demonstrate that the axis-oriented rasterization PE reduces area by 38 and power by 39 , while reconfiguration incurs only a 5 increase in area and a 6 increase in power. Overall, our axis-oriented method effi- ciently reduces the MAC count and holds the potential to become the mainstream choice for future rasterization hardware design. 6.4 Ablation Study We conduct an ablation study to investigate the performance improvement of different optimizations. We evaluate four design variants: (i) Baseline (BS), which comprises a conventional 16 16 rasterization array integrated with a 32-parallel bitonic sorting 9 Initialization PSNR: 20.68 1k epochs PSNR: 24.54 5k epochs PSNR: 27.46 Initialization PSNR: 20.01 1k epochs PSNR: 22.82 5k epochs PSNR: 25.16 Figure 12: Visualization of our neural sorting training process with an indoor (left) and an outdoor (right) scene.\n\n--- Segment 36 ---\n6.4 Ablation Study We conduct an ablation study to investigate the performance improvement of different optimizations. We evaluate four design variants: (i) Baseline (BS), which comprises a conventional 16 16 rasterization array integrated with a 32-parallel bitonic sorting 9 Initialization PSNR: 20.68 1k epochs PSNR: 24.54 5k epochs PSNR: 27.46 Initialization PSNR: 20.01 1k epochs PSNR: 22.82 5k epochs PSNR: 25.16 Figure 12: Visualization of our neural sorting training process with an indoor (left) and an outdoor (right) scene. 25.1 25.3 25.5 25.7 25.9 26.1 2l-2n 3l-2n 2l-3n 3l-3n PNSR 0.8 0.805 0.81 0.815 0.82 0.825 2l-2n 3l-2n 2l-3n 3l-3n SSIM 0.095 0.1 0.105 0.11 0.115 0.12 2l-2n 3l-2n 2l-3n 3l-3n LPIPS 25.1 25.3 25.5 25.7 25.9 26.1 ReLU-Sig. LReLU-Sig. ReLU-Exp LReLU-Exp PNSR 0.795 0.8 0.805 0.81 0.815 0.82 0.825 ReLU-Sig. LReLU-Sig. ReLU-Exp LReLU-Exp SSIM 0.1 0.102 0.104 0.106 0.108 0.11 0.112 ReLU-Sig. LReLU-Sig. ReLU-Exp LReLU-Exp LPIPS Figure 13: Design exploration of activation function (top) and network structure (bottom). Green marks denote our choice. Table 2: Area and power of our design. Coponent Configuration Area [mm2] Power [W] Reconfigurable PE Array 16 16 Reconfigurable PE 2.958 1.48 Support Modules X-PE Line Y-PE line Coord. Gen. Div. Array (4 Div.)\n\n--- Segment 37 ---\nGen. Div. Array (4 Div.) 0.064 0.02 On-chip Buffer GS Feature(88KB) Output (4KB) Depth(4KB) 0.826 0.14 Total 3.85 1.64 0 0.5 1 1.5 2 2.5 Garden Bicycle Counter Kitchen Room Stump BS BS AR BS NS AR BS NS AR IP Figure 14: Speedup of variants isolating each optimization. network [17]. (ii) BS AR, in which the conventional rasterization array in the baseline is replaced by an axis-oriented rasterization (AR) array. (iii) BS AR NS further applies neural sorting (NS) to eliminate the sorting network, supporting its execution on our re- configurable array. (iv) BS AR NS IP further incorporates our in- terleaved pipeline (IP). Since both arrays share the same dimensions, the axis-oriented rasterization array achieves equivalent through- put to the conventional design while occupying a smaller area. Thus, the latency of the conventional array scales up according to the area difference for fairness. Fig. 14 demonstrates that the axis-oriented rasterization achieves a geometric mean speedup of 1.37 . When neural sorting is integrated, the speedup increases to 2.16 , and ultimately, our complete optimization scheme achieves a geometric mean speedup of 2.27 . Tile schedule trajectory study. Fig. 15 (left) presents the aver- age cache hit rate across scenes for three tile scheduling methods: 0 0.2 0.4 0.6 0.8 1 Î - Trajectory Z-Trajectory Baseline Cache Hit Rate 0.0 0.2 0.4 0.6 0.8 1.0 w o Cache Baseline Z-Trajectory Î -Trajectory Off-chip Energy Figure 15: Cache hit rate and energy comparison. the baseline trajectory, the Z-trajectory, and our generalized ğœ‹- trajectory tile schedule. The baseline achieves a hit rate of 43 , the Z-trajectory 55 , and our method improves the hit rate to 62 . The corresponding off-chip access energy is shown on the right, with the configuration without cache normalized to 1.\n\n--- Segment 38 ---\nThe baseline achieves a hit rate of 43 , the Z-trajectory 55 , and our method improves the hit rate to 62 . The corresponding off-chip access energy is shown on the right, with the configuration without cache normalized to 1. Thanks to the hor- izontal, vertical, and hierarchical locality utilized, our ğœ‹-trajectory tile schedule achieves a 2.56 , 1.51 , and 1.23 energy saving over no-cache setting, baseline trajectory, and Z-trajectory. 6.5 Comparison with Other Implementations Comparison with Edge GPU. As shown in Fig. 16 (left), our de- sign achieves a rasterization speedup of 16.9 20.4 over the edge GPU, with a throughput exceeding 150 FPS (frames per second) as indicated on the secondary axis. This speedup is attributed to the axis-oriented rasterization, which effectively avoids redundant computations and reduces the MAC count. Our dedicated hard- ware architecture provides high parallelism and excellent PE uti- lization. Fig. 16 (right) illustrates the remarkable speedup achieved by neural sorting. Even the naive pipeline achieves a speedup of 102 525 over the edge GPU, as neural sorting converts the orig- inally expensive sorting process into a low-cost MAC operation, which is efficiently accelerated by our PE array. Interestingly, in- door scenes specifically counter, kitchen, and room exhibit higher speedups. This is because these scenes contain fewer Gaussians but exhibit more intersections with tiles, and the naive pipeline elim- inates replicated computations by performing the entire sorting at the beginning. However, memory-bound issues significantly de- grade PE utilization. Thanks to our fine-grained interleaved pipeline, the speedup nearly reaches 2000 . Although the fine-grained inter- leaved pipeline incurs some replicated computation for Gaussians across tiles, the small size of the MLP and high PE utilization out- weigh this effect, rendering the sorting latency negligible. Overall comparison: Our rasterization and sorting optimizations are combined for further analysis. To simulate real applications, we also report the end-to-end performance by using the edge GPU to execute the Gaussian projection, from which we extract latency and energy consumption information and incorporate it into our design. Fig.\n\n--- Segment 39 ---\nTo simulate real applications, we also report the end-to-end performance by using the edge GPU to execute the Gaussian projection, from which we extract latency and energy consumption information and incorporate it into our design. Fig. 17 (left) shows that our sorting and rasterization optimizations collectively achieve a speedup of 23.4 27.8 over the GPU imple- mentation, while the end-to-end speedup is reduced to 7.8 11.5 , 10 Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization 18.2 16.9 18.4 17.8 17.0 20.4 0 50 100 150 200 250 300 350 400 0 5 10 15 20 25 Garden Bicycle Counter Kitchen Room Stump FPS Ras. Speedup Our Rasterization FPS 139 141 525 340 385 102 1,972 1,687 1,915 1,806 1,885 2,416 1E 0 1E 1 1E 2 1E 3 1E 4 Garden Bicycle Counter Kitchen Room Stump Naive Neural Sort. Speedup Pipeline Interleaved Sort. Speedup Figure 16: Speedup of rasterization and sorting over GPU. 34.9 36.6 51.4 42.4 44.5 28.8 5.1 6.0 8.8 8.3 7.4 4.1 1E 0 1E 1 1E 2 Garden Bicycle Counter Kitchen Room Stump Ras. Sort. Energy Saving End-to-end Energy Saving 25.8 23.4 25.8 24.7 24.3 27.8 8.5 8.3 11.5 11.1 10.2 7.8 0 20 40 60 80 100 0 5 10 15 20 25 30 Garden Bicycle Counter Kitchen Room Stump FPS Ras. Sort. Speedup End-to-end Speedup End-to-end FPS 50 Figure 17: Overall speedup and energy saving over GPU. with the Gaussian projection being the major source of latency. The secondary axis indicates our end-to-end throughput, with our design consistently achieving over 50 FPS, thereby meeting the real-time rendering requirements on edge devices. Fig. 17 (right) illustrates the energy savings, with our combined rasterization and sorting optimizations achieving between 28.8 and 51.4 energy savings.\n\n--- Segment 40 ---\nFig. 17 (right) illustrates the energy savings, with our combined rasterization and sorting optimizations achieving between 28.8 and 51.4 energy savings. These savings result from our dedicated architecture, which features low on-chip power consumption, uniformly supporting efficient neural sorting and rasterization, and from the cache with a ğœ‹-trajectory tile scheduling scheme that exploits spatial locality to enhance data reuse, thereby further reducing off-chip power consumption. Even with the energy overhead from the Gaussian projection which dominates overall energy consumption, the end- to-end savings still amount to between 4.1 and 8.8 . 0 0.5 1 1.5 2 2.5 3 Garden Bicycle Counter Kitchen Room Stump Area Efficiency Gain Over GSCore 0.0 0.4 0.8 1.2 1.6 2.0 2.4 Garden Bicycle Counter Kitchen Room Stump Energy Efficiency Over GSCore Figure 18: Comparison with SOTA accelerator [22]. Comparison with SOTA Accelerators. We select GSCore [22] for comparison because it also focuses on the original 3DGS infer- ence and accelerates the sorting and rasterization through dedi- cated hardware design, while others focus on specific applications [12, 23, 36]. Based on the details provided in [22], we implement a cycle-accurate simulator for GSCore to estimate performance. GScore is also designed using a 28 nm process and operates at 1GHz. We extract the power and area metrics for GSCore s sort- ing and rasterization modules for comparison. Since GScore has a different scale from our design, for fairness we evaluate area effi- ciency defined as the throughput of sorting rasterization divided by area, and energy efficiency, defined as energy consumption of sorting and rasterization per rendered image1. As shown in Fig. 18, our design achieves an area efficiency improvement of 1.94 2.39 . On the one hand, our rasterization effectively reduces the MAC count to only 6 per PE nearly a 50 reduction; on the other hand, the sorting and rasterization functions are uniformly supported via the reconfiguration of the PE array, incurring the cost of a single engine. For energy efficiency, our design achieves an improvement of 1.36 1.89 . Firstly, our design nearly eliminates the sorting latency, which is significant in GSCore.\n\n--- Segment 41 ---\nFor energy efficiency, our design achieves an improvement of 1.36 1.89 . Firstly, our design nearly eliminates the sorting latency, which is significant in GSCore. Secondly, the unified engine reduces both static and dynamic power consumption. Thirdly, our ğœ‹- trajectory tile scheduling, combined with an efficient cache design, effectively reduces the substantial energy consumption associated with off-chip access. 7 Related Work Efficient 3D Gaussian Splatting algorithm. Many studies have been proposed to accelerate rendering or reduce the mem- ory overhead in 3DGS by pruning the number of Gaussians [7 10, 29, 40], while accelerating the sorting process remains a rela- tively unexplored direction. Recently, [16] propose several simple, monotonically decreasing functions of depth to achieve nearly sort- free rendering; however, these fixed-form functions are inadequate for representing the wide range of real-world scenes. To overcome this, our neural sorting algorithm employs an expressive, tiny MLP, optimized by our training framework to enhance rendering quality, without compromising speed due to the co-designed hardware. 3D Gaussian Splatting accelerators. To address the real-time rendering requirements on edge devices, several specialized hard- ware accelerators have been proposed. GScore [22] streamlines the Gaussian splatting pipeline by reducing the number of Gaussians assigned to tiles via a shape-aware intersection test, thereby ef- fectively accelerating both sorting and rasterization. Recently, the Gaussian Blending Unit [39] proposed an edge GPU plug-in module for rasterization that renders each row of pixels sequentially from left to right, by leveraging intermediate values shared between adjacent pixels. This approach significantly reduces computational cost; however, it sacrifices parallelism because the row dimension is processed sequentially. Several 3DGS accelerators are designed to target segmented regions. For example, MetaSapiens [23] accel- erates foveated rendering using efficiency-aware pruning, GauSPU [36] proposes a co-processor for SLAM, and GsArch [12] accelerates the training process by enhancing memory access efficiency. Our approach diverges from these methods in two key aspects.\n\n--- Segment 42 ---\nFor example, MetaSapiens [23] accel- erates foveated rendering using efficiency-aware pruning, GauSPU [36] proposes a co-processor for SLAM, and GsArch [12] accelerates the training process by enhancing memory access efficiency. Our approach diverges from these methods in two key aspects. First, we uniquely identify the inherent redundancy in the rasterization step and propose an axis-oriented computation flow, supported by a highly parallel rasterization array, which effectively reduces the MAC count without compromising generality. Second, we focus on optimizing the sorting process through our algorithm-hardware co-design, wherein our reconfigurable array with an interleaved pipeline dramatically reduces sorting latency to a negligible level. 1Since GSCore proposes the shape-aware intersection test, and this technique is com- patible with our design, to guarantee fairness we assume that neither design employs this technique. 11 8 Conclusion This work proposes an architecture-algorithm co-design ap- proach to overcome the key challenges of enabling real-time 3D Gaussian Splatting on edge devices. By introducing axis-oriented rasterization, we effectively reduce redundant MAC operations by effectively reusing the common terms expressions. Furthermore, a tiny neural network is employed to replace the conventional sorting process, mitigating pipeline bottlenecks and resource overheads. To support both rasterization and inference efficiently, we develop a reconfigurable PE array, together with an interleaved pipeline and ğœ‹-trajectory tile schedule to enhance PE utilization and mem- ory efficiency. Experimental results demonstrate that our approach achieves up to 27.8 speedup and 51.4 energy savings on real- world scenes with minimal quality loss. We plan to open-source our design to foster further development in this field. References [1] ADLINK Technology Inc. 2022. NVIDIA Jetson Xavier NX-based AI Vision System. vision-system [2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. 2022. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE CVF conference on computer vision and pattern recognition. 5470 5479. [3] Loren Carpenter. 1984. The A-buffer, an antialiased hidden surface method.\n\n--- Segment 43 ---\n1984. The A-buffer, an antialiased hidden surface method. In Proceedings of the 11th annual conference on Computer graphics and interactive techniques. 103 108. [4] Karthik Chandrasekar, Christian Weis, Yonghui Li, Benny Akesson, Norbert Wehn, and Kees Goossens. 2012. DRAMPower: Open-source DRAM power energy estimation tool. URL: drampower. info 22 (2012). [5] Eric Enderton, Erik Sintorn, Peter Shirley, and David Luebke. 2010. Stochastic transparency. In Proceedings of the 2010 ACM SIGGRAPH symposium on Interactive 3D Graphics and Games. 157 164. [6] Cass Everitt. 2001. Interactive order-independent transparency. White paper, nVIDIA 2, 6 (2001), 7. [7] Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, Zhangyang Wang, et al. 2024. Lightgaussian: Unbounded 3d gaussian compression with 15x reduc- tion and 200 fps. Advances in neural information processing systems 37 (2024), 140138 140158. [8] Guangchi Fang and Bing Wang. 2024. Mini-splatting: Representing scenes with a constrained number of gaussians. In European Conference on Computer Vision. Springer, 165 181. [9] Guangchi Fang and Bing Wang. 2024. Mini-Splatting2: Building 360 Scenes within Minutes via Aggressive Gaussian Densification. arXiv preprint arXiv:2411.12788 (2024). [10] Sharath Girish, Kamal Gupta, and Abhinav Shrivastava. 2024. Eagles: Efficient accelerated 3d gaussians with lightweight encodings. In European Conference on Computer Vision. Springer, 54 71. [11] Frank Gray. 1953. Pulse code communication. United States Patent Number 2632058 (1953). [12] Houshu He, Gang Li, Fangxin Liu, Li Jiang, Xiaoyao Liang, and Zhuoran Song. 2025. GSArch: Breaking Memory Barriers in 3D Gaussian Splatting Training via Architectural Support. In Proceedings of the 2025 IEEE International Symposium on High Performance Computer Architecture (HPCA).\n\n--- Segment 44 ---\nGSArch: Breaking Memory Barriers in 3D Gaussian Splatting Training via Architectural Support. In Proceedings of the 2025 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE. [13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision. 1026 1034. [14] David Hilbert and David Hilbert. 1935. Ãœber die stetige Abbildung einer Linie auf ein FlÃ¤chenstÃ¼ck. Dritter Band: Analysis Grundlagen der Mathematik Physik Verschiedenes: Nebst Einer Lebensgeschichte (1935), 1 2. [15] Charles Antony Richard Hoare. 1961. Algorithm 64: quicksort. Commun. ACM 4, 7 (1961), 321. [16] Qiqi Hou, Randall Rauwendaal, Zifeng Li, Hoang Le, Farzad Farhadzadeh, Fatih Porikli, Alexei Bourd, and Amir Said. 2024. Sort-free Gaussian Splatting via Weighted Sum Rendering. arXiv preprint arXiv:2410.18931 (2024). [17] Mihai F Ionescu and Klaus E Schauser. 1997. Optimizing parallel bitonic sort. In Proceedings 11th International Parallel Processing Symposium. IEEE, 303 309. [18] Bernhard Kerbl, Georgios Kopanas, Thomas LeimkÃ¼hler, and George Drettakis. 2023. 3D Gaussian splatting for real-time radiance field rendering. ACM Trans. Graph. 42, 4 (2023), 139 1. [19] Mustafa Khan, Hamidreza Fazlali, Dhruv Sharma, Tongtong Cao, Dongfeng Bai, Yuan Ren, and Bingbing Liu. 2024. Autosplat: Constrained gaussian splatting for autonomous driving scene reconstruction. arXiv preprint arXiv:2407.02598 (2024). [20] Yoongu Kim, Weikun Yang, and Onur Mutlu. 2015.\n\n--- Segment 45 ---\n[20] Yoongu Kim, Weikun Yang, and Onur Mutlu. 2015. Ramulator: A fast and extensible DRAM simulator. IEEE Computer architecture letters 15, 1 (2015), 45 49. [21] Siddharth Krishna Kumar. 2017. On weight initialization in deep neural networks. arXiv preprint arXiv:1704.08863 (2017). [22] Junseo Lee, Seokwon Lee, Jungi Lee, Junyong Park, and Jaewoong Sim. 2024. Gscore: Efficient radiance field rendering via architectural support for 3d gaussian splatting. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3. 497 511. [23] Weikai Lin, Yu Feng, and Yuhao Zhu. 2025. MetaSapiens: Real-Time Neural Rendering with Efficiency-Aware Pruning and Accelerated Foveated Rendering. In Proceedings of the 30th ACM International Conference on Architectural Sup- port for Programming Languages and Operating Systems, Volume 1 (Rotterdam, Netherlands) (ASPLOS 25). Association for Computing Machinery, New York, NY, USA, 669 682. [24] Morgan McGuire and Louis Bavoil. 2013. Weighted blended order-independent transparency. Journal of Computer Graphics Techniques 2, 4 (2013). [25] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. 2021. Nerf: Representing scenes as neural radiance fields for view synthesis. Commun. ACM 65, 1 (2021), 99 106. [26] Guy M Morton. 1966. A computer oriented geodetic data base and a new tech- nique in file sequencing. (1966). [27] Cedrick MÃ¼nstermann, Stefan Krumpen, Reinhard Klein, and Christoph Peters. 2018. Moment-based order-independent transparency. Proceedings of the ACM on Computer Graphics and Interactive Techniques 1, 1 (2018), 1 20. [28] Naveen Muralimanohar, Rajeev Balasubramonian, and Norman P Jouppi. 2009. CACTI 6.0: A tool to model large caches.\n\n--- Segment 46 ---\n2009. CACTI 6.0: A tool to model large caches. HP laboratories 27 (2009), 28. [29] Michael Niemeyer, Fabian Manhardt, Marie-Julie Rakotosaona, Michael Oechsle, Daniel Duckworth, Rama Gosula, Keisuke Tateno, John Bates, Dominik Kaeser, and Federico Tombari. 2024. Radsplat: Radiance field-informed gaussian splatting for robust real-time rendering with 900 fps. arXiv preprint arXiv:2403.13806 (2024). [30] Thomas Porter and Tom Duff. 1984. Compositing digital images. In Proceedings of the 11th annual conference on Computer graphics and interactive techniques. 253 259. [31] David P Rodgers. 1985. Improvements in multiprocessor system design. ACM SIGARCH Computer Architecture News 13, 3 (1985), 225 231. [32] Alvy Ray Smith. 1995. Alpha and the history of digital compositing. Technical Report. Citeseer. [33] Synopsys, Inc. [n. d.]. DesignWare Library. designware-ip soc-infrastructure-ip designware-library.html. [34] Xuechang Tu, Bernhard Kerbl, and Fernando de la Torre. 2024. Fast and robust 3D Gaussian splatting for virtual reality. In SIGGRAPH Asia 2024 Posters. 1 3. [35] Samuel Williams, Andrew Waterman, and David Patterson. 2009. Roofline: an insightful visual performance model for multicore architectures. Commun. ACM 52, 4 (2009), 65 76. [36] Lizhou Wu, Haozhe Zhu, Siqi He, Jiapei Zheng, Chixiao Chen, and Xiaoyang Zeng. 2024. GauSPU: 3D Gaussian Splatting Processor for Real-Time SLAM Systems. In 2024 57th IEEE ACM International Symposium on Microarchitecture (MICRO). IEEE, 1562 1573. [37] Jin Xu, Zishan Li, Bowen Du, Miaomiao Zhang, and Jing Liu. 2020. Reluplex made more practical: Leaky ReLU. In 2020 IEEE Symposium on Computers and communications (ISCC). IEEE, 1 7.\n\n--- Segment 47 ---\nIn 2020 IEEE Symposium on Computers and communications (ISCC). IEEE, 1 7. [38] Vickie Ye, Ruilong Li, Justin Kerr, Matias Turkulainen, Brent Yi, Zhuoyang Pan, Otto Seiskari, Jianbo Ye, Jeffrey Hu, Matthew Tancik, et al. 2025. gsplat: An open-source library for Gaussian splatting. Journal of Machine Learning Research 26, 34 (2025), 1 17. [39] Zhifan Ye, Yonggan Fu, Jingqun Zhang, Leshu Li, Yongan Zhang, Sixu Li, Cheng Wan, Chenxi Wan, Chaojian Li, Sreemanth Prathipati, and Yingyan (Celine) Lin. 2025. Gaussian Blending Unit: An Edge GPU Plug-in for Real-Time Gaussian- Based Rendering in AR VR. In Proceedings of the 2025 IEEE International Sympo- sium on High Performance Computer Architecture (HPCA). IEEE. [40] Zhifan Ye, Chenxi Wan, Chaojian Li, Jihoon Hong, Sixu Li, Leshu Li, Yongan Zhang, and Yingyan Celine Lin. 2024. 3D Gaussian Rendering Can Be Sparser: Ef- ficient Rendering via Learned Fragment Pruning. Advances in Neural Information Processing Systems 37 (2024), 5850 5869. [41] Hongjia Zhai, Xiyu Zhang, Boming Zhao, Hai Li, Yijia He, Zhaopeng Cui, Hujun Bao, and Guofeng Zhang. 2025. Splatloc: 3D Gaussian splatting-based visual lo- calization for augmented reality. IEEE Transactions on Visualization and Computer Graphics (2025). [42] Siting Zhu, Guangming Wang, Xin Kong, Dezhi Kong, and Hesheng Wang. 2024. 3D Gaussian splatting in robotics: A survey. arXiv preprint arXiv:2410.12262 (2024). 12\n\n