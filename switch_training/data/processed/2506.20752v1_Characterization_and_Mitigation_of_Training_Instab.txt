=== ORIGINAL PDF: 2506.20752v1_Characterization_and_Mitigation_of_Training_Instab.pdf ===\n\nRaw text length: 82617 characters\nCleaned text length: 82048 characters\nNumber of segments: 64\n\n=== CLEANED TEXT ===\n\narXiv:2506.20752v1 [cs.LG] 25 Jun 2025 Characterization and Mitigation of Training Instabilities in Microscaling Formats Chloe Huangyuan Su 1,2 Mujin Kwun1 Stephanie Gil2 Sham Kakade1,2 Nikhil Anand 1 1Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University 2Department of Computer Science, Harvard University Abstract Training large language models is an expensive, compute-bound process that must be repeated as models scale, algorithms improve, and new data is collected. To address this, next-generation hardware accelerators increasingly support lower- precision arithmetic formats, such as the Microscaling (MX) formats introduced in NVIDIA s Blackwell architecture. These formats use a shared scale within blocks of parameters to extend representable range and perform forward backward GEMM operations in reduced precision for efficiency gains. In this work, we investigate the challenges and viability of block-scaled precision formats during model training. Across nearly one thousand language models trained from scratch spanning com- pute budgets from 2 1017 to 4.8 1019 FLOPs and sweeping over a broad range of weight activation precision combinations we consistently observe that training in MX formats exhibits sharp, stochastic instabilities in the loss, particularly at larger compute scales. To explain this phenomenon, we conduct controlled exper- iments and ablations on a smaller proxy model that exhibits similar behavior as the language model, sweeping across architectural settings, hyperparameters, and precision formats. These experiments motivate a simple model in which multiplica- tive gradient bias introduced by the quantization of layer-norm affine parameters and a small fraction of activations can trigger runaway divergence. Through in situ intervention experiments on our proxy model, we demonstrate that instabilities can be averted or delayed by modifying precision schemes mid-training. Guided by these findings, we evaluate stabilization strategies in the LLM setting and show that certain hybrid configurations recover performance competitive with full-precision training. We release our code at 1 Introduction Large language models (LLMs) have dramatically improved in capabilities in recent years, largely driven by scaling their capacity and the quantity of training data (Kaplan et al., 2020; OpenAI, 2025; DeepMind, 2025; Anthropic, 2025; Grattafiori et al., 2024). For instance, training the Llama 3.1 405B model required more than 1025 FLOPs and utilized up to 16,000 H100 GPUs (Grattafiori et al., 2024). Scaling these models involves not only the initial, compute-intensive pretraining phase but also frequent retraining as new data becomes available, algorithms evolve, or architectural modifications are introduced, as well as post-training protocols that prepare the model for inference deployment. Pretraining itself also adds the additional challenge of carefully tuned hyperparameter settings, such Equal contribution; correspondence: Preprint. Under review. as learning rate schedules that decay toward zero, making it difficult to resume a completed run without resetting the optimization state. As a result, even small updates such as incorporating new data or minor architectural changes often require restarting training from early checkpoints, further compounding compute demands (Ibrahim et al., 2024). To reduce these computational burdens, recent hardware advancements have introduced native support for lower-precision computations, such as FP8 training in NVIDIA H100 GPUs (Micikevicius et al., 2022b). Upcoming hardware accelerators powered by NVIDIA s Blackwell architecture will further extend these capabilities with standardized, shared-scale Microscaling (MX) formats like MXFP8 and MXFP6 (NVIDIA, 2025). These formats store a per-block shared scale, which expands the effective dynamic range with minimal memory overhead, while simultaneously enabling GEMMs at lower precision (Rouhani et al., 2023; Darvish Rouhani et al., 2023b). While model pretraining is typically done in 16 or 32-bit precision, some quantization schemes are already seeing industry adoption; for example, DeepSeek-V3 employs tile-wise FP8 quantization within large tensors (Liu et al., 2024), while Cohere s Command A model was trained in FP8 while reserving higher-precision operations for, e.g., activation functions and attention mechanisms (Cohere et al., 2025). At an even larger scale, the Llama-4 series of models is reported to have been pretrained in FP8 precision across nearly 32,000 GPUs (Meta, 2025). On the deployment side, workflows like quantization-aware training (QAT) and mixed-precision fine-tuning further underscore that understanding low-precision training dynamics is essential throughout a model s lifecycle (Jacob et al., 2017; Abdolrashidi et al., 2021; Shao et al., 2024). There are two primary challenges that accompany the adoption of low-precision formats for training. First, there is a potential performance tradeoff, where reducing precision may result in degradation or "plateauing" of training loss and downstream accuracy, which can be characterized through scaling laws that account for both compute and precision (Kumar et al., 2024). Second, instabilities may arise in training, often manifesting as abrupt spikes in the loss curve that disrupt convergence (Fishman et al., 2024; Lee et al., 2025). When these instabilities push optimization into regions from which recovery is impossible, they obstruct our ability to extract valid scaling laws, making it impossible to even assess the tradeoffs introduced by low-precision training. In this work, we set out to understand the training dynamics of low-precision MX precision formats and extract valid scaling laws to identify format prescriptions for language model training on next- generation hardware. However, consistent with prior observations by Fishman et al. (2024); Lee et al. (2025), we found that training frequently became unstable particularly for larger, compute-intensive models. In contrast to earlier work, these instabilities appeared more pervasive, emerging across a broad range of activation functions, model scales, MX formats, and hyperparameter settings. Because large-scale language model sweeps are computationally intensive and involve many entangled components, we turn to a controlled synthetic setting to better understand the origin of these failures. Specifically, we employ a residual multi-layer perceptron (MLP) student-teacher model trained on random (Gaussian) inputs, which enables fine-grained ablations over architecture and optimization. This proxy model allows us to isolate the effects of low-precision arithmetic and identify conditions under which training becomes unstable. In this setting, we observe two qualitatively distinct modes of instability. The first is expected" and stems from stochastic optimization dynamics: for instance, aggressive learning rates can amplify small incorrect gradient updates, resulting in loss spikes and divergence. The second, perhaps more interesting, mode is directly induced by low-precision arithmetic: even under stable hyperparameters, quantization introduces systematic gradient bias that can accumulate and destabilize training. To better understand the second failure mode, we perform ablations across format configurations, quantization schemes (e.g., forward-only vs. full quantization), and activation functions, and ana- lyze their effects on stability. Our results support a phenomenological explanation: low-precision instabilities primarily arise from systematic bias in gradient estimates introduced by quantization. We find that a key driver of this bias is the quantization of the layer normalization affine parameters, whose values often become tightly clustered over the course of training. When the values within a block converge too closely, division by the shared block scale can clamp all values in that block to the largest representable number, destabilizing training. We verify that this mechanism is not limited to synthetic settings but also emerges in language model setting. 2 With these insights, we propose two fixes to mitigate the instabilities in the language model setting, which involve keeping the MX activations in higher precision or only including quantization in the forward pass. Applying these fixes, we then train another set of language models in both MXFP8 precision schemes (E4M3, E5M2) and fit valid empirical scaling laws in both cases. Main contributions: MX sweeps: We perform extensive language model pretraining sweeps across weight and activation MX precision formats, and consistently observe training instabilities, particularly in larger models that are trained for longer. While prior work has reported instabilities in low-precision training (Fishman et al., 2024; Lee et al., 2025), we find them in shared- scale (MX) formats and occurring across a wide range of quantization configurations and activation functions. Mechanistic analysis: Through a suite of ablations in a simplified student teacher MLP model, we isolate two modes of instability: one arising from stochastic optimization dy- namics, and another induced by quantization noise. For the second case we show that the gradient becomes systematically biased, derive a norm-based condition that predicts when this bias will dominate, and trace its origin to the data being quantized: most of the layer-norm affine weights (and, to a lesser extent, roughly 1 of activations) are clustered such that MX block-scaling forces them into the same quantization bin. Effective mitigations: We evaluate two stabilization strategies in full-scale language model training: (1) disabling backward-pass quantization and (2) retaining higher-precision activations. Both approaches enable the recovery of valid empirical scaling laws. Notably, we find that MXFP8 weights (E4M3) paired with BF16 activations can match the performance of full bfloat16 baselines, at least up to the model scales we explored. Our work provides both a diagnostic methodology and empirical reference points for evaluating these nascent low-precision training formats. Broader relevance: While our experiments focus on models trained from scratch, the gradient-bias phenomena we study are intrinsic to block-scaled quantization and may also inform post-training workflows (e.g. QAT) or other low-precision scenarios. 2 Related Work Low-Precision Instabilities Training large Transformer models at scale can reveal instabilities that can disrupt or even halt learning (Liu et al., 2024; Chowdhery et al., 2022; Dehghani et al., 2023; Zhang et al., 2022; Molybog et al., 2023; Fishman et al., 2024; Zoph et al., 2022; Ma et al., 2025). In many cases, these issues are exacerbated or directly triggered by low-precision quantization. For example, Fishman et al. (2024) demonstrate that FP8 pretraining becomes unstable when combined with the SwiGLU activation function, attributing the issue to an outlier amplification effect that worsens due to progressive weight alignment over the course of training. Similarly, Lee et al. (2025) report that approximately 10 of FP8 runs using the NanoGPT codebase fail to converge, whereas full-precision (BF16) training exhibits no such failures. Other works (Sun et al., 2024; Bondarenko et al., 2023; Xu et al., 2023), point to activation outliers and gradient norm growth as contributors to these failures while Tseng et al. (2025) proposes a stochastic rounding based algorithm to stabilize training in MXFP4 formats. Meanwhile, DeepSeek-V3 also attributes certain training failures due to blockwise quantization of activation gradients (Liu et al., 2024), underscoring the breadth of challenges introduced by quantization schemes. Wortsman et al. (2024) use small-scale proxy models to study training instabilities in the context of growth of output and layer logits. We adopt a similar approach, and use a simplified student-teacher proxy model that replicates low-precision instabilities in LLMs and introduce a phenomenological framework to distinguish between stochastic and quantization-induced failure modes. Precision Scaling Laws In parallel, several works have examined how model performance scales under precision constraints. This includes both quantization-aware training (QAT), which injects quan- tization noise during the forward pass to prepare models for low-bit inference, and full low-precision training, which applies reduced-precision arithmetic in both passes to accelerate training (Jacob et al., 2017; Abdolrashidi et al., 2021; Shao et al., 2024; Chen et al., 2025). 3 Kumar et al. (2024) compare full-precision, QAT, and low-precision training, finding that overtrained models are especially sensitive to quantization and that even 16-bit training may not be optimal. However, their setup restricts quantization to the forward pass to allow for a fair comparison to QAT methods and does not analyze instability dynamics. We evaluate full MX quantization including both forward and backward passes and propose stabilization techniques that recover valid scaling laws in those lower-precision MX formats. Liu et al. (2025) explore QAT scaling laws in low-bit regimes, finding that fine-tuning outperforms both post-training quantization and QAT from scratch, even down to binary and ternary formats. Ouyang et al. (2024) examine how quantization-induced degradation (QiD) varies across training scales, while Dettmers and Zettlemoyer (2023) study inference-time scaling laws, concluding that 4- and 6-bit models often lie on the Pareto frontier of accuracy and efficiency. 2.1 Review of MX Formats and Experimental Approach Microscaling (MX) formats are a class of low-precision numerical representations designed to enhance the efficiency of deep learning models (Darvish Rouhani et al., 2023a; Rouhani et al., 2023). The idea is conceptually simple as described in Algorithm 1: we represent a block of k values, {Vi}k i 1, using a single shared scale factor X and k corresponding low-precision elements {Pi} where the Pi are obtained by casting Vi X to the specified low-precision format2. The scale X can be calculated using X 2 log2(maxi( Vi )) emax elem with emax elem being the exponent of the largest normal number representable in the chosen element data format. Common element types include 8-bit floating point (FP8) such as E4M3 and E5M2, and 6-bit (FP6) formats like E2M3 and E3M2, typically utilizing an 8-bit exponent (E8M0) for the shared scale. In our experiments, we quantize both weights and activations using these MX formats using the MX Pytorch Emulation Library (Microsoft, 2024). This quantization is applied dynamically to the inputs of matrix multiplication operations (e.g., within Linear, MatMul, BMM layers) across both the forward and backward passes, with results dequantized to a higher precision format (e.g., bfloat16) after the operation. We mainly explore various MX configurations, including the aforementioned 6-bit and 8-bit element formats, and scenarios where MX-formatted tensors are mixed with bfloat16 tensors. We defer a more detailed review of the MX scheme to Appendix A. 3 LLM Experiments 3.1 Setup For our language model experiments, we use OLMo (Groeneveld et al., 2024) in combination with the MX PyTorch Emulation Library (Microsoft, 2024) to enable training under various low-precision configurations. All language models use the GeLU activation function; full hyperparameter details are provided in Table 3. We sweep over a wide range of MX precision formats for both weights and activations, including two FP6 variants (E3M2, E2M3), two FP8 variants (E4M3, E5M2), and a bfloat16 baseline. Each configuration applies full quantization to both forward and backward passes to both weights and activations, as implemented in the Microscaling library (Microsoft, 2024). For each format, we train approximately 70 models3 spanning compute budgets from 2 1017 to 4 1019 FLOPs. Model sizes range from 20M to 1.7B parameters. Token counts are determined using an adapted version of the FLOP accounting code from Brandfonbrener et al. (2024), originally developed for OLMo scaling law experiments. Token-to-parameter ratios in our sweep range from approximately 2 to 156. All models are trained on the Fineweb-Edu dataset Penedo et al. (2024), with the longest runs trained on 35B tokens and the shortest runs corresponding to models trained on 301M tokens. 2We present results for a block size k 32 to match what will be hardware supported, but we also experimented with different choies of k and did not observe qualitatively different results than those presented. 3Some runs crashed and could not always be resumed, leading to small differences in number of models trained for each format. 4 0 20000 40000 60000 80000 Step 1.5 2.0 2.5 3.0 3.5 4.0 4.5 Loss Train CrossEntropyLoss 0 20000 40000 60000 80000 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 25 50 75 100 125 150 175 Ratio bfloat16 weights, bfloat16 activations (a) Train loss and grad norm for weights and activations in bfloat16-bfloat16 format. All runs converge. 0 10000 20000 30000 40000 50000 60000 Step 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Loss Train CrossEntropyLoss 0 10000 20000 30000 40000 50000 60000 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 25 50 75 100 125 150 175 Ratio MXFP8 (E5M2 weights, E5M2 activations) (b) Train loss and grad norm for weights and activations in MXFP8 E5M2-E5M2. Some runs particularly larger models that are trained for longer become unstable and never recover. Figure 1: Shows stable (bfloat16) OLMo training runs (top) compared to lower precision (MXFP8 E5M2, bottom). The low precision computations are done in both forward and backward steps, on both weights and activations. Color bar on the right shows the token-to-parameter ratio. 3.2 Instabilities in Low Precision Figure 1a shows the training loss and gradient norm trajectories for bfloat16 models. Training remains stable, with smooth convergence and bounded gradients. By contrast, Figure 1b illustrates example instabilities in the MXFP8 E5M2-E5M2 weights-activations configuration, where some training runs exhibit sharp upward spikes in loss and large increases in gradient norm magnitude. We find these instabilities to be universal across other low-precision MX configurations and hyperparameter settings, as documented in Appendix F. We observe the instabilities mainly occur in larger, longer-trained models and that importantly, when training is destabilized, training does not recover, and the loss continues to diverge. While the loss spikes appear abruptly, the gradient norm typically grows more gradually (bottom right of Figure 1b) and fails to decrease over time as seen in stable bfloat16 training. This behavior strongly suggests systematic errors in gradient computation, a point that we will investigate further in subsequent sections. 4 Synthetic Experiments 4.1 Setup Our language model experiments with OLMo involve many potentially interacting components, and it is challenging to determine exactly where the low-precision failure mode occurs. To better understand the origins of the instabilities, following the logic of Wortsman et al. (2024), we develop a small-scale proxy model. Despite its simplicity, this model exhibits many of the same qualitative behaviors seen in large-scale training. 5 Given an input x Rdmodel, we consider a student network composed of L residual layers indexed by k 0, . . . , L 1. The hidden state at each layer is computed as: A0 x hk W(1) k LN(Ak 1) Ak 0 Ak 1 W(2) k ϕ(hk) , (1) where LN denotes layer normalization and ϕ is the activation function (e.g., ReLU, GeLU, SwiGLU). Each residual block contains two weight matrices: W(1) k projects to the hidden dimension, and W(2) k projects back to dmodel. By default, the hidden size is set to 4dmodel4) The targets are generated by a fixed teacher model whose architecture can be taken to be the same as the student s without the layer normalization (for sweeps where we change the depth and width of the student, we similarly scale the teacher model). A small Gaussian label noise (σ 10 3) is added to the outputs. The inputs x are drawn i.i.d. from a standard Gaussian, without cycling, using a fixed seed to ensure consistent batch order. To isolate the effect of precision, we train two copies of the student model from the same initialization. The first is trained in full precision (FP32). After training, the weights are reset to their initial state and retrained using a low-precision MX format, with quantization applied to both forward and backward passes as described in Section 2.1. Because the initialization, data, and batch order are identical, any behavioral difference is attributable primarily to the change in numerical precision5. All models are trained with an MSE loss. Hyperparameter choices A key point is that there are regions in hyperparameter space for which the model in Equation (1) will give rise to train instabilities (even in FP32 precision). This is not necessarily due to the precision scheme chosen, but rather due to the fact that in any SGD method there exists some small probability of taking wrong gradient step(s). If the size of the steps are large due to, e.g., a high learning rate, this will be visible as a sudden spike in the loss. Our goal is to choose hyperparameters and make architectural choices (such as depth and width) in order to move away from these expected" instabilities and to ones that could be caused by low precision. For the same reason, we fix a moderately large batch size (2048) throughout to reduce variance in gradient estimates. 4.2 Sweeping over learning rates and architectures Learning rates We begin by sweeping over learning rates η (1 10 5, 5 10 5, 1 10 4, 5 10 4, 1 10 3) across a range of model depths and widths, in two low precision formats: (1) MXFP8 E4M3 in the forward pass and MXFP8 E5M2 in the backward pass6, and (2) MXFP6 E4M3 in both forward and backward passes. Results from this sweep are shown in Figure 2. We observe the following patterns: for low learning rates η 1 10 4, all precision formats remain stable. At η 5 10 4, differences between FP32 and lower-precision formats begin to emerge: FP32 exhibits two unstable runs, while FP8 shows six. At the highest learning rate (η 1 10 3), instabilities are observed across all formats, with larger models failing earlier in training. Interestingly, we find that recovery from an instability is more rapid in FP32, whereas instability in lower-precision formats particularly FP6 is often more persistent. We also experimented with a cosine learning rate schedule that starts at 1 10 3 and decays to 1 10 5 and found that the effect of the schedule was mainly to suppress instabilities at later training times, though we still observe the same differences between high and low precision if the instability does not happen late in training. 4In the case of SwiGLU, following Shazeer (2020) we reduce the hidden dimension from 4dmodel to 8 3dmodel to maintain parity in parameter count. 5To qualify this statement, we observe that some minor sources non-determinism remain even after controlling for random seed, batch order, model initialization, using deterministic Pytorch kernels, etc. but these effects are small compared to the typical precision scales we work with. 6We use this asymmetric format to allow greater dynamic range in the backward pass, following Micikevicius et al. (2022a), and because it exhibited marginally greater stability than using E4M3 for both passes. Our results are not sensitive to this particular choice of low-precision formats. 6 10 3 10 2 10 1 100 101 FP32 MSE loss 1 10 5 5 10 5 1 10 4 5 10 4 1 10 3 10 3 10 2 10 1 100 101 MX FP8 MSE loss 0 2 4 6 10 3 10 2 10 1 100 101 MX FP6 E2M3 MSE loss 0 2 4 6 0 2 4 6 0 2 4 6 0 2 4 6 Training step ( 10³) L 3,W 256 L 3,W 384 L 3,W 512 L 4,W 256 L 4,W 384 L 4,W 512 L 5,W 256 L 5,W 384 L 5,W 512 L 6,W 256 L 6,W 384 L 6,W 512 L 7,W 256 L 7,W 384 L 7,W 512 L 8,W 256 L 8,W 384 L 8,W 512 Figure 2: Comparing FP32 with MXFP6 and MXFP8 formats across different choices for the learning rate. Color corresponds to model size, determined by the depth L and dmodel D on the legend. Effect of Depth and Width In Appendix B, we fix the learning rate to η 5 10 4 and conduct a broader sweep over network depth and width across three MX precision formats. While the number of instability spikes in any individual run is an O(1) integer, aggregating across all runs we observe a general trend: for this fixed learning rate, high-precision training remains stable at larger model sizes than low-precision training. We find that instability differences between high and low precision seem to occur more frequently in networks of intermediate size, for model dimensions in the range 384 dmodel 768 and depths 3 L 6. Intuitively, this makes sense since these models appear to be large enough to exhibit sensitivity to low-precision effects, yet not large enough where overall stochasticity causes generally unstable training at this learning rate. Based on this observation, we fix the learning rate to η 5 10 4 in the remainder of our experiments and zoom in on this intermediate model regime. 4.3 The Effect of Activation Functions and Layernorms Having identified a hyperparameter regime in which instabilities are more prevalent in low precision than high precision, we next ablate the choice of activation function and the inclusion of layer normalization. In Equation (1), this corresponds to varying ϕ( ) and toggling the presence of LN( ). In Figure 3a, we observe that with layer normalization enabled, both GeLU and SwiGLU activations exhibit instability in low precision, with SwiGLU being significantly more prone to divergence. This is consistent with the findings of Fishman et al. (2024), though our results show that SwiGLU also destabilizes training in high precision, suggesting that it generally increases training stochasticity at least for this architecture. We observe two instabilities in GeLU under low precision that are absent in high precision, one of which recovers quickly and the other persists. When layer normalization is removed (Figure 3b), SwiGLU becomes more stable, while GeLU shows increased variance in high-precision training but not in low precision. For ReLU, the removal of layer norm leads to noisier and sometimes unstable dynamics in high precision but not low precision. We note that the loss improves with the removal of layernorm; this is expected as the teacher network does not contain a layernorm so that student model is able to more accurately represent its outputs. At first glance, these results are perplexing they suggest that layernorm destabilizes low-precison training 7 10 3 10 2 10 1 100 101 FP32 MSE GELU RELU SWIGLU No act. function (only LN) 0 2 4 6 10 3 10 2 10 1 100 101 MX FP8 MSE 0 2 4 6 0 2 4 6 0 2 4 6 Training step ( 10³) L 3,W 256 L 3,W 384 L 3,W 512 L 4,W 256 L 4,W 384 L 4,W 512 L 5,W 256 L 5,W 384 L 5,W 512 (a) Loss curves of different activation functions with the inclusion of layernorm. 10 5 10 4 10 3 10 2 10 1 100 101 FP32 MSE GELU (No LN) RELU (No LN) SWIGLU (No LN) LINEAR (No act. function, no LN) 0 2 4 6 10 5 10 4 10 3 10 2 10 1 100 101 MX FP8 MSE 0 2 4 6 0 2 4 6 0 2 4 6 Training step ( 10³) L 3,W 256 L 3,W 384 L 3,W 512 L 4,W 256 L 4,W 384 L 4,W 512 L 5,W 256 L 5,W 384 L 5,W 512 (b) Loss curves of different activation functions without layernorm. Figure 3: Shows the comparison between full and low precision training across different activation functions, with and without layernorm. while simultaneously stabilizing high-precision training. We will return to this point in Section 6 when we explicate the subtleties of layernorms in block scaling formats. 5 Multiplicative Noise Our synthetic experiments reveal that training instabilities in low-precision settings can arise from both stochastic optimization effects and quantization-induced bias. These failures appear to result from a complex interplay between architectural choices, activation functions, layer normalization, and learning rate. One hypothesis, motivated by the growth of the gradient norm in Figure 1, is that lower precision is systematically biasing the gradient. In this section, we examine this hypothesis through a multiplicative noise model and show that it is consistent with the instability patterns seen in low-precision training. 5.1 Behavior of the Noise Let εt egt gt, (2) 8 where gt denotes the exact gradient at time step t, and egt is its low-precision counterpart. Under a multiplicative noise model, we posit that egt (1 ζt) gt, (3) where ζt is a (possibly data and parameter-dependent) noise matrix induced by quantization. Although ζt is not directly measurable (and may not even be uniquely defined e.g., due to weight permutations), we can estimate the magnitude of its effect. Specifically, the deviation vector εt satisfies εt 2 ζt op gt 2, (4) where op denotes the operator norm. In Section 5.2, we argue for a heuristic bound that ζt op must satisfy through training and how a runaway loss divergence may occur in this model. To test this model empirically, we replicate the synthetic experiment setup from Section 4. For each configuration, we fix the random seed and weight initialization, then train one model in FP32 to log the exact gradient gt at each step. We then retrain the same model under MXFP8 precision and compute the deviation εt egt gt at every step. This allows us to extract both the norm ratio εt 2 gt 2 and the cosine similarity between egt and gt. Results are shown in Figure 4. Early in training, the estimate of ζt op (as inferred from Equation (4)) gradually decreases. However, as training progresses, the estimate begins to rise. Once ζt op 2, we observe divergence in the loss. A similar trend is observed in the cosine angle between gradients: it slowly degrades over several thousand steps and eventually flatlines near zero, indicating that the low-precision gradient is no longer aligned with the true descent direction. 10 2 10 1 100 FP32 train loss 10 2 10 1 100 101 MX train loss 0 1 2 3 4 5 6 7 1.0 0.5 0.0 0.5 1.0 cos 0 1 2 3 4 5 6 7 10 1 100 101 102 t op Training step ( 10³) L 4,W 256 L 4,W 384 L 4,W 512 L 6,W 512 Figure 4: Shows the bound on the operator norm ζt op (as inferred from Equation (4)), and the cosine angle between the low precision gradient and high precision gradient. Dashed line in the lower right plot shows when the bound on ζt op is equal to 2. 5.2 A Crude Bound To understand the behavior of ζ op, consider that we have some optimum w such that wL(w ) 0. Linearizing around the minimum we have wL(wt) H(wt w ), (5) where H 2 wL is the Hessian. The equation above makes no reference to precision the only approximation we ve made is ignore terms of order (wt w )2 and higher. Defining δt wt w , we then have gt Hδt. (6) With some manipulations the GD update rule is7 δt 1 δt ηt(I ζt)Hδt (7) 7Strictly speaking, we are using the stochastic Adam update rule and not GD in our experiments, and so the resulting bound should not be regarded as rigorous. 9 and so δt 1 (I ηtH)δt ηtζtHδt. (8) We can therefore see that there is a driving term proportional to the noise ζt; if the noise operator norm is large enough, it can flip a contracting direction into an expanding one. The stability criteria is therefore that the operator I ηt(1 ζt)H has spectral radius less than one. In terms of the maximum eigenvalue of H, λmax, this means that a crude bound for stability is 1 ηtλmax ηt ζt op λmax 1. (9) Clearly, when the norm of ζt grows, the region of stable ηtλmax shrinks. However, from the edge of stability viewpoint of Cohen et al. (2021), in the absence of multiplicative noise, λmax is expected to increase until it hovers at or just above 2 η. Once the multiplicative term ζt is introduced, we may then expect that the stability region defined by Equation (9) contracts. Developing a precise theory for this regime building on the analysis of Jastrzebski et al. (2020); Damian et al. (2023); Cohen et al. (2021) is an interesting direction for future work. In the meantime, we bypass an explicit spectral calculation by estimating a lower bound on ζt op directly in our synthetic experiments through Equation (4). Empirically, we observe a pattern where the running average of this lower bound first drifts downward, later turns upward (lower right of Figure 4). When it stabilizes around 2, training instabilities tend to follow; this observation marks a strong (but not perfect) qualitative correlate of divergence. 6 What can cause the noise? Bound (9) predicts roughly when instability occurs but not why ζt op grows. Typically, instabilities in low precision happen due to over underflow or clamping issues that can bias the gradient. However, in a block scaling format such as MX, it is unclear how such gradient bias may accumulate when the shared scale explicitly puts nearly all values within a representable range. 6.1 Overflow Issues with Layernorms To understand this, we begin by examining a concrete example of MXFP8 E4M3 as specified in Darvish Rouhani et al. (2023a). The left panel of Fig. 5 plots the relative gap (xt 1 xt) xt between successive positive codes in this format, ordered from index 0 (the smallest sub-normal, 2 9) up to index 125 (448). The index stops at 125 (rather than the expected 27 1 127) because S 1111 1112 is reserved for the NaN symbol, which would otherwise correspond to a value of 480, and S 0000 0002 is the zero code, leaving 126 remaining codes (Darvish Rouhani et al., 2023a). We can note the following: 1. For a fixed exponent bin the relative gap starts at 12.5 and decays to 6.6 as the mantissa increases. 2. There is an overflow region (left of Figure 5) when the value becomes larger than the largest representable normal number (448). Typically, these values are clamped down to 448. The latter observation above means that if a block of values lies within a sufficiently small band, these values may end up in the gray overflow region of Figure 5 after dividing by the block scale. For example, from Algorithm 1, for the case of MXFP8 E4M3 which has eelem max 8 the overflow criteria for a given value v within a block with a shared scale X is v X 448 v 0.875 (abs. max within block). (10) This type of overflow region was flagged for the case of narrower MXFP4 format in Tseng et al. (2025). We show that, while MXFP8 E4M3 has a larger dynamic range, the same effect becomes consequential in practice because layernorm affine weights are tightly clustered and particularly susceptible to having all values within a block falling in this range. For example, layernorm weights typically follow log-normal distributions with scale eµ 1 and deviation σ 1, and so a block of weights might look something like [0.89740956, 0.89628334, 0.88358812, 0.88474816, 0.90372837 ... ] 10 which all end up in the overflow region of Figure 5 after dividing by X 2 log2(abs. max) eelem max 2 8. In our experiments, the impact of this effect is shown in the middle plot of Figure 5. In the synthetic case, in some cases, nearly all of the layer norm weights fall within the band required to flow into the last bucket, losing heterogeneity in nearly all blocks when they are clamped to the maximum normal value after scale division. Note that this explains, at least partially, why removing the layernorms stabilized low-precision training in Figure 3b. While a different format, like MXFP8 E5M2 may avoid this issue, the loss of precision from having only two mantissa bits is a different source of bias and still leads to training instabilities. 2 9 2 5 2 3 2 1 1 2 4 8 16 32 64 128 448 0 20 40 60 80 100 120 code index t 0 20 40 60 80 100 relative gap 12.5 6.6 overflow region 448 FP8 E4M3 relative gaps 0.0 0.2 0.4 0.6 0.8 1.0 0.00 0.25 0.50 0.75 1.00 LN overflow frac. LayerNorm Layer 0 Affine Overflow vs. Train Time synthetic LN Olmo FFN LN Olmo Attn LN 0.0 0.2 0.4 0.6 0.8 1.0 10 2 10 1 syn. MX loss 0.0 0.2 0.4 0.6 0.8 1.0 normalized training progress 2 4 OLMo loss 0.0 0.2 0.4 0.6 0.8 1.0 0.000 0.005 0.010 0.015 act. overflow frac. Activation Overflow (Avg. Across All Layers) vs. Train Time synthetic Olmo 0.0 0.2 0.4 0.6 0.8 1.0 10 2 10 1 syn. MX loss 0.0 0.2 0.4 0.6 0.8 1.0 normalized training progress 2 4 OLMo loss Figure 5: Left: relative gap (xt 1 xt) xt for successive positive FP8 E4M3 codes (sign bit stripped). Within each exponent band the gap decays from 12.5 to 6.6 ; the hatched region marks values that would be clamped once the scaled magnitude exceeds the representable limit of 448. Center: Top subplot shows what fraction of layernorm affine parameters end up in the last quantization bin after division of the shared scale in the first layer of the network. For OLMo, we look at the FFN layernorm and the attention layernorm. The synthetic loss in this case exhibits a divergence in MX precision (but is stable in FP32 precision), and corresponds to the student-teacher setup of Equation (1) with four layers and dmodel 512 and η 6 10 4. Right: Shows the fraction of activation values that end up in the last quantization bin after division by the shared scale. We average across all layers for both synthetic and OLMo runs. In OLMo, there are a large number of layernorms which experience different degrees of clamping to the last quantization bin. Some components, such as the attention layernorms, remain relatively well behaved throughout training, whereas others like the FFN layernorms or the QK layernorms (Henry et al., 2020) can experience large, sudden overflow issues. While it s possible to disable the affine transformation of layernorms, this greatly diminishes performance in the language model setting. More broadly, this issue indicates a problem with applying shared-scales to blocks of weights that follow approximately log-normal distributions (such as layernorm affine parameters), which may not have a well-defined notion of a max" relative to a resolution fixed by a given precision scheme. A scale that adapts to both min and max might avoid the bias; we defer this to future work. On the activation side, we find that this effect does affect roughly 1 of values in our synthetic experiments and 0.5 of values in OLMo (shown in the right subplot of Figure 5). 6.2 Potential Mitigations Given these observations, we next ask whether training instabilities can be mitigated by modifying the quantization scheme, for example by increasing the precision of activation LN elements or by restricting quantization to the forward pass only. To investigate this, we perform a sweep over model sizes comparing three setups: (1) full quantization in both forward and backward passes (baseline), (2) forward-pass-only quantization, and (3) higher-precision activation formats (including layernorms) in the MX scheme. As shown in Figure 6, both mitigation strategies do improve stability. Each reduces the number of divergent runs to 2, down from 6 in the fully quantized baseline. One of the instability observed in the forward-only setting also occurs in high-precision training, leaving only one instability unique to the MXFP8 E4M3 baseline unmitigated. 11 0 1 2 3 4 5 6 7 10 3 10 2 10 1 100 101 MSE loss FP32 (skyline) 0 1 2 3 4 5 6 7 MXFP8 quantize fwd backward 0 1 2 3 4 5 6 7 MXFP8 quantize fwd only (backward in fp32) 0 1 2 3 4 5 6 7 MXFP8 quantize fwd (act. backward in bf16) Training step ( 10³) L 2, W 256 L 2, W 384 L 2, W 512 L 3, W 256 L 3, W 384 L 3, W 512 L 4, W 256 L 4, W 384 L 4, W 512 L 5, W 256 L 5, W 384 L 5, W 512 L 6, W 256 L 6, W 384 L 6, W 512 Figure 6: Shows two mitigations (quantization of only forward pass) and activation elements in higher precision, compared to fully quantized baseline and FP32 skyline. 0 2000 4000 6000 8000 training step 10 3 10 2 10 1 train loss intervene well before instability Intervention at Step 4500 MX FP8 E4M3 (orig. instability) FP32 baseline All FP32 (no MX ops) Bump overflow exponent No LayerNorm quant No back-prop quant BF16 weights BF16 acts fwd-only BF16 acts (all) 0 2000 4000 6000 8000 training step intervene just before instability Intervention at Step 5080 MX FP8 E4M3 (orig. instability) FP32 baseline All FP32 (no MX ops) Bump overflow exponent No LayerNorm quant No back-prop quant BF16 weights BF16 acts fwd-only BF16 acts (all) Figure 7: Intervention experiment for a synthetic student-teacher model with dmodel 512, four layers, and learning rate η 6 10 4. Training is stable in FP32 (blue) but diverges in MXFP8 E4M3 (yellow) around step 5100. We test two intervention timings: step 4500 (left, well before instability) and step 5080 (right, just before instability). Early interventions, like disabling backward-pass quantization or switching to high-precision (FP32), successfully prevent divergence, while using high precision for the activations (bfloat16) can greatly delay it. Late interventions cannot avert instability but can only delay it; the most effective are switching to FP32 or skipping quantization of layernorm weights. We can actually go further and ask whether an impending divergence can be averted by in-situ interventions to the training recipe. Figure 7 tracks a configuration that is stable in FP32 but diverges in MXFP8 E4M3. This setting corresponds to the previously described student-teacher scenario with four layers and model dimension dmodel 512. The instability starts approximately at step 5090 and we consider interventions just before the instability at step 5080 and well before the instability at step 4500. For each intervention we keep the random seed, model state, and batch sequence identical, so the training state at the intervention step is the same as in the baseline run; any divergence afterward is therefore attributable to the intervention. Switching entirely to FP32 precision for remaining training steps. Implementing FP32 significantly stabilizes training if the change is made sufficiently early (step 4500), but it is ineffective if applied immediately before instability (step 5080). However, even at the later intervention, FP32 prolongs training stability more effectively than the other approaches. Increasing the shared exponent by one (bumping exponent). Adjusting the exponent to avoid the last bucket overflow for blocks that have values that fall into the range in Equa- tion (10) does not mitigate instability, possibly due to insufficient precision improvement from a single increment. Avoiding MX quantization for LayerNorm affine parameters. Omitting quantization of these parameters partially stabilizes training and delays instability significantly at both inter- vention steps, indicating that LayerNorm parameters do contribute to instability dynamics. However, eventual instability suggests a residual effect from quantized activations. Precision adjustments in forward and backward passes, where we explored: 12 quantizing weights and activations only during the forward pass (no backward-pass quantization); maintaining weights in bfloat16 and activations in MXFP8 (both passes); maintaining activations in bfloat16 for the forward pass but MXFP8 for backward (with MXFP8 weights); using BF16 activations for both forward and backward passes while quantizing weights with MXFP8. Among these, applying the intervention just before instability (step 5080), bfloat16 activation precision in both passes consistently provides the strongest immediate stabilization, closely followed by disabling backward-pass quantization. When interventions occur earlier (step 4500), not quantizing the backward step performs comparably to the FP32 baseline, while fully bfloat16 activations delay instability considerably yet eventually become unstable. These results are consistent with a stochastic noise model, suggesting that multiple inter- acting factors influence instability likelihood. They are also consistent with our earlier observations about the effectiveness of higher activation precision and forward-only quanti- zation schemes in mitigating instability in MXFP8 training. The mixed weight activation precision strategy may be a pragmatic approach, as long as careful attention is given to the behavior of layernorm weights throughout training. Other Sweeps In Appendix B, we report additional ablations over optimizer choices (SGD with and without momentum, and Adam) and weight initialization schemes with reduced variance. While these variations can partially reduce the frequency of instabilities, they do not address the underlying bias in the computation. As we show in Section 5 and in Section 6.1, many instabilities stem from biased gradient estimates, and these choices do not reliably eliminate that source of error. Key Takewaways Our synthetic experiments reveal that training instabilities in block-scaled low- precision settings arise from both stochastic optimization effects and quantization-induced bias. These failures result from a complex and subtle interplay between architectural choices, activation quantization, quantization of layer normalization parameters, and learning rate. The dominant precision-specific bias comes from overflow of tightly clustered layer-norm affine weights (and a small fraction of activations). Our intervention experiments show that while simple tweaks such as nudging the shared exponent do not remove this bias, mitigations which raise precision in key parts of the computation such as increasing the precision of activations or not quantizing in the backward step generally improve stability. 7 Stabilization Strategies in Language Model Setting Motivated by the effective mitigations observed in our synthetic experiments, we return to the language model (OLMo) setting and apply the same two strategies: (1) retaining bfloat16 as the element format for activations and layer-norms, and (2) applying MX quantization only to the forward pass. In both cases, we find that training remains stable across all FP8 configurations, including E4M3 and E5M2 formats. Weight Activation D N Ratio 140.96 99.19 70.91 37.86 21.28 16.23 12.51 N 0.16B N 0.19B N 0.23B N 0.31B N 0.42B N 0.48B N 0.54B bfloat16 bfloat16 0.710 0.703 0.698 0.691 0.688 0.686 0.686 MXFP8 E4M3 bfloat16 0.0 -0.002 -0.002 0.0 0.0 0.0 0.0 MXFP8 E5M2 bfloat16 0.105 0.107 0.112 0.004 0.002 -0.001 -0.001 MXFP8 E4M3 MXFP8 E4M3 0.005 0.002 0.002 0.004 0.002 -0.001 -0.001 MXFP8 E5M2 MXFP8 E5M2 0.010 0.012 0.057 0.019 0.007 0.004 0.004 Table 1: The validation loss on Fineweb-Edu of high precision runs versus low precision with mitigations applied (values are shown as differences with respect to bfloat16 baseline; lower is better). For the last two rows, we quantize only the forward pass. 13 Table 1 reports validation loss differences relative to full-bfloat16 baselines. MXFP8 E4M3 weights paired with bfloat16 activations in particular match full-precision performance across all tested model sizes.As a proof of concept, Figure 8 shows a Chinchilla-style scaling law fit to the stabilized MXFP8 E4M3 bfloat16 runs. This demonstrates that, at least at the scales we trained, valid empirical scaling laws can still be extracted under this hybrid format. Whether this continues to hold at larger scales remains an open question, as such lower precision may exhibit bottoming-out effects or other nonlinearities in training dynamics. Full loss curves and scaling law fits for both mitigation strategies compared to bfloat16 baselines are provided in Appendix C. 8 Conclusion We have shown that training large language models in block-scaled low-precision formats (MXFP8, MXFP6) often leads to sharp, unrecoverable instabilities. By combining large-scale LLM sweeps with a controlled student-teacher proxy model trained on synthetic data, we isolate two distinct failure modes: Stochastic optimization breakdown, driven by a complex interplay between hyperparameters choices (e.g. high learning rates) or architectural factors (such as width, depth, choice of activation function), can alone can trigger instabilities even in high precision. Quantization-induced gradient bias, where shared-scale clamping (particularly of layer-norm affine weights and to a lesser extent, other activations) injects multiplicative gradient noise that ultimately destabilizes training. Guided by these observations, we identify two effective mitigation strategies: maintaining higher precision for activations or limiting quantization exclusively to the forward pass. Notably, we find that using MXFP8 E4M3 weights in combination with bfloat16 activations matches the performance of full-bfloat16 baselines. While our implementation applied higher-precision activations network-wide, it is likely that stability could be preserved using higher precision selectively in key layers or modules. Looking ahead, continued hardware advances will expand the frontier of what s computationally feasible. Some concrete directions for future research include: extending our proxy model to include attention mechanisms, mixture-of-experts with many layers, and other transformer-specific components to better predict instabilities in state-of- the-art architectures; developing a clear theoretical picture of the stochastic noise model and how different factors can amplify or reduce the risk of a training instability; designing new blockwise scaling schemes that adapt to skewed or tightly clustered distribu- tions. 14 1017 1018 1019 1020 1021 FLOPs 107 108 109 Parameters Weight: MXFP8 E4M3, Act: bfloat16 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 Train Loss (a) Scaling law fit for FP8 E4M3-bfloat16. 1017 1018 1019 1020 1021 FLOPs 107 108 109 Parameters Weight: MXFP8 E5M2, Act: bfloat16 0.7 0.8 0.9 1.0 1.1 Train Loss (b) Scaling law fit for FP8 E5M2-bfloat16. Figure 8: Scaling law fit for combinations of precision formats of weights and keep the activations in high precision. Fit was calculated using a Chinchilla model for the loss; details and fit parameters are given in Appendix C. 15 Limitations In this work, we provide insights into when instabilities arise in student-teacher training setups using a residual MLP, and show that these findings generalize to more complex architectures. Specifically, we identify sources of training instability such as batch size, batch quality, and learning rate that affect both high- and low-precision regimes. We also highlight instability factors specific to low-precision training, including sensitivity to learning rate, model depth and width, and the choice of activation function (though not exhaustively explored, they represent the primary contributors). To mitigate these instabilities in full-scale models, we propose retaining gradient computations and or activation functions in full precision. These approaches stabilize training and enable us to fit precision-aware scaling laws, specifically capturing how validation performance scales with model size and number of training tokens. However, there are many interactions in model training and we cannot exhaustively cover them all in our sweeps. It is likely that there exist other mitigation strategies not evaluated in this work. We emphasize, however, that a core contribution of this paper is to propose simplified proxies for reasoning about complex low-precision dynamics, rather than prescribe a single universal fix. Our results are primarily derived from decoder-only language models and residual MLPs; future work is needed to determine whether similar behaviors arise in MoE architectures. Finally, while we emulate MX formats in PyTorch, all experiments are conducted in software. Real-world deployment on Blackwell-class hardware may introduce additional sources of error due to rounding behavior, memory layout, or fused kernel execution not captured by our implementation. We also performed most of our experiments on relatively small models; our synthetic models run on NVIDIA H100s in a matter of minutes, and the largest language models used 16 H100s per run. While this allowed us to reach the 1B scale, results may not extrapolate well to models that are an order of magnitude larger. 16 A Review of Shared-Scale Quantization In this section we provide a self-contained review of block scaling quantization schemes, largely following Rouhani et al. (2023); Darvish Rouhani et al. (2023a). Taking a step back, the idea in shared-scale quantization methods is to introduce a number which represents the shared scale among a group of values that could, e.g., represent weights or activations. The idea is that low-precision data types tend to have a small representable range and quantization can clip very large values or zero-out smaller values. By dividing by the shared scale, the goal is to put these numbers in a representable range and save the scale such that it may be multiplied at the end of the computation. There are many choices for how to pick the scale, with pros and cons for each. For example, one approach is to have a single scale factor for the entire tensor, which has a very low memory overhead but is usually too coarse-grained and lead to saturation issues. On the opposite end, one could keep a scale factor for every value in the tensor which obviously allows for higher accuracy but involves much more memory. Other approaches include tilewise scaling, where a scale factor is used for a fixed-size submatrix. This was the approach taken in Liu et al. (2024). In this work, we focus on block scaling methods, where a single 1-dimensional block of values shares a scale. In particular, we focus on the microscaling" (MX) format, where each block consists of 32 values, with a shared scale that can be computed using Algorithm 1. When performing matrix multiplications or dot products, these shared scales are carried around and multiplied at the end of the computation (see Darvish Rouhani et al. (2023a) for the exact specifications). Algorithm 1 Convert V HP_DTYPEk to an MX block {X, P LP_DTYPEk} Require: k 32 (hardware block size), 1: eelem max exponent of the largest normal value in LP_DTYPE Ensure: Scale factor X and low-precision elements P1, . . . , Pk 2: m maxi Vi 3: shared_exp log2(m) eelem max 4: X 2shared_exp block scale (a power of two) 5: for i 1 to k do 6: r Vi X 7: Pi QUANTIZETOLP(r) clamp if r overflows 8: end for 9: return (X, {Pi}k i 1) The shared scale in MX formats can therefore be regarded as the largest power-of-two that can represent the maximum within a block, shifted by the exponent of the largest normal value in that type. What gets quantized in a typical training setup? There are several toggles. Let us illustrate by a simple example i.e. our synthetic model in Equation (1) when there is only one layer: A0 x h W(1)LN(x) A1 x W(2)ϕ(h) . (11) Roughly speaking, we can choose to apply MX quantization to weights, activations in the forward and or backward pass. In the forward pass, we can apply Algorithm 1 at every stage. Ife denotes the quantization resulting from Algorithm 1 and perform matmuls using the shared scale, then if we quantize both weights and activations we compute A0 x h f W(1)LN(ex) A1 x f W(2)ϕ(eh). (12) One additional subtlety is that vector operations such as vector addition such as those present in layernorm computations are typically carried out in bfloat16, i.e. the operands are first cast once to bfloat16, and the addition itself runs in bfloat16. Meanwhile, in the backward pass, we can actually quantize in three different ways (or not at all if we turn off quantization in backpropagation): quantize weights, quantize input activation gradients, or quantize output activation gradients. Suppose we want to compute W(1)L with the MSE loss L 1 2(A1 y )2. This means computing L A1 A1 W(1) . The term L A1 can be quantized in one format the output gradient quantization, while the second 17 term A1 W(1) can be quantized in a separate format the input gradient quantization. In general, in this work, unless we state otherwise, these two formats are taken to be the same. In backpropagating, the input gradient itself will involve computing ϕ h. Again, the exterior gradient ϕ h h W (1) is quantized according to its chosen format. The second piece, h W(1) involves the input to the linear layer LN(x) which is quantized to the input gradient format. B Additional Synthetic Sweeps In this section, we present additional synthetic experiments to further examine the sources and mitigation of low-precision instabilities. Figure 9 summarizes the frequency of instability spikes across our depth-width sweep at a fixed learning rate of η 5 10 4. The MX-mix format refers to the asymmetric configuration using MXFP8 E4M3 in the forward pass and E5M2 in the backward pass. Spikes were determined by the heuristic criteria that the loss at time step t had to be a factor of 100 lager than the loss at time step t 1; this gives a rough lower bound on the number of spikes. Figure 10 compares the impact of optimizer choice, focusing on SGD with momentum, and vanilla SGD (momentum 0). These experiments used a slightly higher learning rate of η 1 10 2 to exaggerate differences. Compared with Figure 2, we observe that SGD variants are more stable than Adam, perhaps due to Adam s use of second-moment accumulation, which may amplify quantization- induced bias in low-precision regimes. Figure 11 evaluates the effect of different weight initialization schemes. We compare standard Pytorch initialization, typically taken to be a Kaiming uniform distribution between [ 1 fan in, 1 fan in], against a variant using lower gain (gain 0.5) under the Xavier normal distribution. Reducing the variance of initial weights appears to improve loss spikes. 64 96 128 256 384 512 768 896 1024 Width 1 2 3 4 5 6 7 8 Depth 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 2 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 FP32 64 96 128 256 384 512 768 896 1024 Width 1 2 3 4 5 6 7 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 2 1 1 2 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 2 3 2 0 0 0 0 0 2 0 1 1 MX-mix 64 96 128 256 384 512 768 896 1024 Width 1 2 3 4 5 6 7 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 2 1 0 0 0 0 0 1 1 1 2 0 0 0 0 3 0 1 2 1 MX-e4m3 64 96 128 256 384 512 768 896 1024 Width 1 2 3 4 5 6 7 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 2 0 0 0 0 0 1 1 2 0 0 0 0 0 1 1 1 1 1 MX-fp6 0.0 0.5 1.0 1.5 2.0 2.5 3.0 spikes Figure 9: Instability spikes measured in training, for different model depths and widths. 0 1 2 3 4 5 6 7 10 3 10 2 10 1 100 101 MSE loss FP32 w SGD (momentum 0) 0 1 2 3 4 5 6 7 FP32 w SGD (momentum 0.9) 0 1 2 3 4 5 6 7 MXFP8 w SGD (momentum 0) 0 1 2 3 4 5 6 7 MXFP8 w SGD (momentum 0.9) Training step ( 10³) L 3, W 256 L 3, W 384 L 3, W 512 L 4, W 256 L 4, W 384 L 4, W 512 L 5, W 256 L 5, W 384 L 5, W 512 Figure 10: SGD with and without momentum; a larger learning rate was used η 1 10 2. 18 0 1 2 3 4 5 6 7 10 3 10 2 10 1 100 101 MSE loss FP32 baseline (default init) 0 1 2 3 4 5 6 7 FP32 w lower gain Xavier normal 0 1 2 3 4 5 6 7 MXFP8 (default init) 0 1 2 3 4 5 6 7 MXFP8 w lower gain Xavier normal Training step ( 10³) L 2, W 256 L 2, W 384 L 2, W 512 L 3, W 256 L 3, W 384 L 3, W 512 L 4, W 256 L 4, W 384 L 4, W 512 L 5, W 256 L 5, W 384 L 5, W 512 L 6, W 256 L 6, W 384 L 6, W 512 Figure 11: Baseline versus using a lower gain Xavier normal weight initialization. 19 C Scaling Law Fits and Loss Curves after Mitigation In addition to Figure 8 we provide scaling law for the mitigation where we quantize only the forward pass; this is shown in Figure 12 which can be compared against the bfloat16 baseline in Figure 13. Scaling law fits were performed using the methods described in Hoffmann et al. (2022); Brandfonbrener et al. (2024) where the validation loss was fit with a functional form L(N, D) E A N α B Dβ , (13) for constants A, B, E, α, and β. The fitted values of these constants are given in Table 2. We also provide the loss curves after implementing these mitigation strategies; these are shown in Figure 14 and Figure 15. 1017 1018 1019 1020 1021 FLOPs 107 108 109 Parameters Weight: MXFP8 E4M3, Act: MXFP8 E4M3, Quantize Forward Only 0.7 0.8 0.9 1.0 1.1 Train Loss (a) Scaling law fit for MXFP8-FP8 E4M3. 1017 1018 1019 1020 1021 FLOPs 107 108 109 Parameters Weight: MXFP8 E5M2, Act: MXFP8 E5M2, Quantize Forward Only 0.7 0.8 0.9 1.0 1.1 Train Loss (b) Scaling law fit for MXFP8 E5M2-E5M2. Figure 12: Scaling law fits for fixed stable of precision formats of weights and activations quantizing only the forward pass. 1017 1018 1019 1020 1021 FLOPs 107 108 109 Parameters Weight: bfloat16, Act: bfloat16 0.7 0.8 0.9 1.0 1.1 Train Loss (a) Scaling law fit for bfloat16-bfloat16. 1017 1018 1019 1020 1021 FLOPs 107 108 109 Parameters Weight: MXFP6 E2M3, Act: MXFP6 E2M3 0.7 0.8 0.9 1.0 1.1 Train Loss (b) Scaling law fit for FP6 E2M3-FP6 E2M3. Figure 13: Scaling law fits for bfloat16-bfloat16 (baseline) and for MXFP6 format. 20 Weight Activation A B E α β a MXFP6 E2M3 bfloat16 1.84e 03 8.77e 03 0.52 0.50 0.51 0.51 MXFP8 E4M3 bfloat16 2.82e 03 2.04e 04 0.54 0.52 0.55 0.51 MXFP8 E5M2 bfloat16 1.68e 03 1.84e 04 0.52 0.49 0.55 0.53 bfloat16 bfloat16 1.94e 03 2.18e 04 0.53 0.50 0.56 0.53 MXFP8 E4M3 MXFP8 E4M3 1.57e 03 2.11e 04 0.52 0.49 0.55 0.53 MXFP8 E5M2 MXFP8 E5M2 2.20e 03 3.98e 04 0.54 0.51 0.59 0.54 Table 2: Fitted scaling law parameters. For the last two rows, we quantize only the forward pass. The last column is equal to the ratio a β (α β), the exponent of the optimal model size relative to FLOPs. 0 20000 40000 60000 80000 Step 1.5 2.0 2.5 3.0 3.5 4.0 4.5 Loss Train CrossEntropyLoss 0 20000 40000 60000 80000 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 25 50 75 100 125 150 175 Ratio MXFP8 (E4M3 weights, E4M3 activations) Quantize Forward Only (a) Train loss and gradient norm for weights: MXFP8 E4M3, Activation: MXFP8 E4M3 while quantizing only the forward pass. 0 20000 40000 60000 80000 100000 120000 Step 1.5 2.0 2.5 3.0 3.5 4.0 4.5 Loss Train CrossEntropyLoss 0 20000 40000 60000 80000 100000 120000 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 25 50 75 100 125 150 175 Ratio MXFP8 (E5M2 weights, E5M2 activations) Quantize Forward Only (b) Train loss and gradient norm for weights: MXFP8 E5M2, Activation: MXFP8 E5M2. Figure 14: Train loss and gradient norm when quantizing only the forward pass. 21 0 20000 40000 60000 80000 Step 1.5 2.0 2.5 3.0 3.5 4.0 4.5 Loss Train CrossEntropyLoss 0 20000 40000 60000 80000 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 25 50 75 100 125 150 175 Ratio MXFP8 E4M3 weights, bfloat16 activations (a) Train loss and gradient norm for MXFP8 E4M3-MXFP8 E4M3. 0 20000 40000 60000 80000 100000 120000 140000 Step 1.5 2.0 2.5 3.0 3.5 4.0 4.5 Loss Train CrossEntropyLoss 0 20000 40000 60000 80000 100000 120000 140000 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 25 50 75 100 125 150 175 Ratio MXFP8 E5M2 weights, bfloat16 activations (b) Train loss and gradient norm for MXFP8 E5M2-MXFP8 E5M2. Figure 15: Train loss and gradient norm when activations are kept in high precision (bfloat16). 22 D Details of Language Model Training All models are trained on the Fineweb-Edu dataset (Penedo et al., 2024) using the Olmo code- base (Groeneveld et al., 2024), with the longest runs trained on 35B tokens and the shortest runs corresponding to models trained on 301M tokens. Models were trained with a learning rate schedule with a linear warmup starting at 2e-5 increasing to 2e-4, followed by cosine decay back to 2e-5 (Porian et al., 2025). Training runs that involved using MX precision formats were done performed using MX Pytorch Emulation Library (Microsoft, 2024). Parameter Value n 6 24 for small models Number of heads n Head dimension 64 MLP hidden multiplier 4 Depth n Context length 512 Activation GeLU Positional encoding RoPE Biases False Normalization PyTorch Layernorm QK normalization True Tokenizer Llama2 Table 3: Model parameters used for training. 23 E Validation Losses in Language Models Quantizing Table 4 and continued in 5 shows validation losses for all models with mitigations applied (quantiza- tion only in the forward pass, or activations in high precision), trained using our at different FLOP budgets relative to bfloat16 baseline. D N formats bfloat16 E4M3 E5M2 E4M3 E5M2 bfloat16 bfloat16 bfloat16 E4M3 E5M2 87.35 2e 17 1.1522 -0.027 -0.027 -0.027 -0.012 46.99 1.1084 0.002 0.007 0.007 0.012 26.897 1.1011 0.004 0.001 0.004 0.009 16.06 1.0956 -0.001 0.014 0.004 0.009 9.92 1.0971 0.003 0.003 0.008 0.013 6.30 1.0950 0.0 -0.005 -0.01 0.01 4.10 1.1042 0.001 -0.006 0.006 0.006 2.73 1.1255 -0.001 -0.004 0.004 0.019 191.02 4.37e 17 1.030 0.005 0.0 0.010 0.01 102.78 1.0464 -0.016 0.036 -0.011 -0.021 58.81 0.9898 0.005 0.005 0.005 0.015 35.14 0.9806 -0.001 0.004 0.004 0.009 21.70 0.9765 0.003 0.003 0.003 0.013 13.78 0.9717 0.003 0.003 0.003 0.008 8.97 0.9732 0.002 0.002 0.002 0.012 5.97 2.3174 0.303 0.843 2.763 1.237 4.05 0.9839 0.001 0.006 0.006 0.006 2.80 0.9949 0.0 0.0 0.0 0.005 128.62 9.56e 17 0.9198 0.0 0.0 0.005 0.015 76.84 0.9052 0.0 0.005 0.005 0.015 47.46 0.8969 0.002 0.003 0.003 0.008 30.14 0.8894 0.001 0.001 0.006 0.011 19.62 0.8846 0.0 0.005 0.005 0.01 13.05 0.8879 0.002 0.002 0.002 0.012 8.86 0.8849 0.0 0.005 0.005 0.005 6.13 0.8882 0.002 0.002 0.002 0.007 4.31 0.8933 0.002 0.002 0.002 0.007 3.08 0.8961 0.004 0.004 0.004 0.009 2.24 0.9059 -0.001 0.004 0.064 0.004 168.03 2.09e 18 0.8546 0.0 0.005 0.005 0.015 103.78 0.8430 0.002 0.002 0.187 0.012 65.91 0.8335 0.001 0.001 0.001 0.011 42.896 0.8258 -0.001 0.004 0.004 0.009 28.54 0.8242 0.001 0.001 0.001 0.011 19.37 0.8200 0.0 0.0 0.0 0.005 13.399 0.8197 0.0 0.0 0.0 0.005 9.428 0.8187 0.001 0.001 0.001 0.006 6.74 0.8192 0.001 0.001 0.001 0.006 4.89 0.8215 0.003 0.006 0.003 0.003 2.02 0.8327 0.002 0.002 0.002 0.002 Table 4: Validation loss table, with separate columns for various weight and activation precisions. For the last 2 columns, we quantize only the forward pass. The second column indicates the total FLOP count used for those values of tokens-to-parameter ratios (D N). Values are shown as differences with respect to bfloat16 baseline (lower is better). 24 D N formats bfloat16 E4M3 E5M2 E4M3 E5M2 bfloat16 bfloat16 bfloat16 E4M3 E5M2 144.14 4.57e 18 0.794 0.001 0.006 0.006 0.011 93.81 0.784 0.001 0.001 0.001 0.011 62.41 0.780 0.0 0.005 0.005 0.01 42.37 0.774 0.001 0.001 0.001 0.006 29.30 0.772 -0.002 0.003 0.003 0.003 14.74 0.767 -0.002 0.003 0.003 0.003 10.70 0.766 -0.001 0.004 -0.001 0.004 7.87 0.766 -0.001 0.004 -0.001 0.004 4.42 0.769 0.001 0.001 0.001 0.006 3.37 0.772 -0.002 0.003 0.003 0.003 2.60 0.775 0.0 0.0 0.0 0.005 2.02 0.779 0.001 0.001 0.001 0.001 136.47458 1e 19 0.748 0.002 0.002 0.002 0.002 92.646 0.741 -0.001 0.004 0.004 0.009 64.075 0.736 -0.001 0.004 0.004 0.009 45.084 0.731 -0.001 0.004 0.004 0.009 32.233 0.728 0.002 0.002 0.002 0.007 23.391 0.725 0.0 0.005 0.0 0.005 17.210 0.724 0.001 0.001 0.001 0.006 12.826 0.724 0.001 0.001 0.001 0.311 9.674 0.723 0.002 0.002 0.002 0.002 7.38 0.723 0.002 0.002 0.002 0.077 4.43 0.727 -0.002 0.003 0.003 0.003 2.75 0.732 -0.002 0.023 0.003 0.003 Table 5: MXFP8 of the validation loss table, with separate rows for Weight and Activation precisions. For the last 2 columns, we quantize only the forward pass. The second column indicates the FLOP count used. 25 F Additional Unstable Language Model Sweeps In Figure 16 and Figure 17 we show some other examples of weight activation MX precision combinations we found to be unstable. In general, we were not able to find any stable combinations of weights and activations in lower precision across the formats we tested. 0 20000 40000 60000 80000 100000 120000 140000 Step 2 4 6 8 10 Loss Train CrossEntropyLoss 0 20000 40000 60000 80000 100000 120000 140000 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 20 40 60 80 100 120 140 160 Ratio MXFP8 E5M2 weights, MXFP8 E4M3 activations (a) Train loss and grad norm for MXFP8 E5M2-MXFP8 E4M3. 0 10000 20000 30000 40000 50000 60000 Step 2 3 4 5 6 Loss Train CrossEntropyLoss 0 10000 20000 30000 40000 50000 60000 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 25 50 75 100 125 150 175 Ratio MXFP8 E4M3 weights, MXFP8 E5M2 activations (b) Train loss and grad norm for MXFP8 E4M3-MXFP E5M2. Figure 16: Unstable MXFP8 combinations of precision formats of weights and activations. 26 0 100 200 300 400 500 Step 2 3 4 5 6 7 Loss Train CrossEntropyLoss 0 100 200 300 400 500 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 25 50 75 100 125 150 175 Ratio MXFP6 E2M3 weights, MXFP8 E4M3 activations (a) for MXFP6 E2M3-MXFP8 E4M3. 0 50 100 150 200 250 300 350 400 Step 1.5 2.0 2.5 3.0 3.5 4.0 4.5 Loss Train CrossEntropyLoss 0 50 100 150 200 250 300 350 400 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 25 50 75 100 125 150 175 Ratio MXFP6 E2M3 weights, MXFP8 E5M2 activations (b) Train loss and for MXFP6 E2M3-MXFP8 E5M2. Figure 17: Unstable combinations of precision formats of weights and activations for MXFP6 weights. 27 References Abdolrashidi, A., Wang, L., Agrawal, S., Malmaud, J., Rybakov, O., Leichner, C., and Lew, L. (2021). Pareto-optimal quantized resnet is mostly 4-bit. In 2021 IEEE CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), page 3085 3093. IEEE. Anthropic (2025). Claude 4 system card. 4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf. Accessed: 2025-06-20. Bondarenko, Y., Nagel, M., and Blankevoort, T. (2023). Quantizable transformers: Removing outliers by helping attention heads do nothing. Brandfonbrener, D., Anand, N., Vyas, N., Malach, E., and Kakade, S. (2024). Loss-to-loss prediction: Scaling laws for all datasets. arXiv preprint arXiv:2411.12925. Chen, M., Zhang, C., Liu, J., Zeng, Y., Xue, Z., Liu, Z., Li, Y., Ma, J., Huang, J., Zhou, X., and Luo, P. (2025). Scaling law for quantization-aware training. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. (2022). Palm: Scaling language modeling with pathways. Cohen, J., Kaur, S., Li, Y., Kolter, J. Z., and Talwalkar, A. (2021). Gradient descent on neural networks typically occurs at the edge of stability. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Cohere, T., :, Aakanksha, Ahmadian, A., Ahmed, M., Alammar, J., Alizadeh, M., Alnumay, Y., Althammer, S., Arkhangorodsky, A., Aryabumi, V., Aumiller, D., Avalos, R., Aviv, Z., Bae, S., Baji, S., Barbet, A., Bartolo, M., Bebensee, B., Beladia, N., Beller-Morales, W., Bérard, A., Berneshawi, A., Bialas, A., Blunsom, P., Bobkin, M., Bongale, A., Braun, S., Brunet, M., Cahyawijaya, S., Cairuz, D., Campos, J. A., Cao, C., Cao, K., Castagné, R., Cendrero, J., Currie, L. C., Chandak, Y., Chang, D., Chatziveroglou, G., Chen, H., Cheng, C., Chevalier, A., Chiu, J. T., Cho, E., Choi, E., Choi, E., Chung, T., Cirik, V., Cismaru, A., Clavier, P., Conklin, H., Crawhall-Stein, L., Crouse, D., Cruz-Salinas, A. F., Cyrus, B., D souza, D., Dalla-Torre, H., Dang, J., Darling, W., Domingues, O. D., Dash, S., Debugne, A., Dehaze, T., Desai, S., Devassy, J., Dholakia, R., Duffy, K., Edalati, A., Eldeib, A., Elkady, A., Elsharkawy, S., Ergün, I., Ermis, B., Fadaee, M., Fan, B., Fayoux, L., Flet-Berliac, Y., Frosst, N., Gallé, M., Galuba, W., Garg, U., Geist, M., Azar, M. G., Gilsenan-McMahon, E., Goldfarb-Tarrant, S., Goldsack, T., Gomez, A., Gonzaga, V. M., Govindarajan, N., Govindassamy, M., Grinsztajn, N., Gritsch, N., Gu, P., Guo, S., Haefeli, K., Hajjar, R., Hawes, T., He, J., Hofstätter, S., Hong, S., Hooker, S., Hosking, T., Howe, S., Hu, E., Huang, R., Jain, H., Jain, R., Jakobi, N., Jenkins, M., Jordan, J., Joshi, D., Jung, J., Kalyanpur, T., Kamalakara, S. R., Kedrzycki, J., Keskin, G., Kim, E., Kim, J., Ko, W.-Y., Kocmi, T., Kozakov, M., Kry sci nski, W., Jain, A. K., Teru, K. K., Land, S., Lasby, M., Lasche, O., Lee, J., Lewis, P., Li, J., Li, J., Lin, H., Locatelli, A., Luong, K., Ma, R., Mach, L., Machado, M., Magbitang, J., Lopez, B. M., Mann, A., Marchisio, K., Markham, O., Matton, A., McKinney, A., McLoughlin, D., Mokry, J., Morisot, A., Moulder, A., Moynehan, H., Mozes, M., Muppalla, V., Murakhovska, L., Nagarajan, H., Nandula, A., Nasir, H., Nehra, S., Netto-Rosen, J., Ohashi, D., Owers-Bardsley, J., Ozuzu, J., Padilla, D., Park, G., Passaglia, S., Pekmez, J., Penstone, L., Piktus, A., Ploeg, C., Poulton, A., Qi, Y., Raghvendra, S., Ramos, M., Ranjan, E., Richemond, P., Robert-Michon, C., Rodriguez, A., Roy, S., Ruder, S., Ruis, L., Rust, L., Sachan, A., Salamanca, A., Saravanakumar, K. K., Satyakam, I., Sebag, A. S., Sen, P., Sepehri, S., Seshadri, P., Shen, Y., Sherborne, T., Shi, S. S., Shivaprasad, S., Shmyhlo, V., Shrinivason, A., Shteinbuk, I., Shukayev, A., Simard, M., Snyder, E., Spataru, A., Spooner, V., Starostina, T., Strub, F., Su, Y., Sun, J., Talupuru, D., Tarassov, E., Tommasone, E., Tracey, J., Trend, B., Tumer, E., Üstün, A., Venkitesh, B., Venuto, D., Verga, P., Voisin, M., Wang, A., Wang, D., Wang, S., Wen, E., White, N., Willman, J., Winkels, M., Xia, 28 C., Xie, J., Xu, M., Yang, B., Yi-Chern, T., Zhang, I., Zhao, Z., and Zhao, Z. (2025). Command a: An enterprise-ready large language model. Damian, A., Nichani, E., and Lee, J. D. (2023). Self-stabilization: The implicit bias of gradient descent at the edge of stability. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Darvish Rouhani, B., Garegrat, N., Savell, T., More, A., Han, K.-N., Zhao, R., and Hall, M. (2023a). Open compute project. Darvish Rouhani, B., Zhao, R., Elango, V., Shafipour, R., Hall, M., Mesmakhosroshahi, M., More, A., Melnick, L., Golub, M., Varatkar, G., et al. (2023b). With shared microexponents, a little shifting goes a long way. In Proceedings of the 50th Annual International Symposium on Computer Architecture, pages 1 13. DeepMind, G. (2025). Gemini 2.5 technical report. deepmind-media gemini gemini_v2_5_report.pdf. Accessed: 2025-06-20. Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., Steiner, A., Caron, M., Geirhos, R., Alabdulmohsin, I., Jenatton, R., Beyer, L., Tschannen, M., Arnab, A., Wang, X., Riquelme, C., Minderer, M., Puigcerver, J., Evci, U., Kumar, M., van Steenkiste, S., Elsayed, G. F., Mahendran, A., Yu, F., Oliver, A., Huot, F., Bastings, J., Collier, M. P., Gritsenko, A., Birodkar, V., Vasconcelos, C., Tay, Y., Mensink, T., Kolesnikov, A., Paveti c, F., Tran, D., Kipf, T., Luˇci c, M., Zhai, X., Keysers, D., Harmsen, J., and Houlsby, N. (2023). Scaling vision transformers to 22 billion parameters. Dettmers, T. and Zettlemoyer, L. (2023). The case for 4-bit precision: k-bit inference scaling laws. In International Conference on Machine Learning, pages 7750 7774. PMLR. Fishman, M., Chmiel, B., Banner, R., and Soudry, D. (2024). Scaling fp8 training to trillion-token llms. arXiv preprint arXiv:2409.12517. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. (2024). The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Groeneveld, D., Beltagy, I., Walsh, P., Bhagia, A., Kinney, R., Tafjord, O., Jha, A. H., Ivison, H., Magnusson, I., Wang, Y., et al. (2024). Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838. Henry, A., Dachapally, P. R., Pawar, S. S., and Chen, Y. (2020). Query-key normalization for transformers. CoRR, abs 2010.04245. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. (2022). Training compute-optimal large language models. arXiv preprint arXiv:2203.15556. Ibrahim, A., Thérien, B., Gupta, K., Richter, M. L., Anthony, Q. G., Belilovsky, E., Lesort, T., and Rish, I. (2024). Simple and scalable strategies to continually pre-train large language models. Transactions on Machine Learning Research. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., and Kalenichenko, D. (2017). Quantization and training of neural networks for efficient integer-arithmetic-only inference. Jastrzebski, S., Szymczak, M., Fort, S., Arpit, D., Tabor, J., Cho, K., and Geras, K. (2020). The break-even point on optimization trajectories of deep neural networks. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. Kumar, T., Ankner, Z., Spector, B. F., Bordelon, B., Muennighoff, N., Paul, M., Pehlevan, C., Ré, C., and Raghunathan, A. (2024). Scaling laws for precision. arXiv preprint arXiv:2411.04330. 29 Lee, J., Bae, J., Kim, B., Kwon, S. J., and Lee, D. (2025). To fp8 and back again: Quantifying reduced precision effects on llm training stability. Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al. (2024). Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437. Liu, Z., Zhao, C., Huang, H., Chen, S., Zhang, J., Zhao, J., Roy, S., Jin, L., Xiong, Y., Shi, Y., et al. (2025). Paretoq: Scaling laws in extremely low-bit llm quantization. arXiv preprint arXiv:2502.02631. Ma, J., Pei, H., Lausen, L., and Karypis, G. (2025). Understanding silent data corruption in llm training. Meta, A. (2025). The llama 4 herd: The beginning of a new era of natively multimodal ai innovation. Micikevicius, P., Stosic, D., Burgess, N., Cornea, M., Dubey, P., Grisenthwaite, R., Ha, S., Hei- necke, A., Judd, P., Kamalu, J., et al. (2022a). Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433. Micikevicius, P., Stosic, D., Burgess, N., Cornea, M., Dubey, P., Grisenthwaite, R., Ha, S., Heinecke, A., Judd, P., Kamalu, J., Mellempudi, N., Oberman, S., Shoeybi, M., Siu, M., and Wu, H. (2022b). Fp8 formats for deep learning. Microsoft (2024). Mx pytorch emulation library. Molybog, I., Albert, P., Chen, M., DeVito, Z., Esiobu, D., Goyal, N., Koura, P. S., Narang, S., Poulton, A., Silva, R., Tang, B., Liskovich, D., Xu, P., Zhang, Y., Kambadur, M., Roller, S., and Zhang, S. (2023). A theory on adam instability in large-scale machine learning. NVIDIA (2025). Nvidia blackwell architecture. OpenAI (2025). Gpt-4.5 system card. gpt-4-5-system-card-2272025.pdf. Accessed: 2025-06-20. Ouyang, X., Ge, T., Hartvigsen, T., Zhang, Z., Mi, H., and Yu, D. (2024). Low-bit quantization favors undertrained llms: Scaling laws for quantized llms with 100t training tokens. arXiv preprint arXiv:2411.17691. Penedo, G., Kydlíˇcek, H., allal, L. B., Lozhkov, A., Mitchell, M., Raffel, C., Werra, L. V., and Wolf, T. (2024). The fineweb datasets: Decanting the web for the finest text data at scale. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Porian, T., Wortsman, M., Jitsev, J., Schmidt, L., and Carmon, Y. (2025). Resolving discrepancies in compute-optimal scaling of language models. Rouhani, B. D., Zhao, R., More, A., Hall, M., Khodamoradi, A., Deng, S., Choudhary, D., Cornea, M., Dellinger, E., Denolf, K., et al. (2023). Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537. Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z., Zhang, K., Gao, P., Qiao, Y., and Luo, P. (2024). Omniquant: Omnidirectionally calibrated quantization for large language models. Shazeer, N. (2020). Glu variants improve transformer. Sun, M., Chen, X., Kolter, J. Z., and Liu, Z. (2024). Massive activations in large language models. arXiv preprint arXiv:2402.17762. Tseng, A., Yu, T., and Park, Y. (2025). Training llms with mxfp4. Wortsman, M., Liu, P. J., Xiao, L., Everett, K. E., Alemi, A. A., Adlam, B., Co-Reyes, J. D., Gur, I., Kumar, A., Novak, R., Pennington, J., Sohl-Dickstein, J., Xu, K., Lee, J., Gilmer, J., and Kornblith, S. (2024). Small-scale proxies for large-scale transformer training instabilities. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. 30 Xu, K., Lin, J., Wang, Z., Hu, P., and Zhao, Z. (2023). Improved fully quantized training via rectifying batch normalization. arXiv preprint arXiv:. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. (2022). Opt: Open pre-trained transformer language models. Zoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J., Shazeer, N., and Fedus, W. (2022). St-moe: Designing stable and transferable sparse expert models. 31\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2506.20752v1 [cs.LG] 25 Jun 2025 Characterization and Mitigation of Training Instabilities in Microscaling Formats Chloe Huangyuan Su 1,2 Mujin Kwun1 Stephanie Gil2 Sham Kakade1,2 Nikhil Anand 1 1Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University 2Department of Computer Science, Harvard University Abstract Training large language models is an expensive, compute-bound process that must be repeated as models scale, algorithms improve, and new data is collected. To address this, next-generation hardware accelerators increasingly support lower- precision arithmetic formats, such as the Microscaling (MX) formats introduced in NVIDIA s Blackwell architecture. These formats use a shared scale within blocks of parameters to extend representable range and perform forward backward GEMM operations in reduced precision for efficiency gains. In this work, we investigate the challenges and viability of block-scaled precision formats during model training. Across nearly one thousand language models trained from scratch spanning com- pute budgets from 2 1017 to 4.8 1019 FLOPs and sweeping over a broad range of weight activation precision combinations we consistently observe that training in MX formats exhibits sharp, stochastic instabilities in the loss, particularly at larger compute scales. To explain this phenomenon, we conduct controlled exper- iments and ablations on a smaller proxy model that exhibits similar behavior as the language model, sweeping across architectural settings, hyperparameters, and precision formats. These experiments motivate a simple model in which multiplica- tive gradient bias introduced by the quantization of layer-norm affine parameters and a small fraction of activations can trigger runaway divergence. Through in situ intervention experiments on our proxy model, we demonstrate that instabilities can be averted or delayed by modifying precision schemes mid-training. Guided by these findings, we evaluate stabilization strategies in the LLM setting and show that certain hybrid configurations recover performance competitive with full-precision training. We release our code at 1 Introduction Large language models (LLMs) have dramatically improved in capabilities in recent years, largely driven by scaling their capacity and the quantity of training data (Kaplan et al., 2020; OpenAI, 2025; DeepMind, 2025; Anthropic, 2025; Grattafiori et al., 2024).\n\n--- Segment 2 ---\nGuided by these findings, we evaluate stabilization strategies in the LLM setting and show that certain hybrid configurations recover performance competitive with full-precision training. We release our code at 1 Introduction Large language models (LLMs) have dramatically improved in capabilities in recent years, largely driven by scaling their capacity and the quantity of training data (Kaplan et al., 2020; OpenAI, 2025; DeepMind, 2025; Anthropic, 2025; Grattafiori et al., 2024). For instance, training the Llama 3.1 405B model required more than 1025 FLOPs and utilized up to 16,000 H100 GPUs (Grattafiori et al., 2024). Scaling these models involves not only the initial, compute-intensive pretraining phase but also frequent retraining as new data becomes available, algorithms evolve, or architectural modifications are introduced, as well as post-training protocols that prepare the model for inference deployment. Pretraining itself also adds the additional challenge of carefully tuned hyperparameter settings, such Equal contribution; correspondence: Preprint. Under review. as learning rate schedules that decay toward zero, making it difficult to resume a completed run without resetting the optimization state. As a result, even small updates such as incorporating new data or minor architectural changes often require restarting training from early checkpoints, further compounding compute demands (Ibrahim et al., 2024). To reduce these computational burdens, recent hardware advancements have introduced native support for lower-precision computations, such as FP8 training in NVIDIA H100 GPUs (Micikevicius et al., 2022b). Upcoming hardware accelerators powered by NVIDIA s Blackwell architecture will further extend these capabilities with standardized, shared-scale Microscaling (MX) formats like MXFP8 and MXFP6 (NVIDIA, 2025). These formats store a per-block shared scale, which expands the effective dynamic range with minimal memory overhead, while simultaneously enabling GEMMs at lower precision (Rouhani et al., 2023; Darvish Rouhani et al., 2023b).\n\n--- Segment 3 ---\nUpcoming hardware accelerators powered by NVIDIA s Blackwell architecture will further extend these capabilities with standardized, shared-scale Microscaling (MX) formats like MXFP8 and MXFP6 (NVIDIA, 2025). These formats store a per-block shared scale, which expands the effective dynamic range with minimal memory overhead, while simultaneously enabling GEMMs at lower precision (Rouhani et al., 2023; Darvish Rouhani et al., 2023b). While model pretraining is typically done in 16 or 32-bit precision, some quantization schemes are already seeing industry adoption; for example, DeepSeek-V3 employs tile-wise FP8 quantization within large tensors (Liu et al., 2024), while Cohere s Command A model was trained in FP8 while reserving higher-precision operations for, e.g., activation functions and attention mechanisms (Cohere et al., 2025). At an even larger scale, the Llama-4 series of models is reported to have been pretrained in FP8 precision across nearly 32,000 GPUs (Meta, 2025). On the deployment side, workflows like quantization-aware training (QAT) and mixed-precision fine-tuning further underscore that understanding low-precision training dynamics is essential throughout a model s lifecycle (Jacob et al., 2017; Abdolrashidi et al., 2021; Shao et al., 2024). There are two primary challenges that accompany the adoption of low-precision formats for training. First, there is a potential performance tradeoff, where reducing precision may result in degradation or "plateauing" of training loss and downstream accuracy, which can be characterized through scaling laws that account for both compute and precision (Kumar et al., 2024). Second, instabilities may arise in training, often manifesting as abrupt spikes in the loss curve that disrupt convergence (Fishman et al., 2024; Lee et al., 2025). When these instabilities push optimization into regions from which recovery is impossible, they obstruct our ability to extract valid scaling laws, making it impossible to even assess the tradeoffs introduced by low-precision training. In this work, we set out to understand the training dynamics of low-precision MX precision formats and extract valid scaling laws to identify format prescriptions for language model training on next- generation hardware. However, consistent with prior observations by Fishman et al. (2024); Lee et al.\n\n--- Segment 4 ---\nHowever, consistent with prior observations by Fishman et al. (2024); Lee et al. (2025), we found that training frequently became unstable particularly for larger, compute-intensive models. In contrast to earlier work, these instabilities appeared more pervasive, emerging across a broad range of activation functions, model scales, MX formats, and hyperparameter settings. Because large-scale language model sweeps are computationally intensive and involve many entangled components, we turn to a controlled synthetic setting to better understand the origin of these failures. Specifically, we employ a residual multi-layer perceptron (MLP) student-teacher model trained on random (Gaussian) inputs, which enables fine-grained ablations over architecture and optimization. This proxy model allows us to isolate the effects of low-precision arithmetic and identify conditions under which training becomes unstable. In this setting, we observe two qualitatively distinct modes of instability. The first is expected" and stems from stochastic optimization dynamics: for instance, aggressive learning rates can amplify small incorrect gradient updates, resulting in loss spikes and divergence. The second, perhaps more interesting, mode is directly induced by low-precision arithmetic: even under stable hyperparameters, quantization introduces systematic gradient bias that can accumulate and destabilize training. To better understand the second failure mode, we perform ablations across format configurations, quantization schemes (e.g., forward-only vs. full quantization), and activation functions, and ana- lyze their effects on stability. Our results support a phenomenological explanation: low-precision instabilities primarily arise from systematic bias in gradient estimates introduced by quantization. We find that a key driver of this bias is the quantization of the layer normalization affine parameters, whose values often become tightly clustered over the course of training. When the values within a block converge too closely, division by the shared block scale can clamp all values in that block to the largest representable number, destabilizing training. We verify that this mechanism is not limited to synthetic settings but also emerges in language model setting. 2 With these insights, we propose two fixes to mitigate the instabilities in the language model setting, which involve keeping the MX activations in higher precision or only including quantization in the forward pass. Applying these fixes, we then train another set of language models in both MXFP8 precision schemes (E4M3, E5M2) and fit valid empirical scaling laws in both cases.\n\n--- Segment 5 ---\n2 With these insights, we propose two fixes to mitigate the instabilities in the language model setting, which involve keeping the MX activations in higher precision or only including quantization in the forward pass. Applying these fixes, we then train another set of language models in both MXFP8 precision schemes (E4M3, E5M2) and fit valid empirical scaling laws in both cases. Main contributions: MX sweeps: We perform extensive language model pretraining sweeps across weight and activation MX precision formats, and consistently observe training instabilities, particularly in larger models that are trained for longer. While prior work has reported instabilities in low-precision training (Fishman et al., 2024; Lee et al., 2025), we find them in shared- scale (MX) formats and occurring across a wide range of quantization configurations and activation functions. Mechanistic analysis: Through a suite of ablations in a simplified student teacher MLP model, we isolate two modes of instability: one arising from stochastic optimization dy- namics, and another induced by quantization noise. For the second case we show that the gradient becomes systematically biased, derive a norm-based condition that predicts when this bias will dominate, and trace its origin to the data being quantized: most of the layer-norm affine weights (and, to a lesser extent, roughly 1 of activations) are clustered such that MX block-scaling forces them into the same quantization bin. Effective mitigations: We evaluate two stabilization strategies in full-scale language model training: (1) disabling backward-pass quantization and (2) retaining higher-precision activations. Both approaches enable the recovery of valid empirical scaling laws. Notably, we find that MXFP8 weights (E4M3) paired with BF16 activations can match the performance of full bfloat16 baselines, at least up to the model scales we explored. Our work provides both a diagnostic methodology and empirical reference points for evaluating these nascent low-precision training formats. Broader relevance: While our experiments focus on models trained from scratch, the gradient-bias phenomena we study are intrinsic to block-scaled quantization and may also inform post-training workflows (e.g. QAT) or other low-precision scenarios.\n\n--- Segment 6 ---\nBroader relevance: While our experiments focus on models trained from scratch, the gradient-bias phenomena we study are intrinsic to block-scaled quantization and may also inform post-training workflows (e.g. QAT) or other low-precision scenarios. 2 Related Work Low-Precision Instabilities Training large Transformer models at scale can reveal instabilities that can disrupt or even halt learning (Liu et al., 2024; Chowdhery et al., 2022; Dehghani et al., 2023; Zhang et al., 2022; Molybog et al., 2023; Fishman et al., 2024; Zoph et al., 2022; Ma et al., 2025). In many cases, these issues are exacerbated or directly triggered by low-precision quantization. For example, Fishman et al. (2024) demonstrate that FP8 pretraining becomes unstable when combined with the SwiGLU activation function, attributing the issue to an outlier amplification effect that worsens due to progressive weight alignment over the course of training. Similarly, Lee et al. (2025) report that approximately 10 of FP8 runs using the NanoGPT codebase fail to converge, whereas full-precision (BF16) training exhibits no such failures. Other works (Sun et al., 2024; Bondarenko et al., 2023; Xu et al., 2023), point to activation outliers and gradient norm growth as contributors to these failures while Tseng et al. (2025) proposes a stochastic rounding based algorithm to stabilize training in MXFP4 formats. Meanwhile, DeepSeek-V3 also attributes certain training failures due to blockwise quantization of activation gradients (Liu et al., 2024), underscoring the breadth of challenges introduced by quantization schemes. Wortsman et al. (2024) use small-scale proxy models to study training instabilities in the context of growth of output and layer logits. We adopt a similar approach, and use a simplified student-teacher proxy model that replicates low-precision instabilities in LLMs and introduce a phenomenological framework to distinguish between stochastic and quantization-induced failure modes. Precision Scaling Laws In parallel, several works have examined how model performance scales under precision constraints.\n\n--- Segment 7 ---\nWe adopt a similar approach, and use a simplified student-teacher proxy model that replicates low-precision instabilities in LLMs and introduce a phenomenological framework to distinguish between stochastic and quantization-induced failure modes. Precision Scaling Laws In parallel, several works have examined how model performance scales under precision constraints. This includes both quantization-aware training (QAT), which injects quan- tization noise during the forward pass to prepare models for low-bit inference, and full low-precision training, which applies reduced-precision arithmetic in both passes to accelerate training (Jacob et al., 2017; Abdolrashidi et al., 2021; Shao et al., 2024; Chen et al., 2025). 3 Kumar et al. (2024) compare full-precision, QAT, and low-precision training, finding that overtrained models are especially sensitive to quantization and that even 16-bit training may not be optimal. However, their setup restricts quantization to the forward pass to allow for a fair comparison to QAT methods and does not analyze instability dynamics. We evaluate full MX quantization including both forward and backward passes and propose stabilization techniques that recover valid scaling laws in those lower-precision MX formats. Liu et al. (2025) explore QAT scaling laws in low-bit regimes, finding that fine-tuning outperforms both post-training quantization and QAT from scratch, even down to binary and ternary formats. Ouyang et al. (2024) examine how quantization-induced degradation (QiD) varies across training scales, while Dettmers and Zettlemoyer (2023) study inference-time scaling laws, concluding that 4- and 6-bit models often lie on the Pareto frontier of accuracy and efficiency. 2.1 Review of MX Formats and Experimental Approach Microscaling (MX) formats are a class of low-precision numerical representations designed to enhance the efficiency of deep learning models (Darvish Rouhani et al., 2023a; Rouhani et al., 2023). The idea is conceptually simple as described in Algorithm 1: we represent a block of k values, {Vi}k i 1, using a single shared scale factor X and k corresponding low-precision elements {Pi} where the Pi are obtained by casting Vi X to the specified low-precision format2.\n\n--- Segment 8 ---\n2.1 Review of MX Formats and Experimental Approach Microscaling (MX) formats are a class of low-precision numerical representations designed to enhance the efficiency of deep learning models (Darvish Rouhani et al., 2023a; Rouhani et al., 2023). The idea is conceptually simple as described in Algorithm 1: we represent a block of k values, {Vi}k i 1, using a single shared scale factor X and k corresponding low-precision elements {Pi} where the Pi are obtained by casting Vi X to the specified low-precision format2. The scale X can be calculated using X 2 log2(maxi( Vi )) emax elem with emax elem being the exponent of the largest normal number representable in the chosen element data format. Common element types include 8-bit floating point (FP8) such as E4M3 and E5M2, and 6-bit (FP6) formats like E2M3 and E3M2, typically utilizing an 8-bit exponent (E8M0) for the shared scale. In our experiments, we quantize both weights and activations using these MX formats using the MX Pytorch Emulation Library (Microsoft, 2024). This quantization is applied dynamically to the inputs of matrix multiplication operations (e.g., within Linear, MatMul, BMM layers) across both the forward and backward passes, with results dequantized to a higher precision format (e.g., bfloat16) after the operation. We mainly explore various MX configurations, including the aforementioned 6-bit and 8-bit element formats, and scenarios where MX-formatted tensors are mixed with bfloat16 tensors. We defer a more detailed review of the MX scheme to Appendix A. 3 LLM Experiments 3.1 Setup For our language model experiments, we use OLMo (Groeneveld et al., 2024) in combination with the MX PyTorch Emulation Library (Microsoft, 2024) to enable training under various low-precision configurations. All language models use the GeLU activation function; full hyperparameter details are provided in Table 3. We sweep over a wide range of MX precision formats for both weights and activations, including two FP6 variants (E3M2, E2M3), two FP8 variants (E4M3, E5M2), and a bfloat16 baseline.\n\n--- Segment 9 ---\nAll language models use the GeLU activation function; full hyperparameter details are provided in Table 3. We sweep over a wide range of MX precision formats for both weights and activations, including two FP6 variants (E3M2, E2M3), two FP8 variants (E4M3, E5M2), and a bfloat16 baseline. Each configuration applies full quantization to both forward and backward passes to both weights and activations, as implemented in the Microscaling library (Microsoft, 2024). For each format, we train approximately 70 models3 spanning compute budgets from 2 1017 to 4 1019 FLOPs. Model sizes range from 20M to 1.7B parameters. Token counts are determined using an adapted version of the FLOP accounting code from Brandfonbrener et al. (2024), originally developed for OLMo scaling law experiments. Token-to-parameter ratios in our sweep range from approximately 2 to 156. All models are trained on the Fineweb-Edu dataset Penedo et al. (2024), with the longest runs trained on 35B tokens and the shortest runs corresponding to models trained on 301M tokens. 2We present results for a block size k 32 to match what will be hardware supported, but we also experimented with different choies of k and did not observe qualitatively different results than those presented. 3Some runs crashed and could not always be resumed, leading to small differences in number of models trained for each format. 4 0 20000 40000 60000 80000 Step 1.5 2.0 2.5 3.0 3.5 4.0 4.5 Loss Train CrossEntropyLoss 0 20000 40000 60000 80000 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 25 50 75 100 125 150 175 Ratio bfloat16 weights, bfloat16 activations (a) Train loss and grad norm for weights and activations in bfloat16-bfloat16 format. All runs converge.\n\n--- Segment 10 ---\n4 0 20000 40000 60000 80000 Step 1.5 2.0 2.5 3.0 3.5 4.0 4.5 Loss Train CrossEntropyLoss 0 20000 40000 60000 80000 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 25 50 75 100 125 150 175 Ratio bfloat16 weights, bfloat16 activations (a) Train loss and grad norm for weights and activations in bfloat16-bfloat16 format. All runs converge. 0 10000 20000 30000 40000 50000 60000 Step 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Loss Train CrossEntropyLoss 0 10000 20000 30000 40000 50000 60000 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 25 50 75 100 125 150 175 Ratio MXFP8 (E5M2 weights, E5M2 activations) (b) Train loss and grad norm for weights and activations in MXFP8 E5M2-E5M2. Some runs particularly larger models that are trained for longer become unstable and never recover. Figure 1: Shows stable (bfloat16) OLMo training runs (top) compared to lower precision (MXFP8 E5M2, bottom). The low precision computations are done in both forward and backward steps, on both weights and activations. Color bar on the right shows the token-to-parameter ratio. 3.2 Instabilities in Low Precision Figure 1a shows the training loss and gradient norm trajectories for bfloat16 models. Training remains stable, with smooth convergence and bounded gradients. By contrast, Figure 1b illustrates example instabilities in the MXFP8 E5M2-E5M2 weights-activations configuration, where some training runs exhibit sharp upward spikes in loss and large increases in gradient norm magnitude.\n\n--- Segment 11 ---\nTraining remains stable, with smooth convergence and bounded gradients. By contrast, Figure 1b illustrates example instabilities in the MXFP8 E5M2-E5M2 weights-activations configuration, where some training runs exhibit sharp upward spikes in loss and large increases in gradient norm magnitude. We find these instabilities to be universal across other low-precision MX configurations and hyperparameter settings, as documented in Appendix F. We observe the instabilities mainly occur in larger, longer-trained models and that importantly, when training is destabilized, training does not recover, and the loss continues to diverge. While the loss spikes appear abruptly, the gradient norm typically grows more gradually (bottom right of Figure 1b) and fails to decrease over time as seen in stable bfloat16 training. This behavior strongly suggests systematic errors in gradient computation, a point that we will investigate further in subsequent sections. 4 Synthetic Experiments 4.1 Setup Our language model experiments with OLMo involve many potentially interacting components, and it is challenging to determine exactly where the low-precision failure mode occurs. To better understand the origins of the instabilities, following the logic of Wortsman et al. (2024), we develop a small-scale proxy model. Despite its simplicity, this model exhibits many of the same qualitative behaviors seen in large-scale training. 5 Given an input x Rdmodel, we consider a student network composed of L residual layers indexed by k 0, . . . , L 1. The hidden state at each layer is computed as: A0 x hk W(1) k LN(Ak 1) Ak 0 Ak 1 W(2) k ϕ(hk) , (1) where LN denotes layer normalization and ϕ is the activation function (e.g., ReLU, GeLU, SwiGLU). Each residual block contains two weight matrices: W(1) k projects to the hidden dimension, and W(2) k projects back to dmodel. By default, the hidden size is set to 4dmodel4) The targets are generated by a fixed teacher model whose architecture can be taken to be the same as the student s without the layer normalization (for sweeps where we change the depth and width of the student, we similarly scale the teacher model). A small Gaussian label noise (σ 10 3) is added to the outputs. The inputs x are drawn i.i.d.\n\n--- Segment 12 ---\nA small Gaussian label noise (σ 10 3) is added to the outputs. The inputs x are drawn i.i.d. from a standard Gaussian, without cycling, using a fixed seed to ensure consistent batch order. To isolate the effect of precision, we train two copies of the student model from the same initialization. The first is trained in full precision (FP32). After training, the weights are reset to their initial state and retrained using a low-precision MX format, with quantization applied to both forward and backward passes as described in Section 2.1. Because the initialization, data, and batch order are identical, any behavioral difference is attributable primarily to the change in numerical precision5. All models are trained with an MSE loss. Hyperparameter choices A key point is that there are regions in hyperparameter space for which the model in Equation (1) will give rise to train instabilities (even in FP32 precision). This is not necessarily due to the precision scheme chosen, but rather due to the fact that in any SGD method there exists some small probability of taking wrong gradient step(s). If the size of the steps are large due to, e.g., a high learning rate, this will be visible as a sudden spike in the loss. Our goal is to choose hyperparameters and make architectural choices (such as depth and width) in order to move away from these expected" instabilities and to ones that could be caused by low precision. For the same reason, we fix a moderately large batch size (2048) throughout to reduce variance in gradient estimates. 4.2 Sweeping over learning rates and architectures Learning rates We begin by sweeping over learning rates η (1 10 5, 5 10 5, 1 10 4, 5 10 4, 1 10 3) across a range of model depths and widths, in two low precision formats: (1) MXFP8 E4M3 in the forward pass and MXFP8 E5M2 in the backward pass6, and (2) MXFP6 E4M3 in both forward and backward passes. Results from this sweep are shown in Figure 2. We observe the following patterns: for low learning rates η 1 10 4, all precision formats remain stable. At η 5 10 4, differences between FP32 and lower-precision formats begin to emerge: FP32 exhibits two unstable runs, while FP8 shows six.\n\n--- Segment 13 ---\nWe observe the following patterns: for low learning rates η 1 10 4, all precision formats remain stable. At η 5 10 4, differences between FP32 and lower-precision formats begin to emerge: FP32 exhibits two unstable runs, while FP8 shows six. At the highest learning rate (η 1 10 3), instabilities are observed across all formats, with larger models failing earlier in training. Interestingly, we find that recovery from an instability is more rapid in FP32, whereas instability in lower-precision formats particularly FP6 is often more persistent. We also experimented with a cosine learning rate schedule that starts at 1 10 3 and decays to 1 10 5 and found that the effect of the schedule was mainly to suppress instabilities at later training times, though we still observe the same differences between high and low precision if the instability does not happen late in training. 4In the case of SwiGLU, following Shazeer (2020) we reduce the hidden dimension from 4dmodel to 8 3dmodel to maintain parity in parameter count. 5To qualify this statement, we observe that some minor sources non-determinism remain even after controlling for random seed, batch order, model initialization, using deterministic Pytorch kernels, etc. but these effects are small compared to the typical precision scales we work with. 6We use this asymmetric format to allow greater dynamic range in the backward pass, following Micikevicius et al. (2022a), and because it exhibited marginally greater stability than using E4M3 for both passes. Our results are not sensitive to this particular choice of low-precision formats.\n\n--- Segment 14 ---\n(2022a), and because it exhibited marginally greater stability than using E4M3 for both passes. Our results are not sensitive to this particular choice of low-precision formats. 6 10 3 10 2 10 1 100 101 FP32 MSE loss 1 10 5 5 10 5 1 10 4 5 10 4 1 10 3 10 3 10 2 10 1 100 101 MX FP8 MSE loss 0 2 4 6 10 3 10 2 10 1 100 101 MX FP6 E2M3 MSE loss 0 2 4 6 0 2 4 6 0 2 4 6 0 2 4 6 Training step ( 10³) L 3,W 256 L 3,W 384 L 3,W 512 L 4,W 256 L 4,W 384 L 4,W 512 L 5,W 256 L 5,W 384 L 5,W 512 L 6,W 256 L 6,W 384 L 6,W 512 L 7,W 256 L 7,W 384 L 7,W 512 L 8,W 256 L 8,W 384 L 8,W 512 Figure 2: Comparing FP32 with MXFP6 and MXFP8 formats across different choices for the learning rate. Color corresponds to model size, determined by the depth L and dmodel D on the legend. Effect of Depth and Width In Appendix B, we fix the learning rate to η 5 10 4 and conduct a broader sweep over network depth and width across three MX precision formats. While the number of instability spikes in any individual run is an O(1) integer, aggregating across all runs we observe a general trend: for this fixed learning rate, high-precision training remains stable at larger model sizes than low-precision training. We find that instability differences between high and low precision seem to occur more frequently in networks of intermediate size, for model dimensions in the range 384 dmodel 768 and depths 3 L 6. Intuitively, this makes sense since these models appear to be large enough to exhibit sensitivity to low-precision effects, yet not large enough where overall stochasticity causes generally unstable training at this learning rate. Based on this observation, we fix the learning rate to η 5 10 4 in the remainder of our experiments and zoom in on this intermediate model regime.\n\n--- Segment 15 ---\nIntuitively, this makes sense since these models appear to be large enough to exhibit sensitivity to low-precision effects, yet not large enough where overall stochasticity causes generally unstable training at this learning rate. Based on this observation, we fix the learning rate to η 5 10 4 in the remainder of our experiments and zoom in on this intermediate model regime. 4.3 The Effect of Activation Functions and Layernorms Having identified a hyperparameter regime in which instabilities are more prevalent in low precision than high precision, we next ablate the choice of activation function and the inclusion of layer normalization. In Equation (1), this corresponds to varying ϕ( ) and toggling the presence of LN( ). In Figure 3a, we observe that with layer normalization enabled, both GeLU and SwiGLU activations exhibit instability in low precision, with SwiGLU being significantly more prone to divergence. This is consistent with the findings of Fishman et al. (2024), though our results show that SwiGLU also destabilizes training in high precision, suggesting that it generally increases training stochasticity at least for this architecture. We observe two instabilities in GeLU under low precision that are absent in high precision, one of which recovers quickly and the other persists. When layer normalization is removed (Figure 3b), SwiGLU becomes more stable, while GeLU shows increased variance in high-precision training but not in low precision. For ReLU, the removal of layer norm leads to noisier and sometimes unstable dynamics in high precision but not low precision. We note that the loss improves with the removal of layernorm; this is expected as the teacher network does not contain a layernorm so that student model is able to more accurately represent its outputs. At first glance, these results are perplexing they suggest that layernorm destabilizes low-precison training 7 10 3 10 2 10 1 100 101 FP32 MSE GELU RELU SWIGLU No act.\n\n--- Segment 16 ---\nWe note that the loss improves with the removal of layernorm; this is expected as the teacher network does not contain a layernorm so that student model is able to more accurately represent its outputs. At first glance, these results are perplexing they suggest that layernorm destabilizes low-precison training 7 10 3 10 2 10 1 100 101 FP32 MSE GELU RELU SWIGLU No act. function (only LN) 0 2 4 6 10 3 10 2 10 1 100 101 MX FP8 MSE 0 2 4 6 0 2 4 6 0 2 4 6 Training step ( 10³) L 3,W 256 L 3,W 384 L 3,W 512 L 4,W 256 L 4,W 384 L 4,W 512 L 5,W 256 L 5,W 384 L 5,W 512 (a) Loss curves of different activation functions with the inclusion of layernorm. 10 5 10 4 10 3 10 2 10 1 100 101 FP32 MSE GELU (No LN) RELU (No LN) SWIGLU (No LN) LINEAR (No act. function, no LN) 0 2 4 6 10 5 10 4 10 3 10 2 10 1 100 101 MX FP8 MSE 0 2 4 6 0 2 4 6 0 2 4 6 Training step ( 10³) L 3,W 256 L 3,W 384 L 3,W 512 L 4,W 256 L 4,W 384 L 4,W 512 L 5,W 256 L 5,W 384 L 5,W 512 (b) Loss curves of different activation functions without layernorm. Figure 3: Shows the comparison between full and low precision training across different activation functions, with and without layernorm. while simultaneously stabilizing high-precision training. We will return to this point in Section 6 when we explicate the subtleties of layernorms in block scaling formats. 5 Multiplicative Noise Our synthetic experiments reveal that training instabilities in low-precision settings can arise from both stochastic optimization effects and quantization-induced bias. These failures appear to result from a complex interplay between architectural choices, activation functions, layer normalization, and learning rate. One hypothesis, motivated by the growth of the gradient norm in Figure 1, is that lower precision is systematically biasing the gradient.\n\n--- Segment 17 ---\nThese failures appear to result from a complex interplay between architectural choices, activation functions, layer normalization, and learning rate. One hypothesis, motivated by the growth of the gradient norm in Figure 1, is that lower precision is systematically biasing the gradient. In this section, we examine this hypothesis through a multiplicative noise model and show that it is consistent with the instability patterns seen in low-precision training. 5.1 Behavior of the Noise Let εt egt gt, (2) 8 where gt denotes the exact gradient at time step t, and egt is its low-precision counterpart. Under a multiplicative noise model, we posit that egt (1 ζt) gt, (3) where ζt is a (possibly data and parameter-dependent) noise matrix induced by quantization. Although ζt is not directly measurable (and may not even be uniquely defined e.g., due to weight permutations), we can estimate the magnitude of its effect. Specifically, the deviation vector εt satisfies εt 2 ζt op gt 2, (4) where op denotes the operator norm. In Section 5.2, we argue for a heuristic bound that ζt op must satisfy through training and how a runaway loss divergence may occur in this model. To test this model empirically, we replicate the synthetic experiment setup from Section 4. For each configuration, we fix the random seed and weight initialization, then train one model in FP32 to log the exact gradient gt at each step. We then retrain the same model under MXFP8 precision and compute the deviation εt egt gt at every step. This allows us to extract both the norm ratio εt 2 gt 2 and the cosine similarity between egt and gt. Results are shown in Figure 4. Early in training, the estimate of ζt op (as inferred from Equation (4)) gradually decreases. However, as training progresses, the estimate begins to rise. Once ζt op 2, we observe divergence in the loss. A similar trend is observed in the cosine angle between gradients: it slowly degrades over several thousand steps and eventually flatlines near zero, indicating that the low-precision gradient is no longer aligned with the true descent direction.\n\n--- Segment 18 ---\nOnce ζt op 2, we observe divergence in the loss. A similar trend is observed in the cosine angle between gradients: it slowly degrades over several thousand steps and eventually flatlines near zero, indicating that the low-precision gradient is no longer aligned with the true descent direction. 10 2 10 1 100 FP32 train loss 10 2 10 1 100 101 MX train loss 0 1 2 3 4 5 6 7 1.0 0.5 0.0 0.5 1.0 cos 0 1 2 3 4 5 6 7 10 1 100 101 102 t op Training step ( 10³) L 4,W 256 L 4,W 384 L 4,W 512 L 6,W 512 Figure 4: Shows the bound on the operator norm ζt op (as inferred from Equation (4)), and the cosine angle between the low precision gradient and high precision gradient. Dashed line in the lower right plot shows when the bound on ζt op is equal to 2. 5.2 A Crude Bound To understand the behavior of ζ op, consider that we have some optimum w such that wL(w ) 0. Linearizing around the minimum we have wL(wt) H(wt w ), (5) where H 2 wL is the Hessian. The equation above makes no reference to precision the only approximation we ve made is ignore terms of order (wt w )2 and higher. Defining δt wt w , we then have gt Hδt. (6) With some manipulations the GD update rule is7 δt 1 δt ηt(I ζt)Hδt (7) 7Strictly speaking, we are using the stochastic Adam update rule and not GD in our experiments, and so the resulting bound should not be regarded as rigorous. 9 and so δt 1 (I ηtH)δt ηtζtHδt. (8) We can therefore see that there is a driving term proportional to the noise ζt; if the noise operator norm is large enough, it can flip a contracting direction into an expanding one. The stability criteria is therefore that the operator I ηt(1 ζt)H has spectral radius less than one.\n\n--- Segment 19 ---\n(8) We can therefore see that there is a driving term proportional to the noise ζt; if the noise operator norm is large enough, it can flip a contracting direction into an expanding one. The stability criteria is therefore that the operator I ηt(1 ζt)H has spectral radius less than one. In terms of the maximum eigenvalue of H, λmax, this means that a crude bound for stability is 1 ηtλmax ηt ζt op λmax 1. (9) Clearly, when the norm of ζt grows, the region of stable ηtλmax shrinks. However, from the edge of stability viewpoint of Cohen et al. (2021), in the absence of multiplicative noise, λmax is expected to increase until it hovers at or just above 2 η. Once the multiplicative term ζt is introduced, we may then expect that the stability region defined by Equation (9) contracts. Developing a precise theory for this regime building on the analysis of Jastrzebski et al. (2020); Damian et al. (2023); Cohen et al. (2021) is an interesting direction for future work. In the meantime, we bypass an explicit spectral calculation by estimating a lower bound on ζt op directly in our synthetic experiments through Equation (4). Empirically, we observe a pattern where the running average of this lower bound first drifts downward, later turns upward (lower right of Figure 4). When it stabilizes around 2, training instabilities tend to follow; this observation marks a strong (but not perfect) qualitative correlate of divergence. 6 What can cause the noise? Bound (9) predicts roughly when instability occurs but not why ζt op grows. Typically, instabilities in low precision happen due to over underflow or clamping issues that can bias the gradient. However, in a block scaling format such as MX, it is unclear how such gradient bias may accumulate when the shared scale explicitly puts nearly all values within a representable range. 6.1 Overflow Issues with Layernorms To understand this, we begin by examining a concrete example of MXFP8 E4M3 as specified in Darvish Rouhani et al. (2023a). The left panel of Fig.\n\n--- Segment 20 ---\n(2023a). The left panel of Fig. 5 plots the relative gap (xt 1 xt) xt between successive positive codes in this format, ordered from index 0 (the smallest sub-normal, 2 9) up to index 125 (448). The index stops at 125 (rather than the expected 27 1 127) because S 1111 1112 is reserved for the NaN symbol, which would otherwise correspond to a value of 480, and S 0000 0002 is the zero code, leaving 126 remaining codes (Darvish Rouhani et al., 2023a). We can note the following: 1. For a fixed exponent bin the relative gap starts at 12.5 and decays to 6.6 as the mantissa increases. 2. There is an overflow region (left of Figure 5) when the value becomes larger than the largest representable normal number (448). Typically, these values are clamped down to 448. The latter observation above means that if a block of values lies within a sufficiently small band, these values may end up in the gray overflow region of Figure 5 after dividing by the block scale. For example, from Algorithm 1, for the case of MXFP8 E4M3 which has eelem max 8 the overflow criteria for a given value v within a block with a shared scale X is v X 448 v 0.875 (abs. max within block). (10) This type of overflow region was flagged for the case of narrower MXFP4 format in Tseng et al. (2025). We show that, while MXFP8 E4M3 has a larger dynamic range, the same effect becomes consequential in practice because layernorm affine weights are tightly clustered and particularly susceptible to having all values within a block falling in this range. For example, layernorm weights typically follow log-normal distributions with scale eµ 1 and deviation σ 1, and so a block of weights might look something like [0.89740956, 0.89628334, 0.88358812, 0.88474816, 0.90372837 ... ] 10 which all end up in the overflow region of Figure 5 after dividing by X 2 log2(abs. max) eelem max 2 8. In our experiments, the impact of this effect is shown in the middle plot of Figure 5.\n\n--- Segment 21 ---\nmax) eelem max 2 8. In our experiments, the impact of this effect is shown in the middle plot of Figure 5. In the synthetic case, in some cases, nearly all of the layer norm weights fall within the band required to flow into the last bucket, losing heterogeneity in nearly all blocks when they are clamped to the maximum normal value after scale division. Note that this explains, at least partially, why removing the layernorms stabilized low-precision training in Figure 3b. While a different format, like MXFP8 E5M2 may avoid this issue, the loss of precision from having only two mantissa bits is a different source of bias and still leads to training instabilities. 2 9 2 5 2 3 2 1 1 2 4 8 16 32 64 128 448 0 20 40 60 80 100 120 code index t 0 20 40 60 80 100 relative gap 12.5 6.6 overflow region 448 FP8 E4M3 relative gaps 0.0 0.2 0.4 0.6 0.8 1.0 0.00 0.25 0.50 0.75 1.00 LN overflow frac. LayerNorm Layer 0 Affine Overflow vs. Train Time synthetic LN Olmo FFN LN Olmo Attn LN 0.0 0.2 0.4 0.6 0.8 1.0 10 2 10 1 syn. MX loss 0.0 0.2 0.4 0.6 0.8 1.0 normalized training progress 2 4 OLMo loss 0.0 0.2 0.4 0.6 0.8 1.0 0.000 0.005 0.010 0.015 act. overflow frac. Activation Overflow (Avg. Across All Layers) vs. Train Time synthetic Olmo 0.0 0.2 0.4 0.6 0.8 1.0 10 2 10 1 syn. MX loss 0.0 0.2 0.4 0.6 0.8 1.0 normalized training progress 2 4 OLMo loss Figure 5: Left: relative gap (xt 1 xt) xt for successive positive FP8 E4M3 codes (sign bit stripped). Within each exponent band the gap decays from 12.5 to 6.6 ; the hatched region marks values that would be clamped once the scaled magnitude exceeds the representable limit of 448.\n\n--- Segment 22 ---\nMX loss 0.0 0.2 0.4 0.6 0.8 1.0 normalized training progress 2 4 OLMo loss Figure 5: Left: relative gap (xt 1 xt) xt for successive positive FP8 E4M3 codes (sign bit stripped). Within each exponent band the gap decays from 12.5 to 6.6 ; the hatched region marks values that would be clamped once the scaled magnitude exceeds the representable limit of 448. Center: Top subplot shows what fraction of layernorm affine parameters end up in the last quantization bin after division of the shared scale in the first layer of the network. For OLMo, we look at the FFN layernorm and the attention layernorm. The synthetic loss in this case exhibits a divergence in MX precision (but is stable in FP32 precision), and corresponds to the student-teacher setup of Equation (1) with four layers and dmodel 512 and η 6 10 4. Right: Shows the fraction of activation values that end up in the last quantization bin after division by the shared scale. We average across all layers for both synthetic and OLMo runs. In OLMo, there are a large number of layernorms which experience different degrees of clamping to the last quantization bin. Some components, such as the attention layernorms, remain relatively well behaved throughout training, whereas others like the FFN layernorms or the QK layernorms (Henry et al., 2020) can experience large, sudden overflow issues. While it s possible to disable the affine transformation of layernorms, this greatly diminishes performance in the language model setting. More broadly, this issue indicates a problem with applying shared-scales to blocks of weights that follow approximately log-normal distributions (such as layernorm affine parameters), which may not have a well-defined notion of a max" relative to a resolution fixed by a given precision scheme. A scale that adapts to both min and max might avoid the bias; we defer this to future work. On the activation side, we find that this effect does affect roughly 1 of values in our synthetic experiments and 0.5 of values in OLMo (shown in the right subplot of Figure 5).\n\n--- Segment 23 ---\nA scale that adapts to both min and max might avoid the bias; we defer this to future work. On the activation side, we find that this effect does affect roughly 1 of values in our synthetic experiments and 0.5 of values in OLMo (shown in the right subplot of Figure 5). 6.2 Potential Mitigations Given these observations, we next ask whether training instabilities can be mitigated by modifying the quantization scheme, for example by increasing the precision of activation LN elements or by restricting quantization to the forward pass only. To investigate this, we perform a sweep over model sizes comparing three setups: (1) full quantization in both forward and backward passes (baseline), (2) forward-pass-only quantization, and (3) higher-precision activation formats (including layernorms) in the MX scheme. As shown in Figure 6, both mitigation strategies do improve stability. Each reduces the number of divergent runs to 2, down from 6 in the fully quantized baseline. One of the instability observed in the forward-only setting also occurs in high-precision training, leaving only one instability unique to the MXFP8 E4M3 baseline unmitigated. 11 0 1 2 3 4 5 6 7 10 3 10 2 10 1 100 101 MSE loss FP32 (skyline) 0 1 2 3 4 5 6 7 MXFP8 quantize fwd backward 0 1 2 3 4 5 6 7 MXFP8 quantize fwd only (backward in fp32) 0 1 2 3 4 5 6 7 MXFP8 quantize fwd (act. backward in bf16) Training step ( 10³) L 2, W 256 L 2, W 384 L 2, W 512 L 3, W 256 L 3, W 384 L 3, W 512 L 4, W 256 L 4, W 384 L 4, W 512 L 5, W 256 L 5, W 384 L 5, W 512 L 6, W 256 L 6, W 384 L 6, W 512 Figure 6: Shows two mitigations (quantization of only forward pass) and activation elements in higher precision, compared to fully quantized baseline and FP32 skyline. 0 2000 4000 6000 8000 training step 10 3 10 2 10 1 train loss intervene well before instability Intervention at Step 4500 MX FP8 E4M3 (orig.\n\n--- Segment 24 ---\nbackward in bf16) Training step ( 10³) L 2, W 256 L 2, W 384 L 2, W 512 L 3, W 256 L 3, W 384 L 3, W 512 L 4, W 256 L 4, W 384 L 4, W 512 L 5, W 256 L 5, W 384 L 5, W 512 L 6, W 256 L 6, W 384 L 6, W 512 Figure 6: Shows two mitigations (quantization of only forward pass) and activation elements in higher precision, compared to fully quantized baseline and FP32 skyline. 0 2000 4000 6000 8000 training step 10 3 10 2 10 1 train loss intervene well before instability Intervention at Step 4500 MX FP8 E4M3 (orig. instability) FP32 baseline All FP32 (no MX ops) Bump overflow exponent No LayerNorm quant No back-prop quant BF16 weights BF16 acts fwd-only BF16 acts (all) 0 2000 4000 6000 8000 training step intervene just before instability Intervention at Step 5080 MX FP8 E4M3 (orig. instability) FP32 baseline All FP32 (no MX ops) Bump overflow exponent No LayerNorm quant No back-prop quant BF16 weights BF16 acts fwd-only BF16 acts (all) Figure 7: Intervention experiment for a synthetic student-teacher model with dmodel 512, four layers, and learning rate η 6 10 4. Training is stable in FP32 (blue) but diverges in MXFP8 E4M3 (yellow) around step 5100. We test two intervention timings: step 4500 (left, well before instability) and step 5080 (right, just before instability). Early interventions, like disabling backward-pass quantization or switching to high-precision (FP32), successfully prevent divergence, while using high precision for the activations (bfloat16) can greatly delay it. Late interventions cannot avert instability but can only delay it; the most effective are switching to FP32 or skipping quantization of layernorm weights. We can actually go further and ask whether an impending divergence can be averted by in-situ interventions to the training recipe. Figure 7 tracks a configuration that is stable in FP32 but diverges in MXFP8 E4M3. This setting corresponds to the previously described student-teacher scenario with four layers and model dimension dmodel 512.\n\n--- Segment 25 ---\nFigure 7 tracks a configuration that is stable in FP32 but diverges in MXFP8 E4M3. This setting corresponds to the previously described student-teacher scenario with four layers and model dimension dmodel 512. The instability starts approximately at step 5090 and we consider interventions just before the instability at step 5080 and well before the instability at step 4500. For each intervention we keep the random seed, model state, and batch sequence identical, so the training state at the intervention step is the same as in the baseline run; any divergence afterward is therefore attributable to the intervention. Switching entirely to FP32 precision for remaining training steps. Implementing FP32 significantly stabilizes training if the change is made sufficiently early (step 4500), but it is ineffective if applied immediately before instability (step 5080). However, even at the later intervention, FP32 prolongs training stability more effectively than the other approaches. Increasing the shared exponent by one (bumping exponent). Adjusting the exponent to avoid the last bucket overflow for blocks that have values that fall into the range in Equa- tion (10) does not mitigate instability, possibly due to insufficient precision improvement from a single increment. Avoiding MX quantization for LayerNorm affine parameters. Omitting quantization of these parameters partially stabilizes training and delays instability significantly at both inter- vention steps, indicating that LayerNorm parameters do contribute to instability dynamics. However, eventual instability suggests a residual effect from quantized activations. Precision adjustments in forward and backward passes, where we explored: 12 quantizing weights and activations only during the forward pass (no backward-pass quantization); maintaining weights in bfloat16 and activations in MXFP8 (both passes); maintaining activations in bfloat16 for the forward pass but MXFP8 for backward (with MXFP8 weights); using BF16 activations for both forward and backward passes while quantizing weights with MXFP8. Among these, applying the intervention just before instability (step 5080), bfloat16 activation precision in both passes consistently provides the strongest immediate stabilization, closely followed by disabling backward-pass quantization. When interventions occur earlier (step 4500), not quantizing the backward step performs comparably to the FP32 baseline, while fully bfloat16 activations delay instability considerably yet eventually become unstable. These results are consistent with a stochastic noise model, suggesting that multiple inter- acting factors influence instability likelihood.\n\n--- Segment 26 ---\nWhen interventions occur earlier (step 4500), not quantizing the backward step performs comparably to the FP32 baseline, while fully bfloat16 activations delay instability considerably yet eventually become unstable. These results are consistent with a stochastic noise model, suggesting that multiple inter- acting factors influence instability likelihood. They are also consistent with our earlier observations about the effectiveness of higher activation precision and forward-only quanti- zation schemes in mitigating instability in MXFP8 training. The mixed weight activation precision strategy may be a pragmatic approach, as long as careful attention is given to the behavior of layernorm weights throughout training. Other Sweeps In Appendix B, we report additional ablations over optimizer choices (SGD with and without momentum, and Adam) and weight initialization schemes with reduced variance. While these variations can partially reduce the frequency of instabilities, they do not address the underlying bias in the computation. As we show in Section 5 and in Section 6.1, many instabilities stem from biased gradient estimates, and these choices do not reliably eliminate that source of error. Key Takewaways Our synthetic experiments reveal that training instabilities in block-scaled low- precision settings arise from both stochastic optimization effects and quantization-induced bias. These failures result from a complex and subtle interplay between architectural choices, activation quantization, quantization of layer normalization parameters, and learning rate. The dominant precision-specific bias comes from overflow of tightly clustered layer-norm affine weights (and a small fraction of activations). Our intervention experiments show that while simple tweaks such as nudging the shared exponent do not remove this bias, mitigations which raise precision in key parts of the computation such as increasing the precision of activations or not quantizing in the backward step generally improve stability. 7 Stabilization Strategies in Language Model Setting Motivated by the effective mitigations observed in our synthetic experiments, we return to the language model (OLMo) setting and apply the same two strategies: (1) retaining bfloat16 as the element format for activations and layer-norms, and (2) applying MX quantization only to the forward pass. In both cases, we find that training remains stable across all FP8 configurations, including E4M3 and E5M2 formats.\n\n--- Segment 27 ---\n7 Stabilization Strategies in Language Model Setting Motivated by the effective mitigations observed in our synthetic experiments, we return to the language model (OLMo) setting and apply the same two strategies: (1) retaining bfloat16 as the element format for activations and layer-norms, and (2) applying MX quantization only to the forward pass. In both cases, we find that training remains stable across all FP8 configurations, including E4M3 and E5M2 formats. Weight Activation D N Ratio 140.96 99.19 70.91 37.86 21.28 16.23 12.51 N 0.16B N 0.19B N 0.23B N 0.31B N 0.42B N 0.48B N 0.54B bfloat16 bfloat16 0.710 0.703 0.698 0.691 0.688 0.686 0.686 MXFP8 E4M3 bfloat16 0.0 -0.002 -0.002 0.0 0.0 0.0 0.0 MXFP8 E5M2 bfloat16 0.105 0.107 0.112 0.004 0.002 -0.001 -0.001 MXFP8 E4M3 MXFP8 E4M3 0.005 0.002 0.002 0.004 0.002 -0.001 -0.001 MXFP8 E5M2 MXFP8 E5M2 0.010 0.012 0.057 0.019 0.007 0.004 0.004 Table 1: The validation loss on Fineweb-Edu of high precision runs versus low precision with mitigations applied (values are shown as differences with respect to bfloat16 baseline; lower is better). For the last two rows, we quantize only the forward pass. 13 Table 1 reports validation loss differences relative to full-bfloat16 baselines. MXFP8 E4M3 weights paired with bfloat16 activations in particular match full-precision performance across all tested model sizes.As a proof of concept, Figure 8 shows a Chinchilla-style scaling law fit to the stabilized MXFP8 E4M3 bfloat16 runs. This demonstrates that, at least at the scales we trained, valid empirical scaling laws can still be extracted under this hybrid format.\n\n--- Segment 28 ---\nMXFP8 E4M3 weights paired with bfloat16 activations in particular match full-precision performance across all tested model sizes.As a proof of concept, Figure 8 shows a Chinchilla-style scaling law fit to the stabilized MXFP8 E4M3 bfloat16 runs. This demonstrates that, at least at the scales we trained, valid empirical scaling laws can still be extracted under this hybrid format. Whether this continues to hold at larger scales remains an open question, as such lower precision may exhibit bottoming-out effects or other nonlinearities in training dynamics. Full loss curves and scaling law fits for both mitigation strategies compared to bfloat16 baselines are provided in Appendix C. 8 Conclusion We have shown that training large language models in block-scaled low-precision formats (MXFP8, MXFP6) often leads to sharp, unrecoverable instabilities. By combining large-scale LLM sweeps with a controlled student-teacher proxy model trained on synthetic data, we isolate two distinct failure modes: Stochastic optimization breakdown, driven by a complex interplay between hyperparameters choices (e.g. high learning rates) or architectural factors (such as width, depth, choice of activation function), can alone can trigger instabilities even in high precision. Quantization-induced gradient bias, where shared-scale clamping (particularly of layer-norm affine weights and to a lesser extent, other activations) injects multiplicative gradient noise that ultimately destabilizes training. Guided by these observations, we identify two effective mitigation strategies: maintaining higher precision for activations or limiting quantization exclusively to the forward pass. Notably, we find that using MXFP8 E4M3 weights in combination with bfloat16 activations matches the performance of full-bfloat16 baselines. While our implementation applied higher-precision activations network-wide, it is likely that stability could be preserved using higher precision selectively in key layers or modules. Looking ahead, continued hardware advances will expand the frontier of what s computationally feasible.\n\n--- Segment 29 ---\nWhile our implementation applied higher-precision activations network-wide, it is likely that stability could be preserved using higher precision selectively in key layers or modules. Looking ahead, continued hardware advances will expand the frontier of what s computationally feasible. Some concrete directions for future research include: extending our proxy model to include attention mechanisms, mixture-of-experts with many layers, and other transformer-specific components to better predict instabilities in state-of- the-art architectures; developing a clear theoretical picture of the stochastic noise model and how different factors can amplify or reduce the risk of a training instability; designing new blockwise scaling schemes that adapt to skewed or tightly clustered distribu- tions. 14 1017 1018 1019 1020 1021 FLOPs 107 108 109 Parameters Weight: MXFP8 E4M3, Act: bfloat16 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 Train Loss (a) Scaling law fit for FP8 E4M3-bfloat16. 1017 1018 1019 1020 1021 FLOPs 107 108 109 Parameters Weight: MXFP8 E5M2, Act: bfloat16 0.7 0.8 0.9 1.0 1.1 Train Loss (b) Scaling law fit for FP8 E5M2-bfloat16. Figure 8: Scaling law fit for combinations of precision formats of weights and keep the activations in high precision. Fit was calculated using a Chinchilla model for the loss; details and fit parameters are given in Appendix C. 15 Limitations In this work, we provide insights into when instabilities arise in student-teacher training setups using a residual MLP, and show that these findings generalize to more complex architectures. Specifically, we identify sources of training instability such as batch size, batch quality, and learning rate that affect both high- and low-precision regimes. We also highlight instability factors specific to low-precision training, including sensitivity to learning rate, model depth and width, and the choice of activation function (though not exhaustively explored, they represent the primary contributors). To mitigate these instabilities in full-scale models, we propose retaining gradient computations and or activation functions in full precision.\n\n--- Segment 30 ---\nWe also highlight instability factors specific to low-precision training, including sensitivity to learning rate, model depth and width, and the choice of activation function (though not exhaustively explored, they represent the primary contributors). To mitigate these instabilities in full-scale models, we propose retaining gradient computations and or activation functions in full precision. These approaches stabilize training and enable us to fit precision-aware scaling laws, specifically capturing how validation performance scales with model size and number of training tokens. However, there are many interactions in model training and we cannot exhaustively cover them all in our sweeps. It is likely that there exist other mitigation strategies not evaluated in this work. We emphasize, however, that a core contribution of this paper is to propose simplified proxies for reasoning about complex low-precision dynamics, rather than prescribe a single universal fix. Our results are primarily derived from decoder-only language models and residual MLPs; future work is needed to determine whether similar behaviors arise in MoE architectures. Finally, while we emulate MX formats in PyTorch, all experiments are conducted in software. Real-world deployment on Blackwell-class hardware may introduce additional sources of error due to rounding behavior, memory layout, or fused kernel execution not captured by our implementation. We also performed most of our experiments on relatively small models; our synthetic models run on NVIDIA H100s in a matter of minutes, and the largest language models used 16 H100s per run. While this allowed us to reach the 1B scale, results may not extrapolate well to models that are an order of magnitude larger. 16 A Review of Shared-Scale Quantization In this section we provide a self-contained review of block scaling quantization schemes, largely following Rouhani et al. (2023); Darvish Rouhani et al. (2023a). Taking a step back, the idea in shared-scale quantization methods is to introduce a number which represents the shared scale among a group of values that could, e.g., represent weights or activations. The idea is that low-precision data types tend to have a small representable range and quantization can clip very large values or zero-out smaller values. By dividing by the shared scale, the goal is to put these numbers in a representable range and save the scale such that it may be multiplied at the end of the computation. There are many choices for how to pick the scale, with pros and cons for each.\n\n--- Segment 31 ---\nBy dividing by the shared scale, the goal is to put these numbers in a representable range and save the scale such that it may be multiplied at the end of the computation. There are many choices for how to pick the scale, with pros and cons for each. For example, one approach is to have a single scale factor for the entire tensor, which has a very low memory overhead but is usually too coarse-grained and lead to saturation issues. On the opposite end, one could keep a scale factor for every value in the tensor which obviously allows for higher accuracy but involves much more memory. Other approaches include tilewise scaling, where a scale factor is used for a fixed-size submatrix. This was the approach taken in Liu et al. (2024). In this work, we focus on block scaling methods, where a single 1-dimensional block of values shares a scale. In particular, we focus on the microscaling" (MX) format, where each block consists of 32 values, with a shared scale that can be computed using Algorithm 1. When performing matrix multiplications or dot products, these shared scales are carried around and multiplied at the end of the computation (see Darvish Rouhani et al. (2023a) for the exact specifications). Algorithm 1 Convert V HP_DTYPEk to an MX block {X, P LP_DTYPEk} Require: k 32 (hardware block size), 1: eelem max exponent of the largest normal value in LP_DTYPE Ensure: Scale factor X and low-precision elements P1, . . . , Pk 2: m maxi Vi 3: shared_exp log2(m) eelem max 4: X 2shared_exp block scale (a power of two) 5: for i 1 to k do 6: r Vi X 7: Pi QUANTIZETOLP(r) clamp if r overflows 8: end for 9: return (X, {Pi}k i 1) The shared scale in MX formats can therefore be regarded as the largest power-of-two that can represent the maximum within a block, shifted by the exponent of the largest normal value in that type. What gets quantized in a typical training setup? There are several toggles. Let us illustrate by a simple example i.e.\n\n--- Segment 32 ---\nThere are several toggles. Let us illustrate by a simple example i.e. our synthetic model in Equation (1) when there is only one layer: A0 x h W(1)LN(x) A1 x W(2)ϕ(h) . (11) Roughly speaking, we can choose to apply MX quantization to weights, activations in the forward and or backward pass. In the forward pass, we can apply Algorithm 1 at every stage. Ife denotes the quantization resulting from Algorithm 1 and perform matmuls using the shared scale, then if we quantize both weights and activations we compute A0 x h f W(1)LN(ex) A1 x f W(2)ϕ(eh). (12) One additional subtlety is that vector operations such as vector addition such as those present in layernorm computations are typically carried out in bfloat16, i.e. the operands are first cast once to bfloat16, and the addition itself runs in bfloat16. Meanwhile, in the backward pass, we can actually quantize in three different ways (or not at all if we turn off quantization in backpropagation): quantize weights, quantize input activation gradients, or quantize output activation gradients. Suppose we want to compute W(1)L with the MSE loss L 1 2(A1 y )2. This means computing L A1 A1 W(1) . The term L A1 can be quantized in one format the output gradient quantization, while the second 17 term A1 W(1) can be quantized in a separate format the input gradient quantization. In general, in this work, unless we state otherwise, these two formats are taken to be the same. In backpropagating, the input gradient itself will involve computing ϕ h. Again, the exterior gradient ϕ h h W (1) is quantized according to its chosen format. The second piece, h W(1) involves the input to the linear layer LN(x) which is quantized to the input gradient format. B Additional Synthetic Sweeps In this section, we present additional synthetic experiments to further examine the sources and mitigation of low-precision instabilities.\n\n--- Segment 33 ---\nThe second piece, h W(1) involves the input to the linear layer LN(x) which is quantized to the input gradient format. B Additional Synthetic Sweeps In this section, we present additional synthetic experiments to further examine the sources and mitigation of low-precision instabilities. Figure 9 summarizes the frequency of instability spikes across our depth-width sweep at a fixed learning rate of η 5 10 4. The MX-mix format refers to the asymmetric configuration using MXFP8 E4M3 in the forward pass and E5M2 in the backward pass. Spikes were determined by the heuristic criteria that the loss at time step t had to be a factor of 100 lager than the loss at time step t 1; this gives a rough lower bound on the number of spikes. Figure 10 compares the impact of optimizer choice, focusing on SGD with momentum, and vanilla SGD (momentum 0). These experiments used a slightly higher learning rate of η 1 10 2 to exaggerate differences. Compared with Figure 2, we observe that SGD variants are more stable than Adam, perhaps due to Adam s use of second-moment accumulation, which may amplify quantization- induced bias in low-precision regimes. Figure 11 evaluates the effect of different weight initialization schemes. We compare standard Pytorch initialization, typically taken to be a Kaiming uniform distribution between [ 1 fan in, 1 fan in], against a variant using lower gain (gain 0.5) under the Xavier normal distribution. Reducing the variance of initial weights appears to improve loss spikes.\n\n--- Segment 34 ---\nWe compare standard Pytorch initialization, typically taken to be a Kaiming uniform distribution between [ 1 fan in, 1 fan in], against a variant using lower gain (gain 0.5) under the Xavier normal distribution. Reducing the variance of initial weights appears to improve loss spikes. 64 96 128 256 384 512 768 896 1024 Width 1 2 3 4 5 6 7 8 Depth 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 2 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 FP32 64 96 128 256 384 512 768 896 1024 Width 1 2 3 4 5 6 7 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 2 1 1 2 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 2 3 2 0 0 0 0 0 2 0 1 1 MX-mix 64 96 128 256 384 512 768 896 1024 Width 1 2 3 4 5 6 7 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 2 1 0 0 0 0 0 1 1 1 2 0 0 0 0 3 0 1 2 1 MX-e4m3 64 96 128 256 384 512 768 896 1024 Width 1 2 3 4 5 6 7 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 2 0 0 0 0 0 1 1 2 0 0 0 0 0 1 1 1 1 1 MX-fp6 0.0 0.5 1.0 1.5 2.0 2.5 3.0 spikes Figure 9: Instability spikes measured in training, for different model depths and widths.\n\n--- Segment 35 ---\nReducing the variance of initial weights appears to improve loss spikes. 64 96 128 256 384 512 768 896 1024 Width 1 2 3 4 5 6 7 8 Depth 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 2 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 FP32 64 96 128 256 384 512 768 896 1024 Width 1 2 3 4 5 6 7 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 2 1 1 2 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 2 3 2 0 0 0 0 0 2 0 1 1 MX-mix 64 96 128 256 384 512 768 896 1024 Width 1 2 3 4 5 6 7 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 2 1 0 0 0 0 0 1 1 1 2 0 0 0 0 3 0 1 2 1 MX-e4m3 64 96 128 256 384 512 768 896 1024 Width 1 2 3 4 5 6 7 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 2 0 0 0 0 0 1 1 2 0 0 0 0 0 1 1 1 1 1 MX-fp6 0.0 0.5 1.0 1.5 2.0 2.5 3.0 spikes Figure 9: Instability spikes measured in training, for different model depths and widths. 0 1 2 3 4 5 6 7 10 3 10 2 10 1 100 101 MSE loss FP32 w SGD (momentum 0) 0 1 2 3 4 5 6 7 FP32 w SGD (momentum 0.9) 0 1 2 3 4 5 6 7 MXFP8 w SGD (momentum 0) 0 1 2 3 4 5 6 7 MXFP8 w SGD (momentum 0.9) Training step ( 10³) L 3, W 256 L 3, W 384 L 3, W 512 L 4, W 256 L 4, W 384 L 4, W 512 L 5, W 256 L 5, W 384 L 5, W 512 Figure 10: SGD with and without momentum; a larger learning rate was used η 1 10 2.\n\n--- Segment 36 ---\n64 96 128 256 384 512 768 896 1024 Width 1 2 3 4 5 6 7 8 Depth 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 2 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 FP32 64 96 128 256 384 512 768 896 1024 Width 1 2 3 4 5 6 7 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 2 1 1 2 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 2 3 2 0 0 0 0 0 2 0 1 1 MX-mix 64 96 128 256 384 512 768 896 1024 Width 1 2 3 4 5 6 7 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 2 1 0 0 0 0 0 1 1 1 2 0 0 0 0 3 0 1 2 1 MX-e4m3 64 96 128 256 384 512 768 896 1024 Width 1 2 3 4 5 6 7 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 2 0 0 0 0 0 1 1 2 0 0 0 0 0 1 1 1 1 1 MX-fp6 0.0 0.5 1.0 1.5 2.0 2.5 3.0 spikes Figure 9: Instability spikes measured in training, for different model depths and widths. 0 1 2 3 4 5 6 7 10 3 10 2 10 1 100 101 MSE loss FP32 w SGD (momentum 0) 0 1 2 3 4 5 6 7 FP32 w SGD (momentum 0.9) 0 1 2 3 4 5 6 7 MXFP8 w SGD (momentum 0) 0 1 2 3 4 5 6 7 MXFP8 w SGD (momentum 0.9) Training step ( 10³) L 3, W 256 L 3, W 384 L 3, W 512 L 4, W 256 L 4, W 384 L 4, W 512 L 5, W 256 L 5, W 384 L 5, W 512 Figure 10: SGD with and without momentum; a larger learning rate was used η 1 10 2. 18 0 1 2 3 4 5 6 7 10 3 10 2 10 1 100 101 MSE loss FP32 baseline (default init) 0 1 2 3 4 5 6 7 FP32 w lower gain Xavier normal 0 1 2 3 4 5 6 7 MXFP8 (default init) 0 1 2 3 4 5 6 7 MXFP8 w lower gain Xavier normal Training step ( 10³) L 2, W 256 L 2, W 384 L 2, W 512 L 3, W 256 L 3, W 384 L 3, W 512 L 4, W 256 L 4, W 384 L 4, W 512 L 5, W 256 L 5, W 384 L 5, W 512 L 6, W 256 L 6, W 384 L 6, W 512 Figure 11: Baseline versus using a lower gain Xavier normal weight initialization.\n\n--- Segment 37 ---\n0 1 2 3 4 5 6 7 10 3 10 2 10 1 100 101 MSE loss FP32 w SGD (momentum 0) 0 1 2 3 4 5 6 7 FP32 w SGD (momentum 0.9) 0 1 2 3 4 5 6 7 MXFP8 w SGD (momentum 0) 0 1 2 3 4 5 6 7 MXFP8 w SGD (momentum 0.9) Training step ( 10³) L 3, W 256 L 3, W 384 L 3, W 512 L 4, W 256 L 4, W 384 L 4, W 512 L 5, W 256 L 5, W 384 L 5, W 512 Figure 10: SGD with and without momentum; a larger learning rate was used η 1 10 2. 18 0 1 2 3 4 5 6 7 10 3 10 2 10 1 100 101 MSE loss FP32 baseline (default init) 0 1 2 3 4 5 6 7 FP32 w lower gain Xavier normal 0 1 2 3 4 5 6 7 MXFP8 (default init) 0 1 2 3 4 5 6 7 MXFP8 w lower gain Xavier normal Training step ( 10³) L 2, W 256 L 2, W 384 L 2, W 512 L 3, W 256 L 3, W 384 L 3, W 512 L 4, W 256 L 4, W 384 L 4, W 512 L 5, W 256 L 5, W 384 L 5, W 512 L 6, W 256 L 6, W 384 L 6, W 512 Figure 11: Baseline versus using a lower gain Xavier normal weight initialization. 19 C Scaling Law Fits and Loss Curves after Mitigation In addition to Figure 8 we provide scaling law for the mitigation where we quantize only the forward pass; this is shown in Figure 12 which can be compared against the bfloat16 baseline in Figure 13. Scaling law fits were performed using the methods described in Hoffmann et al. (2022); Brandfonbrener et al. (2024) where the validation loss was fit with a functional form L(N, D) E A N α B Dβ , (13) for constants A, B, E, α, and β. The fitted values of these constants are given in Table 2. We also provide the loss curves after implementing these mitigation strategies; these are shown in Figure 14 and Figure 15.\n\n--- Segment 38 ---\nThe fitted values of these constants are given in Table 2. We also provide the loss curves after implementing these mitigation strategies; these are shown in Figure 14 and Figure 15. 1017 1018 1019 1020 1021 FLOPs 107 108 109 Parameters Weight: MXFP8 E4M3, Act: MXFP8 E4M3, Quantize Forward Only 0.7 0.8 0.9 1.0 1.1 Train Loss (a) Scaling law fit for MXFP8-FP8 E4M3. 1017 1018 1019 1020 1021 FLOPs 107 108 109 Parameters Weight: MXFP8 E5M2, Act: MXFP8 E5M2, Quantize Forward Only 0.7 0.8 0.9 1.0 1.1 Train Loss (b) Scaling law fit for MXFP8 E5M2-E5M2. Figure 12: Scaling law fits for fixed stable of precision formats of weights and activations quantizing only the forward pass. 1017 1018 1019 1020 1021 FLOPs 107 108 109 Parameters Weight: bfloat16, Act: bfloat16 0.7 0.8 0.9 1.0 1.1 Train Loss (a) Scaling law fit for bfloat16-bfloat16. 1017 1018 1019 1020 1021 FLOPs 107 108 109 Parameters Weight: MXFP6 E2M3, Act: MXFP6 E2M3 0.7 0.8 0.9 1.0 1.1 Train Loss (b) Scaling law fit for FP6 E2M3-FP6 E2M3. Figure 13: Scaling law fits for bfloat16-bfloat16 (baseline) and for MXFP6 format.\n\n--- Segment 39 ---\n1017 1018 1019 1020 1021 FLOPs 107 108 109 Parameters Weight: MXFP6 E2M3, Act: MXFP6 E2M3 0.7 0.8 0.9 1.0 1.1 Train Loss (b) Scaling law fit for FP6 E2M3-FP6 E2M3. Figure 13: Scaling law fits for bfloat16-bfloat16 (baseline) and for MXFP6 format. 20 Weight Activation A B E α β a MXFP6 E2M3 bfloat16 1.84e 03 8.77e 03 0.52 0.50 0.51 0.51 MXFP8 E4M3 bfloat16 2.82e 03 2.04e 04 0.54 0.52 0.55 0.51 MXFP8 E5M2 bfloat16 1.68e 03 1.84e 04 0.52 0.49 0.55 0.53 bfloat16 bfloat16 1.94e 03 2.18e 04 0.53 0.50 0.56 0.53 MXFP8 E4M3 MXFP8 E4M3 1.57e 03 2.11e 04 0.52 0.49 0.55 0.53 MXFP8 E5M2 MXFP8 E5M2 2.20e 03 3.98e 04 0.54 0.51 0.59 0.54 Table 2: Fitted scaling law parameters. For the last two rows, we quantize only the forward pass. The last column is equal to the ratio a β (α β), the exponent of the optimal model size relative to FLOPs.\n\n--- Segment 40 ---\nFor the last two rows, we quantize only the forward pass. The last column is equal to the ratio a β (α β), the exponent of the optimal model size relative to FLOPs. 0 20000 40000 60000 80000 Step 1.5 2.0 2.5 3.0 3.5 4.0 4.5 Loss Train CrossEntropyLoss 0 20000 40000 60000 80000 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 25 50 75 100 125 150 175 Ratio MXFP8 (E4M3 weights, E4M3 activations) Quantize Forward Only (a) Train loss and gradient norm for weights: MXFP8 E4M3, Activation: MXFP8 E4M3 while quantizing only the forward pass. 0 20000 40000 60000 80000 100000 120000 Step 1.5 2.0 2.5 3.0 3.5 4.0 4.5 Loss Train CrossEntropyLoss 0 20000 40000 60000 80000 100000 120000 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 25 50 75 100 125 150 175 Ratio MXFP8 (E5M2 weights, E5M2 activations) Quantize Forward Only (b) Train loss and gradient norm for weights: MXFP8 E5M2, Activation: MXFP8 E5M2. Figure 14: Train loss and gradient norm when quantizing only the forward pass. 21 0 20000 40000 60000 80000 Step 1.5 2.0 2.5 3.0 3.5 4.0 4.5 Loss Train CrossEntropyLoss 0 20000 40000 60000 80000 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 25 50 75 100 125 150 175 Ratio MXFP8 E4M3 weights, bfloat16 activations (a) Train loss and gradient norm for MXFP8 E4M3-MXFP8 E4M3.\n\n--- Segment 41 ---\nFigure 14: Train loss and gradient norm when quantizing only the forward pass. 21 0 20000 40000 60000 80000 Step 1.5 2.0 2.5 3.0 3.5 4.0 4.5 Loss Train CrossEntropyLoss 0 20000 40000 60000 80000 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 25 50 75 100 125 150 175 Ratio MXFP8 E4M3 weights, bfloat16 activations (a) Train loss and gradient norm for MXFP8 E4M3-MXFP8 E4M3. 0 20000 40000 60000 80000 100000 120000 140000 Step 1.5 2.0 2.5 3.0 3.5 4.0 4.5 Loss Train CrossEntropyLoss 0 20000 40000 60000 80000 100000 120000 140000 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 25 50 75 100 125 150 175 Ratio MXFP8 E5M2 weights, bfloat16 activations (b) Train loss and gradient norm for MXFP8 E5M2-MXFP8 E5M2. Figure 15: Train loss and gradient norm when activations are kept in high precision (bfloat16). 22 D Details of Language Model Training All models are trained on the Fineweb-Edu dataset (Penedo et al., 2024) using the Olmo code- base (Groeneveld et al., 2024), with the longest runs trained on 35B tokens and the shortest runs corresponding to models trained on 301M tokens. Models were trained with a learning rate schedule with a linear warmup starting at 2e-5 increasing to 2e-4, followed by cosine decay back to 2e-5 (Porian et al., 2025). Training runs that involved using MX precision formats were done performed using MX Pytorch Emulation Library (Microsoft, 2024).\n\n--- Segment 42 ---\nModels were trained with a learning rate schedule with a linear warmup starting at 2e-5 increasing to 2e-4, followed by cosine decay back to 2e-5 (Porian et al., 2025). Training runs that involved using MX precision formats were done performed using MX Pytorch Emulation Library (Microsoft, 2024). Parameter Value n 6 24 for small models Number of heads n Head dimension 64 MLP hidden multiplier 4 Depth n Context length 512 Activation GeLU Positional encoding RoPE Biases False Normalization PyTorch Layernorm QK normalization True Tokenizer Llama2 Table 3: Model parameters used for training. 23 E Validation Losses in Language Models Quantizing Table 4 and continued in 5 shows validation losses for all models with mitigations applied (quantiza- tion only in the forward pass, or activations in high precision), trained using our at different FLOP budgets relative to bfloat16 baseline.\n\n--- Segment 43 ---\nParameter Value n 6 24 for small models Number of heads n Head dimension 64 MLP hidden multiplier 4 Depth n Context length 512 Activation GeLU Positional encoding RoPE Biases False Normalization PyTorch Layernorm QK normalization True Tokenizer Llama2 Table 3: Model parameters used for training. 23 E Validation Losses in Language Models Quantizing Table 4 and continued in 5 shows validation losses for all models with mitigations applied (quantiza- tion only in the forward pass, or activations in high precision), trained using our at different FLOP budgets relative to bfloat16 baseline. D N formats bfloat16 E4M3 E5M2 E4M3 E5M2 bfloat16 bfloat16 bfloat16 E4M3 E5M2 87.35 2e 17 1.1522 -0.027 -0.027 -0.027 -0.012 46.99 1.1084 0.002 0.007 0.007 0.012 26.897 1.1011 0.004 0.001 0.004 0.009 16.06 1.0956 -0.001 0.014 0.004 0.009 9.92 1.0971 0.003 0.003 0.008 0.013 6.30 1.0950 0.0 -0.005 -0.01 0.01 4.10 1.1042 0.001 -0.006 0.006 0.006 2.73 1.1255 -0.001 -0.004 0.004 0.019 191.02 4.37e 17 1.030 0.005 0.0 0.010 0.01 102.78 1.0464 -0.016 0.036 -0.011 -0.021 58.81 0.9898 0.005 0.005 0.005 0.015 35.14 0.9806 -0.001 0.004 0.004 0.009 21.70 0.9765 0.003 0.003 0.003 0.013 13.78 0.9717 0.003 0.003 0.003 0.008 8.97 0.9732 0.002 0.002 0.002 0.012 5.97 2.3174 0.303 0.843 2.763 1.237 4.05 0.9839 0.001 0.006 0.006 0.006 2.80 0.9949 0.0 0.0 0.0 0.005 128.62 9.56e 17 0.9198 0.0 0.0 0.005 0.015 76.84 0.9052 0.0 0.005 0.005 0.015 47.46 0.8969 0.002 0.003 0.003 0.008 30.14 0.8894 0.001 0.001 0.006 0.011 19.62 0.8846 0.0 0.005 0.005 0.01 13.05 0.8879 0.002 0.002 0.002 0.012 8.86 0.8849 0.0 0.005 0.005 0.005 6.13 0.8882 0.002 0.002 0.002 0.007 4.31 0.8933 0.002 0.002 0.002 0.007 3.08 0.8961 0.004 0.004 0.004 0.009 2.24 0.9059 -0.001 0.004 0.064 0.004 168.03 2.09e 18 0.8546 0.0 0.005 0.005 0.015 103.78 0.8430 0.002 0.002 0.187 0.012 65.91 0.8335 0.001 0.001 0.001 0.011 42.896 0.8258 -0.001 0.004 0.004 0.009 28.54 0.8242 0.001 0.001 0.001 0.011 19.37 0.8200 0.0 0.0 0.0 0.005 13.399 0.8197 0.0 0.0 0.0 0.005 9.428 0.8187 0.001 0.001 0.001 0.006 6.74 0.8192 0.001 0.001 0.001 0.006 4.89 0.8215 0.003 0.006 0.003 0.003 2.02 0.8327 0.002 0.002 0.002 0.002 Table 4: Validation loss table, with separate columns for various weight and activation precisions.\n\n--- Segment 44 ---\n23 E Validation Losses in Language Models Quantizing Table 4 and continued in 5 shows validation losses for all models with mitigations applied (quantiza- tion only in the forward pass, or activations in high precision), trained using our at different FLOP budgets relative to bfloat16 baseline. D N formats bfloat16 E4M3 E5M2 E4M3 E5M2 bfloat16 bfloat16 bfloat16 E4M3 E5M2 87.35 2e 17 1.1522 -0.027 -0.027 -0.027 -0.012 46.99 1.1084 0.002 0.007 0.007 0.012 26.897 1.1011 0.004 0.001 0.004 0.009 16.06 1.0956 -0.001 0.014 0.004 0.009 9.92 1.0971 0.003 0.003 0.008 0.013 6.30 1.0950 0.0 -0.005 -0.01 0.01 4.10 1.1042 0.001 -0.006 0.006 0.006 2.73 1.1255 -0.001 -0.004 0.004 0.019 191.02 4.37e 17 1.030 0.005 0.0 0.010 0.01 102.78 1.0464 -0.016 0.036 -0.011 -0.021 58.81 0.9898 0.005 0.005 0.005 0.015 35.14 0.9806 -0.001 0.004 0.004 0.009 21.70 0.9765 0.003 0.003 0.003 0.013 13.78 0.9717 0.003 0.003 0.003 0.008 8.97 0.9732 0.002 0.002 0.002 0.012 5.97 2.3174 0.303 0.843 2.763 1.237 4.05 0.9839 0.001 0.006 0.006 0.006 2.80 0.9949 0.0 0.0 0.0 0.005 128.62 9.56e 17 0.9198 0.0 0.0 0.005 0.015 76.84 0.9052 0.0 0.005 0.005 0.015 47.46 0.8969 0.002 0.003 0.003 0.008 30.14 0.8894 0.001 0.001 0.006 0.011 19.62 0.8846 0.0 0.005 0.005 0.01 13.05 0.8879 0.002 0.002 0.002 0.012 8.86 0.8849 0.0 0.005 0.005 0.005 6.13 0.8882 0.002 0.002 0.002 0.007 4.31 0.8933 0.002 0.002 0.002 0.007 3.08 0.8961 0.004 0.004 0.004 0.009 2.24 0.9059 -0.001 0.004 0.064 0.004 168.03 2.09e 18 0.8546 0.0 0.005 0.005 0.015 103.78 0.8430 0.002 0.002 0.187 0.012 65.91 0.8335 0.001 0.001 0.001 0.011 42.896 0.8258 -0.001 0.004 0.004 0.009 28.54 0.8242 0.001 0.001 0.001 0.011 19.37 0.8200 0.0 0.0 0.0 0.005 13.399 0.8197 0.0 0.0 0.0 0.005 9.428 0.8187 0.001 0.001 0.001 0.006 6.74 0.8192 0.001 0.001 0.001 0.006 4.89 0.8215 0.003 0.006 0.003 0.003 2.02 0.8327 0.002 0.002 0.002 0.002 Table 4: Validation loss table, with separate columns for various weight and activation precisions. For the last 2 columns, we quantize only the forward pass.\n\n--- Segment 45 ---\nD N formats bfloat16 E4M3 E5M2 E4M3 E5M2 bfloat16 bfloat16 bfloat16 E4M3 E5M2 87.35 2e 17 1.1522 -0.027 -0.027 -0.027 -0.012 46.99 1.1084 0.002 0.007 0.007 0.012 26.897 1.1011 0.004 0.001 0.004 0.009 16.06 1.0956 -0.001 0.014 0.004 0.009 9.92 1.0971 0.003 0.003 0.008 0.013 6.30 1.0950 0.0 -0.005 -0.01 0.01 4.10 1.1042 0.001 -0.006 0.006 0.006 2.73 1.1255 -0.001 -0.004 0.004 0.019 191.02 4.37e 17 1.030 0.005 0.0 0.010 0.01 102.78 1.0464 -0.016 0.036 -0.011 -0.021 58.81 0.9898 0.005 0.005 0.005 0.015 35.14 0.9806 -0.001 0.004 0.004 0.009 21.70 0.9765 0.003 0.003 0.003 0.013 13.78 0.9717 0.003 0.003 0.003 0.008 8.97 0.9732 0.002 0.002 0.002 0.012 5.97 2.3174 0.303 0.843 2.763 1.237 4.05 0.9839 0.001 0.006 0.006 0.006 2.80 0.9949 0.0 0.0 0.0 0.005 128.62 9.56e 17 0.9198 0.0 0.0 0.005 0.015 76.84 0.9052 0.0 0.005 0.005 0.015 47.46 0.8969 0.002 0.003 0.003 0.008 30.14 0.8894 0.001 0.001 0.006 0.011 19.62 0.8846 0.0 0.005 0.005 0.01 13.05 0.8879 0.002 0.002 0.002 0.012 8.86 0.8849 0.0 0.005 0.005 0.005 6.13 0.8882 0.002 0.002 0.002 0.007 4.31 0.8933 0.002 0.002 0.002 0.007 3.08 0.8961 0.004 0.004 0.004 0.009 2.24 0.9059 -0.001 0.004 0.064 0.004 168.03 2.09e 18 0.8546 0.0 0.005 0.005 0.015 103.78 0.8430 0.002 0.002 0.187 0.012 65.91 0.8335 0.001 0.001 0.001 0.011 42.896 0.8258 -0.001 0.004 0.004 0.009 28.54 0.8242 0.001 0.001 0.001 0.011 19.37 0.8200 0.0 0.0 0.0 0.005 13.399 0.8197 0.0 0.0 0.0 0.005 9.428 0.8187 0.001 0.001 0.001 0.006 6.74 0.8192 0.001 0.001 0.001 0.006 4.89 0.8215 0.003 0.006 0.003 0.003 2.02 0.8327 0.002 0.002 0.002 0.002 Table 4: Validation loss table, with separate columns for various weight and activation precisions. For the last 2 columns, we quantize only the forward pass. The second column indicates the total FLOP count used for those values of tokens-to-parameter ratios (D N).\n\n--- Segment 46 ---\nFor the last 2 columns, we quantize only the forward pass. The second column indicates the total FLOP count used for those values of tokens-to-parameter ratios (D N). Values are shown as differences with respect to bfloat16 baseline (lower is better).\n\n--- Segment 47 ---\nThe second column indicates the total FLOP count used for those values of tokens-to-parameter ratios (D N). Values are shown as differences with respect to bfloat16 baseline (lower is better). 24 D N formats bfloat16 E4M3 E5M2 E4M3 E5M2 bfloat16 bfloat16 bfloat16 E4M3 E5M2 144.14 4.57e 18 0.794 0.001 0.006 0.006 0.011 93.81 0.784 0.001 0.001 0.001 0.011 62.41 0.780 0.0 0.005 0.005 0.01 42.37 0.774 0.001 0.001 0.001 0.006 29.30 0.772 -0.002 0.003 0.003 0.003 14.74 0.767 -0.002 0.003 0.003 0.003 10.70 0.766 -0.001 0.004 -0.001 0.004 7.87 0.766 -0.001 0.004 -0.001 0.004 4.42 0.769 0.001 0.001 0.001 0.006 3.37 0.772 -0.002 0.003 0.003 0.003 2.60 0.775 0.0 0.0 0.0 0.005 2.02 0.779 0.001 0.001 0.001 0.001 136.47458 1e 19 0.748 0.002 0.002 0.002 0.002 92.646 0.741 -0.001 0.004 0.004 0.009 64.075 0.736 -0.001 0.004 0.004 0.009 45.084 0.731 -0.001 0.004 0.004 0.009 32.233 0.728 0.002 0.002 0.002 0.007 23.391 0.725 0.0 0.005 0.0 0.005 17.210 0.724 0.001 0.001 0.001 0.006 12.826 0.724 0.001 0.001 0.001 0.311 9.674 0.723 0.002 0.002 0.002 0.002 7.38 0.723 0.002 0.002 0.002 0.077 4.43 0.727 -0.002 0.003 0.003 0.003 2.75 0.732 -0.002 0.023 0.003 0.003 Table 5: MXFP8 of the validation loss table, with separate rows for Weight and Activation precisions.\n\n--- Segment 48 ---\nValues are shown as differences with respect to bfloat16 baseline (lower is better). 24 D N formats bfloat16 E4M3 E5M2 E4M3 E5M2 bfloat16 bfloat16 bfloat16 E4M3 E5M2 144.14 4.57e 18 0.794 0.001 0.006 0.006 0.011 93.81 0.784 0.001 0.001 0.001 0.011 62.41 0.780 0.0 0.005 0.005 0.01 42.37 0.774 0.001 0.001 0.001 0.006 29.30 0.772 -0.002 0.003 0.003 0.003 14.74 0.767 -0.002 0.003 0.003 0.003 10.70 0.766 -0.001 0.004 -0.001 0.004 7.87 0.766 -0.001 0.004 -0.001 0.004 4.42 0.769 0.001 0.001 0.001 0.006 3.37 0.772 -0.002 0.003 0.003 0.003 2.60 0.775 0.0 0.0 0.0 0.005 2.02 0.779 0.001 0.001 0.001 0.001 136.47458 1e 19 0.748 0.002 0.002 0.002 0.002 92.646 0.741 -0.001 0.004 0.004 0.009 64.075 0.736 -0.001 0.004 0.004 0.009 45.084 0.731 -0.001 0.004 0.004 0.009 32.233 0.728 0.002 0.002 0.002 0.007 23.391 0.725 0.0 0.005 0.0 0.005 17.210 0.724 0.001 0.001 0.001 0.006 12.826 0.724 0.001 0.001 0.001 0.311 9.674 0.723 0.002 0.002 0.002 0.002 7.38 0.723 0.002 0.002 0.002 0.077 4.43 0.727 -0.002 0.003 0.003 0.003 2.75 0.732 -0.002 0.023 0.003 0.003 Table 5: MXFP8 of the validation loss table, with separate rows for Weight and Activation precisions. For the last 2 columns, we quantize only the forward pass.\n\n--- Segment 49 ---\n24 D N formats bfloat16 E4M3 E5M2 E4M3 E5M2 bfloat16 bfloat16 bfloat16 E4M3 E5M2 144.14 4.57e 18 0.794 0.001 0.006 0.006 0.011 93.81 0.784 0.001 0.001 0.001 0.011 62.41 0.780 0.0 0.005 0.005 0.01 42.37 0.774 0.001 0.001 0.001 0.006 29.30 0.772 -0.002 0.003 0.003 0.003 14.74 0.767 -0.002 0.003 0.003 0.003 10.70 0.766 -0.001 0.004 -0.001 0.004 7.87 0.766 -0.001 0.004 -0.001 0.004 4.42 0.769 0.001 0.001 0.001 0.006 3.37 0.772 -0.002 0.003 0.003 0.003 2.60 0.775 0.0 0.0 0.0 0.005 2.02 0.779 0.001 0.001 0.001 0.001 136.47458 1e 19 0.748 0.002 0.002 0.002 0.002 92.646 0.741 -0.001 0.004 0.004 0.009 64.075 0.736 -0.001 0.004 0.004 0.009 45.084 0.731 -0.001 0.004 0.004 0.009 32.233 0.728 0.002 0.002 0.002 0.007 23.391 0.725 0.0 0.005 0.0 0.005 17.210 0.724 0.001 0.001 0.001 0.006 12.826 0.724 0.001 0.001 0.001 0.311 9.674 0.723 0.002 0.002 0.002 0.002 7.38 0.723 0.002 0.002 0.002 0.077 4.43 0.727 -0.002 0.003 0.003 0.003 2.75 0.732 -0.002 0.023 0.003 0.003 Table 5: MXFP8 of the validation loss table, with separate rows for Weight and Activation precisions. For the last 2 columns, we quantize only the forward pass. The second column indicates the FLOP count used.\n\n--- Segment 50 ---\nFor the last 2 columns, we quantize only the forward pass. The second column indicates the FLOP count used. 25 F Additional Unstable Language Model Sweeps In Figure 16 and Figure 17 we show some other examples of weight activation MX precision combinations we found to be unstable. In general, we were not able to find any stable combinations of weights and activations in lower precision across the formats we tested. 0 20000 40000 60000 80000 100000 120000 140000 Step 2 4 6 8 10 Loss Train CrossEntropyLoss 0 20000 40000 60000 80000 100000 120000 140000 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 20 40 60 80 100 120 140 160 Ratio MXFP8 E5M2 weights, MXFP8 E4M3 activations (a) Train loss and grad norm for MXFP8 E5M2-MXFP8 E4M3. 0 10000 20000 30000 40000 50000 60000 Step 2 3 4 5 6 Loss Train CrossEntropyLoss 0 10000 20000 30000 40000 50000 60000 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 25 50 75 100 125 150 175 Ratio MXFP8 E4M3 weights, MXFP8 E5M2 activations (b) Train loss and grad norm for MXFP8 E4M3-MXFP E5M2. Figure 16: Unstable MXFP8 combinations of precision formats of weights and activations. 26 0 100 200 300 400 500 Step 2 3 4 5 6 7 Loss Train CrossEntropyLoss 0 100 200 300 400 500 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 25 50 75 100 125 150 175 Ratio MXFP6 E2M3 weights, MXFP8 E4M3 activations (a) for MXFP6 E2M3-MXFP8 E4M3.\n\n--- Segment 51 ---\nFigure 16: Unstable MXFP8 combinations of precision formats of weights and activations. 26 0 100 200 300 400 500 Step 2 3 4 5 6 7 Loss Train CrossEntropyLoss 0 100 200 300 400 500 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 25 50 75 100 125 150 175 Ratio MXFP6 E2M3 weights, MXFP8 E4M3 activations (a) for MXFP6 E2M3-MXFP8 E4M3. 0 50 100 150 200 250 300 350 400 Step 1.5 2.0 2.5 3.0 3.5 4.0 4.5 Loss Train CrossEntropyLoss 0 50 100 150 200 250 300 350 400 Step 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Grad Norm Optimizer Total Grad Norm 25 50 75 100 125 150 175 Ratio MXFP6 E2M3 weights, MXFP8 E5M2 activations (b) Train loss and for MXFP6 E2M3-MXFP8 E5M2. Figure 17: Unstable combinations of precision formats of weights and activations for MXFP6 weights. 27 References Abdolrashidi, A., Wang, L., Agrawal, S., Malmaud, J., Rybakov, O., Leichner, C., and Lew, L. (2021). Pareto-optimal quantized resnet is mostly 4-bit. In 2021 IEEE CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), page 3085 3093. IEEE. Anthropic (2025). Claude 4 system card. 4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf. Accessed: 2025-06-20. Bondarenko, Y., Nagel, M., and Blankevoort, T. (2023). Quantizable transformers: Removing outliers by helping attention heads do nothing. Brandfonbrener, D., Anand, N., Vyas, N., Malach, E., and Kakade, S. (2024).\n\n--- Segment 52 ---\nQuantizable transformers: Removing outliers by helping attention heads do nothing. Brandfonbrener, D., Anand, N., Vyas, N., Malach, E., and Kakade, S. (2024). Loss-to-loss prediction: Scaling laws for all datasets. arXiv preprint arXiv:2411.12925. Chen, M., Zhang, C., Liu, J., Zeng, Y., Xue, Z., Liu, Z., Li, Y., Ma, J., Huang, J., Zhou, X., and Luo, P. (2025). Scaling law for quantization-aware training. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. (2022). Palm: Scaling language modeling with pathways.\n\n--- Segment 53 ---\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. (2022). Palm: Scaling language modeling with pathways. Cohen, J., Kaur, S., Li, Y., Kolter, J. Z., and Talwalkar, A. (2021). Gradient descent on neural networks typically occurs at the edge of stability. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.\n\n--- Segment 54 ---\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Cohere, T., :, Aakanksha, Ahmadian, A., Ahmed, M., Alammar, J., Alizadeh, M., Alnumay, Y., Althammer, S., Arkhangorodsky, A., Aryabumi, V., Aumiller, D., Avalos, R., Aviv, Z., Bae, S., Baji, S., Barbet, A., Bartolo, M., Bebensee, B., Beladia, N., Beller-Morales, W., Bérard, A., Berneshawi, A., Bialas, A., Blunsom, P., Bobkin, M., Bongale, A., Braun, S., Brunet, M., Cahyawijaya, S., Cairuz, D., Campos, J.\n\n--- Segment 55 ---\nOpenReview.net. Cohere, T., :, Aakanksha, Ahmadian, A., Ahmed, M., Alammar, J., Alizadeh, M., Alnumay, Y., Althammer, S., Arkhangorodsky, A., Aryabumi, V., Aumiller, D., Avalos, R., Aviv, Z., Bae, S., Baji, S., Barbet, A., Bartolo, M., Bebensee, B., Beladia, N., Beller-Morales, W., Bérard, A., Berneshawi, A., Bialas, A., Blunsom, P., Bobkin, M., Bongale, A., Braun, S., Brunet, M., Cahyawijaya, S., Cairuz, D., Campos, J. A., Cao, C., Cao, K., Castagné, R., Cendrero, J., Currie, L. C., Chandak, Y., Chang, D., Chatziveroglou, G., Chen, H., Cheng, C., Chevalier, A., Chiu, J. T., Cho, E., Choi, E., Choi, E., Chung, T., Cirik, V., Cismaru, A., Clavier, P., Conklin, H., Crawhall-Stein, L., Crouse, D., Cruz-Salinas, A. F., Cyrus, B., D souza, D., Dalla-Torre, H., Dang, J., Darling, W., Domingues, O. D., Dash, S., Debugne, A., Dehaze, T., Desai, S., Devassy, J., Dholakia, R., Duffy, K., Edalati, A., Eldeib, A., Elkady, A., Elsharkawy, S., Ergün, I., Ermis, B., Fadaee, M., Fan, B., Fayoux, L., Flet-Berliac, Y., Frosst, N., Gallé, M., Galuba, W., Garg, U., Geist, M., Azar, M. G., Gilsenan-McMahon, E., Goldfarb-Tarrant, S., Goldsack, T., Gomez, A., Gonzaga, V. M., Govindarajan, N., Govindassamy, M., Grinsztajn, N., Gritsch, N., Gu, P., Guo, S., Haefeli, K., Hajjar, R., Hawes, T., He, J., Hofstätter, S., Hong, S., Hooker, S., Hosking, T., Howe, S., Hu, E., Huang, R., Jain, H., Jain, R., Jakobi, N., Jenkins, M., Jordan, J., Joshi, D., Jung, J., Kalyanpur, T., Kamalakara, S. R., Kedrzycki, J., Keskin, G., Kim, E., Kim, J., Ko, W.-Y., Kocmi, T., Kozakov, M., Kry sci nski, W., Jain, A. K., Teru, K. K., Land, S., Lasby, M., Lasche, O., Lee, J., Lewis, P., Li, J., Li, J., Lin, H., Locatelli, A., Luong, K., Ma, R., Mach, L., Machado, M., Magbitang, J., Lopez, B. M., Mann, A., Marchisio, K., Markham, O., Matton, A., McKinney, A., McLoughlin, D., Mokry, J., Morisot, A., Moulder, A., Moynehan, H., Mozes, M., Muppalla, V., Murakhovska, L., Nagarajan, H., Nandula, A., Nasir, H., Nehra, S., Netto-Rosen, J., Ohashi, D., Owers-Bardsley, J., Ozuzu, J., Padilla, D., Park, G., Passaglia, S., Pekmez, J., Penstone, L., Piktus, A., Ploeg, C., Poulton, A., Qi, Y., Raghvendra, S., Ramos, M., Ranjan, E., Richemond, P., Robert-Michon, C., Rodriguez, A., Roy, S., Ruder, S., Ruis, L., Rust, L., Sachan, A., Salamanca, A., Saravanakumar, K. K., Satyakam, I., Sebag, A. S., Sen, P., Sepehri, S., Seshadri, P., Shen, Y., Sherborne, T., Shi, S. S., Shivaprasad, S., Shmyhlo, V., Shrinivason, A., Shteinbuk, I., Shukayev, A., Simard, M., Snyder, E., Spataru, A., Spooner, V., Starostina, T., Strub, F., Su, Y., Sun, J., Talupuru, D., Tarassov, E., Tommasone, E., Tracey, J., Trend, B., Tumer, E., Üstün, A., Venkitesh, B., Venuto, D., Verga, P., Voisin, M., Wang, A., Wang, D., Wang, S., Wen, E., White, N., Willman, J., Winkels, M., Xia, 28 C., Xie, J., Xu, M., Yang, B., Yi-Chern, T., Zhang, I., Zhao, Z., and Zhao, Z.\n\n--- Segment 56 ---\nCohere, T., :, Aakanksha, Ahmadian, A., Ahmed, M., Alammar, J., Alizadeh, M., Alnumay, Y., Althammer, S., Arkhangorodsky, A., Aryabumi, V., Aumiller, D., Avalos, R., Aviv, Z., Bae, S., Baji, S., Barbet, A., Bartolo, M., Bebensee, B., Beladia, N., Beller-Morales, W., Bérard, A., Berneshawi, A., Bialas, A., Blunsom, P., Bobkin, M., Bongale, A., Braun, S., Brunet, M., Cahyawijaya, S., Cairuz, D., Campos, J. A., Cao, C., Cao, K., Castagné, R., Cendrero, J., Currie, L. C., Chandak, Y., Chang, D., Chatziveroglou, G., Chen, H., Cheng, C., Chevalier, A., Chiu, J. T., Cho, E., Choi, E., Choi, E., Chung, T., Cirik, V., Cismaru, A., Clavier, P., Conklin, H., Crawhall-Stein, L., Crouse, D., Cruz-Salinas, A. F., Cyrus, B., D souza, D., Dalla-Torre, H., Dang, J., Darling, W., Domingues, O. D., Dash, S., Debugne, A., Dehaze, T., Desai, S., Devassy, J., Dholakia, R., Duffy, K., Edalati, A., Eldeib, A., Elkady, A., Elsharkawy, S., Ergün, I., Ermis, B., Fadaee, M., Fan, B., Fayoux, L., Flet-Berliac, Y., Frosst, N., Gallé, M., Galuba, W., Garg, U., Geist, M., Azar, M. G., Gilsenan-McMahon, E., Goldfarb-Tarrant, S., Goldsack, T., Gomez, A., Gonzaga, V. M., Govindarajan, N., Govindassamy, M., Grinsztajn, N., Gritsch, N., Gu, P., Guo, S., Haefeli, K., Hajjar, R., Hawes, T., He, J., Hofstätter, S., Hong, S., Hooker, S., Hosking, T., Howe, S., Hu, E., Huang, R., Jain, H., Jain, R., Jakobi, N., Jenkins, M., Jordan, J., Joshi, D., Jung, J., Kalyanpur, T., Kamalakara, S. R., Kedrzycki, J., Keskin, G., Kim, E., Kim, J., Ko, W.-Y., Kocmi, T., Kozakov, M., Kry sci nski, W., Jain, A. K., Teru, K. K., Land, S., Lasby, M., Lasche, O., Lee, J., Lewis, P., Li, J., Li, J., Lin, H., Locatelli, A., Luong, K., Ma, R., Mach, L., Machado, M., Magbitang, J., Lopez, B. M., Mann, A., Marchisio, K., Markham, O., Matton, A., McKinney, A., McLoughlin, D., Mokry, J., Morisot, A., Moulder, A., Moynehan, H., Mozes, M., Muppalla, V., Murakhovska, L., Nagarajan, H., Nandula, A., Nasir, H., Nehra, S., Netto-Rosen, J., Ohashi, D., Owers-Bardsley, J., Ozuzu, J., Padilla, D., Park, G., Passaglia, S., Pekmez, J., Penstone, L., Piktus, A., Ploeg, C., Poulton, A., Qi, Y., Raghvendra, S., Ramos, M., Ranjan, E., Richemond, P., Robert-Michon, C., Rodriguez, A., Roy, S., Ruder, S., Ruis, L., Rust, L., Sachan, A., Salamanca, A., Saravanakumar, K. K., Satyakam, I., Sebag, A. S., Sen, P., Sepehri, S., Seshadri, P., Shen, Y., Sherborne, T., Shi, S. S., Shivaprasad, S., Shmyhlo, V., Shrinivason, A., Shteinbuk, I., Shukayev, A., Simard, M., Snyder, E., Spataru, A., Spooner, V., Starostina, T., Strub, F., Su, Y., Sun, J., Talupuru, D., Tarassov, E., Tommasone, E., Tracey, J., Trend, B., Tumer, E., Üstün, A., Venkitesh, B., Venuto, D., Verga, P., Voisin, M., Wang, A., Wang, D., Wang, S., Wen, E., White, N., Willman, J., Winkels, M., Xia, 28 C., Xie, J., Xu, M., Yang, B., Yi-Chern, T., Zhang, I., Zhao, Z., and Zhao, Z. (2025).\n\n--- Segment 57 ---\nA., Cao, C., Cao, K., Castagné, R., Cendrero, J., Currie, L. C., Chandak, Y., Chang, D., Chatziveroglou, G., Chen, H., Cheng, C., Chevalier, A., Chiu, J. T., Cho, E., Choi, E., Choi, E., Chung, T., Cirik, V., Cismaru, A., Clavier, P., Conklin, H., Crawhall-Stein, L., Crouse, D., Cruz-Salinas, A. F., Cyrus, B., D souza, D., Dalla-Torre, H., Dang, J., Darling, W., Domingues, O. D., Dash, S., Debugne, A., Dehaze, T., Desai, S., Devassy, J., Dholakia, R., Duffy, K., Edalati, A., Eldeib, A., Elkady, A., Elsharkawy, S., Ergün, I., Ermis, B., Fadaee, M., Fan, B., Fayoux, L., Flet-Berliac, Y., Frosst, N., Gallé, M., Galuba, W., Garg, U., Geist, M., Azar, M. G., Gilsenan-McMahon, E., Goldfarb-Tarrant, S., Goldsack, T., Gomez, A., Gonzaga, V. M., Govindarajan, N., Govindassamy, M., Grinsztajn, N., Gritsch, N., Gu, P., Guo, S., Haefeli, K., Hajjar, R., Hawes, T., He, J., Hofstätter, S., Hong, S., Hooker, S., Hosking, T., Howe, S., Hu, E., Huang, R., Jain, H., Jain, R., Jakobi, N., Jenkins, M., Jordan, J., Joshi, D., Jung, J., Kalyanpur, T., Kamalakara, S. R., Kedrzycki, J., Keskin, G., Kim, E., Kim, J., Ko, W.-Y., Kocmi, T., Kozakov, M., Kry sci nski, W., Jain, A. K., Teru, K. K., Land, S., Lasby, M., Lasche, O., Lee, J., Lewis, P., Li, J., Li, J., Lin, H., Locatelli, A., Luong, K., Ma, R., Mach, L., Machado, M., Magbitang, J., Lopez, B. M., Mann, A., Marchisio, K., Markham, O., Matton, A., McKinney, A., McLoughlin, D., Mokry, J., Morisot, A., Moulder, A., Moynehan, H., Mozes, M., Muppalla, V., Murakhovska, L., Nagarajan, H., Nandula, A., Nasir, H., Nehra, S., Netto-Rosen, J., Ohashi, D., Owers-Bardsley, J., Ozuzu, J., Padilla, D., Park, G., Passaglia, S., Pekmez, J., Penstone, L., Piktus, A., Ploeg, C., Poulton, A., Qi, Y., Raghvendra, S., Ramos, M., Ranjan, E., Richemond, P., Robert-Michon, C., Rodriguez, A., Roy, S., Ruder, S., Ruis, L., Rust, L., Sachan, A., Salamanca, A., Saravanakumar, K. K., Satyakam, I., Sebag, A. S., Sen, P., Sepehri, S., Seshadri, P., Shen, Y., Sherborne, T., Shi, S. S., Shivaprasad, S., Shmyhlo, V., Shrinivason, A., Shteinbuk, I., Shukayev, A., Simard, M., Snyder, E., Spataru, A., Spooner, V., Starostina, T., Strub, F., Su, Y., Sun, J., Talupuru, D., Tarassov, E., Tommasone, E., Tracey, J., Trend, B., Tumer, E., Üstün, A., Venkitesh, B., Venuto, D., Verga, P., Voisin, M., Wang, A., Wang, D., Wang, S., Wen, E., White, N., Willman, J., Winkels, M., Xia, 28 C., Xie, J., Xu, M., Yang, B., Yi-Chern, T., Zhang, I., Zhao, Z., and Zhao, Z. (2025). Command a: An enterprise-ready large language model.\n\n--- Segment 58 ---\n(2025). Command a: An enterprise-ready large language model. Damian, A., Nichani, E., and Lee, J. D. (2023). Self-stabilization: The implicit bias of gradient descent at the edge of stability. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Darvish Rouhani, B., Garegrat, N., Savell, T., More, A., Han, K.-N., Zhao, R., and Hall, M. (2023a). Open compute project. Darvish Rouhani, B., Zhao, R., Elango, V., Shafipour, R., Hall, M., Mesmakhosroshahi, M., More, A., Melnick, L., Golub, M., Varatkar, G., et al. (2023b). With shared microexponents, a little shifting goes a long way. In Proceedings of the 50th Annual International Symposium on Computer Architecture, pages 1 13. DeepMind, G. (2025). Gemini 2.5 technical report. deepmind-media gemini gemini_v2_5_report.pdf. Accessed: 2025-06-20.\n\n--- Segment 59 ---\ndeepmind-media gemini gemini_v2_5_report.pdf. Accessed: 2025-06-20. Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., Steiner, A., Caron, M., Geirhos, R., Alabdulmohsin, I., Jenatton, R., Beyer, L., Tschannen, M., Arnab, A., Wang, X., Riquelme, C., Minderer, M., Puigcerver, J., Evci, U., Kumar, M., van Steenkiste, S., Elsayed, G. F., Mahendran, A., Yu, F., Oliver, A., Huot, F., Bastings, J., Collier, M. P., Gritsenko, A., Birodkar, V., Vasconcelos, C., Tay, Y., Mensink, T., Kolesnikov, A., Paveti c, F., Tran, D., Kipf, T., Luˇci c, M., Zhai, X., Keysers, D., Harmsen, J., and Houlsby, N. (2023). Scaling vision transformers to 22 billion parameters. Dettmers, T. and Zettlemoyer, L. (2023). The case for 4-bit precision: k-bit inference scaling laws. In International Conference on Machine Learning, pages 7750 7774. PMLR. Fishman, M., Chmiel, B., Banner, R., and Soudry, D. (2024). Scaling fp8 training to trillion-token llms. arXiv preprint arXiv:2409.12517. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. (2024). The llama 3 herd of models. arXiv preprint arXiv:2407.21783.\n\n--- Segment 60 ---\nThe llama 3 herd of models. arXiv preprint arXiv:2407.21783. Groeneveld, D., Beltagy, I., Walsh, P., Bhagia, A., Kinney, R., Tafjord, O., Jha, A. H., Ivison, H., Magnusson, I., Wang, Y., et al. (2024). Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838. Henry, A., Dachapally, P. R., Pawar, S. S., and Chen, Y. (2020). Query-key normalization for transformers. CoRR, abs 2010.04245. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. (2022). Training compute-optimal large language models. arXiv preprint arXiv:2203.15556. Ibrahim, A., Thérien, B., Gupta, K., Richter, M. L., Anthony, Q. G., Belilovsky, E., Lesort, T., and Rish, I. (2024). Simple and scalable strategies to continually pre-train large language models. Transactions on Machine Learning Research. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., and Kalenichenko, D. (2017). Quantization and training of neural networks for efficient integer-arithmetic-only inference. Jastrzebski, S., Szymczak, M., Fort, S., Arpit, D., Tabor, J., Cho, K., and Geras, K. (2020). The break-even point on optimization trajectories of deep neural networks. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. (2020). Scaling laws for neural language models.\n\n--- Segment 61 ---\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. Kumar, T., Ankner, Z., Spector, B. F., Bordelon, B., Muennighoff, N., Paul, M., Pehlevan, C., Ré, C., and Raghunathan, A. (2024). Scaling laws for precision. arXiv preprint arXiv:2411.04330. 29 Lee, J., Bae, J., Kim, B., Kwon, S. J., and Lee, D. (2025). To fp8 and back again: Quantifying reduced precision effects on llm training stability. Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al. (2024). Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437. Liu, Z., Zhao, C., Huang, H., Chen, S., Zhang, J., Zhao, J., Roy, S., Jin, L., Xiong, Y., Shi, Y., et al. (2025). Paretoq: Scaling laws in extremely low-bit llm quantization. arXiv preprint arXiv:2502.02631. Ma, J., Pei, H., Lausen, L., and Karypis, G. (2025). Understanding silent data corruption in llm training. Meta, A. (2025). The llama 4 herd: The beginning of a new era of natively multimodal ai innovation. Micikevicius, P., Stosic, D., Burgess, N., Cornea, M., Dubey, P., Grisenthwaite, R., Ha, S., Hei- necke, A., Judd, P., Kamalu, J., et al. (2022a). Fp8 formats for deep learning.\n\n--- Segment 62 ---\n(2022a). Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433. Micikevicius, P., Stosic, D., Burgess, N., Cornea, M., Dubey, P., Grisenthwaite, R., Ha, S., Heinecke, A., Judd, P., Kamalu, J., Mellempudi, N., Oberman, S., Shoeybi, M., Siu, M., and Wu, H. (2022b). Fp8 formats for deep learning. Microsoft (2024). Mx pytorch emulation library. Molybog, I., Albert, P., Chen, M., DeVito, Z., Esiobu, D., Goyal, N., Koura, P. S., Narang, S., Poulton, A., Silva, R., Tang, B., Liskovich, D., Xu, P., Zhang, Y., Kambadur, M., Roller, S., and Zhang, S. (2023). A theory on adam instability in large-scale machine learning. NVIDIA (2025). Nvidia blackwell architecture. OpenAI (2025). Gpt-4.5 system card. gpt-4-5-system-card-2272025.pdf. Accessed: 2025-06-20. Ouyang, X., Ge, T., Hartvigsen, T., Zhang, Z., Mi, H., and Yu, D. (2024). Low-bit quantization favors undertrained llms: Scaling laws for quantized llms with 100t training tokens. arXiv preprint arXiv:2411.17691. Penedo, G., Kydlíˇcek, H., allal, L. B., Lozhkov, A., Mitchell, M., Raffel, C., Werra, L. V., and Wolf, T. (2024). The fineweb datasets: Decanting the web for the finest text data at scale. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Porian, T., Wortsman, M., Jitsev, J., Schmidt, L., and Carmon, Y. (2025).\n\n--- Segment 63 ---\nPorian, T., Wortsman, M., Jitsev, J., Schmidt, L., and Carmon, Y. (2025). Resolving discrepancies in compute-optimal scaling of language models. Rouhani, B. D., Zhao, R., More, A., Hall, M., Khodamoradi, A., Deng, S., Choudhary, D., Cornea, M., Dellinger, E., Denolf, K., et al. (2023). Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537. Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z., Zhang, K., Gao, P., Qiao, Y., and Luo, P. (2024). Omniquant: Omnidirectionally calibrated quantization for large language models. Shazeer, N. (2020). Glu variants improve transformer. Sun, M., Chen, X., Kolter, J. Z., and Liu, Z. (2024). Massive activations in large language models. arXiv preprint arXiv:2402.17762. Tseng, A., Yu, T., and Park, Y. (2025). Training llms with mxfp4. Wortsman, M., Liu, P. J., Xiao, L., Everett, K. E., Alemi, A. A., Adlam, B., Co-Reyes, J. D., Gur, I., Kumar, A., Novak, R., Pennington, J., Sohl-Dickstein, J., Xu, K., Lee, J., Gilmer, J., and Kornblith, S. (2024). Small-scale proxies for large-scale transformer training instabilities. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. 30 Xu, K., Lin, J., Wang, Z., Hu, P., and Zhao, Z. (2023). Improved fully quantized training via rectifying batch normalization. arXiv preprint arXiv:.\n\n--- Segment 64 ---\nImproved fully quantized training via rectifying batch normalization. arXiv preprint arXiv:. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. (2022). Opt: Open pre-trained transformer language models. Zoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J., Shazeer, N., and Fedus, W. (2022). St-moe: Designing stable and transferable sparse expert models. 31\n\n