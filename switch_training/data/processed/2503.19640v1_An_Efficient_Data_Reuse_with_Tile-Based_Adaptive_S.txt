=== ORIGINAL PDF: 2503.19640v1_An_Efficient_Data_Reuse_with_Tile-Based_Adaptive_S.pdf ===\n\nRaw text length: 21063 characters\nCleaned text length: 20857 characters\nNumber of segments: 12\n\n=== CLEANED TEXT ===\n\nAn Efficient Data Reuse with Tile-Based Adaptive Stationary for Transformer Accelerators Tseng-Jen Li, and Tian-Sheuan Chang Institute of Electronics, National Yang Ming Chiao Tung University, Hsinchu 300093, Taiwan Abstract Transformer-based models have become the de facto backbone across many fields, such as computer vision and natural language processing. However, as these models scale in size, external memory access (EMA) for weight and activations becomes a critical bottleneck due to its significantly higher energy consumption compared to internal computations. While most prior work has focused on optimizing the self-attention mechanism, little attention has been given to optimizing data transfer during linear projections, where EMA costs are equally important. In this paper, we propose the Tile-based Adaptive Stationary (TAS) scheme that selects the input or weight station- ary in a tile granularity, based on the input sequence length. Our experimental results demonstrate that TAS can significantly reduce EMA by more than 97 compared to traditional sta- tionary schemes, while being compatible with various attention optimization techniques and hardware accelerators. I. INTRODUCTION Transformer-based models [1] are now crucial in deep learning for a range of tasks. They process tokenized text, audio, or images with multiple transformer layers, excelling in handling long-range dependencies and parallel processing with attention mechanisms. Models like Vision Transformer (ViT) [2], BERT [3], and Wav2Vec2.0 [4] have greatly outperformed older convolutional and recurrent models in accuracy. Their success is largely due to parameter scaling, where increased parameters through deeper layers, more attention heads, or larger embeddings improve performance. This strategy has led to large models like ViT-G 14 [5], Wav2Vec2-XLS-R-2B [6], and GPT-3 [7], as shown in Table I, which consistently push the accuracy limits. However, the growing number of param- eters presents challenges for hardware accelerators, requiring efficient data reuse to reduce memory bandwidth constraints. Data reuse through different stationary schemes has been well explored for accelerators of convolutional neural net- works [8]. [8] proposed the input weight output stationary schemes that reuse input weight output, respectively, for the computing flow. However, it is optimized for convolution- based computation. These stationary schemes have been applied in [9] that propose an accelerator for versatile transformer-based models. However, their stationary schemes are fixed when focusing solely on one of the transformer models. This approach is not optimal, since transformer input length could vary dramatically, such as the text or audio input. Using the BERT model as a reference, by inputting the tokenized text of length 3072 into the adaptive stationary strategy, it is possible to obtain over a 75 reduction in the reused matrix as compared to [9], which shows that an optimal stationary scheme shall be input-length dependent. Moreover, their method necessitates concurrent read and write operations to external memory because of data flow conflicts, which imposes further stall penalties on the hardware. To mitigate these challenges, we propose the Tile-based Adaptive Stationary (TAS) that selects either input or weight stationary according to input length to maximize data reuse, reduce external memory access, as well as decrease the amount of simultaneous reading from and writing to external memory access. TAS can reduce external memory accesses by approx- imately 97 compared to the one without any data reuse. Besides, this approach optimizes memory communication by achieving nearly twice the efficiency compared to the previous fixed stationary method [9]. The rest of the paper is organized as follows. Section II reviews fixed stationary methods and its problems. Section III presents our proposed methods. Section IV shows the experimental results. Finally, this paper is concluded in Section V. II. REVIEW OF THE FIXED STATIONARY SCHEME Fig. 1 shows the fixed stationary strategies for matrix- matrix multiplication, often employed in transformer models for operations like linear projections. Though these stationary strategies are similar to those in CNNs, they feature unique data flows and memory access patterns, detailed in this Sec- tion. Details for Fig. 1 are as follows. In Fig. 1(a), M is the input matrix row count, K is the weight matrix column count, and N is the shared dimension between input matrix columns and weight matrix rows. The parameters m, n, and k denote the tile size, where m M, n N, and k K. Circled numbers indicate tile computation order. Arrows with numbers show tile advancement direction post-computation, whereas numbers without arrows suggest tiles remain until aligned with partners. Squared numbers signify the sequence of output results. The different stationary schemes are outlined below. a) Input Stationary: The IS scheme aims to load in- put tile data into internal memory once for reuse, reducing memory access to retrieve identical inputs. Fig. 1(b) portrays data movement in the IS scheme. An arrow labeled 1 shows weight tile data progressing along weight matrix rows, denoted K. Thus, the input tile marked 1 is used K k times. The 2 arrow indicates that the input tile in the input matrix moves arXiv:2503.19640v1 [cs.LG] 25 Mar 2025 TABLE I HIDDEN DIMENSION, PRE-DEFINED TOKEN LENGTH, PARAMETER SIZE AND TOTAL EMA VALUES FOR RESPECTIVE REPRESENTATIVE LARGE MODEL Model ViT-G 14 Wav2Vec2 -XLS-R GPT-3 Hidden Dimension 4,096 2,560 12,288 Token Length 518 1,536 2,048 Parameter Size (B) 1.8 2 175 Total EMA (G) 312.9 353.9 11,132.6 TABLE II COMPARISON OF EXTERNAL MEMORY ACCESS FOR VARIOUS STATIONARY SCHEMES Stationary Scheme Input Matrix Weight Matrix Output Matrix Total Na ıve K MN M NK N MK MNK 3 IS MN M m NK N n MK MNK ( 1 K 1 m 1 n) WS K k MN NK N n MK MNK ( 1 k 1 M 1 n) OS K k MN M m NK MK MNK ( 1 k 1 m 1 N ) IS-OS MN M m NK MK MNK ( 1 K 1 m 1 N ) WS-OS K k MN NK MK MNK ( 1 k 1 M 1 N ) Fig. 1. Matrix Mapping for Matrix-Matrix Multiplication with Conventional Stationary Schemes n units, while the weight tile shifts n units downward in the weight matrix. As the input tile transitions from west to east in the input matrix, it moves m units to the next row, repositioning the weight tile from right-bottom to left-top in the weight matrix. An arrow marked 2 produces the output tile. These steps repeat M m times, as indicated by the arrow marked 3 . b) Weight Stationary: The WS scheme loads each weight tile into internal memory once, enabling repeated use across multiple calculations, which reduces redundant memory ac- cess. Fig. 1(c) illustrates this dataflow. An arrow 1 indicates the fixed position of the weight tile along matrix columns, M, where each tile is reused M m times. Arrow 2 shows the input tile moving k units down the input matrix, while the stationary weight tile remains fixed. As the input tile traverses from east to west, the weight tile remains in place and is reused for subsequent inputs, shifting vertically by k. The output tile generation, marked by 2 , follows this sequence and repeats K k times, indicated by 3 . c) Output Stationary: The OS strategies depicted in Fig. 1(d) and (e) showcase another commonly employed group of techniques. These approaches retain partial sums in internal memory until the computation of output results is completed, thereby optimizing inner products, which are fundamental to matrix-matrix multiplication. This method leverages the spatial locality of processing element arrays to reduce external data retrievals for partial sums. The primary distinction between the row-oriented OS and col-oriented OS schemes lies in the sequence of generating the weight matrix. In the row-oriented OS, outputs are generated row by row, while the column- oriented strategy produces results column by column. Fig. 1(d) shows arrows with black circle indicators where the input and weight tiles slide vertically and horizontally, respectively. Upon generating an output tile marked by 1 , indicated by arrows in 2 , the input tile returns to the initial position in the input matrix, and the weight tile shifts right by k. After repeating the data flow K k times, as shown by arrows marked as 3 , the input tile is moved downward by m, and the weight tile is moved to the starting weight matrix location. On the other hand, the column-oriented strategy initially redirects the weight tile, as shown by arrows in 2 , and later shifts the input tile, indicated by 3 . d) Problems of the fixed stationary scheme: Table II shows the external memory access values for different sta- tionary schemes. These fixed stationary schemes have two problems: higher external memory access and concurrent read write demands. For the first problem, in this table, m, n, and k represent tile dimensions and the available hardware computation resources, typically much smaller than M, N, and K. For instance, with an IS configuration, the EMA Fig. 2. Matrix Mapping for Matrix-Matrix Multiplication with Proposed Stationary Schemes of the input matrix is noticeably less compared to others, highlighting the overall EMA reduction. Therefore, applying a fixed stationary scheme without considering the input length is inadvisable. For the second problem, external memory like DRAM cannot read and write data simultaneously. However, when computing linear projections, two matrices need to be read from external memory, while only one requires writing back. Though this can solved by extra buffers, the required buffer size could be quite large due to the large matrix size in current transformer models. Thus, an effective partial sum reuse scheme is demanded to solve this problem. III. METHODOLOGY This Section first shows the concept of the adaptive station- ary mechanism, then further enhances data reuse with a hybrid stationary scheme in a tile granularity, and finally presents the whole strategy. A. Adaptive Mechanism The adaptive mechanism is constructed to optimize the selection of the most efficient stationary data scheme in a tile granularity for specific EMA needs during matrix com- putations. The core decision focuses on choosing between IS or WS, determined by evaluating the EMA reduction for the matrix. In current accelerators, PE arrays are often organized in a square formation like 8 8 or 16 16. This design choice facilitates efficient tile computation mapping onto the processing elements, where m, n, and k are roughly equal. For existed accelerators, each PE arrays usually consists of a squared sized PEs, such that 8 8, or 16 16. That is, for the consideration of well porting tile computation to the processing elements, m, n, and k are approximately equivalent. As demonstrated in Table II, the input matrix primarily reduces total EMA when the input stationary scheme is applied, decreasing EMA from K (MN) to MN. In contrast, under the WS scheme, the weight matrix EMA is reduced by a factor of M. Thus, selecting either IS or WS depends simply on comparing MN and NK, corresponding to the input and weight matrix sizes, respectively. The decision is governed by the expression MN NK N(M K). When M K, the result is negative, suggesting IS is more efficient than WS. Conversely, when the result is zero or positive (M K), WS is more advantageous and preferred. This straightforward condition highlights the minimal overhead in decision-making hardware, which merely compares the input matrix s row count with the weight ma- trix s column count before performing matrix multiplication. By dynamically adjusting the stationary scheme at each com- puting phase, we achieve notable EMA reductions compared to a static scheme. For example, in a BERT model during the linear projection stage of the query, the EMA reduction for the specified matrix can exceed 94 , as depicted in Table III. This input-length adaptive stationary scheme maximizes resource efficiency. B. Hybrid Strategy in a Tile Granularity Fig. 1(b) and (c) show that the internal memory capacity for partial sums would be up to K and M registers for the respective data-reuse strategies of IS and WS. While (d) and (e) in Fig. 1 indicate that internal memory capacity reduces to n, which also is also equivalent to the shared dimension of input tile s column and weight tile s row. Consequently, to fully leverage the ability of decreasing EMA by IS or WS, and to consider the internal memory usage of partial sums for hardware implementation, we introduce the hybrid strategy based on the above adaptive mechanism. This strategy, in addition to the reuse of temporal data by IS or WS, incorporates a spatial reuse strategy in a tile granularity by combining the input weight stationary scheme with the output stationary scheme. This hybrid approach not only minimizes internal memory usage, but also ensures that partial sums are not stored in internal memory until the final results are generated. Therefore, the spatial reuse of partial sums successfully prevents frequent simultaneous reading and writing of data externally. C. Proposed Stationary Scheme The methodology for optimizing matrix-matrix multiplica- tion through our innovative tile-based adaptive stationary approach is elucidated in Fig. 2. In this illustration, Psum denotes partial sums, IF refers to input features, and WP signifies weight parameters. The choice between IS-OS and WS-OS strategies is based on the comparative dimensions of the input for linear projection computation; specifically, if M is less than K, we select IS-WS as shown in Fig. 2(a). In contrast, if this condition is not met, OS-WS is used, as illustrated in Fig. 2(b). Within the diagram, the symbols k and m signify the number of partial sums stored internally, which is determined by the accelerator s internal memory capacity. For the IS-WS strategy, an input tile remains static for k k iterations, multiplying with weight tiles placed in k different positions along the K dimension, shifting by a distance k each cycle, indicated by 1 . Post temporal reuse via IS, we exploit k partial sums spatial reuse through a row-oriented OS, shown by an arrow at 2 . Once the input tile has exhausted the N dimension of the input matrix, it resets as denoted by 3 , and the weight tile shifts rightward by k to a new column set, continuing this process until the weight matrix is fully processed. Subsequently, the input tile moves downward by m and the sequence repeats. In scenarios where M K, the WS-OS strategy is activated via our adaptive scheme. Initially, the weight tile marked by 1 is fixed and reused for m m iterations, with the input tile shifting downward, as indicated by an arrow with 1 . Upon maximizing temporal locality through WS, we aim to reuse partial sums, leveraging spatial locality. As execution proceeds, both input and weight tiles shift horizontally and vertically, respectively, as portrayed by an arrow at 2 , re- peating the described sequence. Once the weight tile reaches the weight matrix s lower boundary, as shown by an arrow at 3 , it resets to the starting position while the input tile transitions to the subsequent row set, sliding anew from west to east across the input matrix. This series persists for M m iterations until the input tile fully traverses the input matrix once and returns to the matrix s upper-left corner, marked by an arrow at 4 . This comprehensive cycle persists until the weight tile completely explores the weight matrix. IV. EXPERIMENTAL RESULTS TABLE III COMPARISON OF EMA VALUES FOR DIFFERENT SEQUENCE LENGTHS IN THE WAV2VEC2.0-LARGE MODEL, WHERE seq len REPRESENTS FOR THE SEQUENCE LENGTH AND ss. IS FOR STATIONARY SCHEME seq len IS WS IS-WS optimal ss. 115 1.18 105 1.04 106 9.22 105 IS 384 3.93 105 1.04 106 6.47 105 IS 1565 1.60 106 1.05 106 5.54 105 WS 15000 1.54 107 1.06 106 1.43 107 WS To show the advantages of the proposed approach, this Section shows the results evaluated on the audio and nat- ural language processing tasks. Table III presents the EMA differences with the matrix applied in the stationary scheme during inference of the Wav2Vec2.0-large automatic speech recognition model [4] evaluated on the LibriSpeech [10] dataset. In this dataset, the shortest audio is about 2.3 seconds (115 tokens), and the longest one is 31.3 seconds (1565 tokens), with an average length of 7.6 seconds (384 tokens). Table III details the EMA values for the specified matrix where data reuse is applied across these sequence lengths. We also provide EMA values for recognizing lengthy speech sequences. For sequences exceeding the maximum length, they are usually segmented into chunks for inference. Here, the inference for increasing sequence length corresponds to more rows in the input matrix, maintaining the same computation flow and EMA analysis for linear projection. Table 2 shows that varying sequence lengths influence the optimal stationary scheme choice, indicating that TAS s adaptive mechanism compensates for limitations in fixed schemes. TABLE IV MEASUREMENT OF COMPUTING ENERGY COST FOR BERT-BASE WITH NAIVE IMPLEMENTATION, AYAKA [9] S OPTIMIZATION, AND OURS ACROSS LAYERS Layer ID Na ıve (A) [9] (B) Ours (C) Reduction A B A A C A 0 65.81 35.76 1.89 48.47 97.17 1 66.30 35.05 1.90 48.86 97.15 2 67.65 37.30 1.94 49.88 97.09 3 67.44 37.13 1.93 49.72 97.10 4 67.40 36.23 1.93 49.69 97.10 5 67.42 35.35 1.93 49.70 97.10 6 67.35 37.40 1.93 49.65 97.10 7 64.46 35.28 1.85 47.40 97.23 8 67.44 33.44 1.93 49.72 97.10 9 67.55 35.12 1.94 49.80 97.09 10 65.04 34.63 1.86 47.86 97.20 11 64.74 34.59 1.85 47.62 97.21 12 66.55 35.61 1.91 49.03 97.14 Table IV shows the energy consumption in the BERT- Base model by appling our method to [9]. The computational energy cost includes both external data transfer and internal chip processing, following the same approach and energy numbers in [9]. Basically, the energy consumed by external data transmission is 10 to 100 times greater than that of internal chip computation. To simplify the effective simulation of computing energy costs, measurements can be efficiently taken by evaluating the EMA ratio across various stationary schemes. The fixed stationary scheme introduced by [9] results in an approximate 48 reduction in energy usage during BERT-Base model inference, on average, when compared to a basic implementation lacking stationary schemes. Our proposed stationary scheme achieves an approximately 97 reduction in energy consumption, providing double the energy efficiency compared to the stationary scheme from [9]. V. CONCLUSION This paper introduces the data ruse strategy of TAS, an adaptive and efficient approach to lower EMA in transformer accelerators. TAS dynamically adjusts the stationary scheme in a tile granularity according to input sequence length, sig- nificantly cutting EMA and tackling major energy bottleneck challenges in modern transformer accelerators. Experiments reveal TAS consistently outperforms fixed schemes (IS, WS, OS) across different transformer models, reducing EMA by over 97 in most scenarios. Additionally, TAS coordinates well with existing attention optimizations and hardware accel- erators, offering a flexible, energy-saving solution for large- scale transformer models. REFERENCES [1] A. Vaswani, Attention is all you need, Advances in Neural Information Processing Systems, 2017. [2] A. Dosovitskiy, An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929, 2020. [3] J. Devlin, Bert: Pre-training of deep bidirectional transformers for language understanding, arXiv preprint arXiv:1810.04805, 2018. [4] A. Baevski et al., wav2vec 2.0: A framework for self-supervised learning of speech representations, Advances in neural information processing systems, vol. 33, pp. 12 449 12 460, 2020. [5] X. Zhai et al., Scaling vision transformers, in Proceedings of the IEEE CVF conference on computer vision and pattern recognition, 2022, pp. 12 104 12 113. [6] A. Babu et al., Xls-r: Self-supervised cross-lingual speech representa- tion learning at scale, arXiv preprint arXiv:2111.09296, 2021. [7] T. B. Brown, Language models are few-shot learners, arXiv preprint arXiv:2005.14165, 2020. [8] Y.-H. Chen et al., Eyeriss: An energy-efficient reconfigurable accelera- tor for deep convolutional neural networks, IEEE journal of solid-state circuits, vol. 52, no. 1, pp. 127 138, 2016. [9] Y. Qin et al., Ayaka: A versatile transformer accelerator with low-rank estimation and heterogeneous dataflow, IEEE Journal of Solid-State Circuits, 2024. [10] V. Panayotov et al., Librispeech: an asr corpus based on public domain audio books, in 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2015, pp. 5206 5210.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nAn Efficient Data Reuse with Tile-Based Adaptive Stationary for Transformer Accelerators Tseng-Jen Li, and Tian-Sheuan Chang Institute of Electronics, National Yang Ming Chiao Tung University, Hsinchu 300093, Taiwan Abstract Transformer-based models have become the de facto backbone across many fields, such as computer vision and natural language processing. However, as these models scale in size, external memory access (EMA) for weight and activations becomes a critical bottleneck due to its significantly higher energy consumption compared to internal computations. While most prior work has focused on optimizing the self-attention mechanism, little attention has been given to optimizing data transfer during linear projections, where EMA costs are equally important. In this paper, we propose the Tile-based Adaptive Stationary (TAS) scheme that selects the input or weight station- ary in a tile granularity, based on the input sequence length. Our experimental results demonstrate that TAS can significantly reduce EMA by more than 97 compared to traditional sta- tionary schemes, while being compatible with various attention optimization techniques and hardware accelerators. I. INTRODUCTION Transformer-based models [1] are now crucial in deep learning for a range of tasks. They process tokenized text, audio, or images with multiple transformer layers, excelling in handling long-range dependencies and parallel processing with attention mechanisms. Models like Vision Transformer (ViT) [2], BERT [3], and Wav2Vec2.0 [4] have greatly outperformed older convolutional and recurrent models in accuracy. Their success is largely due to parameter scaling, where increased parameters through deeper layers, more attention heads, or larger embeddings improve performance. This strategy has led to large models like ViT-G 14 [5], Wav2Vec2-XLS-R-2B [6], and GPT-3 [7], as shown in Table I, which consistently push the accuracy limits. However, the growing number of param- eters presents challenges for hardware accelerators, requiring efficient data reuse to reduce memory bandwidth constraints. Data reuse through different stationary schemes has been well explored for accelerators of convolutional neural net- works [8]. [8] proposed the input weight output stationary schemes that reuse input weight output, respectively, for the computing flow. However, it is optimized for convolution- based computation.\n\n--- Segment 2 ---\n[8] proposed the input weight output stationary schemes that reuse input weight output, respectively, for the computing flow. However, it is optimized for convolution- based computation. These stationary schemes have been applied in [9] that propose an accelerator for versatile transformer-based models. However, their stationary schemes are fixed when focusing solely on one of the transformer models. This approach is not optimal, since transformer input length could vary dramatically, such as the text or audio input. Using the BERT model as a reference, by inputting the tokenized text of length 3072 into the adaptive stationary strategy, it is possible to obtain over a 75 reduction in the reused matrix as compared to [9], which shows that an optimal stationary scheme shall be input-length dependent. Moreover, their method necessitates concurrent read and write operations to external memory because of data flow conflicts, which imposes further stall penalties on the hardware. To mitigate these challenges, we propose the Tile-based Adaptive Stationary (TAS) that selects either input or weight stationary according to input length to maximize data reuse, reduce external memory access, as well as decrease the amount of simultaneous reading from and writing to external memory access. TAS can reduce external memory accesses by approx- imately 97 compared to the one without any data reuse. Besides, this approach optimizes memory communication by achieving nearly twice the efficiency compared to the previous fixed stationary method [9]. The rest of the paper is organized as follows. Section II reviews fixed stationary methods and its problems. Section III presents our proposed methods. Section IV shows the experimental results. Finally, this paper is concluded in Section V. II. REVIEW OF THE FIXED STATIONARY SCHEME Fig. 1 shows the fixed stationary strategies for matrix- matrix multiplication, often employed in transformer models for operations like linear projections. Though these stationary strategies are similar to those in CNNs, they feature unique data flows and memory access patterns, detailed in this Sec- tion. Details for Fig. 1 are as follows. In Fig. 1(a), M is the input matrix row count, K is the weight matrix column count, and N is the shared dimension between input matrix columns and weight matrix rows. The parameters m, n, and k denote the tile size, where m M, n N, and k K. Circled numbers indicate tile computation order.\n\n--- Segment 3 ---\n1(a), M is the input matrix row count, K is the weight matrix column count, and N is the shared dimension between input matrix columns and weight matrix rows. The parameters m, n, and k denote the tile size, where m M, n N, and k K. Circled numbers indicate tile computation order. Arrows with numbers show tile advancement direction post-computation, whereas numbers without arrows suggest tiles remain until aligned with partners. Squared numbers signify the sequence of output results. The different stationary schemes are outlined below. a) Input Stationary: The IS scheme aims to load in- put tile data into internal memory once for reuse, reducing memory access to retrieve identical inputs. Fig. 1(b) portrays data movement in the IS scheme. An arrow labeled 1 shows weight tile data progressing along weight matrix rows, denoted K. Thus, the input tile marked 1 is used K k times. The 2 arrow indicates that the input tile in the input matrix moves arXiv:2503.19640v1 [cs.LG] 25 Mar 2025 TABLE I HIDDEN DIMENSION, PRE-DEFINED TOKEN LENGTH, PARAMETER SIZE AND TOTAL EMA VALUES FOR RESPECTIVE REPRESENTATIVE LARGE MODEL Model ViT-G 14 Wav2Vec2 -XLS-R GPT-3 Hidden Dimension 4,096 2,560 12,288 Token Length 518 1,536 2,048 Parameter Size (B) 1.8 2 175 Total EMA (G) 312.9 353.9 11,132.6 TABLE II COMPARISON OF EXTERNAL MEMORY ACCESS FOR VARIOUS STATIONARY SCHEMES Stationary Scheme Input Matrix Weight Matrix Output Matrix Total Na ıve K MN M NK N MK MNK 3 IS MN M m NK N n MK MNK ( 1 K 1 m 1 n) WS K k MN NK N n MK MNK ( 1 k 1 M 1 n) OS K k MN M m NK MK MNK ( 1 k 1 m 1 N ) IS-OS MN M m NK MK MNK ( 1 K 1 m 1 N ) WS-OS K k MN NK MK MNK ( 1 k 1 M 1 N ) Fig. 1.\n\n--- Segment 4 ---\nThe 2 arrow indicates that the input tile in the input matrix moves arXiv:2503.19640v1 [cs.LG] 25 Mar 2025 TABLE I HIDDEN DIMENSION, PRE-DEFINED TOKEN LENGTH, PARAMETER SIZE AND TOTAL EMA VALUES FOR RESPECTIVE REPRESENTATIVE LARGE MODEL Model ViT-G 14 Wav2Vec2 -XLS-R GPT-3 Hidden Dimension 4,096 2,560 12,288 Token Length 518 1,536 2,048 Parameter Size (B) 1.8 2 175 Total EMA (G) 312.9 353.9 11,132.6 TABLE II COMPARISON OF EXTERNAL MEMORY ACCESS FOR VARIOUS STATIONARY SCHEMES Stationary Scheme Input Matrix Weight Matrix Output Matrix Total Na ıve K MN M NK N MK MNK 3 IS MN M m NK N n MK MNK ( 1 K 1 m 1 n) WS K k MN NK N n MK MNK ( 1 k 1 M 1 n) OS K k MN M m NK MK MNK ( 1 k 1 m 1 N ) IS-OS MN M m NK MK MNK ( 1 K 1 m 1 N ) WS-OS K k MN NK MK MNK ( 1 k 1 M 1 N ) Fig. 1. Matrix Mapping for Matrix-Matrix Multiplication with Conventional Stationary Schemes n units, while the weight tile shifts n units downward in the weight matrix. As the input tile transitions from west to east in the input matrix, it moves m units to the next row, repositioning the weight tile from right-bottom to left-top in the weight matrix. An arrow marked 2 produces the output tile. These steps repeat M m times, as indicated by the arrow marked 3 . b) Weight Stationary: The WS scheme loads each weight tile into internal memory once, enabling repeated use across multiple calculations, which reduces redundant memory ac- cess. Fig. 1(c) illustrates this dataflow. An arrow 1 indicates the fixed position of the weight tile along matrix columns, M, where each tile is reused M m times. Arrow 2 shows the input tile moving k units down the input matrix, while the stationary weight tile remains fixed.\n\n--- Segment 5 ---\nAn arrow 1 indicates the fixed position of the weight tile along matrix columns, M, where each tile is reused M m times. Arrow 2 shows the input tile moving k units down the input matrix, while the stationary weight tile remains fixed. As the input tile traverses from east to west, the weight tile remains in place and is reused for subsequent inputs, shifting vertically by k. The output tile generation, marked by 2 , follows this sequence and repeats K k times, indicated by 3 . c) Output Stationary: The OS strategies depicted in Fig. 1(d) and (e) showcase another commonly employed group of techniques. These approaches retain partial sums in internal memory until the computation of output results is completed, thereby optimizing inner products, which are fundamental to matrix-matrix multiplication. This method leverages the spatial locality of processing element arrays to reduce external data retrievals for partial sums. The primary distinction between the row-oriented OS and col-oriented OS schemes lies in the sequence of generating the weight matrix. In the row-oriented OS, outputs are generated row by row, while the column- oriented strategy produces results column by column. Fig. 1(d) shows arrows with black circle indicators where the input and weight tiles slide vertically and horizontally, respectively. Upon generating an output tile marked by 1 , indicated by arrows in 2 , the input tile returns to the initial position in the input matrix, and the weight tile shifts right by k. After repeating the data flow K k times, as shown by arrows marked as 3 , the input tile is moved downward by m, and the weight tile is moved to the starting weight matrix location. On the other hand, the column-oriented strategy initially redirects the weight tile, as shown by arrows in 2 , and later shifts the input tile, indicated by 3 . d) Problems of the fixed stationary scheme: Table II shows the external memory access values for different sta- tionary schemes. These fixed stationary schemes have two problems: higher external memory access and concurrent read write demands. For the first problem, in this table, m, n, and k represent tile dimensions and the available hardware computation resources, typically much smaller than M, N, and K. For instance, with an IS configuration, the EMA Fig. 2. Matrix Mapping for Matrix-Matrix Multiplication with Proposed Stationary Schemes of the input matrix is noticeably less compared to others, highlighting the overall EMA reduction.\n\n--- Segment 6 ---\n2. Matrix Mapping for Matrix-Matrix Multiplication with Proposed Stationary Schemes of the input matrix is noticeably less compared to others, highlighting the overall EMA reduction. Therefore, applying a fixed stationary scheme without considering the input length is inadvisable. For the second problem, external memory like DRAM cannot read and write data simultaneously. However, when computing linear projections, two matrices need to be read from external memory, while only one requires writing back. Though this can solved by extra buffers, the required buffer size could be quite large due to the large matrix size in current transformer models. Thus, an effective partial sum reuse scheme is demanded to solve this problem. III. METHODOLOGY This Section first shows the concept of the adaptive station- ary mechanism, then further enhances data reuse with a hybrid stationary scheme in a tile granularity, and finally presents the whole strategy. A. Adaptive Mechanism The adaptive mechanism is constructed to optimize the selection of the most efficient stationary data scheme in a tile granularity for specific EMA needs during matrix com- putations. The core decision focuses on choosing between IS or WS, determined by evaluating the EMA reduction for the matrix. In current accelerators, PE arrays are often organized in a square formation like 8 8 or 16 16. This design choice facilitates efficient tile computation mapping onto the processing elements, where m, n, and k are roughly equal. For existed accelerators, each PE arrays usually consists of a squared sized PEs, such that 8 8, or 16 16. That is, for the consideration of well porting tile computation to the processing elements, m, n, and k are approximately equivalent. As demonstrated in Table II, the input matrix primarily reduces total EMA when the input stationary scheme is applied, decreasing EMA from K (MN) to MN. In contrast, under the WS scheme, the weight matrix EMA is reduced by a factor of M. Thus, selecting either IS or WS depends simply on comparing MN and NK, corresponding to the input and weight matrix sizes, respectively. The decision is governed by the expression MN NK N(M K). When M K, the result is negative, suggesting IS is more efficient than WS. Conversely, when the result is zero or positive (M K), WS is more advantageous and preferred.\n\n--- Segment 7 ---\nWhen M K, the result is negative, suggesting IS is more efficient than WS. Conversely, when the result is zero or positive (M K), WS is more advantageous and preferred. This straightforward condition highlights the minimal overhead in decision-making hardware, which merely compares the input matrix s row count with the weight ma- trix s column count before performing matrix multiplication. By dynamically adjusting the stationary scheme at each com- puting phase, we achieve notable EMA reductions compared to a static scheme. For example, in a BERT model during the linear projection stage of the query, the EMA reduction for the specified matrix can exceed 94 , as depicted in Table III. This input-length adaptive stationary scheme maximizes resource efficiency. B. Hybrid Strategy in a Tile Granularity Fig. 1(b) and (c) show that the internal memory capacity for partial sums would be up to K and M registers for the respective data-reuse strategies of IS and WS. While (d) and (e) in Fig. 1 indicate that internal memory capacity reduces to n, which also is also equivalent to the shared dimension of input tile s column and weight tile s row. Consequently, to fully leverage the ability of decreasing EMA by IS or WS, and to consider the internal memory usage of partial sums for hardware implementation, we introduce the hybrid strategy based on the above adaptive mechanism. This strategy, in addition to the reuse of temporal data by IS or WS, incorporates a spatial reuse strategy in a tile granularity by combining the input weight stationary scheme with the output stationary scheme. This hybrid approach not only minimizes internal memory usage, but also ensures that partial sums are not stored in internal memory until the final results are generated. Therefore, the spatial reuse of partial sums successfully prevents frequent simultaneous reading and writing of data externally. C. Proposed Stationary Scheme The methodology for optimizing matrix-matrix multiplica- tion through our innovative tile-based adaptive stationary approach is elucidated in Fig. 2. In this illustration, Psum denotes partial sums, IF refers to input features, and WP signifies weight parameters. The choice between IS-OS and WS-OS strategies is based on the comparative dimensions of the input for linear projection computation; specifically, if M is less than K, we select IS-WS as shown in Fig. 2(a). In contrast, if this condition is not met, OS-WS is used, as illustrated in Fig. 2(b).\n\n--- Segment 8 ---\nIn contrast, if this condition is not met, OS-WS is used, as illustrated in Fig. 2(b). Within the diagram, the symbols k and m signify the number of partial sums stored internally, which is determined by the accelerator s internal memory capacity. For the IS-WS strategy, an input tile remains static for k k iterations, multiplying with weight tiles placed in k different positions along the K dimension, shifting by a distance k each cycle, indicated by 1 . Post temporal reuse via IS, we exploit k partial sums spatial reuse through a row-oriented OS, shown by an arrow at 2 . Once the input tile has exhausted the N dimension of the input matrix, it resets as denoted by 3 , and the weight tile shifts rightward by k to a new column set, continuing this process until the weight matrix is fully processed. Subsequently, the input tile moves downward by m and the sequence repeats. In scenarios where M K, the WS-OS strategy is activated via our adaptive scheme. Initially, the weight tile marked by 1 is fixed and reused for m m iterations, with the input tile shifting downward, as indicated by an arrow with 1 . Upon maximizing temporal locality through WS, we aim to reuse partial sums, leveraging spatial locality. As execution proceeds, both input and weight tiles shift horizontally and vertically, respectively, as portrayed by an arrow at 2 , re- peating the described sequence. Once the weight tile reaches the weight matrix s lower boundary, as shown by an arrow at 3 , it resets to the starting position while the input tile transitions to the subsequent row set, sliding anew from west to east across the input matrix. This series persists for M m iterations until the input tile fully traverses the input matrix once and returns to the matrix s upper-left corner, marked by an arrow at 4 . This comprehensive cycle persists until the weight tile completely explores the weight matrix. IV. EXPERIMENTAL RESULTS TABLE III COMPARISON OF EMA VALUES FOR DIFFERENT SEQUENCE LENGTHS IN THE WAV2VEC2.0-LARGE MODEL, WHERE seq len REPRESENTS FOR THE SEQUENCE LENGTH AND ss. IS FOR STATIONARY SCHEME seq len IS WS IS-WS optimal ss.\n\n--- Segment 9 ---\nEXPERIMENTAL RESULTS TABLE III COMPARISON OF EMA VALUES FOR DIFFERENT SEQUENCE LENGTHS IN THE WAV2VEC2.0-LARGE MODEL, WHERE seq len REPRESENTS FOR THE SEQUENCE LENGTH AND ss. IS FOR STATIONARY SCHEME seq len IS WS IS-WS optimal ss. 115 1.18 105 1.04 106 9.22 105 IS 384 3.93 105 1.04 106 6.47 105 IS 1565 1.60 106 1.05 106 5.54 105 WS 15000 1.54 107 1.06 106 1.43 107 WS To show the advantages of the proposed approach, this Section shows the results evaluated on the audio and nat- ural language processing tasks. Table III presents the EMA differences with the matrix applied in the stationary scheme during inference of the Wav2Vec2.0-large automatic speech recognition model [4] evaluated on the LibriSpeech [10] dataset. In this dataset, the shortest audio is about 2.3 seconds (115 tokens), and the longest one is 31.3 seconds (1565 tokens), with an average length of 7.6 seconds (384 tokens). Table III details the EMA values for the specified matrix where data reuse is applied across these sequence lengths. We also provide EMA values for recognizing lengthy speech sequences. For sequences exceeding the maximum length, they are usually segmented into chunks for inference. Here, the inference for increasing sequence length corresponds to more rows in the input matrix, maintaining the same computation flow and EMA analysis for linear projection. Table 2 shows that varying sequence lengths influence the optimal stationary scheme choice, indicating that TAS s adaptive mechanism compensates for limitations in fixed schemes.\n\n--- Segment 10 ---\nHere, the inference for increasing sequence length corresponds to more rows in the input matrix, maintaining the same computation flow and EMA analysis for linear projection. Table 2 shows that varying sequence lengths influence the optimal stationary scheme choice, indicating that TAS s adaptive mechanism compensates for limitations in fixed schemes. TABLE IV MEASUREMENT OF COMPUTING ENERGY COST FOR BERT-BASE WITH NAIVE IMPLEMENTATION, AYAKA [9] S OPTIMIZATION, AND OURS ACROSS LAYERS Layer ID Na ıve (A) [9] (B) Ours (C) Reduction A B A A C A 0 65.81 35.76 1.89 48.47 97.17 1 66.30 35.05 1.90 48.86 97.15 2 67.65 37.30 1.94 49.88 97.09 3 67.44 37.13 1.93 49.72 97.10 4 67.40 36.23 1.93 49.69 97.10 5 67.42 35.35 1.93 49.70 97.10 6 67.35 37.40 1.93 49.65 97.10 7 64.46 35.28 1.85 47.40 97.23 8 67.44 33.44 1.93 49.72 97.10 9 67.55 35.12 1.94 49.80 97.09 10 65.04 34.63 1.86 47.86 97.20 11 64.74 34.59 1.85 47.62 97.21 12 66.55 35.61 1.91 49.03 97.14 Table IV shows the energy consumption in the BERT- Base model by appling our method to [9]. The computational energy cost includes both external data transfer and internal chip processing, following the same approach and energy numbers in [9]. Basically, the energy consumed by external data transmission is 10 to 100 times greater than that of internal chip computation. To simplify the effective simulation of computing energy costs, measurements can be efficiently taken by evaluating the EMA ratio across various stationary schemes. The fixed stationary scheme introduced by [9] results in an approximate 48 reduction in energy usage during BERT-Base model inference, on average, when compared to a basic implementation lacking stationary schemes.\n\n--- Segment 11 ---\nTo simplify the effective simulation of computing energy costs, measurements can be efficiently taken by evaluating the EMA ratio across various stationary schemes. The fixed stationary scheme introduced by [9] results in an approximate 48 reduction in energy usage during BERT-Base model inference, on average, when compared to a basic implementation lacking stationary schemes. Our proposed stationary scheme achieves an approximately 97 reduction in energy consumption, providing double the energy efficiency compared to the stationary scheme from [9]. V. CONCLUSION This paper introduces the data ruse strategy of TAS, an adaptive and efficient approach to lower EMA in transformer accelerators. TAS dynamically adjusts the stationary scheme in a tile granularity according to input sequence length, sig- nificantly cutting EMA and tackling major energy bottleneck challenges in modern transformer accelerators. Experiments reveal TAS consistently outperforms fixed schemes (IS, WS, OS) across different transformer models, reducing EMA by over 97 in most scenarios. Additionally, TAS coordinates well with existing attention optimizations and hardware accel- erators, offering a flexible, energy-saving solution for large- scale transformer models. REFERENCES [1] A. Vaswani, Attention is all you need, Advances in Neural Information Processing Systems, 2017. [2] A. Dosovitskiy, An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929, 2020. [3] J. Devlin, Bert: Pre-training of deep bidirectional transformers for language understanding, arXiv preprint arXiv:1810.04805, 2018. [4] A. Baevski et al., wav2vec 2.0: A framework for self-supervised learning of speech representations, Advances in neural information processing systems, vol. 33, pp. 12 449 12 460, 2020. [5] X. Zhai et al., Scaling vision transformers, in Proceedings of the IEEE CVF conference on computer vision and pattern recognition, 2022, pp. 12 104 12 113. [6] A. Babu et al., Xls-r: Self-supervised cross-lingual speech representa- tion learning at scale, arXiv preprint arXiv:2111.09296, 2021. [7] T. B.\n\n--- Segment 12 ---\n[6] A. Babu et al., Xls-r: Self-supervised cross-lingual speech representa- tion learning at scale, arXiv preprint arXiv:2111.09296, 2021. [7] T. B. Brown, Language models are few-shot learners, arXiv preprint arXiv:2005.14165, 2020. [8] Y.-H. Chen et al., Eyeriss: An energy-efficient reconfigurable accelera- tor for deep convolutional neural networks, IEEE journal of solid-state circuits, vol. 52, no. 1, pp. 127 138, 2016. [9] Y. Qin et al., Ayaka: A versatile transformer accelerator with low-rank estimation and heterogeneous dataflow, IEEE Journal of Solid-State Circuits, 2024. [10] V. Panayotov et al., Librispeech: an asr corpus based on public domain audio books, in 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2015, pp. 5206 5210.\n\n