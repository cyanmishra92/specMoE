=== ORIGINAL PDF: 2506.03474v1_CORE_Constraint-Aware_One-Step_Reinforcement_Learn.pdf ===\n\nRaw text length: 44077 characters\nCleaned text length: 43678 characters\nNumber of segments: 29\n\n=== CLEANED TEXT ===\n\narXiv:2506.03474v1 [cs.LG] 4 Jun 2025 CORE: Constraint-Aware One-Step Reinforcement Learning for Simulation-Guided Neural Network Accelerator Design Yifeng Xiao Yurong Xu Ning Yan Masood Mortazavi Pierluigi Nuzzo Abstract Simulation-based design space exploration (DSE) aims to efficiently optimize high-dimensional structured designs under complex constraints and expensive eval- uation costs. Existing approaches, including heuristic and multi-step reinforcement learning (RL) methods, struggle to balance sampling efficiency and constraint satisfaction due to sparse, delayed feedback, and large hybrid action spaces. In this paper, we introduce CORE, a constraint-aware, one-step RL method for simulation- guided DSE. In CORE, the policy agent learns to sample design configurations by defining a structured distribution over them, incorporating dependencies via a scaling-graph-based decoder, and by reward shaping to penalize invalid designs based on the feedback obtained from simulation. CORE updates the policy using a surrogate objective that compares the rewards of designs within a sampled batch, without learning a value function. This critic-free formulation enables efficient learning by encouraging the selection of higher-reward designs. We instantiate CORE for hardware-mapping co-design of neural network accelerators, demonstrat- ing that it significantly improves sample efficiency and achieves better accelerator configurations compared to state-of-the-art baselines. Our approach is general and applicable to a broad class of discrete-continuous constrained design problems. 1 Introduction Simulation-based design space exploration (DSE) plays a critical role in automated hardware-software co-design, compiler tuning, and system-level optimization. Such optimization tasks typically involve complex, hybrid discrete-continuous design spaces, expensive evaluations through black-box sim- ulations, and strict design constraints. Traditional approaches, such as genetic algorithms [1] and Bayesian optimization [2], face significant challenges due to sparse and delayed feedback, scalability issues, and limited mechanisms for enforcing structural constraints. Methods based on reinforcement learning (RL) [3, 4] typically frame DSE as a sequential Markov decision process (MDP) problem [5, 6], requiring long rollout-based exploration or value function approximations that are often impractical for expensive simulation environments. Furthermore, these methods frequently rely on heuristic masking and coarse discretizations, reducing exploration efficiency and potentially violating feasibility constraints during training. Thus, an efficient method explicitly designed for structured, constraint-aware sampling remains an important open challenge. In this paper, we propose CORE (Constraint-aware One-step REinforcement learning), a one-step RL method [7] specifically tailored for simulation-guided structured DSE. CORE generates complete candidate configurations in a single step by learning a structured distribution over design variables, avoiding the inefficiencies of sequential rollout-based methods, the need to maintain intermediate University of California, Berkeley. {yifengx, Futurewei Technologies. {yan.ningyan, Preprint. Under review. design states, and the reward sparsity issues that arise in multi-step RL. To ensure constraint satisfac- tion and exploit parameter dependencies, CORE introduces a novel scaling-graph-based decoder that enforces parameter dependencies during sampling and a reward shaping mechanism to penalize in- valid configurations. The policy is updated by a critic-free surrogate objective based on batch-relative reward [8, 9, 10], which are obtained efficiently through parallel simulation of sampled designs, eliminating the need for value functions. We demonstrate CORE s effectiveness in a challenging application domain: the co-design of hardware and mapping strategies for spatial deep neural network (DNN) accelerators [11, 12, 13] to accelerate DNN inference by leveraging parallelism and data reuse. Given a DNN load, the accelerator architecture is designed to execute a fixed-size tensor computation while the mapping strategy determines how the computation is distributed among the hardware resources to achieve the best performance. This setting presents a representative and challenging testbed due to its rich structure, large combinatorial space, and costly simulation feedback, making it well-suited to assess the strengths of our method. We focus exclusively on this domain to enable a thorough evaluation, but the methodology is designed to generalize to other structured, simulation-based optimization problems. Our contributions can be summarized as follows: We propose CORE, a critic-free, one-step RL framework integrating parallel evaluations for simulation-based structured DSE problems. We introduce a scaling-graph-based decoding strategy that improves the generation of feasible configurations by explicitly modeling parameter dependencies and constraints. We design a constraint-aware reward-shaping mechanism to penalize invalid configurations, significantly enhancing the exploration efficiency. We evaluate CORE on the co-design of hardware and mapping strategies for DNN accelera- tors, showing that our method can achieve at least 15 improvement in both latency and latency-area-sum metrics with fewer sample designs, compared to state-of-the-art methods. The rest of the paper is organized as follows. Section 2 formulates the problem and introduces the background on DSE and DNN accelerator design. Section 3 presents the proposed optimization algorithm while Section 4 reports the evaluation results. Section 5 concludes the paper. 2 Background and Problem Formulation Simulation-Based Design Space Exploration. Simulation-based DSE methods have evolved from traditional heuristic-based algorithms, such as genetic algorithms [1] and simulated annealing [14], to more structured optimization techniques, including surrogate modeling and Bayesian optimization [2]. These problems are generally not amenable to analytical solutions due to the complexity and non- differentiable nature of the simulation-based objective metrics. We formalize the design problem: Problem 1 Given a design space D, a simulator U : D RJ that returns J performance metrics for a design configuration Î¾ D, a set of parameter dependency constraints {gi(Î¾) 0}K i 1, a set of performance constraints {hj(U(Î¾)) 0}M j 1, and a scalar reward function R : RJ R, the goal is to find an optimal configuration Î¾ that solves: maximize Î¾ D R(U(Î¾)) s.t. gi(Î¾) 0, i {1, . . . , K} hj(U(Î¾)) 0, j {1, . . . , M} (1) RL-Based Design Space Exploration. Recent work leverages RL to explore complex design spaces in domains such as memory controller tuning, system-on-chip design [5], and microarchitecture search [15]. These methods typically model the task as a multi-step MDP, where design choices are made sequentially and rewards are obtained only after a full design evaluation. This leads to sparse, delayed feedback and requires maintaining partial design states or heuristic masking [6] to enforce constraints, complicating learning and scaling. We instead adopt a one-step RL formulation [7], where the policy generates complete design candidates in one step, enabling efficient parallel sampling and eliminating the need for sequential rollouts, intermediate state designs, and sparse reward propagation. 2 PE PE PE PE PE PE PE PE PE L2 SP L1 SP MAC (a) S R C K X Y X Y Weight Input Activation Output Activation (b) Parameter Value Hardware resources of PEs 2 : 1024 : 2 L2 buffer size (bytes) 1 : 232 L1 buffer size (bytes) 1 : 232 Mapping strategies of level i in one layer Loop order Si, Ri, Ki, Ci, Xi, Yi Parallelization dimension Pi Si, Ri, Ki, Ci, Xi, Yi Level of parallelism Pi 1 : Pi Si 1 : Si Ri 1 : Ri Ki 1 : Ki Ci 1 : Ci Xi 1 : Xi Yi 1 : Yi (c) Figure 1: (a) Hardware resources for a 2-level spatial DNN accelerator; (b) Tensor dimensions for convolutional layers. (c) Design space of hardware resources and mapping strategies. Spatial DNN Accelerator Design. We instantiate Problem 1 for the co-design of hardware resources and mapping strategies in spatial DNN accelerators. Each design configuration Î¾ D consists of a structured combination of hardware parameters and mapping strategies. The simulator U(Î¾) evaluates a given design based on performance metrics such as latency, area, and power, which are encoded into a scalar reward via R(U(Î¾)). The constraint functions gi(Î¾) enforce dependencies over parameters (e.g., buffer bounds, tile hierarchy), while hj(U(Î¾)) encode system-level performance constraints such as area budgets across target platforms. Prior work typically searches for efficient mappings on fixed hardware or tunes hardware configu- rations under a fixed mapping strategy [16, 17, 5, 18]. While hardware-mapping interdependencies suggest that joint optimization can yield better performance, this remains challenging due to the vast combined design space [19, 17]. Recent approaches to address this problem include heuristic search [19] and two-step optimization [20, 21]. However, these methods are limited by sampling inefficiency and scalability for large design spaces. In contrast, CORE performs joint optimization over the design space by embedding structured design parameters into a continuous distribution. A scaling-graph-based decoder enforces constraints and dependencies during sampling enabling efficient and scalable exploration of valid configurations. Co-Design Space and Parameter Dependencies. As shown in Table 1c, the hardware design space includes the number of processing elements (PEs), and the sizes of L1 and L2 buffers. The mapping design space for each memory level i includes the loop ordering, tile sizes (Si, Ri, Ki, Ci, Xi, Yi), the parallelization dimension Pi, and the level of parallelism Pi. Here, the subscript i denotes the memory hierarchy level (e.g., i 1 for L1, i 2 for L2). Further details on parameter ranges and encoding are provided in Appendix A.1. The design space exhibits rich structural dependencies. For instance, buffer sizes and PEs constrain the feasible L2 buffer size; tile sizes must satisfy Di Di 1 across memory levels to preserve hierarchical partitioning [17], where each Di {Si, . . . , Yi} denotes the tile size of a particular loop dimension at memory level i.; and the number of PEs bounds the degree of parallelism via the tile size in the selected parallel dimension. 3 CORE Framework In this section, we introduce the CORE framework (Fig. 2) for simulation-based DSE. CORE employs an one-step RL formulation, where the policy models conditional probability density functions (PDFs) over structured design actions. Unlike traditional RL methods, CORE decouples policy learning from evaluation via a parallel pipeline consisting of conditional sampling, scaling-graph-based decoding, and simulation-driven reward feedback. This design supports high-throughput exploration under delayed and expensive feedback regimes. The policy is updated using a surrogate objective defined by the relative advantages of the sampled designs, enabling sample-efficient learning without value functions. We describe each component of the framework in the following sections. 3.1 One-Step Markov Decision Process and Sampling Policy Definition 1 (One-Step MDP) A one-step MDP is defined as the tuple M (s0, A, R), where s0 S denotes the single state of the environment, A denotes the action space, and the reward 3 ğ‘ ! ğœƒ" ğœƒ" Policy Update Objective ğ¿ (ğœƒ") Actions Parallel Evaluation Config. PDFs ğ’‡'! Conditional PDF-Based Sampling Scaling Graph-Based Decoding Simulation and Reward Shaping ğ‘“'!(ğ‘ ğ‘(, , ğ‘)) ğ‘“'!(ğ‘) ğ‘)) ğ‘“'!(ğ‘)) Policy Learning Design Space Target Power Latency Design Constraints Cloud Edge Optimal Design ğœ‰ ğ’‚"ğ‘¬ ğ’‚" ğŸ‘ ğ’‚"ğŸ ğ’‚"ğŸ ğœ‰"ğ‘¬ ğœ‰" ğŸ‘ ğœ‰"ğŸ ğœ‰"ğŸ ğ‘¨"ğ‘¬ ğ‘¨" ğŸ‘ ğ‘¨" ğŸ ğ‘¨" ğŸ Adv. Agent ğœ‹'! Entropy Objective ğ¿0(ğœƒ") KL Objective ğ¿1(ğœƒ") Figure 2: Overview of the CORE framework for simulation-based design space exploration. function R is defined as R(s0, a) : {s0} A R, a A. The agent interacts with the environment in episodes of length 1. We model the design exploration process as a one-step MDP, where a policy network Ï€Î¸ represents a joint distribution over design actions conditioned on a fixed input state s0. Since the environment is stateless, s0 acts as a static context vector and remains unchanged throughout training. The joint distribution Ï€Î¸(a1, . . . , aN; s0) is factorized as a product of conditional probability density functions: Ï€Î¸(a1, ..., aN; s0) N Y i 1 Ï€i,Î¸ N Y i 1 fi,Î¸(ai ai 1...aN; s0), (2) where each fi,Î¸ is a conditional PDF corresponding to an action ai, so the compound action a (a1, . . . , aN) is sampled from the joint distribution. This factorization captures statistical dependencies between design actions, allowing the model to learn probabilistic preferences for combinations of actions. These learned correlations are distinct from the structural dependencies encoded in the scaling graph decoder (Section 3.2), which ensures that the sampled actions are mapped to valid, feasible configurations. We consider two types of distributions for fi,Î¸: The categorical distribution models discrete actions drawn from a finite set of categories, parameterized by a probability vector p, where pi represents the probability of selecting the i-th category. The Beta distribution usually models continuous actions on the interval [0, 1], parameterized by two parameters, Î± and Î², which control the shape of the distribution. We use Beta distributions as continuous relaxations for discrete actions when the discrete space is large or context-dependent. A continuous value is sampled from the Beta distribution and rounded into a discrete value, as described in Section 3.2. For the design space shown in Table 1c, we sample the level of parallelism Pi, which includes 6 choices, using the categorical distribution. The remaining parameters are sampled from beta distributions to reduce the output dimension of the policy neural network (NN) and incorporate structural dependencies. 3.2 Decoding Actions to Configurations In each training episode, we use Ï€Î¸ to sample a batch of E compound design actions {ak}E k 1, where each ak can be decoded into a design configuration. We instantiate the evaluation procedure of DNN accelerator in Fig. 3, where lowercase letters represent sampled actions, uppercase letters indicate decoded configuration parameters, and color-coded mapping components distinguish design strategies across layers. We introduce a decoding technique that maps actions from the action space A to structured design space D, while satisfying parameter dependency constraints. 4 ğ‘“! ğ‘“" ğ‘“ ğ‘“ ğ‘“ ğ‘“ Sample ğ‘ƒğ¸! ğ‘ƒğ¸" ğ‘ƒğ¸ !" ğ¿! ğ¿" MAC ğ‘¾ğŸ: ğ¾! ğ¶! ğ‘†! ğ‘…! ğ‘¶ğŸ: ğ¾! ğ‘‹! ğ‘Œ! ğ‘°ğŸ: ğ¶! ğ‘‹! ğ‘Œ! ğ‘¾ğŸ: ğ¾" ğ¶" ğ‘†" ğ‘…" ğ‘°ğŸ: ğ¶" ğ‘‹" ğ‘Œ" ğ‘¶ğŸ: ğ¾" ğ‘‹" ğ‘Œ" Simulate Latency, Area, Power 1 2 ... (a) PDF-Based Sampling (c) Simulation for Structured Configurations ğ¿! ğ‘™! ğ¿" ğ‘™" ğ‘› ğ‘ ğ‘ƒ! ğ‘! P! p! ğ‘†" ğ‘…" ğ‘Œ" ğ¾" ğ¶" ğ‘‹" ğ‘ " ğ‘Ÿ" ğ‘¦" ğ‘˜" ğ‘" ğ‘¥" (b) Scaling Graph-Based Decoding ğ‘†" ğ‘…" ğ‘‹" ğ‘Œ" ğ¾" ğ¶" ğ‘ƒ" ğ‘ ! ğ‘Ÿ! ğ‘¦! ğ‘˜! ğ‘! ğ‘! ğ‘¥! ğ‘†! ğ‘…! ğ‘Œ! ğ¾! ğ¶! ğ‘ƒ! ğ‘‹! ğ‘†! ğ‘…! ğ‘¦! ğ‘˜! ğ‘! ğ‘! ğ‘¥! ğ‘™" ğ‘™! ğ‘› ğ‘ƒ"p! P!p! Decode Figure 3: Evaluation pipeline for hardware-mapping co-design. Action Discretization. For independent discrete parameters with a wide range of values, we use the Beta distribution to sample an action b [0, 1] and then round it into a discrete value B. Generally, assuming a range [Blow, Bup] and a step size Bs, we discretize the action as follows: B Blow Bup Blow Bs 1 b Bs, (3) where x denotes the floor function, and the action b is scaled to match the range of the parameter B. By taking the number of PEs as an example, we have 512 choices, i.e., Npe can be selected in the range from 2 to 1024 with steps of 2. Using the beta distribution to produce an action npe [0, 1], we quantize this action as Npe 2 2 512npe . L2 buffer size L1 buffer size of PEs L1 parallel level L1 parallel dimension L2 tile size L1 tile size Figure 4: Scaling graph for DNN accelerator. Scaling-Graph-Based Decoding. To effectively navigate structured parameter spaces with interde- pendencies, we introduce a decoding strategy based on a scaling graphs, which dynamically adjusts the parameter bounds, improving feasibility and acceler- ating convergence during exploration [22]. As shown in Fig. 4, a scaling graph captures structural depen- dencies between design parameters. Each node rep- resents a design variable, and each directed edge encodes a constraint or scaling relationship from a source (influencing) parameter to a target (dependent) parameter. Specifically, the decoded value of the source constrains the feasible range of the target variable, such as the upper bound. Given a set of source parameters with decoded values {Ai}, and a target parameter bounded within [Blow, Bup] with step size Bs, we consider the upper bound relation and a sampled action b such that: B Blow mini{Ai} Blow Bs 1 b Bs, (4) where we replace Bup with mini{Ai} in Equation (3) to ensure that the upper bound is determined by the source nodes. Intuitively, this scaling ensures that sampled actions respect inter-parameter dependencies by dynamically constraining action ranges according to previously decoded parameters. Decoding follows the topological order of the scaling graph to respect dependencies. In the example shown in Fig. 3 (b), the upper bound for the level of parallelism P1 is constrained by the number of PEs and the corresponding tile size in level 2, which is determined by the selected parallelization dimension P1. Assuming P1 Plow and the selected parallel dimension is X, i.e., P1 X, we decode P1 as follows: P1 Plow (min{Npe, X2} Plow 1)p1 . (5) In summary, the scaling graph enables constraint-aware decoding by dynamically adjusting parameter bounds during sampling. This ensures that the sampled configurations are feasible and accelerates learning. 3.3 Parallel Sampling Policy Optimization We customize proximal policy optimization [4, 23] for design exploration. The objective function R(Î¾) in Problem (1) is obtained from the simulation results of a design point Î¾. We incorporate the 5 constraint-aware reward shaping to formulate a surrogate objective function L(Î¸t), which is used to update the policy NN. Reward and Surrogate Advantage. Let {Î¾k}E k 1 denote a batch of E design samples, where each Î¾k D is a candidate design point from the design space D. Each design is evaluated by a simulator or cost model, which returns J performance metrics (e.g., latency, power, area). We define the simulator as a function: U : D RJ, which returns a set of metrics value for a design point Î¾. Then, the optimization target can be expressed as a weighted sum of the metrics: R(Î¾k) w U(Î¾k), (6) where w RJ is a user-defined weight vector applied to the simulator outputs. If a lower value of a metric (e.g., latency or energy consumption) indicates a better design, we assign the corresponding weight wj 0 so that the scalar reward R(Î¾k) increases as design quality improves. This ensures that all objectives are aligned under a reward maximization framework. At each training episode t, the agent samples a batch of E design points {Î¾k}E k 1 and evaluates them using parallel simulation to obtain a scalar reward R(Î¾k) for each sample. We define the running reward Ë†Rt to track the exponential moving average of batch rewards, which gives higher weight to recent batches while gradually discounting earlier ones: Ë†Rt Î±rEÎ¾ Ï€Î¸t[R(Î¾)] (1 Î±r) Ë†Rt 1, Ë†R0 0, (7) EÎ¾ Ï€Î¸t[R(Î¾)] 1 E E X k 1 R(Î¾k), (8) where the expectation over the policy distribution is approximated by the empirical mean of the current batch, and Î±r [0, 1] is the renewal rate. Based on this estimate, we define the surrogate advantage At(Î¾k) to measure the relative quality of each sampled design in the batch, computed as the difference between its reward and the running average: At(Î¾k) R(Î¾k) Ë†Rt. (9) Constraint-Aware Reward Shaping. If a design point violates a required constraint Ï•, we apply a scaling penalty to quantify the degree of violation. Given a constraint h(U(Î¾k)) 0, if it is violated, the reward is updated as follows: R(Î¾k) w U(Î¾k) Î±ch(U(Î¾k)), (10) where Î±c is the violation penalty rate. If a design point cannot be simulated, possibly due to ignoring certain dependencies or architectural constraints, we call it an anomalous design. We define a penalty to push the reward below the average, discouraging the agent from generating similar samples in the future. The reward for an anomalous design Î¾ in episode t is computed as: Rt(Î¾ ) ( min(EÎ¾ Ï€Î¸t 1[R(Î¾)], Ë†Rt 1) Î±pEÎ¾ Ï€Î¸t[R(Î¾)], t 1 Rano t 1 (11) where Î±p is the anomalous design penalty rate, Rano is the initial reward for the first episode, and the expectation is approximated by the empirical mean of the batch. Surrogate Objective. The surrogate objective function comprises the conditional update objective, the Kullback-Leibler (KL) objective, and the entropy objective [8]. First, we update the policy parameters Î¸ based on the surrogate advantage through the conditional update objective, which is computed as follows: Lup(Î¸t) EÎ¾ Ï€Î¸t Ï€Î¸(a1, ..., aN; s0) Ï€Î¸t(a1, ..., aN; s0)At(Î¾k) . (12) Moreover, a KL-regularizer is added to regulate the update rate of the policy as follows: Lr(Î¸t) Î²r N X i 1 DKL(Ï€i,Î¸ Ï€i,Î¸t), (13) 6 Algorithm 1 Constraint-Aware One-Step RL for Design Exploration Require: Design Space D, Policy NN Ï€Î¸, input state s0, scaling graph G, batch size E, maximum num- ber of episodes tm, target reward R, dependency constraints {gi(Î¾) 0}K i 1, performance constraints {hj(U(Î¾)) 0}M j 1, weights w for objective metrics, and learning rate Î·. 1: t 1, Ë†R0 0. 2: Initialize parameters Î¸1 of policy network Ï€Î¸. 3: while t tm do 4: a1, , aE Ï€Î¸t(s0); Sample E compound actions (Sec. 3.1) 5: Î¾1, , Î¾E Decode( a1, , aE , D, {gi}K i 1); Decode via scaling graph (Sec. 3.2) 6: R(Î¾1), , R(Î¾E) Reward( U(Î¾1), , U(Î¾E) , w, {hj}M j 1); Equation (6), (10), (11) 7: Rmax, Î¾best FindBest(R(Î¾1), , R(Î¾E)); Find Î¾ with the maximum reward 8: if Rmax R then 9: break; 10: Ë†Rt Î±r 1 E PE k 1 R(Î¾k) (1 Î±r) Ë†Rt 1; Equation (7), (8) 11: L(Î¸t) ComputeObj( R(Î¾1), , R(Î¾E) , Ë†Rt, Ï€Î¸t); Equation (9), (12), (13), (14) 12: Î¸t 1 Î¸t Î· Î¸L(Î¸t); Policy update via gradient ascent 13: t t 1; 14: return Î¾best; where DKL is the forward KL divergence [23] and Î²r is a factor for the KL objective, encouraging the updated policy to stay close to the current policy. Lastly, to balance exploration and exploitation, we include an entropy regularization term that encourages the policy to maintain uncertainty in its action distributions. This regularization prevents early convergence to suboptimal, overconfident actions and encourages diverse exploration early in training. To reduce unnecessary randomness later, we multiply the entropy bonus by a decaying factor Î²e [8], which gradually shifts the policy from exploration to exploitation: Le(Î¸t) Î²e N X i 1 H(Ï€i,Î¸), (14) where H is the entropy function. With the objective function L(Î¸t) Lup(Î¸t) Lr(Î¸t) Le(Î¸t), optimization algorithms such as gradient ascent can be used for back-propagation to update the policy NN parameters Î¸, guiding the model toward an optimal policy. Our framework is summarized in Algorithm 1. Training terminates when either the maximum number of episodes tm is reached or the target reward threshold R is achieved. 4 Experiments 4.1 Experiment Setup Policy and Hyperparameters We implement CORE in Python using the PyTorch library and evaluate it on a set of DNN models for various applications. The policy network is a 4-layer multilayer perceptron with ReLU activations (see Appendix B for architecture details). It is trained for 2000 episodes using the Adam optimizer with default beta parameters, a learning rate of 10 5, and a batch size of 32, which corresponds to the number of parallel CPU threads used for simulation. The entropy coefficient Î²e is linearly decayed from 1.0 to 0.02, the surrogate reward is computed with a renewal rate Î±r of 0.2, and other rate factors are fixed at 1.0. No hyperparameter tuning is performed. We implement all experiments on a server with an NVIDIA V100 GPU. DNN Accelerator Setup We consider a 2-level mapping strategy for the accelerator design space in the experiments. We evaluate seven DNN models across vision (ResNet-18, ResNet-50, MobileNetV2, VGG-16), language (BERT), and recommendation (DLRM, NCF) domains [17]. For transformer models like BERT, the mapping space is defined to capture matrix multiplications in attention and feed-forward layers. This work focuses on inference configuration only. Area constraints for PEs and buffers are imposed to reflect realistic platform settings: 0.2 mm2 for edge devices and 7.0 mm2 for cloud platforms [24]. While DRAM bandwidth and interconnects vary in practice, we abstract these under a unified simulation interface to maintain comparability. The 7 Table 1: Log10-scaled performance comparison of baseline methods and ablation studies with CORE on cloud and edge platforms. Objective Latency (log10 cycles) Latency-Area-Sum (log10) Method GA HASCO w o. rs. w o. sc. CORE GA HASCO w o. rs. w o. sc. CORE Cloud Platform Resnet18 7.28 6.80 6.68 5.26 4.62 7.44 6.79 6.76 6.14 5.66 Resnet50 7.29 7.30 7.31 6.47 5.47 7.40 6.92 7.34 6.97 6.18 Mbnet-V2 7.01 6.79 6.20 5.80 4.33 6.89 6.75 6.30 6.54 5.14 BERT 7.31 6.85 6.74 5.95 5.60 7.08 6.74 6.94 6.52 5.99 NCF 3.50 3.48 3.66 3.47 2.17 4.65 3.41 4.53 4.16 4.06 DLRM 2.57 2.60 2.83 3.73 2.03 4.09 3.72 4.06 4.08 4.05 VGG16 7.85 7.43 7.51 5.84 5.05 7.92 7.65 7.52 7.12 6.08 Edge Platform Resnet18 7.20 7.06 7.03 5.33 5.31 7.39 6.79 7.04 6.50 5.59 Resnet50 7.37 7.12 - 6.74 5.47 7.25 6.92 - 7.02 6.26 Mbnet-V2 7.07 6.74 - 6.49 4.97 7.22 6.73 - 6.35 5.16 BERT 7.18 6.76 - 6.37 5.83 7.35 6.70 - 6.66 6.02 NCF 3.86 3.32 3.89 3.65 2.78 4.64 3.65 4.56 4.34 4.06 DLRM 2.68 2.58 3.68 3.88 3.37 4.13 3.12 4.38 4.46 4.05 VGG16 7.58 7.71 7.87 5.84 5.13 7.97 7.66 7.87 6.93 6.14 simulation-based evaluation uses the MAESTRO cost model [25] for performance and Synopsys DC, Cadence Innovus for area estimation (via RTL synthesis with the Nangate 15nm library) [19]. We set a fixed sampling budget of 40,000 for all algorithms to maintain consistency. 4.2 Main Results Comparisons with Baseline Optimization Algorithms. We use a genetic algorithm (GA) in the literature [5] as the baseline for the co-optimization problem. Additionally, we compare our approach with the state-of-the-art open-source optimization algorithm HASCO [20], which is a two-stage algorithm using multi-objective Bayesian optimization to explore the hardware design space and heuristic and Q-learning algorithms to optimize the mappings across all the layers of the network. Because the space defined in Figure 1c is too large for HASCO, we limit it by using larger step sizes when discretizing the ranges of the number of PEs and the buffer sizes. The results are shown in Table 1, where we optimize the latency and the sum of latency and area (LAS) of the accelerator design for each DNN model on cloud and edge platforms. We optimize the entire model as a whole, and the reported values reflect the average reward computed over all layers. While HASCO achieves better metrics on smaller DNNs like NCF and DLRM, CORE consistently obtains optimal results across other cases, demonstrating its superiority in terms of larger DNNs and more complex design spaces. We average the results across all models for each platform and illustrate in Figure 5 the average latency and LAS for each method. CORE outperforms both GA and HASCO in terms of latency and LAS of the resulting design, achieving at least a 15 improvement in the optimal reward. Ablation Study. We also conduct an ablation study to evaluate the effectiveness of the reward design and the graph-based scale-decoding strategy in CORE. As detailed in Table 1, the column w o. rs. indicates the results when a negative penalty reward replaces the reward shaping upon constraint violations and anomalous designs, while the column w o. sc. shows the outcomes when configurations are decoded independently from the produced actions, ignoring parameter dependencies. The performance benefits are due to the structure-awareness and the mechanism to reason about constraints in CORE, which we expect to be transferable to other constrained domains. The reward shaping can provide feedback when design points are invalid, enabling the agent to learn quantitative information, which facilitates the exploration process. In some cases with edge platforms, the algorithm without the reward shaping fails to find a valid design point throughout the exploration process, shown as -" in the table. Similarly, the dependency decoding strategy is instrumental in maintaining the feasibility of the decoded configurations, helping the agent understand the parameter dependencies and select feasible actions. The ablation study shows a performance drop when either method is removed. 8 GA HASCO w o. rs. w o. sc. CORE 105 106 107 Latency (a) Average latency GA HASCO w o. rs. w o. sc. CORE 105 106 107 LAS Cloud Edge (b) Average LAS Figure 5: Average target metrics on different platforms for different methods. 0 1 2 3 104 106 107 108 of Samples Latency (cycles) GA w o. sc. CORE (a) 0 1 2 3 104 106 107 108 109 of Samples LAS (b) Figure 6: Optimized latency and LAS as the number of samples increases: (a) Change in optimized latency for ResNet50 with the cloud platform; (b) Change in optimized LAS for BERT with the edge platform. Sample Efficiency. The overall runtime can vary across different machines and parallelization set- tings, and it is mainly determined by the number of simulations re- quired to achieve the optimized design. Therefore, we measure the number of sampled designs to achieve optimized latency and LAS to evaluate the space explo- ration efficiency. The changes in latency for ResNet50 and a cloud platform and the changes in LAS for BERT with an edge platform can be seen in Fig. 6, where the x-axis represents the number of samples, and the y-axis denotes the value of the optimized objective. HASCO is not included in this comparison, as it is a two-stage algorithm that optimizes the hardware design space and mappings separately, which is not directly comparable to our single-step approach. The method without reward shaping is also omitted. Both GA and the non-reward shaping method converge faster, but our work achieves convergence with significantly better metrics. The results show that CORE requires fewer samples to achieve lower latency and LAS in both cases. Discussion. While our approach demonstrates strong performance across simulated metrics in efficiently navigating structured design spaces, its effectiveness depends on the fidelity of the un- derlying cost models. In the context of DNN accelerator design, our formulation further assumes static, compile-time mapping strategies. Extending the framework to support dynamic or runtime- adaptive dataflows presents a promising direction for future work. Interestingly, a recent method [10] independently proposes a similar idea to compute the advantage via intra-batch reward comparisons in the context of optimizing LLM training dynamics, such as token sampling and update strategies. This convergence highlights the generality of our approach, suggesting its applicability beyond DNN accelerator design to other complex, constraint-rich domains such as compiler tuning, robotic system co-design, and automated system architecture search, where simulation costs and structural constraints are similarly restrictive. 5 Conclusion We presented CORE, a critic-free, one-step reinforcement learning approach for constraint-aware, simulation-guided design space exploration. CORE addresses critical limitations of existing meth- ods by incorporating a surrogate objective based on relative advantages and enforcing feasibility constraints through a scaling-graph-based decoder. We validated CORE s effectiveness on the chal- lenging task of neural network accelerator hardware-mapping co-design, demonstrating significantly improved sampling efficiency and better optimization outcomes compared to state-of-the-art methods. Although demonstrated in the accelerator design domain, the CORE framework and its representa- tions generalize broadly to other complex design tasks. Future work includes extending CORE to 9 dynamic design environments, incorporating runtime adaptivity, and integrating it with symbolic reasoning or large language models for enhanced interpretability and scalability. References [1] John H Holland. Genetic algorithms. Scientific american, 267(1):66 73, 1992. [2] Bowen Lei, Tanner Quinn Kirk, Anirban Bhattacharya, Debdeep Pati, Xiaoning Qian, Raymundo Arroyave, and Bani K Mallick. Bayesian optimization with adaptive surrogate models for automated experimental design. Npj Computational Materials, 7(1):194, 2021. [3] Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A survey. Journal of artificial intelligence research, 4:237 285, 1996. [4] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [5] Srivatsan Krishnan, Amir Yazdanbakhsh, Shvetank Prakash, Jason Jabbour, Ikechukwu Uchendu, Susobhan Ghosh, Behzad Boroujerdian, Daniel Richins, Devashree Tripathy, Aleksandra Faust, et al. Archgym: An open-source gymnasium for machine learning assisted architecture design. In Proceedings of the 50th Annual International Symposium on Computer Architecture, pages 1 16, 2023. [6] Zixuan Jiang, Ebrahim Songhori, Shen Wang, Anna Goldie, Azalia Mirhoseini, Joe Jiang, Young-Joon Lee, and David Z Pan. Delving into macro placement with reinforcement learning. In 2021 ACM IEEE 3rd Workshop on Machine Learning for CAD (MLCAD), pages 1 3. IEEE, 2021. [7] Hassan Ghraieb, Jonathan Viquerat, AurÃ©lien Larcher, Philippe Meliga, and Elie Hachem. Single-step deep reinforcement learning for open-loop control of laminar and turbulent flows. Physical Review Fluids, 6(5):053902, 2021. [8] Masood S. Mortazavi, Tiancheng Qin, and Ning Yan. Theta-resonance: A single-step reinforcement learning method for design space exploration, 2022. [9] Jiayu Li, Masood S. Mortazavi, and Ning Yan. Exploring distributed circuit design using single-step reinforcement learning. In 2024 60th ACM IEEE Design Automation Conference (DAC). IEEE, 2024. [10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [11] Yu-Hsin Chen, Tushar Krishna, Joel S Emer, and Vivienne Sze. Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks. IEEE journal of solid-state circuits, 52(1):127 138, 2016. [12] NVDLA Project. Nvdla deep learning accelerator. 2017. [13] Zidong Du, Robert Fasthuber, Tianshi Chen, Paolo Ienne, Ling Li, Tao Luo, Xiaobing Feng, Yunji Chen, and Olivier Temam. Shidiannao: Shifting vision processing closer to the sensor. In Proceedings of the 42nd annual international symposium on computer architecture, pages 92 104, 2015. [14] Dimitris Bertsimas and John Tsitsiklis. Simulated annealing. Statistical science, 8(1):10 15, 1993. [15] Chen Bai, Jianwang Zhai, Yuzhe Ma, Bei Yu, and Martin DF Wong. Towards automated risc-v mi- croarchitecture design with reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 12 20, 2024. [16] Sheng-Chun Kao, Geonhwa Jeong, and Tushar Krishna. Confuciux: Autonomous hardware resource as- signment for dnn accelerators using reinforcement learning. In 2020 53rd Annual IEEE ACM International Symposium on Microarchitecture (MICRO), pages 622 636. IEEE, 2020. [17] Sheng-Chun Kao and Tushar Krishna. Gamma: Automating the hw mapping of dnn models on accelerators via genetic algorithm. In Proceedings of the 39th International Conference on Computer-Aided Design, pages 1 9, 2020. [18] Size Zheng, Yun Liang, Shuo Wang, Renze Chen, and Kaiwen Sheng. Flextensor: An automatic schedule exploration and optimization framework for tensor computation on heterogeneous system. In Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, pages 859 873, 2020. 10 [19] Sheng-Chun Kao, Michael Pellauer, Angshuman Parashar, and Tushar Krishna. Digamma: Domain-aware genetic algorithm for hw-mapping co-optimization for dnn accelerators. In DATE, 2022. [20] Qingcheng Xiao, Size Zheng, Bingzhe Wu, Pengcheng Xu, Xuehai Qian, and Yun Liang. Hasco: To- wards agile hardware and software co-design for tensor computation. In 2021 ACM IEEE 48th Annual International Symposium on Computer Architecture (ISCA), pages 1055 1068. IEEE, 2021. [21] Bahador Rashidi, Chao Gao, Shan Lu, Zhisheng Wang, Chunhua Zhou, Di Niu, and Fengyu Sun. Unico: Unified hardware software co-optimization for robust neural network acceleration. In Proceedings of the 56th Annual IEEE ACM International Symposium on Microarchitecture, pages 77 90, 2023. [22] Stijn Eyerman, Lieven Eeckhout, Tejas Karkhanis, and James E Smith. A mechanistic performance model for superscalar out-of-order processors. ACM Transactions on Computer Systems (TOCS), 27(2):1 37, 2009. [23] Chloe Ching-Yun Hsu, Celestine Mendler-DÃ¼nner, and Moritz Hardt. Revisiting design choices in proximal policy optimization. arXiv preprint arXiv:2009.10897, 2020. [24] Yakun Sophia Shao, Jason Clemons, Rangharajan Venkatesan, Brian Zimmer, Matthew Fojtik, Nan Jiang, Ben Keller, Alicia Klinefelter, Nathaniel Pinckney, Priyanka Raina, et al. Simba: Scaling deep-learning inference with multi-chip-module-based architecture. In Proceedings of the 52nd Annual IEEE ACM International Symposium on Microarchitecture, pages 14 27, 2019. [25] Hyoukjun Kwon, Prasanth Chatarasi, Vivek Sarkar, Tushar Krishna, Michael Pellauer, and Angshuman Parashar. Maestro: A data-centric approach to understand reuse, performance, and hardware cost of dnn mappings. IEEE micro, 40(3):20 29, 2020. [26] Angshuman Parashar, Priyanka Raina, Yakun Sophia Shao, Yu-Hsin Chen, Victor A Ying, Anurag Mukkara, Rangharajan Venkatesan, Brucek Khailany, Stephen W Keckler, and Joel Emer. Timeloop: A systematic approach to dnn accelerator evaluation. In 2019 IEEE international symposium on performance analysis of systems and software (ISPASS), pages 304 315. IEEE, 2019. A Detailed Description of the Spatial DNN Accelerator This appendix provides additional information on the spatial Deep Neural Network (DNN) accelerator co-design framework, facilitating understanding of hardware resource allocation, mapping strategies, and associated performance modeling. A.1 Spatial DNN Accelerator Resources A.1.1 Hardware Resources A spatial DNN accelerator primarily comprises an array of processing elements (PEs), each equipped with a multiply-accumulate (MAC) unit and Level-1 (L1) buffers for local data storage. An accelerator typically also includes a shared Level-2 (L2) buffer that acts as a bridge between off-chip memory and the on-chip L1 buffers, optimizing data fetching and reducing memory bandwidth demands. Networks-on-Chip (NoCs) facilitate efficient operand distribution from the L2 buffer to individual PEs and the collection of partial outputs, subsequently stored back into the L2 buffer. A.1.2 Mapping Strategies Mapping strategies determine how computations of a DNN layer are partitioned and executed across hardware resources. Tiling Strategy Tiling divides large tensors, such as weights and activations, into smaller blocks or "tiles" to fit within the L1 buffers of the PEs. Computation is conducted on one tile at a time, enhancing data reuse and minimizing redundant memory transfers. The choice of tile size impacts the amount of data that must be simultaneously stored within on-chip buffers. Computation Order The sequence in which loop computations are executed significantly affects performance. For instance, the six-loop nest structure of 2D convolution operations (Fig. 7) provides multiple possible orderings (6! possibilities), each with distinct implications for data reuse and performance. 11 S R C K X Y X Y Weight Input Activation Output Activation Figure 7: Notation for the dimensions used in convolution operations. K: output channels, C: input channels, Y: input height, X: input width, R: filter height, S: filter width, Y : output height, X : output width. Parallelism Strategy This strategy specifies the number of PEs concurrently utilized and identifies which loop dimensions to parallelize at each memory hierarchy level, greatly influencing accelerator throughput and efficiency. A.2 Accelerator Performance and Cost Modeling A.2.1 DNN Model Characteristics Different DNN architectures (e.g., convolutional, fully connected, transformer layers) have unique computational patterns and data reuse opportunities. Convolutional layers dominate image-processing models, whereas fully connected layers appear frequently in language and recommendation models, each requiring tailored accelerator resources and mappings. A.2.2 Cost Models Accurate and efficient estimation of accelerator performance and resource usage is provided by analytical cost models such as MAESTRO [25] and Timeloop [26]. These tools analyze hardware configurations and mapping strategies to quantify runtime, area, and power consumption, thus enabling informed decisions during accelerator design optimization. B Policy Network Architecture In the experiment, we use a 4-layer multilayer perceptron with ReLU activations after each hidden layer. The input dimension is 512. The hidden layers have widths 4096, and the output layer matches the action space size. The layer dimensions are: 512 4096 4096 4096 parameters for Ï€Î¸(a1, ..., aN; s0). The network outputs conditional probability distributions for the design actions. Specifically, each design parameter pi is modeled using a parametric distribution: Beta distribution: requires 2 parameters (e.g., Î±i, Î²i) Categorical distribution: requires k parameters for k categories (e.g., logits) Let N be the number of design parameters, and let dim(pi) denote the number of parameters required for pi. Then, the total output size of the policy network is: parameters for Ï€Î¸(a1, ..., aN; s0) N X i 1 dim(pi) C Broader Impacts This work proposes a constraint-aware reinforcement learning framework to accelerate simulation- based DSE for DNN accelerator co-design. The primary positive societal impact lies in reducing 12 the computational and time cost of designing efficient AI hardware, which may enable deployment in resource-constrained domains such as edge computing, mobile healthcare, and environmental monitoring. Our method promotes sustainability by optimizing design with fewer simulations, contributing to greener hardware design workflows. However, potential negative societal impacts include the risk of such optimization techniques being used to develop accelerators for ethically concerning applications. Furthermore, design automation could displace roles traditionally held by domain experts. These concerns warrant thoughtful deployment and governance when adopting such techniques. We believe that transparent reporting, open-access implementation, and continued interdisciplinary dialogue are important to mitigate misuse and align technical advancement with societal values. 13\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2506.03474v1 [cs.LG] 4 Jun 2025 CORE: Constraint-Aware One-Step Reinforcement Learning for Simulation-Guided Neural Network Accelerator Design Yifeng Xiao Yurong Xu Ning Yan Masood Mortazavi Pierluigi Nuzzo Abstract Simulation-based design space exploration (DSE) aims to efficiently optimize high-dimensional structured designs under complex constraints and expensive eval- uation costs. Existing approaches, including heuristic and multi-step reinforcement learning (RL) methods, struggle to balance sampling efficiency and constraint satisfaction due to sparse, delayed feedback, and large hybrid action spaces. In this paper, we introduce CORE, a constraint-aware, one-step RL method for simulation- guided DSE. In CORE, the policy agent learns to sample design configurations by defining a structured distribution over them, incorporating dependencies via a scaling-graph-based decoder, and by reward shaping to penalize invalid designs based on the feedback obtained from simulation. CORE updates the policy using a surrogate objective that compares the rewards of designs within a sampled batch, without learning a value function. This critic-free formulation enables efficient learning by encouraging the selection of higher-reward designs. We instantiate CORE for hardware-mapping co-design of neural network accelerators, demonstrat- ing that it significantly improves sample efficiency and achieves better accelerator configurations compared to state-of-the-art baselines. Our approach is general and applicable to a broad class of discrete-continuous constrained design problems. 1 Introduction Simulation-based design space exploration (DSE) plays a critical role in automated hardware-software co-design, compiler tuning, and system-level optimization. Such optimization tasks typically involve complex, hybrid discrete-continuous design spaces, expensive evaluations through black-box sim- ulations, and strict design constraints. Traditional approaches, such as genetic algorithms [1] and Bayesian optimization [2], face significant challenges due to sparse and delayed feedback, scalability issues, and limited mechanisms for enforcing structural constraints. Methods based on reinforcement learning (RL) [3, 4] typically frame DSE as a sequential Markov decision process (MDP) problem [5, 6], requiring long rollout-based exploration or value function approximations that are often impractical for expensive simulation environments.\n\n--- Segment 2 ---\nTraditional approaches, such as genetic algorithms [1] and Bayesian optimization [2], face significant challenges due to sparse and delayed feedback, scalability issues, and limited mechanisms for enforcing structural constraints. Methods based on reinforcement learning (RL) [3, 4] typically frame DSE as a sequential Markov decision process (MDP) problem [5, 6], requiring long rollout-based exploration or value function approximations that are often impractical for expensive simulation environments. Furthermore, these methods frequently rely on heuristic masking and coarse discretizations, reducing exploration efficiency and potentially violating feasibility constraints during training. Thus, an efficient method explicitly designed for structured, constraint-aware sampling remains an important open challenge. In this paper, we propose CORE (Constraint-aware One-step REinforcement learning), a one-step RL method [7] specifically tailored for simulation-guided structured DSE. CORE generates complete candidate configurations in a single step by learning a structured distribution over design variables, avoiding the inefficiencies of sequential rollout-based methods, the need to maintain intermediate University of California, Berkeley. {yifengx, Futurewei Technologies. {yan.ningyan, Preprint. Under review. design states, and the reward sparsity issues that arise in multi-step RL. To ensure constraint satisfac- tion and exploit parameter dependencies, CORE introduces a novel scaling-graph-based decoder that enforces parameter dependencies during sampling and a reward shaping mechanism to penalize in- valid configurations. The policy is updated by a critic-free surrogate objective based on batch-relative reward [8, 9, 10], which are obtained efficiently through parallel simulation of sampled designs, eliminating the need for value functions. We demonstrate CORE s effectiveness in a challenging application domain: the co-design of hardware and mapping strategies for spatial deep neural network (DNN) accelerators [11, 12, 13] to accelerate DNN inference by leveraging parallelism and data reuse. Given a DNN load, the accelerator architecture is designed to execute a fixed-size tensor computation while the mapping strategy determines how the computation is distributed among the hardware resources to achieve the best performance. This setting presents a representative and challenging testbed due to its rich structure, large combinatorial space, and costly simulation feedback, making it well-suited to assess the strengths of our method.\n\n--- Segment 3 ---\nGiven a DNN load, the accelerator architecture is designed to execute a fixed-size tensor computation while the mapping strategy determines how the computation is distributed among the hardware resources to achieve the best performance. This setting presents a representative and challenging testbed due to its rich structure, large combinatorial space, and costly simulation feedback, making it well-suited to assess the strengths of our method. We focus exclusively on this domain to enable a thorough evaluation, but the methodology is designed to generalize to other structured, simulation-based optimization problems. Our contributions can be summarized as follows: We propose CORE, a critic-free, one-step RL framework integrating parallel evaluations for simulation-based structured DSE problems. We introduce a scaling-graph-based decoding strategy that improves the generation of feasible configurations by explicitly modeling parameter dependencies and constraints. We design a constraint-aware reward-shaping mechanism to penalize invalid configurations, significantly enhancing the exploration efficiency. We evaluate CORE on the co-design of hardware and mapping strategies for DNN accelera- tors, showing that our method can achieve at least 15 improvement in both latency and latency-area-sum metrics with fewer sample designs, compared to state-of-the-art methods. The rest of the paper is organized as follows. Section 2 formulates the problem and introduces the background on DSE and DNN accelerator design. Section 3 presents the proposed optimization algorithm while Section 4 reports the evaluation results. Section 5 concludes the paper. 2 Background and Problem Formulation Simulation-Based Design Space Exploration. Simulation-based DSE methods have evolved from traditional heuristic-based algorithms, such as genetic algorithms [1] and simulated annealing [14], to more structured optimization techniques, including surrogate modeling and Bayesian optimization [2]. These problems are generally not amenable to analytical solutions due to the complexity and non- differentiable nature of the simulation-based objective metrics. We formalize the design problem: Problem 1 Given a design space D, a simulator U : D RJ that returns J performance metrics for a design configuration Î¾ D, a set of parameter dependency constraints {gi(Î¾) 0}K i 1, a set of performance constraints {hj(U(Î¾)) 0}M j 1, and a scalar reward function R : RJ R, the goal is to find an optimal configuration Î¾ that solves: maximize Î¾ D R(U(Î¾)) s.t.\n\n--- Segment 4 ---\nThese problems are generally not amenable to analytical solutions due to the complexity and non- differentiable nature of the simulation-based objective metrics. We formalize the design problem: Problem 1 Given a design space D, a simulator U : D RJ that returns J performance metrics for a design configuration Î¾ D, a set of parameter dependency constraints {gi(Î¾) 0}K i 1, a set of performance constraints {hj(U(Î¾)) 0}M j 1, and a scalar reward function R : RJ R, the goal is to find an optimal configuration Î¾ that solves: maximize Î¾ D R(U(Î¾)) s.t. gi(Î¾) 0, i {1, . . . , K} hj(U(Î¾)) 0, j {1, . . . , M} (1) RL-Based Design Space Exploration. Recent work leverages RL to explore complex design spaces in domains such as memory controller tuning, system-on-chip design [5], and microarchitecture search [15]. These methods typically model the task as a multi-step MDP, where design choices are made sequentially and rewards are obtained only after a full design evaluation. This leads to sparse, delayed feedback and requires maintaining partial design states or heuristic masking [6] to enforce constraints, complicating learning and scaling. We instead adopt a one-step RL formulation [7], where the policy generates complete design candidates in one step, enabling efficient parallel sampling and eliminating the need for sequential rollouts, intermediate state designs, and sparse reward propagation.\n\n--- Segment 5 ---\nThis leads to sparse, delayed feedback and requires maintaining partial design states or heuristic masking [6] to enforce constraints, complicating learning and scaling. We instead adopt a one-step RL formulation [7], where the policy generates complete design candidates in one step, enabling efficient parallel sampling and eliminating the need for sequential rollouts, intermediate state designs, and sparse reward propagation. 2 PE PE PE PE PE PE PE PE PE L2 SP L1 SP MAC (a) S R C K X Y X Y Weight Input Activation Output Activation (b) Parameter Value Hardware resources of PEs 2 : 1024 : 2 L2 buffer size (bytes) 1 : 232 L1 buffer size (bytes) 1 : 232 Mapping strategies of level i in one layer Loop order Si, Ri, Ki, Ci, Xi, Yi Parallelization dimension Pi Si, Ri, Ki, Ci, Xi, Yi Level of parallelism Pi 1 : Pi Si 1 : Si Ri 1 : Ri Ki 1 : Ki Ci 1 : Ci Xi 1 : Xi Yi 1 : Yi (c) Figure 1: (a) Hardware resources for a 2-level spatial DNN accelerator; (b) Tensor dimensions for convolutional layers. (c) Design space of hardware resources and mapping strategies. Spatial DNN Accelerator Design. We instantiate Problem 1 for the co-design of hardware resources and mapping strategies in spatial DNN accelerators. Each design configuration Î¾ D consists of a structured combination of hardware parameters and mapping strategies. The simulator U(Î¾) evaluates a given design based on performance metrics such as latency, area, and power, which are encoded into a scalar reward via R(U(Î¾)). The constraint functions gi(Î¾) enforce dependencies over parameters (e.g., buffer bounds, tile hierarchy), while hj(U(Î¾)) encode system-level performance constraints such as area budgets across target platforms. Prior work typically searches for efficient mappings on fixed hardware or tunes hardware configu- rations under a fixed mapping strategy [16, 17, 5, 18]. While hardware-mapping interdependencies suggest that joint optimization can yield better performance, this remains challenging due to the vast combined design space [19, 17]. Recent approaches to address this problem include heuristic search [19] and two-step optimization [20, 21].\n\n--- Segment 6 ---\nWhile hardware-mapping interdependencies suggest that joint optimization can yield better performance, this remains challenging due to the vast combined design space [19, 17]. Recent approaches to address this problem include heuristic search [19] and two-step optimization [20, 21]. However, these methods are limited by sampling inefficiency and scalability for large design spaces. In contrast, CORE performs joint optimization over the design space by embedding structured design parameters into a continuous distribution. A scaling-graph-based decoder enforces constraints and dependencies during sampling enabling efficient and scalable exploration of valid configurations. Co-Design Space and Parameter Dependencies. As shown in Table 1c, the hardware design space includes the number of processing elements (PEs), and the sizes of L1 and L2 buffers. The mapping design space for each memory level i includes the loop ordering, tile sizes (Si, Ri, Ki, Ci, Xi, Yi), the parallelization dimension Pi, and the level of parallelism Pi. Here, the subscript i denotes the memory hierarchy level (e.g., i 1 for L1, i 2 for L2). Further details on parameter ranges and encoding are provided in Appendix A.1. The design space exhibits rich structural dependencies. For instance, buffer sizes and PEs constrain the feasible L2 buffer size; tile sizes must satisfy Di Di 1 across memory levels to preserve hierarchical partitioning [17], where each Di {Si, . . . , Yi} denotes the tile size of a particular loop dimension at memory level i.; and the number of PEs bounds the degree of parallelism via the tile size in the selected parallel dimension. 3 CORE Framework In this section, we introduce the CORE framework (Fig. 2) for simulation-based DSE. CORE employs an one-step RL formulation, where the policy models conditional probability density functions (PDFs) over structured design actions. Unlike traditional RL methods, CORE decouples policy learning from evaluation via a parallel pipeline consisting of conditional sampling, scaling-graph-based decoding, and simulation-driven reward feedback. This design supports high-throughput exploration under delayed and expensive feedback regimes. The policy is updated using a surrogate objective defined by the relative advantages of the sampled designs, enabling sample-efficient learning without value functions. We describe each component of the framework in the following sections.\n\n--- Segment 7 ---\nThe policy is updated using a surrogate objective defined by the relative advantages of the sampled designs, enabling sample-efficient learning without value functions. We describe each component of the framework in the following sections. 3.1 One-Step Markov Decision Process and Sampling Policy Definition 1 (One-Step MDP) A one-step MDP is defined as the tuple M (s0, A, R), where s0 S denotes the single state of the environment, A denotes the action space, and the reward 3 ğ‘ ! ğœƒ" ğœƒ" Policy Update Objective ğ¿ (ğœƒ") Actions Parallel Evaluation Config. PDFs ğ’‡'! Conditional PDF-Based Sampling Scaling Graph-Based Decoding Simulation and Reward Shaping ğ‘“'! (ğ‘ ğ‘(, , ğ‘)) ğ‘“'! (ğ‘) ğ‘)) ğ‘“'! (ğ‘)) Policy Learning Design Space Target Power Latency Design Constraints Cloud Edge Optimal Design ğœ‰ ğ’‚"ğ‘¬ ğ’‚" ğŸ‘ ğ’‚"ğŸ ğ’‚"ğŸ ğœ‰"ğ‘¬ ğœ‰" ğŸ‘ ğœ‰"ğŸ ğœ‰"ğŸ ğ‘¨"ğ‘¬ ğ‘¨" ğŸ‘ ğ‘¨" ğŸ ğ‘¨" ğŸ Adv. Agent ğœ‹'! Entropy Objective ğ¿0(ğœƒ") KL Objective ğ¿1(ğœƒ") Figure 2: Overview of the CORE framework for simulation-based design space exploration. function R is defined as R(s0, a) : {s0} A R, a A. The agent interacts with the environment in episodes of length 1. We model the design exploration process as a one-step MDP, where a policy network Ï€Î¸ represents a joint distribution over design actions conditioned on a fixed input state s0. Since the environment is stateless, s0 acts as a static context vector and remains unchanged throughout training. The joint distribution Ï€Î¸(a1, . . .\n\n--- Segment 8 ---\n. . , aN; s0) is factorized as a product of conditional probability density functions: Ï€Î¸(a1, ..., aN; s0) N Y i 1 Ï€i,Î¸ N Y i 1 fi,Î¸(ai ai 1...aN; s0), (2) where each fi,Î¸ is a conditional PDF corresponding to an action ai, so the compound action a (a1, . . . , aN) is sampled from the joint distribution. This factorization captures statistical dependencies between design actions, allowing the model to learn probabilistic preferences for combinations of actions. These learned correlations are distinct from the structural dependencies encoded in the scaling graph decoder (Section 3.2), which ensures that the sampled actions are mapped to valid, feasible configurations. We consider two types of distributions for fi,Î¸: The categorical distribution models discrete actions drawn from a finite set of categories, parameterized by a probability vector p, where pi represents the probability of selecting the i-th category. The Beta distribution usually models continuous actions on the interval [0, 1], parameterized by two parameters, Î± and Î², which control the shape of the distribution. We use Beta distributions as continuous relaxations for discrete actions when the discrete space is large or context-dependent. A continuous value is sampled from the Beta distribution and rounded into a discrete value, as described in Section 3.2. For the design space shown in Table 1c, we sample the level of parallelism Pi, which includes 6 choices, using the categorical distribution. The remaining parameters are sampled from beta distributions to reduce the output dimension of the policy neural network (NN) and incorporate structural dependencies. 3.2 Decoding Actions to Configurations In each training episode, we use Ï€Î¸ to sample a batch of E compound design actions {ak}E k 1, where each ak can be decoded into a design configuration. We instantiate the evaluation procedure of DNN accelerator in Fig. 3, where lowercase letters represent sampled actions, uppercase letters indicate decoded configuration parameters, and color-coded mapping components distinguish design strategies across layers. We introduce a decoding technique that maps actions from the action space A to structured design space D, while satisfying parameter dependency constraints. 4 ğ‘“!\n\n--- Segment 9 ---\nWe introduce a decoding technique that maps actions from the action space A to structured design space D, while satisfying parameter dependency constraints. 4 ğ‘“! ğ‘“" ğ‘“ ğ‘“ ğ‘“ ğ‘“ Sample ğ‘ƒğ¸! ğ‘ƒğ¸" ğ‘ƒğ¸ !" ğ¿! ğ¿" MAC ğ‘¾ğŸ: ğ¾! ğ¶! ğ‘†! ğ‘…! ğ‘¶ğŸ: ğ¾! ğ‘‹! ğ‘Œ! ğ‘°ğŸ: ğ¶! ğ‘‹! ğ‘Œ! ğ‘¾ğŸ: ğ¾" ğ¶" ğ‘†" ğ‘…" ğ‘°ğŸ: ğ¶" ğ‘‹" ğ‘Œ" ğ‘¶ğŸ: ğ¾" ğ‘‹" ğ‘Œ" Simulate Latency, Area, Power 1 2 ... (a) PDF-Based Sampling (c) Simulation for Structured Configurations ğ¿! ğ‘™! ğ¿" ğ‘™" ğ‘› ğ‘ ğ‘ƒ! ğ‘! P! p! ğ‘†" ğ‘…" ğ‘Œ" ğ¾" ğ¶" ğ‘‹" ğ‘ " ğ‘Ÿ" ğ‘¦" ğ‘˜" ğ‘" ğ‘¥" (b) Scaling Graph-Based Decoding ğ‘†" ğ‘…" ğ‘‹" ğ‘Œ" ğ¾" ğ¶" ğ‘ƒ" ğ‘ ! ğ‘Ÿ! ğ‘¦! ğ‘˜! ğ‘! ğ‘! ğ‘¥! ğ‘†! ğ‘…! ğ‘Œ! ğ¾! ğ¶! ğ‘ƒ! ğ‘‹! ğ‘†! ğ‘…! ğ‘¦! ğ‘˜! ğ‘! ğ‘! ğ‘¥! ğ‘™" ğ‘™! ğ‘› ğ‘ƒ"p! P!p!\n\n--- Segment 10 ---\nğ‘› ğ‘ƒ"p! P!p! Decode Figure 3: Evaluation pipeline for hardware-mapping co-design. Action Discretization. For independent discrete parameters with a wide range of values, we use the Beta distribution to sample an action b [0, 1] and then round it into a discrete value B. Generally, assuming a range [Blow, Bup] and a step size Bs, we discretize the action as follows: B Blow Bup Blow Bs 1 b Bs, (3) where x denotes the floor function, and the action b is scaled to match the range of the parameter B. By taking the number of PEs as an example, we have 512 choices, i.e., Npe can be selected in the range from 2 to 1024 with steps of 2. Using the beta distribution to produce an action npe [0, 1], we quantize this action as Npe 2 2 512npe . L2 buffer size L1 buffer size of PEs L1 parallel level L1 parallel dimension L2 tile size L1 tile size Figure 4: Scaling graph for DNN accelerator. Scaling-Graph-Based Decoding. To effectively navigate structured parameter spaces with interde- pendencies, we introduce a decoding strategy based on a scaling graphs, which dynamically adjusts the parameter bounds, improving feasibility and acceler- ating convergence during exploration [22]. As shown in Fig. 4, a scaling graph captures structural depen- dencies between design parameters. Each node rep- resents a design variable, and each directed edge encodes a constraint or scaling relationship from a source (influencing) parameter to a target (dependent) parameter. Specifically, the decoded value of the source constrains the feasible range of the target variable, such as the upper bound. Given a set of source parameters with decoded values {Ai}, and a target parameter bounded within [Blow, Bup] with step size Bs, we consider the upper bound relation and a sampled action b such that: B Blow mini{Ai} Blow Bs 1 b Bs, (4) where we replace Bup with mini{Ai} in Equation (3) to ensure that the upper bound is determined by the source nodes. Intuitively, this scaling ensures that sampled actions respect inter-parameter dependencies by dynamically constraining action ranges according to previously decoded parameters.\n\n--- Segment 11 ---\nGiven a set of source parameters with decoded values {Ai}, and a target parameter bounded within [Blow, Bup] with step size Bs, we consider the upper bound relation and a sampled action b such that: B Blow mini{Ai} Blow Bs 1 b Bs, (4) where we replace Bup with mini{Ai} in Equation (3) to ensure that the upper bound is determined by the source nodes. Intuitively, this scaling ensures that sampled actions respect inter-parameter dependencies by dynamically constraining action ranges according to previously decoded parameters. Decoding follows the topological order of the scaling graph to respect dependencies. In the example shown in Fig. 3 (b), the upper bound for the level of parallelism P1 is constrained by the number of PEs and the corresponding tile size in level 2, which is determined by the selected parallelization dimension P1. Assuming P1 Plow and the selected parallel dimension is X, i.e., P1 X, we decode P1 as follows: P1 Plow (min{Npe, X2} Plow 1)p1 . (5) In summary, the scaling graph enables constraint-aware decoding by dynamically adjusting parameter bounds during sampling. This ensures that the sampled configurations are feasible and accelerates learning. 3.3 Parallel Sampling Policy Optimization We customize proximal policy optimization [4, 23] for design exploration. The objective function R(Î¾) in Problem (1) is obtained from the simulation results of a design point Î¾. We incorporate the 5 constraint-aware reward shaping to formulate a surrogate objective function L(Î¸t), which is used to update the policy NN. Reward and Surrogate Advantage. Let {Î¾k}E k 1 denote a batch of E design samples, where each Î¾k D is a candidate design point from the design space D. Each design is evaluated by a simulator or cost model, which returns J performance metrics (e.g., latency, power, area). We define the simulator as a function: U : D RJ, which returns a set of metrics value for a design point Î¾. Then, the optimization target can be expressed as a weighted sum of the metrics: R(Î¾k) w U(Î¾k), (6) where w RJ is a user-defined weight vector applied to the simulator outputs.\n\n--- Segment 12 ---\nWe define the simulator as a function: U : D RJ, which returns a set of metrics value for a design point Î¾. Then, the optimization target can be expressed as a weighted sum of the metrics: R(Î¾k) w U(Î¾k), (6) where w RJ is a user-defined weight vector applied to the simulator outputs. If a lower value of a metric (e.g., latency or energy consumption) indicates a better design, we assign the corresponding weight wj 0 so that the scalar reward R(Î¾k) increases as design quality improves. This ensures that all objectives are aligned under a reward maximization framework. At each training episode t, the agent samples a batch of E design points {Î¾k}E k 1 and evaluates them using parallel simulation to obtain a scalar reward R(Î¾k) for each sample. We define the running reward Ë†Rt to track the exponential moving average of batch rewards, which gives higher weight to recent batches while gradually discounting earlier ones: Ë†Rt Î±rEÎ¾ Ï€Î¸t[R(Î¾)] (1 Î±r) Ë†Rt 1, Ë†R0 0, (7) EÎ¾ Ï€Î¸t[R(Î¾)] 1 E E X k 1 R(Î¾k), (8) where the expectation over the policy distribution is approximated by the empirical mean of the current batch, and Î±r [0, 1] is the renewal rate. Based on this estimate, we define the surrogate advantage At(Î¾k) to measure the relative quality of each sampled design in the batch, computed as the difference between its reward and the running average: At(Î¾k) R(Î¾k) Ë†Rt. (9) Constraint-Aware Reward Shaping. If a design point violates a required constraint Ï•, we apply a scaling penalty to quantify the degree of violation. Given a constraint h(U(Î¾k)) 0, if it is violated, the reward is updated as follows: R(Î¾k) w U(Î¾k) Î±ch(U(Î¾k)), (10) where Î±c is the violation penalty rate. If a design point cannot be simulated, possibly due to ignoring certain dependencies or architectural constraints, we call it an anomalous design.\n\n--- Segment 13 ---\nGiven a constraint h(U(Î¾k)) 0, if it is violated, the reward is updated as follows: R(Î¾k) w U(Î¾k) Î±ch(U(Î¾k)), (10) where Î±c is the violation penalty rate. If a design point cannot be simulated, possibly due to ignoring certain dependencies or architectural constraints, we call it an anomalous design. We define a penalty to push the reward below the average, discouraging the agent from generating similar samples in the future. The reward for an anomalous design Î¾ in episode t is computed as: Rt(Î¾ ) ( min(EÎ¾ Ï€Î¸t 1[R(Î¾)], Ë†Rt 1) Î±pEÎ¾ Ï€Î¸t[R(Î¾)], t 1 Rano t 1 (11) where Î±p is the anomalous design penalty rate, Rano is the initial reward for the first episode, and the expectation is approximated by the empirical mean of the batch. Surrogate Objective. The surrogate objective function comprises the conditional update objective, the Kullback-Leibler (KL) objective, and the entropy objective [8]. First, we update the policy parameters Î¸ based on the surrogate advantage through the conditional update objective, which is computed as follows: Lup(Î¸t) EÎ¾ Ï€Î¸t Ï€Î¸(a1, ..., aN; s0) Ï€Î¸t(a1, ..., aN; s0)At(Î¾k) . (12) Moreover, a KL-regularizer is added to regulate the update rate of the policy as follows: Lr(Î¸t) Î²r N X i 1 DKL(Ï€i,Î¸ Ï€i,Î¸t), (13) 6 Algorithm 1 Constraint-Aware One-Step RL for Design Exploration Require: Design Space D, Policy NN Ï€Î¸, input state s0, scaling graph G, batch size E, maximum num- ber of episodes tm, target reward R, dependency constraints {gi(Î¾) 0}K i 1, performance constraints {hj(U(Î¾)) 0}M j 1, weights w for objective metrics, and learning rate Î·. 1: t 1, Ë†R0 0.\n\n--- Segment 14 ---\n(12) Moreover, a KL-regularizer is added to regulate the update rate of the policy as follows: Lr(Î¸t) Î²r N X i 1 DKL(Ï€i,Î¸ Ï€i,Î¸t), (13) 6 Algorithm 1 Constraint-Aware One-Step RL for Design Exploration Require: Design Space D, Policy NN Ï€Î¸, input state s0, scaling graph G, batch size E, maximum num- ber of episodes tm, target reward R, dependency constraints {gi(Î¾) 0}K i 1, performance constraints {hj(U(Î¾)) 0}M j 1, weights w for objective metrics, and learning rate Î·. 1: t 1, Ë†R0 0. 2: Initialize parameters Î¸1 of policy network Ï€Î¸. 3: while t tm do 4: a1, , aE Ï€Î¸t(s0); Sample E compound actions (Sec. 3.1) 5: Î¾1, , Î¾E Decode( a1, , aE , D, {gi}K i 1); Decode via scaling graph (Sec.\n\n--- Segment 15 ---\n3: while t tm do 4: a1, , aE Ï€Î¸t(s0); Sample E compound actions (Sec. 3.1) 5: Î¾1, , Î¾E Decode( a1, , aE , D, {gi}K i 1); Decode via scaling graph (Sec. 3.2) 6: R(Î¾1), , R(Î¾E) Reward( U(Î¾1), , U(Î¾E) , w, {hj}M j 1); Equation (6), (10), (11) 7: Rmax, Î¾best FindBest(R(Î¾1), , R(Î¾E)); Find Î¾ with the maximum reward 8: if Rmax R then 9: break; 10: Ë†Rt Î±r 1 E PE k 1 R(Î¾k) (1 Î±r) Ë†Rt 1; Equation (7), (8) 11: L(Î¸t) ComputeObj( R(Î¾1), , R(Î¾E) , Ë†Rt, Ï€Î¸t); Equation (9), (12), (13), (14) 12: Î¸t 1 Î¸t Î· Î¸L(Î¸t); Policy update via gradient ascent 13: t t 1; 14: return Î¾best; where DKL is the forward KL divergence [23] and Î²r is a factor for the KL objective, encouraging the updated policy to stay close to the current policy. Lastly, to balance exploration and exploitation, we include an entropy regularization term that encourages the policy to maintain uncertainty in its action distributions. This regularization prevents early convergence to suboptimal, overconfident actions and encourages diverse exploration early in training. To reduce unnecessary randomness later, we multiply the entropy bonus by a decaying factor Î²e [8], which gradually shifts the policy from exploration to exploitation: Le(Î¸t) Î²e N X i 1 H(Ï€i,Î¸), (14) where H is the entropy function.\n\n--- Segment 16 ---\nThis regularization prevents early convergence to suboptimal, overconfident actions and encourages diverse exploration early in training. To reduce unnecessary randomness later, we multiply the entropy bonus by a decaying factor Î²e [8], which gradually shifts the policy from exploration to exploitation: Le(Î¸t) Î²e N X i 1 H(Ï€i,Î¸), (14) where H is the entropy function. With the objective function L(Î¸t) Lup(Î¸t) Lr(Î¸t) Le(Î¸t), optimization algorithms such as gradient ascent can be used for back-propagation to update the policy NN parameters Î¸, guiding the model toward an optimal policy. Our framework is summarized in Algorithm 1. Training terminates when either the maximum number of episodes tm is reached or the target reward threshold R is achieved. 4 Experiments 4.1 Experiment Setup Policy and Hyperparameters We implement CORE in Python using the PyTorch library and evaluate it on a set of DNN models for various applications. The policy network is a 4-layer multilayer perceptron with ReLU activations (see Appendix B for architecture details). It is trained for 2000 episodes using the Adam optimizer with default beta parameters, a learning rate of 10 5, and a batch size of 32, which corresponds to the number of parallel CPU threads used for simulation. The entropy coefficient Î²e is linearly decayed from 1.0 to 0.02, the surrogate reward is computed with a renewal rate Î±r of 0.2, and other rate factors are fixed at 1.0. No hyperparameter tuning is performed. We implement all experiments on a server with an NVIDIA V100 GPU. DNN Accelerator Setup We consider a 2-level mapping strategy for the accelerator design space in the experiments. We evaluate seven DNN models across vision (ResNet-18, ResNet-50, MobileNetV2, VGG-16), language (BERT), and recommendation (DLRM, NCF) domains [17]. For transformer models like BERT, the mapping space is defined to capture matrix multiplications in attention and feed-forward layers. This work focuses on inference configuration only. Area constraints for PEs and buffers are imposed to reflect realistic platform settings: 0.2 mm2 for edge devices and 7.0 mm2 for cloud platforms [24].\n\n--- Segment 17 ---\nThis work focuses on inference configuration only. Area constraints for PEs and buffers are imposed to reflect realistic platform settings: 0.2 mm2 for edge devices and 7.0 mm2 for cloud platforms [24]. While DRAM bandwidth and interconnects vary in practice, we abstract these under a unified simulation interface to maintain comparability. The 7 Table 1: Log10-scaled performance comparison of baseline methods and ablation studies with CORE on cloud and edge platforms. Objective Latency (log10 cycles) Latency-Area-Sum (log10) Method GA HASCO w o. rs. w o. sc. CORE GA HASCO w o. rs. w o. sc.\n\n--- Segment 18 ---\nCORE GA HASCO w o. rs. w o. sc. CORE Cloud Platform Resnet18 7.28 6.80 6.68 5.26 4.62 7.44 6.79 6.76 6.14 5.66 Resnet50 7.29 7.30 7.31 6.47 5.47 7.40 6.92 7.34 6.97 6.18 Mbnet-V2 7.01 6.79 6.20 5.80 4.33 6.89 6.75 6.30 6.54 5.14 BERT 7.31 6.85 6.74 5.95 5.60 7.08 6.74 6.94 6.52 5.99 NCF 3.50 3.48 3.66 3.47 2.17 4.65 3.41 4.53 4.16 4.06 DLRM 2.57 2.60 2.83 3.73 2.03 4.09 3.72 4.06 4.08 4.05 VGG16 7.85 7.43 7.51 5.84 5.05 7.92 7.65 7.52 7.12 6.08 Edge Platform Resnet18 7.20 7.06 7.03 5.33 5.31 7.39 6.79 7.04 6.50 5.59 Resnet50 7.37 7.12 - 6.74 5.47 7.25 6.92 - 7.02 6.26 Mbnet-V2 7.07 6.74 - 6.49 4.97 7.22 6.73 - 6.35 5.16 BERT 7.18 6.76 - 6.37 5.83 7.35 6.70 - 6.66 6.02 NCF 3.86 3.32 3.89 3.65 2.78 4.64 3.65 4.56 4.34 4.06 DLRM 2.68 2.58 3.68 3.88 3.37 4.13 3.12 4.38 4.46 4.05 VGG16 7.58 7.71 7.87 5.84 5.13 7.97 7.66 7.87 6.93 6.14 simulation-based evaluation uses the MAESTRO cost model [25] for performance and Synopsys DC, Cadence Innovus for area estimation (via RTL synthesis with the Nangate 15nm library) [19].\n\n--- Segment 19 ---\nw o. sc. CORE Cloud Platform Resnet18 7.28 6.80 6.68 5.26 4.62 7.44 6.79 6.76 6.14 5.66 Resnet50 7.29 7.30 7.31 6.47 5.47 7.40 6.92 7.34 6.97 6.18 Mbnet-V2 7.01 6.79 6.20 5.80 4.33 6.89 6.75 6.30 6.54 5.14 BERT 7.31 6.85 6.74 5.95 5.60 7.08 6.74 6.94 6.52 5.99 NCF 3.50 3.48 3.66 3.47 2.17 4.65 3.41 4.53 4.16 4.06 DLRM 2.57 2.60 2.83 3.73 2.03 4.09 3.72 4.06 4.08 4.05 VGG16 7.85 7.43 7.51 5.84 5.05 7.92 7.65 7.52 7.12 6.08 Edge Platform Resnet18 7.20 7.06 7.03 5.33 5.31 7.39 6.79 7.04 6.50 5.59 Resnet50 7.37 7.12 - 6.74 5.47 7.25 6.92 - 7.02 6.26 Mbnet-V2 7.07 6.74 - 6.49 4.97 7.22 6.73 - 6.35 5.16 BERT 7.18 6.76 - 6.37 5.83 7.35 6.70 - 6.66 6.02 NCF 3.86 3.32 3.89 3.65 2.78 4.64 3.65 4.56 4.34 4.06 DLRM 2.68 2.58 3.68 3.88 3.37 4.13 3.12 4.38 4.46 4.05 VGG16 7.58 7.71 7.87 5.84 5.13 7.97 7.66 7.87 6.93 6.14 simulation-based evaluation uses the MAESTRO cost model [25] for performance and Synopsys DC, Cadence Innovus for area estimation (via RTL synthesis with the Nangate 15nm library) [19]. We set a fixed sampling budget of 40,000 for all algorithms to maintain consistency.\n\n--- Segment 20 ---\nCORE Cloud Platform Resnet18 7.28 6.80 6.68 5.26 4.62 7.44 6.79 6.76 6.14 5.66 Resnet50 7.29 7.30 7.31 6.47 5.47 7.40 6.92 7.34 6.97 6.18 Mbnet-V2 7.01 6.79 6.20 5.80 4.33 6.89 6.75 6.30 6.54 5.14 BERT 7.31 6.85 6.74 5.95 5.60 7.08 6.74 6.94 6.52 5.99 NCF 3.50 3.48 3.66 3.47 2.17 4.65 3.41 4.53 4.16 4.06 DLRM 2.57 2.60 2.83 3.73 2.03 4.09 3.72 4.06 4.08 4.05 VGG16 7.85 7.43 7.51 5.84 5.05 7.92 7.65 7.52 7.12 6.08 Edge Platform Resnet18 7.20 7.06 7.03 5.33 5.31 7.39 6.79 7.04 6.50 5.59 Resnet50 7.37 7.12 - 6.74 5.47 7.25 6.92 - 7.02 6.26 Mbnet-V2 7.07 6.74 - 6.49 4.97 7.22 6.73 - 6.35 5.16 BERT 7.18 6.76 - 6.37 5.83 7.35 6.70 - 6.66 6.02 NCF 3.86 3.32 3.89 3.65 2.78 4.64 3.65 4.56 4.34 4.06 DLRM 2.68 2.58 3.68 3.88 3.37 4.13 3.12 4.38 4.46 4.05 VGG16 7.58 7.71 7.87 5.84 5.13 7.97 7.66 7.87 6.93 6.14 simulation-based evaluation uses the MAESTRO cost model [25] for performance and Synopsys DC, Cadence Innovus for area estimation (via RTL synthesis with the Nangate 15nm library) [19]. We set a fixed sampling budget of 40,000 for all algorithms to maintain consistency. 4.2 Main Results Comparisons with Baseline Optimization Algorithms.\n\n--- Segment 21 ---\nWe set a fixed sampling budget of 40,000 for all algorithms to maintain consistency. 4.2 Main Results Comparisons with Baseline Optimization Algorithms. We use a genetic algorithm (GA) in the literature [5] as the baseline for the co-optimization problem. Additionally, we compare our approach with the state-of-the-art open-source optimization algorithm HASCO [20], which is a two-stage algorithm using multi-objective Bayesian optimization to explore the hardware design space and heuristic and Q-learning algorithms to optimize the mappings across all the layers of the network. Because the space defined in Figure 1c is too large for HASCO, we limit it by using larger step sizes when discretizing the ranges of the number of PEs and the buffer sizes. The results are shown in Table 1, where we optimize the latency and the sum of latency and area (LAS) of the accelerator design for each DNN model on cloud and edge platforms. We optimize the entire model as a whole, and the reported values reflect the average reward computed over all layers. While HASCO achieves better metrics on smaller DNNs like NCF and DLRM, CORE consistently obtains optimal results across other cases, demonstrating its superiority in terms of larger DNNs and more complex design spaces. We average the results across all models for each platform and illustrate in Figure 5 the average latency and LAS for each method. CORE outperforms both GA and HASCO in terms of latency and LAS of the resulting design, achieving at least a 15 improvement in the optimal reward. Ablation Study. We also conduct an ablation study to evaluate the effectiveness of the reward design and the graph-based scale-decoding strategy in CORE. As detailed in Table 1, the column w o. rs. indicates the results when a negative penalty reward replaces the reward shaping upon constraint violations and anomalous designs, while the column w o. sc. shows the outcomes when configurations are decoded independently from the produced actions, ignoring parameter dependencies. The performance benefits are due to the structure-awareness and the mechanism to reason about constraints in CORE, which we expect to be transferable to other constrained domains. The reward shaping can provide feedback when design points are invalid, enabling the agent to learn quantitative information, which facilitates the exploration process.\n\n--- Segment 22 ---\nThe performance benefits are due to the structure-awareness and the mechanism to reason about constraints in CORE, which we expect to be transferable to other constrained domains. The reward shaping can provide feedback when design points are invalid, enabling the agent to learn quantitative information, which facilitates the exploration process. In some cases with edge platforms, the algorithm without the reward shaping fails to find a valid design point throughout the exploration process, shown as -" in the table. Similarly, the dependency decoding strategy is instrumental in maintaining the feasibility of the decoded configurations, helping the agent understand the parameter dependencies and select feasible actions. The ablation study shows a performance drop when either method is removed. 8 GA HASCO w o. rs. w o. sc. CORE 105 106 107 Latency (a) Average latency GA HASCO w o. rs. w o. sc. CORE 105 106 107 LAS Cloud Edge (b) Average LAS Figure 5: Average target metrics on different platforms for different methods. 0 1 2 3 104 106 107 108 of Samples Latency (cycles) GA w o. sc. CORE (a) 0 1 2 3 104 106 107 108 109 of Samples LAS (b) Figure 6: Optimized latency and LAS as the number of samples increases: (a) Change in optimized latency for ResNet50 with the cloud platform; (b) Change in optimized LAS for BERT with the edge platform. Sample Efficiency. The overall runtime can vary across different machines and parallelization set- tings, and it is mainly determined by the number of simulations re- quired to achieve the optimized design. Therefore, we measure the number of sampled designs to achieve optimized latency and LAS to evaluate the space explo- ration efficiency. The changes in latency for ResNet50 and a cloud platform and the changes in LAS for BERT with an edge platform can be seen in Fig. 6, where the x-axis represents the number of samples, and the y-axis denotes the value of the optimized objective. HASCO is not included in this comparison, as it is a two-stage algorithm that optimizes the hardware design space and mappings separately, which is not directly comparable to our single-step approach. The method without reward shaping is also omitted. Both GA and the non-reward shaping method converge faster, but our work achieves convergence with significantly better metrics.\n\n--- Segment 23 ---\nThe method without reward shaping is also omitted. Both GA and the non-reward shaping method converge faster, but our work achieves convergence with significantly better metrics. The results show that CORE requires fewer samples to achieve lower latency and LAS in both cases. Discussion. While our approach demonstrates strong performance across simulated metrics in efficiently navigating structured design spaces, its effectiveness depends on the fidelity of the un- derlying cost models. In the context of DNN accelerator design, our formulation further assumes static, compile-time mapping strategies. Extending the framework to support dynamic or runtime- adaptive dataflows presents a promising direction for future work. Interestingly, a recent method [10] independently proposes a similar idea to compute the advantage via intra-batch reward comparisons in the context of optimizing LLM training dynamics, such as token sampling and update strategies. This convergence highlights the generality of our approach, suggesting its applicability beyond DNN accelerator design to other complex, constraint-rich domains such as compiler tuning, robotic system co-design, and automated system architecture search, where simulation costs and structural constraints are similarly restrictive. 5 Conclusion We presented CORE, a critic-free, one-step reinforcement learning approach for constraint-aware, simulation-guided design space exploration. CORE addresses critical limitations of existing meth- ods by incorporating a surrogate objective based on relative advantages and enforcing feasibility constraints through a scaling-graph-based decoder. We validated CORE s effectiveness on the chal- lenging task of neural network accelerator hardware-mapping co-design, demonstrating significantly improved sampling efficiency and better optimization outcomes compared to state-of-the-art methods. Although demonstrated in the accelerator design domain, the CORE framework and its representa- tions generalize broadly to other complex design tasks. Future work includes extending CORE to 9 dynamic design environments, incorporating runtime adaptivity, and integrating it with symbolic reasoning or large language models for enhanced interpretability and scalability. References [1] John H Holland. Genetic algorithms. Scientific american, 267(1):66 73, 1992. [2] Bowen Lei, Tanner Quinn Kirk, Anirban Bhattacharya, Debdeep Pati, Xiaoning Qian, Raymundo Arroyave, and Bani K Mallick. Bayesian optimization with adaptive surrogate models for automated experimental design. Npj Computational Materials, 7(1):194, 2021.\n\n--- Segment 24 ---\nBayesian optimization with adaptive surrogate models for automated experimental design. Npj Computational Materials, 7(1):194, 2021. [3] Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A survey. Journal of artificial intelligence research, 4:237 285, 1996. [4] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [5] Srivatsan Krishnan, Amir Yazdanbakhsh, Shvetank Prakash, Jason Jabbour, Ikechukwu Uchendu, Susobhan Ghosh, Behzad Boroujerdian, Daniel Richins, Devashree Tripathy, Aleksandra Faust, et al. Archgym: An open-source gymnasium for machine learning assisted architecture design. In Proceedings of the 50th Annual International Symposium on Computer Architecture, pages 1 16, 2023. [6] Zixuan Jiang, Ebrahim Songhori, Shen Wang, Anna Goldie, Azalia Mirhoseini, Joe Jiang, Young-Joon Lee, and David Z Pan. Delving into macro placement with reinforcement learning. In 2021 ACM IEEE 3rd Workshop on Machine Learning for CAD (MLCAD), pages 1 3. IEEE, 2021. [7] Hassan Ghraieb, Jonathan Viquerat, AurÃ©lien Larcher, Philippe Meliga, and Elie Hachem. Single-step deep reinforcement learning for open-loop control of laminar and turbulent flows. Physical Review Fluids, 6(5):053902, 2021. [8] Masood S. Mortazavi, Tiancheng Qin, and Ning Yan. Theta-resonance: A single-step reinforcement learning method for design space exploration, 2022. [9] Jiayu Li, Masood S. Mortazavi, and Ning Yan. Exploring distributed circuit design using single-step reinforcement learning. In 2024 60th ACM IEEE Design Automation Conference (DAC). IEEE, 2024.\n\n--- Segment 25 ---\nIn 2024 60th ACM IEEE Design Automation Conference (DAC). IEEE, 2024. [10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [11] Yu-Hsin Chen, Tushar Krishna, Joel S Emer, and Vivienne Sze. Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks. IEEE journal of solid-state circuits, 52(1):127 138, 2016. [12] NVDLA Project. Nvdla deep learning accelerator. 2017. [13] Zidong Du, Robert Fasthuber, Tianshi Chen, Paolo Ienne, Ling Li, Tao Luo, Xiaobing Feng, Yunji Chen, and Olivier Temam. Shidiannao: Shifting vision processing closer to the sensor. In Proceedings of the 42nd annual international symposium on computer architecture, pages 92 104, 2015. [14] Dimitris Bertsimas and John Tsitsiklis. Simulated annealing. Statistical science, 8(1):10 15, 1993. [15] Chen Bai, Jianwang Zhai, Yuzhe Ma, Bei Yu, and Martin DF Wong. Towards automated risc-v mi- croarchitecture design with reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 12 20, 2024. [16] Sheng-Chun Kao, Geonhwa Jeong, and Tushar Krishna. Confuciux: Autonomous hardware resource as- signment for dnn accelerators using reinforcement learning. In 2020 53rd Annual IEEE ACM International Symposium on Microarchitecture (MICRO), pages 622 636. IEEE, 2020. [17] Sheng-Chun Kao and Tushar Krishna. Gamma: Automating the hw mapping of dnn models on accelerators via genetic algorithm. In Proceedings of the 39th International Conference on Computer-Aided Design, pages 1 9, 2020.\n\n--- Segment 26 ---\nGamma: Automating the hw mapping of dnn models on accelerators via genetic algorithm. In Proceedings of the 39th International Conference on Computer-Aided Design, pages 1 9, 2020. [18] Size Zheng, Yun Liang, Shuo Wang, Renze Chen, and Kaiwen Sheng. Flextensor: An automatic schedule exploration and optimization framework for tensor computation on heterogeneous system. In Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, pages 859 873, 2020. 10 [19] Sheng-Chun Kao, Michael Pellauer, Angshuman Parashar, and Tushar Krishna. Digamma: Domain-aware genetic algorithm for hw-mapping co-optimization for dnn accelerators. In DATE, 2022. [20] Qingcheng Xiao, Size Zheng, Bingzhe Wu, Pengcheng Xu, Xuehai Qian, and Yun Liang. Hasco: To- wards agile hardware and software co-design for tensor computation. In 2021 ACM IEEE 48th Annual International Symposium on Computer Architecture (ISCA), pages 1055 1068. IEEE, 2021. [21] Bahador Rashidi, Chao Gao, Shan Lu, Zhisheng Wang, Chunhua Zhou, Di Niu, and Fengyu Sun. Unico: Unified hardware software co-optimization for robust neural network acceleration. In Proceedings of the 56th Annual IEEE ACM International Symposium on Microarchitecture, pages 77 90, 2023. [22] Stijn Eyerman, Lieven Eeckhout, Tejas Karkhanis, and James E Smith. A mechanistic performance model for superscalar out-of-order processors. ACM Transactions on Computer Systems (TOCS), 27(2):1 37, 2009. [23] Chloe Ching-Yun Hsu, Celestine Mendler-DÃ¼nner, and Moritz Hardt. Revisiting design choices in proximal policy optimization. arXiv preprint arXiv:2009.10897, 2020.\n\n--- Segment 27 ---\nRevisiting design choices in proximal policy optimization. arXiv preprint arXiv:2009.10897, 2020. [24] Yakun Sophia Shao, Jason Clemons, Rangharajan Venkatesan, Brian Zimmer, Matthew Fojtik, Nan Jiang, Ben Keller, Alicia Klinefelter, Nathaniel Pinckney, Priyanka Raina, et al. Simba: Scaling deep-learning inference with multi-chip-module-based architecture. In Proceedings of the 52nd Annual IEEE ACM International Symposium on Microarchitecture, pages 14 27, 2019. [25] Hyoukjun Kwon, Prasanth Chatarasi, Vivek Sarkar, Tushar Krishna, Michael Pellauer, and Angshuman Parashar. Maestro: A data-centric approach to understand reuse, performance, and hardware cost of dnn mappings. IEEE micro, 40(3):20 29, 2020. [26] Angshuman Parashar, Priyanka Raina, Yakun Sophia Shao, Yu-Hsin Chen, Victor A Ying, Anurag Mukkara, Rangharajan Venkatesan, Brucek Khailany, Stephen W Keckler, and Joel Emer. Timeloop: A systematic approach to dnn accelerator evaluation. In 2019 IEEE international symposium on performance analysis of systems and software (ISPASS), pages 304 315. IEEE, 2019. A Detailed Description of the Spatial DNN Accelerator This appendix provides additional information on the spatial Deep Neural Network (DNN) accelerator co-design framework, facilitating understanding of hardware resource allocation, mapping strategies, and associated performance modeling. A.1 Spatial DNN Accelerator Resources A.1.1 Hardware Resources A spatial DNN accelerator primarily comprises an array of processing elements (PEs), each equipped with a multiply-accumulate (MAC) unit and Level-1 (L1) buffers for local data storage. An accelerator typically also includes a shared Level-2 (L2) buffer that acts as a bridge between off-chip memory and the on-chip L1 buffers, optimizing data fetching and reducing memory bandwidth demands. Networks-on-Chip (NoCs) facilitate efficient operand distribution from the L2 buffer to individual PEs and the collection of partial outputs, subsequently stored back into the L2 buffer.\n\n--- Segment 28 ---\nAn accelerator typically also includes a shared Level-2 (L2) buffer that acts as a bridge between off-chip memory and the on-chip L1 buffers, optimizing data fetching and reducing memory bandwidth demands. Networks-on-Chip (NoCs) facilitate efficient operand distribution from the L2 buffer to individual PEs and the collection of partial outputs, subsequently stored back into the L2 buffer. A.1.2 Mapping Strategies Mapping strategies determine how computations of a DNN layer are partitioned and executed across hardware resources. Tiling Strategy Tiling divides large tensors, such as weights and activations, into smaller blocks or "tiles" to fit within the L1 buffers of the PEs. Computation is conducted on one tile at a time, enhancing data reuse and minimizing redundant memory transfers. The choice of tile size impacts the amount of data that must be simultaneously stored within on-chip buffers. Computation Order The sequence in which loop computations are executed significantly affects performance. For instance, the six-loop nest structure of 2D convolution operations (Fig. 7) provides multiple possible orderings (6! possibilities), each with distinct implications for data reuse and performance. 11 S R C K X Y X Y Weight Input Activation Output Activation Figure 7: Notation for the dimensions used in convolution operations. K: output channels, C: input channels, Y: input height, X: input width, R: filter height, S: filter width, Y : output height, X : output width. Parallelism Strategy This strategy specifies the number of PEs concurrently utilized and identifies which loop dimensions to parallelize at each memory hierarchy level, greatly influencing accelerator throughput and efficiency. A.2 Accelerator Performance and Cost Modeling A.2.1 DNN Model Characteristics Different DNN architectures (e.g., convolutional, fully connected, transformer layers) have unique computational patterns and data reuse opportunities. Convolutional layers dominate image-processing models, whereas fully connected layers appear frequently in language and recommendation models, each requiring tailored accelerator resources and mappings. A.2.2 Cost Models Accurate and efficient estimation of accelerator performance and resource usage is provided by analytical cost models such as MAESTRO [25] and Timeloop [26].\n\n--- Segment 29 ---\nConvolutional layers dominate image-processing models, whereas fully connected layers appear frequently in language and recommendation models, each requiring tailored accelerator resources and mappings. A.2.2 Cost Models Accurate and efficient estimation of accelerator performance and resource usage is provided by analytical cost models such as MAESTRO [25] and Timeloop [26]. These tools analyze hardware configurations and mapping strategies to quantify runtime, area, and power consumption, thus enabling informed decisions during accelerator design optimization. B Policy Network Architecture In the experiment, we use a 4-layer multilayer perceptron with ReLU activations after each hidden layer. The input dimension is 512. The hidden layers have widths 4096, and the output layer matches the action space size. The layer dimensions are: 512 4096 4096 4096 parameters for Ï€Î¸(a1, ..., aN; s0). The network outputs conditional probability distributions for the design actions. Specifically, each design parameter pi is modeled using a parametric distribution: Beta distribution: requires 2 parameters (e.g., Î±i, Î²i) Categorical distribution: requires k parameters for k categories (e.g., logits) Let N be the number of design parameters, and let dim(pi) denote the number of parameters required for pi. Then, the total output size of the policy network is: parameters for Ï€Î¸(a1, ..., aN; s0) N X i 1 dim(pi) C Broader Impacts This work proposes a constraint-aware reinforcement learning framework to accelerate simulation- based DSE for DNN accelerator co-design. The primary positive societal impact lies in reducing 12 the computational and time cost of designing efficient AI hardware, which may enable deployment in resource-constrained domains such as edge computing, mobile healthcare, and environmental monitoring. Our method promotes sustainability by optimizing design with fewer simulations, contributing to greener hardware design workflows. However, potential negative societal impacts include the risk of such optimization techniques being used to develop accelerators for ethically concerning applications. Furthermore, design automation could displace roles traditionally held by domain experts. These concerns warrant thoughtful deployment and governance when adopting such techniques. We believe that transparent reporting, open-access implementation, and continued interdisciplinary dialogue are important to mitigate misuse and align technical advancement with societal values. 13\n\n