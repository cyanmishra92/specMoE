{
  "source": "kraken.pdf",
  "raw_length": 82135,
  "cleaned_length": 81091,
  "base_segments": 321,
  "augmented_segments": 642,
  "segments": [
    {
      "text": "Abstract The growing popularity of microservices has led to the pro- liferation of online cloud service-based applications, which are typically modelled as Directed Acyclic Graphs (DAGs) comprising of tens to hundreds of microservices. The vast majority of these applications are user-facing, and hence, have stringent SLO requirements. Serverless functions, hav- ing short resource provisioning times and instant scalability, are suitable candidates for developing such latency-critical applications.",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": "Serverless functions, hav- ing short resource provisioning times and instant scalability, are suitable candidates for developing such latency-critical applications. However, existing serverless providers are un- aware of the workflow characteristics of application DAGs, leading to container over-provisioning in many cases. This is further exacerbated in the case of dynamic DAGs, where the function chain for an application is not known a pri- ori.",
      "type": "sliding_window",
      "tokens": 98
    },
    {
      "text": "This is further exacerbated in the case of dynamic DAGs, where the function chain for an application is not known a pri- ori. Motivated by these observations, we propose  Kraken , a workflow-aware resource management framework that minimizes the number of containers provisioned for an ap- plication DAG while ensuring SLO-compliance. We design and implement  Kraken  on  OpenFaaS  and evaluate it on a multi-node  Kubernetes -managed cluster.",
      "type": "sliding_window",
      "tokens": 118
    },
    {
      "text": "We design and implement  Kraken  on  OpenFaaS  and evaluate it on a multi-node  Kubernetes -managed cluster. Our extensive ex- perimental evaluation using  DeathStarbench  workload suite and real-world traces demonstrates that  Kraken  spawns up to 76% fewer containers, thereby improving container uti- lization and saving cluster-wide energy by up to 4 ×  and 48%, respectively, when compared to state-of-the art schedulers employed in serverless platforms. CCS Concepts •  Computer systems organization  → Cloud Comput- ing ;  Resource-Management ; Scheduling.",
      "type": "sliding_window",
      "tokens": 148
    },
    {
      "text": "CCS Concepts •  Computer systems organization  → Cloud Comput- ing ;  Resource-Management ; Scheduling. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.",
      "type": "sliding_window",
      "tokens": 60
    },
    {
      "text": "To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SoCC ’21, November 1–4, 2021, Seattle, WA, USA © 2021 Association for Computing Machinery.",
      "type": "sliding_window",
      "tokens": 75
    },
    {
      "text": "SoCC ’21, November 1–4, 2021, Seattle, WA, USA © 2021 Association for Computing Machinery. ACM ISBN 978-1-4503-8638-8/21/11...$15.00 https://doi.org/10.1145/3472883.3486992 \nKeywords serverless, resource-management, scheduling, queuing \nACM Reference Format: Vivek M. Bhasi, Jashwant Raj Gunasekaran, Prashanth Thinakaran, Cyan Subhra Mishra, Mahmut Taylan Kandemir, and Chita Das. 2021.",
      "type": "sliding_window",
      "tokens": 142
    },
    {
      "text": "2021. Kraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms. In  ACM Symposium on Cloud Computing (SoCC ’21), November 1–4, 2021, Seattle, WA, USA.",
      "type": "sliding_window",
      "tokens": 56
    },
    {
      "text": "In  ACM Symposium on Cloud Computing (SoCC ’21), November 1–4, 2021, Seattle, WA, USA. ACM, New York, NY, USA, 15 pages. https://doi.org/10.1145/3472883.",
      "type": "sliding_window",
      "tokens": 54
    },
    {
      "text": "https://doi.org/10.1145/3472883. 3486992 \n1 Introduction Cloud applications are embracing microservices as a pre- mier application model, owing to their advantages in terms of simplified development and ease of scalability [ 29 ,  40 ]. Many of these real-world services often comprise of tens or even hundreds of loosely-coupled microservices [ 42 ] (e.g.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "Many of these real-world services often comprise of tens or even hundreds of loosely-coupled microservices [ 42 ] (e.g. Ex- pedia [ 15 ] and Airbnb [ 2 ]). Typically, these online service ap- plications are user-facing and hence, are administered under strict Service Level Objectives (SLOs) [ 47 ,  48 ] and response latency requirements.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "Typically, these online service ap- plications are user-facing and hence, are administered under strict Service Level Objectives (SLOs) [ 47 ,  48 ] and response latency requirements. Therefore, choosing the underlying resources (virtual machines or containers) from a plethora of public cloud resource offerings [ 31 ,  33 ,  37 ,  41 ,  45 ,  50 ] becomes crucial due to their characteristics (such as provisioning la- tency) that determine the response latency. Serverless com- puting (FaaS) has recently emerged as a first-class platform to deploy latency-critical user facing applications as it miti- gates resource management overheads for developers while simultaneously offering instantaneous scalability.",
      "type": "sliding_window",
      "tokens": 173
    },
    {
      "text": "Serverless com- puting (FaaS) has recently emerged as a first-class platform to deploy latency-critical user facing applications as it miti- gates resource management overheads for developers while simultaneously offering instantaneous scalability. However, deploying complex microservice-based applications on FaaS has unique challenges owing to its design limitations. First, due to the stateless nature of FaaS, individual mi- croservices have to be designed as functions and explicitly chained together using tools to compose the entire appli- cation, thus forming a Directed Acyclic Graph (DAG) [ 33 ].",
      "type": "sliding_window",
      "tokens": 147
    },
    {
      "text": "First, due to the stateless nature of FaaS, individual mi- croservices have to be designed as functions and explicitly chained together using tools to compose the entire appli- cation, thus forming a Directed Acyclic Graph (DAG) [ 33 ]. Second, the state management between dependent functions has to be explicitly handled using a predefined state ma- chine and made available to the cloud provider [ 6 ,  23 ]. Third, the presence of conditional branches in some DAGs can lead to uncertainties in determining which functions will \n153 \nSoCC ’21, November 1–4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al.",
      "type": "sliding_window",
      "tokens": 169
    },
    {
      "text": "Third, the presence of conditional branches in some DAGs can lead to uncertainties in determining which functions will \n153 \nSoCC ’21, November 1–4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al. be invoked by different requests to the same application. For instance, in a train-ticket application [ 40 ], actions like make_reservation  can trigger different paths/workflows (sub- set of functions) within the application.",
      "type": "sliding_window",
      "tokens": 116
    },
    {
      "text": "For instance, in a train-ticket application [ 40 ], actions like make_reservation  can trigger different paths/workflows (sub- set of functions) within the application. These design chal- lenges, when combined with the scheduling and container provisioning policies of current serverless platforms, result in crucial inefficiencies with respect to application performance and provider-side resource utilization. Two such inefficien- cies are described below: •  The majority of serverless platforms [ 32 ,  44 ,  46 ,  50 ] assume that DAGs in applications are static, implying that all com- posite functions will be invoked by a single request to the application.",
      "type": "sliding_window",
      "tokens": 151
    },
    {
      "text": "Two such inefficien- cies are described below: •  The majority of serverless platforms [ 32 ,  44 ,  46 ,  50 ] assume that DAGs in applications are static, implying that all com- posite functions will be invoked by a single request to the application. This assumption leads to the spawning of equal number of containers for all functions in proportion to the application load, resulting in container over-provisioning. •  Dynamic DAGs, where only a subset of functions within each DAG are invoked per request type, necessitate the ap- portioning of containers to each function.",
      "type": "sliding_window",
      "tokens": 146
    },
    {
      "text": "•  Dynamic DAGs, where only a subset of functions within each DAG are invoked per request type, necessitate the ap- portioning of containers to each function. Recent frame- works like Xanadu [ 27 ], predict the most likely functions to be used in the DAG. This results in container provisioning along a single function chain.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "This results in container provisioning along a single function chain. However, not proportionately allocating containers to all functions in the application can lead to under-provisioning containers for some functions when requests deviate from the predicted path. To address these challenges, we propose  Kraken , a DAG workflow-aware resource management framework specifi- cally catered to dynamic DAGs, that minimizes resource con- sumption, while remaining SLO compliant.",
      "type": "sliding_window",
      "tokens": 104
    },
    {
      "text": "To address these challenges, we propose  Kraken , a DAG workflow-aware resource management framework specifi- cally catered to dynamic DAGs, that minimizes resource con- sumption, while remaining SLO compliant. The key compo- nents of  Kraken  are (i)  Kraken  employs a Proactive Weighted Scaler (PWS) which deploys containers for functions in ad- vance by utilizing a request arrival estimation model. The number of containers to be deployed is jointly determined by the estimation model and function weights.",
      "type": "sliding_window",
      "tokens": 130
    },
    {
      "text": "The number of containers to be deployed is jointly determined by the estimation model and function weights. These weights are assigned by the PWS by taking into account function invocation probabilities and parameters pertaining to the DAG structure, namely,  Commonality  (functions common to multiple workflows) and  Connectivity  (number of descen- dant functions), (ii) In addition to the PWS,  Kraken  employs a Reactive Scaler (RS) to scale containers appropriately to re- cover from potential resource mismanagement by the PWS, (iii) Further, we batch multiple requests to each container in order to minimize resource consumption. We have developed a prototype of  Kraken  using  OpenFaaS , an open source serverless framework [ 11 ], and extensively evaluated it using real-world datacenter traces on a 160 core Kubernetes  cluster.",
      "type": "sliding_window",
      "tokens": 193
    },
    {
      "text": "We have developed a prototype of  Kraken  using  OpenFaaS , an open source serverless framework [ 11 ], and extensively evaluated it using real-world datacenter traces on a 160 core Kubernetes  cluster. Our results show that  Kraken  spawns up to 76% fewer containers on average, thereby improving container utilization and cluster-wide energy savings by up to 4 ×  and 48%, respectively, when compared to state-of-the art serverless schedulers. Furthermore,  Kraken  guarantees SLO requirements for up to 99.97% of requests.",
      "type": "sliding_window",
      "tokens": 128
    },
    {
      "text": "Furthermore,  Kraken  guarantees SLO requirements for up to 99.97% of requests. 2 Background and Motivation We start with providing an overview of serverless DAGs along with related work (Table 1) and discuss the challenges which motivate the need for  Kraken . 2.1 Serverless Function Chains (DAGs) Many applications are modeled as function chains and typically administered under strict SLOs (hundreds of mil- liseconds) [ 30 ].",
      "type": "sliding_window",
      "tokens": 101
    },
    {
      "text": "2.1 Serverless Function Chains (DAGs) Many applications are modeled as function chains and typically administered under strict SLOs (hundreds of mil- liseconds) [ 30 ]. Serverless function chains are formed by stitching together various individual serverless functions using some form of synchronization to provide the func- tionality of a full-fledged application. Function chains are supported in commercial serverless platforms such as AWS Step Functions [4, 23], IBM Cloud Functions [8], and Azure Durable functions [ 6 ].",
      "type": "sliding_window",
      "tokens": 124
    },
    {
      "text": "Function chains are supported in commercial serverless platforms such as AWS Step Functions [4, 23], IBM Cloud Functions [8], and Azure Durable functions [ 6 ]. By characterizing production appli- cation traces from Azure, Shahrad et.al [ 42 ] have elucidated that 46% of applications have 2-10 functions. Excluding the most general (and rare) cases where applications can have loops/cycles within a function chain [ 27 ], applications can be modeled as a  Directed Acyclic Graph  (DAG) where each ver- tex/stage is a function [ 26 ] Henceforth, we will use the terms ‘function’ and ‘stage’ interchangeably.",
      "type": "sliding_window",
      "tokens": 164
    },
    {
      "text": "Excluding the most general (and rare) cases where applications can have loops/cycles within a function chain [ 27 ], applications can be modeled as a  Directed Acyclic Graph  (DAG) where each ver- tex/stage is a function [ 26 ] Henceforth, we will use the terms ‘function’ and ‘stage’ interchangeably. We define a  workflow or  path  within an application as a sequence of vertices and the edges that connect them, starting from the first vertex (or vertices) and ending at the last vertex (or vertices). An application invokes functions in the sequence as specified by the path in the DAG.",
      "type": "sliding_window",
      "tokens": 158
    },
    {
      "text": "An application invokes functions in the sequence as specified by the path in the DAG. Based on the nature of the workflow, function chains can be classified as Static or Dynamic. 2.1.1 Static DAGs : In static function chains (or DAGs), the workflows are specified in advance by the developer (using a schema), which is then orchestrated by the provider.",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "2.1.1 Static DAGs : In static function chains (or DAGs), the workflows are specified in advance by the developer (using a schema), which is then orchestrated by the provider. This re- sults in a predetermined path being traversed in the event of an application invocation. For example, in  Hotel Reservation (Figure 1c), if only one path (say,  NGINX - Make_Reservation ) is always chosen, it represents a static function chain.",
      "type": "sliding_window",
      "tokens": 116
    },
    {
      "text": "For example, in  Hotel Reservation (Figure 1c), if only one path (say,  NGINX - Make_Reservation ) is always chosen, it represents a static function chain. Hence- forth, we refer to static function chains as Static DAG Ap- plications (SDAs). Clearly, having prior knowledge of what functions will be invoked for an application makes container provisioning easier for SDAs.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "Clearly, having prior knowledge of what functions will be invoked for an application makes container provisioning easier for SDAs. 2.1.2 Dynamic DAGs : Although the application DAG con- sists of multiple functions that may be invoked, there are cases where the functions can themselves invoke other func- tions depending on the inputs they receive. We refer to such functions as Dynamic Branch Points (DBPs), and the chains they are a part of as Dynamic Function Chains.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "We refer to such functions as Dynamic Branch Points (DBPs), and the chains they are a part of as Dynamic Function Chains. In such cases, deploying containers without prior knowledge about the possible paths in the workflow leads to sub-optimal con- tainer provisioning for individual functions. Figure 1 shows the DAGs for three Dynamic Function Chains.",
      "type": "sliding_window",
      "tokens": 82
    },
    {
      "text": "Figure 1 shows the DAGs for three Dynamic Function Chains. Social Net- work  (Figure 1a), for example, is one such chain that has 11 functions in total, with each subset of functions contribut- ing to multiple paths (7 paths in total). For instance, from \n154 \nKraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC ’21, November 1–4, 2021, Seattle, WA, USA \nFeatures \nArchipelago [44] \nPower-chief [51] \nFifer [32] \nXanadu [27] \nGrandSLAm [34] \nSequoia [46] \nHybrid Histogram [42] \nCirrus [25] \nKraken \nSLO Guarantees ✓ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Dynamic DAG Applications ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ Slack-aware batching ✗ ✗ ✓ ✗ ✓ ✗ ✗ ✗ ✓ Cold Start Spillover Prevention ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓ Function Weight Apportioning ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓ Energy Efficieny ✗ ✓ ✓ ✓ ✗ ✗ ✓ ✓ ✓ Request Arrival Prediction ✓ ✗ ✓ ✓ ✓ ✗ ✓ ✗ ✓ Satisfactory Tail Latency ✓ ✗ ✓ ✗ ✓ ✓ ✓ ✓ ✓ \nTable 1: Comparing the features of  Kraken  with other state-of-the- art resource management frameworks.",
      "type": "sliding_window",
      "tokens": 377
    },
    {
      "text": "For instance, from \n154 \nKraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC ’21, November 1–4, 2021, Seattle, WA, USA \nFeatures \nArchipelago [44] \nPower-chief [51] \nFifer [32] \nXanadu [27] \nGrandSLAm [34] \nSequoia [46] \nHybrid Histogram [42] \nCirrus [25] \nKraken \nSLO Guarantees ✓ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Dynamic DAG Applications ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ Slack-aware batching ✗ ✗ ✓ ✗ ✓ ✗ ✗ ✗ ✓ Cold Start Spillover Prevention ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓ Function Weight Apportioning ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓ Energy Efficieny ✗ ✓ ✓ ✓ ✗ ✗ ✓ ✓ ✓ Request Arrival Prediction ✓ ✗ ✓ ✓ ✓ ✗ ✓ ✗ ✓ Satisfactory Tail Latency ✓ ✗ ✓ ✗ ✓ ✓ ✓ ✓ ✓ \nTable 1: Comparing the features of  Kraken  with other state-of-the- art resource management frameworks. App DBP Total Fanout Possible Paths Max Depth Social Network 2 8 7 5 Media Service 3 7 5 6 Hotel Reservation 1 2 2 4 Table 2: Analyzing Variability in Application Workflows. the start function  NGINX , any one of  Search ,  Make_Post , Read_Timeline  and  Follow  can be taken.",
      "type": "sliding_window",
      "tokens": 393
    },
    {
      "text": "the start function  NGINX , any one of  Search ,  Make_Post , Read_Timeline  and  Follow  can be taken. Henceforth, we refer to such Dynamic DAG Applications as DDAs. 2.2 Motivation Two specific challenges in the context of DDAs along with potential opportunities to resolve them are described below: Challenge 1: Path Prediction in DDAs.",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "2.2 Motivation Two specific challenges in the context of DDAs along with potential opportunities to resolve them are described below: Challenge 1: Path Prediction in DDAs. DDAs will only have a subset of their functions invoked for an incoming request to the application due to the presence of conditional paths within their DAGs. Figure 1 depicts the DAGs of three such applications from the  𝐷𝑒𝑎𝑡ℎ𝑆𝑡𝑎𝑟 benchmark suite [ 29 ], and Table 2 summarizes the various workflows that can be triggered by an incoming request to them.",
      "type": "sliding_window",
      "tokens": 117
    },
    {
      "text": "Figure 1 depicts the DAGs of three such applications from the  𝐷𝑒𝑎𝑡ℎ𝑆𝑡𝑎𝑟 benchmark suite [ 29 ], and Table 2 summarizes the various workflows that can be triggered by an incoming request to them. ‘Total fan-out’ and ‘Max Depth’ denotes the total number of outgoing branches and maximum distance between the start function and any other function in a DAG, respectively. Note that each func- tion triggers only one other function in the application at a time.",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "Note that each func- tion triggers only one other function in the application at a time. The decision to trigger the next function typically depends on the input to the current function, although there are cases like  Media Service  where this decision may de- pend on previous function inputs as well. Therefore, there is considerable variation in the functions that can be invoked in DDAs, thus, negating the inherent assumption in many frameworks [ 32 ,  42 ,  44 ,  50 ] that all functions will be invoked with the same frequency as the application.",
      "type": "sliding_window",
      "tokens": 121
    },
    {
      "text": "Therefore, there is considerable variation in the functions that can be invoked in DDAs, thus, negating the inherent assumption in many frameworks [ 32 ,  42 ,  44 ,  50 ] that all functions will be invoked with the same frequency as the application. This discrepancy can lead to substantial container overprovisioning. Opportunity 1:  In order to reduce overprovisioning of contain- ers, it is vital to design a workflow-aware resource management (RM) framework that can dynamically scale containers for each function, as opposed to uniformly scaling for all functions.",
      "type": "sliding_window",
      "tokens": 128
    },
    {
      "text": "Opportunity 1:  In order to reduce overprovisioning of contain- ers, it is vital to design a workflow-aware resource management (RM) framework that can dynamically scale containers for each function, as opposed to uniformly scaling for all functions. To design such a policy, the RM framework needs to know each function’s invocation frequency, which is a good estimator of its relative popularity. We introduce weights to estimate the appropriate number of containers to be spawned for each function.",
      "type": "sliding_window",
      "tokens": 110
    },
    {
      "text": "We introduce weights to estimate the appropriate number of containers to be spawned for each function. A function’s weight is calculated using the relative invocation frequency of a function along with other DAG-specific parameters  (explained in the next section). The relative invocation frequency of a function is measured with respect to the application it consti- tutes.",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "The relative invocation frequency of a function is measured with respect to the application it consti- tutes. The same function belonging to multiple applications can, therefore, have distinct weights in each application. To analyze the benefits of using invocation frequency, we designed a probability-based policy that employs weighted container scaling.",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "To analyze the benefits of using invocation frequency, we designed a probability-based policy that employs weighted container scaling. For the purposes of this experiment, we base our function weights only on invocation frequencies that are periodically calculated at the beginning of each scaling window. Figure 2 depicts the number of containers provisioned per function for three container provisioning policies subject to a Poisson arrival trace ( 𝜇 = 25 requests per second (rps)) for three applications.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "Figure 2 depicts the number of containers provisioned per function for three container provisioning policies subject to a Poisson arrival trace ( 𝜇 = 25 requests per second (rps)) for three applications. The static provision- ing policy is representative of current platforms [ 50 ] which spawn containers for functions in a workflow-agnostic fash- ion. Xanadu  [ 27 ] represents the policy that scales containers only along the Most Likely Path (MLP), which is the request’s expected path.",
      "type": "sliding_window",
      "tokens": 121
    },
    {
      "text": "Xanadu  [ 27 ] represents the policy that scales containers only along the Most Likely Path (MLP), which is the request’s expected path. If the request takes a different path,  Xanadu provisions containers along the path actually taken, in a reactive  fashion, and scales down the containers it provi- sioned along the MLP. Consequently,  Xanadu , when subject to moderate/heavy load, over-provisions containers by 32% compared to the Probability-based policy (from Figure 2) as a result of being locked into provisioning containers for the MLP until it is able to recalculate it.",
      "type": "sliding_window",
      "tokens": 151
    },
    {
      "text": "Consequently,  Xanadu , when subject to moderate/heavy load, over-provisions containers by 32% compared to the Probability-based policy (from Figure 2) as a result of being locked into provisioning containers for the MLP until it is able to recalculate it. Our probability-based policy, on the other hand, provisions containers for func- tions along  every possible path in proportion to their assigned weights . Note that variability in application usage patterns can lead to changes in function probabilities within each DDA, which the policy will have to account for.",
      "type": "sliding_window",
      "tokens": 130
    },
    {
      "text": "Note that variability in application usage patterns can lead to changes in function probabilities within each DDA, which the policy will have to account for. Challenge 2: Adaptive Container Provisioning. While probability-based container provisioning can significantly reduce the number of containers, the presence of container cold-starts leads to SLO violations (requests not meeting their expected response latency).",
      "type": "sliding_window",
      "tokens": 80
    },
    {
      "text": "While probability-based container provisioning can significantly reduce the number of containers, the presence of container cold-starts leads to SLO violations (requests not meeting their expected response latency). This is because cold starts can take up a significant proportion of a function’s response time (up to 10s of seconds [ 13 ,  14 ]). A significant amount of research [ 18 ,  22 ,  24 ,  38 ,  39 ,  43 ,  52 ] has been focused to- wards reducing cold-start overheads (in particular, proactive container provisioning [ 3 ,  32 ,  44 ,  46 ]).",
      "type": "sliding_window",
      "tokens": 140
    },
    {
      "text": "A significant amount of research [ 18 ,  22 ,  24 ,  38 ,  39 ,  43 ,  52 ] has been focused to- wards reducing cold-start overheads (in particular, proactive container provisioning [ 3 ,  32 ,  44 ,  46 ]). However, in the case of DDAs, DBPs make it unclear as to how many containers should be provisioned in advance for the functions along each path in the DAG. We identify two interlinked factors, in the context of DDAs, that need to be accounted for when making container scaling decisions.",
      "type": "sliding_window",
      "tokens": 132
    },
    {
      "text": "We identify two interlinked factors, in the context of DDAs, that need to be accounted for when making container scaling decisions. The first, is what we call  critical functions . These are functions within a DAG that have a high number of descendant functions that are linked to it and we use the \n155 \nSoCC ’21, November 1–4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "These are functions within a DAG that have a high number of descendant functions that are linked to it and we use the \n155 \nSoCC ’21, November 1–4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al. SEARCH \nNGINX \nMAKE_POST \nREAD_TIMELINE \nFOLLOW \nTEXT \nMEDIA \nUSER_TAG \nURL_SHORTENER \nCOMPOSE_POST \nPOST_STORAGE \n(a) Social Network. NGINX ID \nMOVIE_ID \nTEXT_SERVICE \nUSER_SERVICE \nRATING \nCOMPOSE_REVIEW \nMOVIE_REVIEW \nUSER_REVIEW \nREVIEW_STORAGE \n(b) Media Service.",
      "type": "sliding_window",
      "tokens": 175
    },
    {
      "text": "NGINX ID \nMOVIE_ID \nTEXT_SERVICE \nUSER_SERVICE \nRATING \nCOMPOSE_REVIEW \nMOVIE_REVIEW \nUSER_REVIEW \nREVIEW_STORAGE \n(b) Media Service. NGINX \nCHECK_RESERVATION GET_PROFILES SEARCH \nMAKE_RESERVATION \n(c) Hotel Reservation. Figure 1: DAGs of Dynamic Function Chains.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "Figure 1: DAGs of Dynamic Function Chains. 0 100 200 300 \nStatic Provisioning \nProbability-based \nXanadu \n# Containers NGINX Search Make_Post Text Media User_Tag URL_Shortener Compose_Post Post_Storage Read_Timeline Follow \n(a) Social Network. 0 100 200 300 \nStatic Provisioning \nProbability-based \nXanadu \n# Containers NGINX ID Movie_ID Text User_Service Rating Compose_Review Movie_Review User_Review Review_Storage \n(b) Media Service.",
      "type": "sliding_window",
      "tokens": 132
    },
    {
      "text": "0 100 200 300 \nStatic Provisioning \nProbability-based \nXanadu \n# Containers NGINX ID Movie_ID Text User_Service Rating Compose_Review Movie_Review User_Review Review_Storage \n(b) Media Service. 0 50 100 150 \nStatic Provisioning \nProbability-based \nXanadu \n# Containers \nNGINX Check_Reservation Get_Profiles Search Make_Reservation \n(c) Hotel Reservation. Figure 2: Function-wise Breakdown of Container Provisioning across Applications.",
      "type": "sliding_window",
      "tokens": 120
    },
    {
      "text": "Figure 2: Function-wise Breakdown of Container Provisioning across Applications. 98.10% \n98.55% \n99.00% \n99.45% \n99.90% \n0 \n200 \n400 \n600 \n800 \nCritical Non-Critical Critical Non-Critical Critical Non-Critical \nSocial Network Media Service Hotel Reservation \nPercentage \nResponse Time (ms) \nEnd-to-End Response Time SLO Guarantee \nFigure 3: Performance Deterioration resulting from Container De- ficiency at Critical Functions. The Primary Y-axis denotes the Av- erage End-to-End Response Time, the Secondary Y-axis represents the percentage of SLOs satisfied and the X-axis indicates the Appli- cation under consideration.",
      "type": "sliding_window",
      "tokens": 164
    },
    {
      "text": "The Primary Y-axis denotes the Av- erage End-to-End Response Time, the Secondary Y-axis represents the percentage of SLOs satisfied and the X-axis indicates the Appli- cation under consideration. term  Connectivity  to denote the ratio of number of descen- dant functions to the total number of functions. Inadequately provisioning containers for such functions causes requests to queue up as containers are spawned in the background.",
      "type": "sliding_window",
      "tokens": 114
    },
    {
      "text": "Inadequately provisioning containers for such functions causes requests to queue up as containers are spawned in the background. Moreover, this additional request load trickles down to all the descendants, adversely affecting their response times as well. We refer to this effect as  Cold Start Spillover .",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "We refer to this effect as  Cold Start Spillover . Fig- ure 3 compares the performance degradation resulting from underprovisioning both Critical and Non-Critical functions. The (Critical, Non-Critical) function pairs chosen for this experiment were ( Make_Post ,  Text ), ( ID ,  Rating ) and ( NGINX , Search ) for  Social Network ,  Media Service  and  Hotel Reserva- tion , respectively.",
      "type": "sliding_window",
      "tokens": 106
    },
    {
      "text": "The (Critical, Non-Critical) function pairs chosen for this experiment were ( Make_Post ,  Text ), ( ID ,  Rating ) and ( NGINX , Search ) for  Social Network ,  Media Service  and  Hotel Reserva- tion , respectively. It can be observed that underprovisioning containers for just one Critical function has a greater im- pact on application performance than doing so for a single Non-Critical function, with the end-to-end response time and SLO guarantees becoming 24ms and 0.25% worse on average. This effect can worsen if the same were to happen with multiple critical functions.",
      "type": "sliding_window",
      "tokens": 148
    },
    {
      "text": "This effect can worsen if the same were to happen with multiple critical functions. In addition to critical functions, it is also crucial to assign higher weights to common functions as well. Common func- tions refer to those which are a part of two or more paths within an application DAG.",
      "type": "sliding_window",
      "tokens": 64
    },
    {
      "text": "Common func- tions refer to those which are a part of two or more paths within an application DAG. Figure 4 shows the ‘hit rate’ of \nfunctions within an application that is subject to a constant load where any path in the application is equally likely to be picked. It can be seen that functions which are common to a larger number of paths are invoked at a higher rate by such a request arrival pattern.",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "It can be seen that functions which are common to a larger number of paths are invoked at a higher rate by such a request arrival pattern. Therefore, common functions have a higher chance of experiencing increased load due to be- ing present in multiple paths. Consequently, higher weights have to be assigned to such functions to ensure resilience in the presence of varying application usage patterns.",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "Consequently, higher weights have to be assigned to such functions to ensure resilience in the presence of varying application usage patterns. Opportunity 2:  Although proactive provisioning combined with probability-based scaling is useful, it is essential to iden- tify critical and common functions in each DDA and assign them higher weights in comparison to standard functions. Hence, rather than simply measuring the weights only in terms of function invocation frequency, we also need to account for DAG specific factors like  Commonality  and  Con- nectivity .",
      "type": "sliding_window",
      "tokens": 114
    },
    {
      "text": "Hence, rather than simply measuring the weights only in terms of function invocation frequency, we also need to account for DAG specific factors like  Commonality  and  Con- nectivity . The above discourse motivates us to rethink the design of serverless RM frameworks to cater to DDAs as well. One key driver for the design lies in a  Probability Estimation Model  for individual functions, which is explained below.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "One key driver for the design lies in a  Probability Estimation Model  for individual functions, which is explained below. 3 Function Probability Estimation Model As elucidated in  Opportunity-1 , to specifically address the container over-provisioning problem for DDAs, we need to estimate the weights to be assigned to their composite func- tions, a key component of which is the function invocation probability. In this section, we model the function probability estimation problem using a Variable Order Markov Model (VOMM) [ 21 ].",
      "type": "sliding_window",
      "tokens": 120
    },
    {
      "text": "In this section, we model the function probability estimation problem using a Variable Order Markov Model (VOMM) [ 21 ]. VOMMs are effective in capturing the invo- cation patterns of functions within each application while simultaneously isolating the effects of other applications that share them. This aids us in the calculation of function invocation probabilities.",
      "type": "sliding_window",
      "tokens": 80
    },
    {
      "text": "This aids us in the calculation of function invocation probabilities. Wherever appropriate, we draw in- spiration from related works that model user web surfing \n156 \nKraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC ’21, November 1–4, 2021, Seattle, WA, USA \n0 \n0.25 \n0.5 \n0.75 \n1 \nHit Rate \n(a) Social Network. 0 \n0.25 \n0.5 \n0.75 \n1 \nHit Rate \n(b) Media Service.",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": "0 \n0.25 \n0.5 \n0.75 \n1 \nHit Rate \n(b) Media Service. 0 \n0.25 \n0.5 \n0.75 \n1 \nHit Rate \n(c) Hotel Reservation. Figure 4: Function Hit Rate for an Evenly Distributed Load across all Paths in each Application.",
      "type": "sliding_window",
      "tokens": 58
    },
    {
      "text": "Figure 4: Function Hit Rate for an Evenly Distributed Load across all Paths in each Application. NGINX \nSearch \nMake_Post \nRead_Timeline \nFollow \nText \nMedia \nUser_tag \nURL_Shortener \nCompose_Post \nPost_Storage User_Tag URL Compose_Post \nFollow Text \nend \nSearch Make_Post Read_Timeline \nNGINX \nPost_Storage \n0.08 \n0.4 \n0.32 \n0.2 \n0.5 \n0.3 \n0.1 \n0.1 \n1 \n1 \n1 \n1 1 \n1 \nFigure 5: Transforming the Social Network DAG into a Transition Matrix. behavior [ 19 ,  20 ].",
      "type": "sliding_window",
      "tokens": 133
    },
    {
      "text": "behavior [ 19 ,  20 ]. VOMMs are an extension of Markov Mod- els [ 28 ], where the transition probability from the current state to the next state depends not only on the current state, but possibly on its predecessors (which we refer to as the ‘context’ of the state). Such behavior is seen in some of our workloads such as  𝑀𝑒𝑑𝑖𝑎𝑆𝑒𝑟𝑣𝑖𝑐𝑒 .",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "Such behavior is seen in some of our workloads such as  𝑀𝑒𝑑𝑖𝑎𝑆𝑒𝑟𝑣𝑖𝑐𝑒 . The order of the VOMM denotes the number of predecessors that influence the tran- sition decision. An application DAG can map neatly onto a Markov model wherein the functions within the application DAG are mod- eled as states of the VOMM.",
      "type": "sliding_window",
      "tokens": 80
    },
    {
      "text": "An application DAG can map neatly onto a Markov model wherein the functions within the application DAG are mod- eled as states of the VOMM. The process of one function invoking another function corresponds to a transition from the caller function state to the callee function state. The weight for each function corresponds to the state transition probability from the start state to the current one (note that this may require possibly transitioning through a number of intermediate states).",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "The weight for each function corresponds to the state transition probability from the start state to the current one (note that this may require possibly transitioning through a number of intermediate states). Thus, for a DAG with  𝑛 functions, the transition probabil- ity matrix,  𝑇 , is an  𝑛 ×  𝑛 matrix, where  𝑛 is the total number of states and each entry,  𝑡 𝑗𝑖 , is the transition probability from the state corresponding to the function along the col- umn j, ( 𝑓 𝑗 ), to that of the function along the row i, ( 𝑓 𝑖 ). An example of a Transition Matrix for the  Social Network , with 11 functions, is depicted in Figure 5.",
      "type": "sliding_window",
      "tokens": 163
    },
    {
      "text": "An example of a Transition Matrix for the  Social Network , with 11 functions, is depicted in Figure 5. An additional state,  end , is added to represent the state the model transitions to after a path in the DAG is completely executed. In Figure 5, as- suming both column and row indices of 𝑇 start at 0, an entry 𝑡 0 4  represents the transition probability from  NGINX ’s state to  Follow ’s state and is equal to 0.2.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "In Figure 5, as- suming both column and row indices of 𝑇 start at 0, an entry 𝑡 0 4  represents the transition probability from  NGINX ’s state to  Follow ’s state and is equal to 0.2. In general, this transition probability,  𝑡 𝑗𝑖 , is calculated as the number of requests from 𝑓 𝑗 to  𝑓 𝑖 divided by the number of incoming requests to  𝑓 𝑖 in the context of the application being considered. The Probability Vector is an  𝑛 × 1 column vector that cap- tures the probabilities of the model being in different states after a number of time steps have elapsed, given that the model was initialized at a known state.",
      "type": "sliding_window",
      "tokens": 159
    },
    {
      "text": "The Probability Vector is an  𝑛 × 1 column vector that cap- tures the probabilities of the model being in different states after a number of time steps have elapsed, given that the model was initialized at a known state. A ‘time step’ refers to a unit of measuring state change in the Markov Model. For practical purposes, we fix it to be the execution time of the slowest function at the current function depth.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "For practical purposes, we fix it to be the execution time of the slowest function at the current function depth. The ‘depth’ of a function, in this context, is defined as the distance, in terms of the number of edges in the DAG, from the start state to the current state. The Probability Vector after  𝑑 number of time steps can be represented as  𝑃 𝑑 .",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "The Probability Vector after  𝑑 number of time steps can be represented as  𝑃 𝑑 . Then, the Probability Vector for the next time step,  𝑑 +  1, is given by the  transition equation ,  𝑃 𝑡 + 1  =  𝑇 ·  𝑃 𝑡 . This equation infers that the Proba- bility Vector at the next time step is obtained by performing a transition operation across all possible current states.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "This equation infers that the Proba- bility Vector at the next time step is obtained by performing a transition operation across all possible current states. Repeatedly carrying out this transition process, starting from the initial Probability Vector, enables the estimation of probabilities of each function along all possible workflows. Iterating this process for  𝑑 time steps would yield the proba- bilities of functions at a depth of  𝑑 from the start function, given by  𝑃 𝑑 =  𝑇 𝑑 ·  𝑃 0 .",
      "type": "sliding_window",
      "tokens": 118
    },
    {
      "text": "Iterating this process for  𝑑 time steps would yield the proba- bilities of functions at a depth of  𝑑 from the start function, given by  𝑃 𝑑 =  𝑇 𝑑 ·  𝑃 0 . Thus, we can compute the probability of any function in the DAG by varying the depth,  𝑑 , using this equation. In order to apply this to proactive container allocation decisions, we can adopt the following procedure.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "In order to apply this to proactive container allocation decisions, we can adopt the following procedure. The incoming load to the application at time stamp,  𝑡 , is denoted as  𝑃𝐿 𝑡 and can be predicted using a load estimation model. Assuming each request to a function within the appli- cation spawns one container for that function, the number of containers to be provisioned in advance for functions at depth  𝑑 is given by: \n𝑁𝐶 𝑑 𝑡 =  ⌈ PL  𝑡 · ( 𝑇 𝑑 ·  𝑃 0 )⌉ \nHere,  𝑁𝐶 𝑑 𝑡 is a column vector of 𝑛 elements, each correspond- ing to the number of elements required to be provisioned for functions at a depth,  𝑑 , from the start function.",
      "type": "sliding_window",
      "tokens": 176
    },
    {
      "text": "Assuming each request to a function within the appli- cation spawns one container for that function, the number of containers to be provisioned in advance for functions at depth  𝑑 is given by: \n𝑁𝐶 𝑑 𝑡 =  ⌈ PL  𝑡 · ( 𝑇 𝑑 ·  𝑃 0 )⌉ \nHere,  𝑁𝐶 𝑑 𝑡 is a column vector of 𝑛 elements, each correspond- ing to the number of elements required to be provisioned for functions at a depth,  𝑑 , from the start function. Provisioning these containers at a fixed time window in advance from  𝑡 prevents cold starts from affecting the end-user experience. For example, if  𝑃𝐿 𝑡 is estimated to be 25 requests, then from Figure 5, we obtain the number of containers needed for functions at depth,  𝑑 =  1, by multiplying 25 with  𝑃 1  (which is  𝑇 1 · P 0 ).",
      "type": "sliding_window",
      "tokens": 206
    },
    {
      "text": "For example, if  𝑃𝐿 𝑡 is estimated to be 25 requests, then from Figure 5, we obtain the number of containers needed for functions at depth,  𝑑 =  1, by multiplying 25 with  𝑃 1  (which is  𝑇 1 · P 0 ). Consequently, the total number of containers re- quired for each function in the application can be computed \n157 \nSoCC ’21, November 1–4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al. Notation Meaning T Transition Matrix P 𝑑 Probability Vector for functions at depth,  d n # functions in application or # states in model f  𝑖 ,  f 𝑗 functions along row,  i  or column,  j  in  T t  𝑗𝑖 Transition probability from  f  𝑗 𝑡𝑜 f 𝑖 W 𝑝 Probability calculation time window t Request arrival time d # time steps for which transitions are done PL 𝑡 Scalar that represents the anticipated # requests at time,  t NC 𝑑 𝑡 # containers needed for functions at depth  d , at time  t Table 3: Notations used in Equations.",
      "type": "sliding_window",
      "tokens": 263
    },
    {
      "text": "Notation Meaning T Transition Matrix P 𝑑 Probability Vector for functions at depth,  d n # functions in application or # states in model f  𝑖 ,  f 𝑗 functions along row,  i  or column,  j  in  T t  𝑗𝑖 Transition probability from  f  𝑗 𝑡𝑜 f 𝑖 W 𝑝 Probability calculation time window t Request arrival time d # time steps for which transitions are done PL 𝑡 Scalar that represents the anticipated # requests at time,  t NC 𝑑 𝑡 # containers needed for functions at depth  d , at time  t Table 3: Notations used in Equations. by performing a summation of  𝑁𝐶 𝑑 𝑡 across all possible depths, 𝑑 , from the start function. We can now transform our previously-assumed Markov Model into a VOMM by splitting up context-dependent states into multiple context-independent states (the number of which is dependent on the DAG structure and the order of the VOMM).",
      "type": "sliding_window",
      "tokens": 228
    },
    {
      "text": "We can now transform our previously-assumed Markov Model into a VOMM by splitting up context-dependent states into multiple context-independent states (the number of which is dependent on the DAG structure and the order of the VOMM). For example, in Figure 5, if the transition from  Com- pose_Post  to  Post_Storage  depended on the immediate prede- cessors of  Compose_Post , the  Compose_Post  state would be context-dependent and would therefore, be split into context- independent states, namely,  𝐶𝑜𝑚𝑝𝑜𝑠𝑒 _ 𝑃𝑜𝑠𝑡 | 𝑇𝑒𝑥𝑡 ( Compose Post  given 𝑇𝑒𝑥𝑡 was already invoked), 𝐶𝑜𝑚𝑝𝑜𝑠𝑒 _ 𝑃𝑜𝑠𝑡 | 𝑀𝑒𝑑𝑖𝑎 etc. for the previous equations to hold.",
      "type": "sliding_window",
      "tokens": 165
    },
    {
      "text": "for the previous equations to hold. This changes the to- tal number of states from  𝑛 to  𝑁 , the number of extended states, resulting in a larger Transition Matrix and Probability Vector. To calculate the required number of containers for a single function that has multiple context-independent states associated with it, we take the sum of the calculated values for all of those states.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "To calculate the required number of containers for a single function that has multiple context-independent states associated with it, we take the sum of the calculated values for all of those states. 4 Overall Design of Kraken \nKraken 1   leverages the function weight estimation model from the above section along with several other design choices as outlined in this section (Figure 6). Users submit requests in the form of invocation triggers to applications  1  hosted on a Serverless platform.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "Users submit requests in the form of invocation triggers to applications  1  hosted on a Serverless platform. In  Kraken , containers are provisioned in advance by the Proactive Weighted Scaler (PWS)  2  to serve these incoming requests by avoiding cold starts. To achieve this, the PWS  2  first fetches relevant system metrics (using a monitoring tool  3  and orchestrator logs).",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "To achieve this, the PWS  2  first fetches relevant system metrics (using a monitoring tool  3  and orchestrator logs). These metrics, in addition to a developer-provided DAG Descriptor  4  , are then used by the Weight Estimation module  2a  of PWS  2  to assign weights to functions on the basis of their invocation probabil- ities. Commonality  and  Connectivity  (parameters in  2a  ) are additional parameters used in weight estimation to account for critical and common functions.",
      "type": "sliding_window",
      "tokens": 113
    },
    {
      "text": "Commonality  and  Connectivity  (parameters in  2a  ) are additional parameters used in weight estimation to account for critical and common functions. Additionally, a Load Pre- dictor module  2b  makes use of the system metrics to predict \n1 Kraken is a legendary sea monster with tentacles akin to multiple paths/chains in a Serverless DAG. Containers \nRequest  \nQueue \nFunction 1 \nFunction 2 \nFunction n \n.",
      "type": "sliding_window",
      "tokens": 98
    },
    {
      "text": "Containers \nRequest  \nQueue \nFunction 1 \nFunction 2 \nFunction n \n. . .",
      "type": "sliding_window",
      "tokens": 20
    },
    {
      "text": ". REPLICA TRACKER \nLOAD MONITOR \nOVERLOAD DETECTOR \nFUNCTION \nIDLER \nPROACTIVE WEIGHTED SCALER \nREACTIVE SCALER \nWEIGHT ESTIMATOR \nLOAD PREDICTOR \nDev-Provided  DAG Descriptor \nScrape Metrics \nAPPLICATIONS \nDECISION \nSCALE \n2a \n2 \n7 \n7a 7b \n1 \n3 \n2b \n4 \n5 \n3a \n3b \n6 \nPROBABILITY CONNECTIVITY COMMONALITY \nKRAKEN \nFigure 6: High-level View of Kraken Architecture incoming load and uses this in conjunction with the calcu- lated function weights to determine the number of function containers to be spawned by the underlying resource orches- trator  6  . However, only a fraction of these containers are actually spawned, as determined by the function’s batch size.",
      "type": "sliding_window",
      "tokens": 200
    },
    {
      "text": "However, only a fraction of these containers are actually spawned, as determined by the function’s batch size. The batch size denotes the number of requests per function each container can simultaneously serve without exceed- ing the SLO. In order to effectively handle mis-predictions in load,  Kraken  also employs a Reactive Scaler (RS)  7  that consists of two major components.",
      "type": "sliding_window",
      "tokens": 89
    },
    {
      "text": "In order to effectively handle mis-predictions in load,  Kraken  also employs a Reactive Scaler (RS)  7  that consists of two major components. First, is an Overload De- tector  7a  that keeps track of request overloading at functions by monitoring queuing delays at containers. Subsequently, it triggers container scaling  6  by calculating the additional containers needed to mitigate the delay.",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "Subsequently, it triggers container scaling  6  by calculating the additional containers needed to mitigate the delay. Second, a Function Idler component  7b  evicts containers from memory  6  when an excess is detected. Thus,  Kraken  makes use of PWS and RS to scale containers to meet the target SLOs while simul- taneously minimizing the number of containers by making use of function invocation probabilities, function batching, and container eviction, where appropriate.",
      "type": "sliding_window",
      "tokens": 110
    },
    {
      "text": "Thus,  Kraken  makes use of PWS and RS to scale containers to meet the target SLOs while simul- taneously minimizing the number of containers by making use of function invocation probabilities, function batching, and container eviction, where appropriate. 4.1 Proactive Weighted Scaler We describe in detail the components of PWS below. 4.1.1 Estimating function weights : Since workflows in SDAs are pre-determined, pre-deploying resources for them is straightforward in comparison to DDAs, whose workflow activation patterns are not known a priori.",
      "type": "sliding_window",
      "tokens": 131
    },
    {
      "text": "4.1.1 Estimating function weights : Since workflows in SDAs are pre-determined, pre-deploying resources for them is straightforward in comparison to DDAs, whose workflow activation patterns are not known a priori. For DDAs, de- ploying containers for each function in proportion to the application load will inevitably lead to resource wastage. To address this, we design a Weight Estimator  2a  to assign weights to all functions so as to allocate resources in propor- tion to them.",
      "type": "sliding_window",
      "tokens": 114
    },
    {
      "text": "To address this, we design a Weight Estimator  2a  to assign weights to all functions so as to allocate resources in propor- tion to them. Explained below is the working of the proce- dure  𝐸𝑠𝑡𝑖𝑚𝑎𝑡𝑒 _ 𝐶𝑜𝑛𝑡𝑎𝑖𝑛𝑒𝑟𝑠 in Algorithm 1 which is used to estimate function weights. Probability:  As alluded to in Section 2, one of the factors used in function weight estimation is its invocation probabil- ity.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "Probability:  As alluded to in Section 2, one of the factors used in function weight estimation is its invocation probabil- ity. The procedure in Section 3 describes how the transition probabilities of the states associated with functions are com- puted through repeated matrix multiplications of the Transi- tion Matrix, 𝑇 with the Probability Vector,  𝑃 . 𝐶𝑜𝑚𝑝𝑢𝑡𝑒 _ 𝑃𝑟𝑜𝑏 , \n158 \nKraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC ’21, November 1–4, 2021, Seattle, WA, USA \nin Algorithm 1, first estimates the invocation probabilities of a function’s immediate predecessors and uses it along with system log information and load measurements of the function to calculate its invocation probability.",
      "type": "sliding_window",
      "tokens": 176
    },
    {
      "text": "𝐶𝑜𝑚𝑝𝑢𝑡𝑒 _ 𝑃𝑟𝑜𝑏 , \n158 \nKraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC ’21, November 1–4, 2021, Seattle, WA, USA \nin Algorithm 1, first estimates the invocation probabilities of a function’s immediate predecessors and uses it along with system log information and load measurements of the function to calculate its invocation probability. Connectivity:  In addition to function invocation probabil- ities, it is necessary to also account for the effects of cold starts on DDAs while estimating function weights. Cold start spillovers (that often occur due to container underprovision- ing), as described in Section 2, can impact the response la- tency of applications harshly.",
      "type": "sliding_window",
      "tokens": 171
    },
    {
      "text": "Cold start spillovers (that often occur due to container underprovision- ing), as described in Section 2, can impact the response la- tency of applications harshly. Provisioning critical functions with more containers helps throttle this at the source. To this end,  Kraken  makes use of a parameter called  Connec- tivity , while assigning function weights.",
      "type": "sliding_window",
      "tokens": 82
    },
    {
      "text": "To this end,  Kraken  makes use of a parameter called  Connec- tivity , while assigning function weights. The  Connectivity of a function is defined as the ratio of number of its descen- dant functions to the total number of functions. The  𝐶𝑜𝑛𝑛 procedure in Algorithm 1 makes use of this formula.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "The  𝐶𝑜𝑛𝑛 procedure in Algorithm 1 makes use of this formula. For ex- ample, in Figure 1c, the  Connectivity  of  𝐶ℎ𝑒𝑐𝑘 _ 𝑅𝑒𝑠𝑒𝑟𝑣𝑎𝑡𝑖𝑜𝑛 is   2 \n5   since it has two descendants and there is a total of five functions. Bringing  Connectivity  into the weight estimation process helps  Kraken  assign a higher weight to critical func- tions, in turn, ensuring that more containers are assigned to them, resulting in improved response times for the functions themselves, as well as their descendants.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "Bringing  Connectivity  into the weight estimation process helps  Kraken  assign a higher weight to critical func- tions, in turn, ensuring that more containers are assigned to them, resulting in improved response times for the functions themselves, as well as their descendants. Commonality:  As described in Section 2, in addition to cold start spillovers, incorrect probability estimations may arise due to variability in workflow activation patterns. This may be due to change in user behavior manifesting itself as variable function input patterns.",
      "type": "sliding_window",
      "tokens": 106
    },
    {
      "text": "This may be due to change in user behavior manifesting itself as variable function input patterns. Such errors can lead to sub- optimal container allocation to DAG stages in proportion to the wrongly-calculated function weights. To cope with this, we introduce a parameter called  Commonality , which is defined as the fraction of number of unique paths that the function can be a part of with respect to the total number of unique paths.",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "To cope with this, we introduce a parameter called  Commonality , which is defined as the fraction of number of unique paths that the function can be a part of with respect to the total number of unique paths. This is how the procedure  𝐶𝑜𝑚𝑚 calculates Commonality  in Algorithm 1. For example, in Figure 1a, the Commonality  of the function  𝐶𝑜𝑚𝑝𝑜𝑠𝑒 _ 𝑃𝑜𝑠𝑡 in the  Social Network  application is given by the fraction   4 \n7   as it is present in four out of the seven possible paths in the DAG.",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": "For example, in Figure 1a, the Commonality  of the function  𝐶𝑜𝑚𝑝𝑜𝑠𝑒 _ 𝑃𝑜𝑠𝑡 in the  Social Network  application is given by the fraction   4 \n7   as it is present in four out of the seven possible paths in the DAG. Using Commonality  in the weight estimation process allows  Kraken to tolerate function probability miscalculations by assigning higher weights to those functions that are statistically more likely to experience rise in usage because of their presence in a larger number of workflows. Note that we deal with the possibility of container overprovisioning due to the in- creased function weights by allowing both  Connectivity  and Commonality  to be capped at a certain value.",
      "type": "sliding_window",
      "tokens": 146
    },
    {
      "text": "Note that we deal with the possibility of container overprovisioning due to the in- creased function weights by allowing both  Connectivity  and Commonality  to be capped at a certain value. 4.1.2 Proactive Container Provisioning : Once function weights are assigned by considering the above factors, they are employed in estimating the number of containers needed per DAG stage ( Estimate_Containers  in Algorithm 1). These containers have to be provisioned in advance to service fu- ture load to shield the end user from the effects of cold starts \nand thereby meet the SLO.",
      "type": "sliding_window",
      "tokens": 131
    },
    {
      "text": "These containers have to be provisioned in advance to service fu- ture load to shield the end user from the effects of cold starts \nand thereby meet the SLO. This load will have to be predicted in order to make timely container provisioning decisions. Algorithm 1  Proactive Scaling with weight estimation \n1:  for  Every Monitor_Interval= PW  do 2: Proactive_Weighted_Scaler ( ∀ 𝑓𝑢𝑛𝑐𝑡𝑖𝑜𝑛𝑠 ) 3:  procedure  Proactive_Weighted_Scaler( func ) 4: cl  ← 𝐶𝑢𝑟𝑟𝑒𝑛𝑡 _ 𝐿𝑜𝑎𝑑 ( 𝑓𝑢𝑛𝑐 ) 5: 𝑝𝑙 𝑡 + 𝑃𝑊 ← Load_Predictor ( 𝑐𝑙, 𝑝𝑙 𝑡 )  a 6: batches  ← l p 𝑙𝑡 + 𝑃𝑊 f 𝑢𝑛𝑐.𝑏𝑎𝑡𝑐ℎ _ 𝑠𝑖𝑧𝑒 m \nb 7: total_con  ← Estimate_Containers ( 𝑏𝑎𝑡𝑐ℎ𝑒𝑠, 𝑓𝑢𝑛𝑐 ) 8: reqd_con  ← 𝑚𝑎𝑥 ( 𝑚𝑖𝑛 _ 𝑐𝑜𝑛,𝑡𝑜𝑡𝑎𝑙 _ 𝑐𝑜𝑛 ) 9: Scale_Containers ( 𝑓𝑢𝑛𝑐,𝑟𝑒𝑞𝑑 _ 𝑐𝑜𝑛 ) 10:  procedure  estimate_containers( load, func ) ⊲ Output:  𝑟𝑒𝑞𝑑 _ 𝑐𝑜𝑛 11: func.prob  ← Compute_Prob (func) 12: reqd_con  ←⌈ 𝑙𝑜𝑎𝑑 ∗ 𝑓𝑢𝑛𝑐.𝑝𝑟𝑜𝑏 ⌉ 13: extra  ←⌈( Comm ( 𝑓𝑢𝑛𝑐 ) +  Conn ( 𝑓𝑢𝑛𝑐 )) ∗ 𝑟𝑒𝑞𝑑 _ 𝑐𝑜𝑛 ⌉ 14: reqd_con  ← reqd_con + extra \nKraken  makes use of a Load Predictor  2b  (Algorithm 1  a ) which uses the EWMA model to predict the incoming load at the end of a fixed time window,  𝑃𝑊 .",
      "type": "sliding_window",
      "tokens": 427
    },
    {
      "text": "Algorithm 1  Proactive Scaling with weight estimation \n1:  for  Every Monitor_Interval= PW  do 2: Proactive_Weighted_Scaler ( ∀ 𝑓𝑢𝑛𝑐𝑡𝑖𝑜𝑛𝑠 ) 3:  procedure  Proactive_Weighted_Scaler( func ) 4: cl  ← 𝐶𝑢𝑟𝑟𝑒𝑛𝑡 _ 𝐿𝑜𝑎𝑑 ( 𝑓𝑢𝑛𝑐 ) 5: 𝑝𝑙 𝑡 + 𝑃𝑊 ← Load_Predictor ( 𝑐𝑙, 𝑝𝑙 𝑡 )  a 6: batches  ← l p 𝑙𝑡 + 𝑃𝑊 f 𝑢𝑛𝑐.𝑏𝑎𝑡𝑐ℎ _ 𝑠𝑖𝑧𝑒 m \nb 7: total_con  ← Estimate_Containers ( 𝑏𝑎𝑡𝑐ℎ𝑒𝑠, 𝑓𝑢𝑛𝑐 ) 8: reqd_con  ← 𝑚𝑎𝑥 ( 𝑚𝑖𝑛 _ 𝑐𝑜𝑛,𝑡𝑜𝑡𝑎𝑙 _ 𝑐𝑜𝑛 ) 9: Scale_Containers ( 𝑓𝑢𝑛𝑐,𝑟𝑒𝑞𝑑 _ 𝑐𝑜𝑛 ) 10:  procedure  estimate_containers( load, func ) ⊲ Output:  𝑟𝑒𝑞𝑑 _ 𝑐𝑜𝑛 11: func.prob  ← Compute_Prob (func) 12: reqd_con  ←⌈ 𝑙𝑜𝑎𝑑 ∗ 𝑓𝑢𝑛𝑐.𝑝𝑟𝑜𝑏 ⌉ 13: extra  ←⌈( Comm ( 𝑓𝑢𝑛𝑐 ) +  Conn ( 𝑓𝑢𝑛𝑐 )) ∗ 𝑟𝑒𝑞𝑑 _ 𝑐𝑜𝑛 ⌉ 14: reqd_con  ← reqd_con + extra \nKraken  makes use of a Load Predictor  2b  (Algorithm 1  a ) which uses the EWMA model to predict the incoming load at the end of a fixed time window,  𝑃𝑊 . This time window is chosen according to the time taken to scale all functions in the respective application. Note that  𝑡 in the algorithm refers to the current time.",
      "type": "sliding_window",
      "tokens": 407
    },
    {
      "text": "Note that  𝑡 in the algorithm refers to the current time. We choose this model so as to have a light-weight load prediction mechanism that has min- imal impact on the end-to-end latency ( ∼ 10 − 3   ms). This Load Predictor  2b  can be used in conjunction with the afore- mentioned Weight Estimator  2a  to calculate the fraction of application load each function will receive.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "This Load Predictor  2b  can be used in conjunction with the afore- mentioned Weight Estimator  2a  to calculate the fraction of application load each function will receive. Kraken  uses this load distribution to pre-provision the requisite number of containers for all functions in the application. 4.2 Request Batching Many serverless frameworks [ 5 ,  10 ,  17 ,  27 ,  44 ,  46 ,  50 ] spawn a single container to serve each incoming request to a function.",
      "type": "sliding_window",
      "tokens": 113
    },
    {
      "text": "4.2 Request Batching Many serverless frameworks [ 5 ,  10 ,  17 ,  27 ,  44 ,  46 ,  50 ] spawn a single container to serve each incoming request to a function. While this approach is beneficial to minimize SLO violations, comparable performance can be achieved by using fewer containers by leveraging the notion of slack [ 32 ,  34 ]. Slack refers to the difference in expected response time and actual execution time of functions within a function chain.",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": "Slack refers to the difference in expected response time and actual execution time of functions within a function chain. Functions in a chain can have widely varying execution times. Allotting stage-wise SLOs to each function in a chain in proportion to their execution times reveals that there are cases where there is significant difference (slack) between the function’s expected SLO and its run-time.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "Allotting stage-wise SLOs to each function in a chain in proportion to their execution times reveals that there are cases where there is significant difference (slack) between the function’s expected SLO and its run-time. Figure 7 depicts this slack for all functions in the applications considered. This slack is leveraged by  Kraken  by batching multiple requests to the functions by queueing requests at their con- tainers.",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "This slack is leveraged by  Kraken  by batching multiple requests to the functions by queueing requests at their con- tainers. Requests are batched onto containers in a fashion similar to the First Fit Bin Packing algorithm [ 36 ]. Batching reduces the number of containers spawned for each function by a factor of its batch size (Algorithm 1  b  ).",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "Batching reduces the number of containers spawned for each function by a factor of its batch size (Algorithm 1  b  ). The batch size \nfor a function,  𝑓 , is defined as  BatchSize  ( 𝑓 )  = j StageSLO  ( 𝑓 ) ExecTime  ( 𝑓 ) k \n(Algorithm 1  b  ). Note that  ExecTime (f)  is estimated by aver- aging the execution times of the function obtained through \n159 \nSoCC ’21, November 1–4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al.",
      "type": "sliding_window",
      "tokens": 158
    },
    {
      "text": "Note that  ExecTime (f)  is estimated by aver- aging the execution times of the function obtained through \n159 \nSoCC ’21, November 1–4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al. 0 \n200 \n400 \n600 \nTime (ms) \nExec Time(ms) Stage-wise SLO(ms) \n(a) Social Network. 0 \nTime (ms) \nExec Time (ms) Stage-wise SLO (ms) \n600 \n450 \n300 \n150 \n(b) Media Service.",
      "type": "sliding_window",
      "tokens": 136
    },
    {
      "text": "0 \nTime (ms) \nExec Time (ms) Stage-wise SLO (ms) \n600 \n450 \n300 \n150 \n(b) Media Service. 0 \nTime (ms) \nExec Time (ms) Stage-wise SLO (ms) \n400 \n300 \n200 \n100 \n(c) Hotel Reservation. Figure 7: Slack for various Functions in each Application.",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "Figure 7: Slack for various Functions in each Application. offline profiling and  StageSLO (f)  is allotted in proportion to it. The batch size represents the number of requests that can be served by a function without violating the allotted stage-wise SLO.",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "The batch size represents the number of requests that can be served by a function without violating the allotted stage-wise SLO. 4.3 Reactive Scaler (RS) Though the introduction of Request Batching  5  allows Kraken  to reduce the containers provisioned, load mispredic- tions and probability miscalculations can still occur, leading to resource mismanagement, which could potentially affect the SLO compliance. To deal with this,  Kraken  also employs the RS  7  to scale containers up or down in response to re- quest overloading at containers (due to under-provisioning) and container over-provisioning, respectively.",
      "type": "sliding_window",
      "tokens": 141
    },
    {
      "text": "To deal with this,  Kraken  also employs the RS  7  to scale containers up or down in response to re- quest overloading at containers (due to under-provisioning) and container over-provisioning, respectively. In case of inadequate container provisioning, the Overload Detector  7a  in the RS  7  detects the number of allocated con- tainers for each DAG stage and calculates the estimated wait times of their queued requests (Algorithm 2  b  ). If it detects requests whose wait times exceed the cost of spawning a new container (the cold start of the function), overloading is said to have occurred at the stage.",
      "type": "sliding_window",
      "tokens": 152
    },
    {
      "text": "If it detects requests whose wait times exceed the cost of spawning a new container (the cold start of the function), overloading is said to have occurred at the stage. In such a scenario,  Kraken batches these requests (# _ 𝑑𝑒𝑙𝑎𝑦𝑒𝑑 _ 𝑟𝑒𝑞𝑢𝑒𝑠𝑡𝑠 in Algorithm 2) onto a newly-spawned container(s) (Algorithm 2  c  ). This is because requests that have to wait longer than the cold start would be served faster at a newly created container than by waiting at an overloaded container.",
      "type": "sliding_window",
      "tokens": 120
    },
    {
      "text": "This is because requests that have to wait longer than the cold start would be served faster at a newly created container than by waiting at an overloaded container. Similarly, for stages where container overprovisioning has occurred, the RS  7  gradually scales down its allocated containers to the appropriate number, if its Function Idler module  7b  detects excess containers for serving the current load (Algorithm 2  a ). Thus, the RS  7  , in combination with the PWS  2  and re- quest batching  5  , helps  Kraken  remain SLO compliant while using minimum resources.",
      "type": "sliding_window",
      "tokens": 130
    },
    {
      "text": "Thus, the RS  7  , in combination with the PWS  2  and re- quest batching  5  , helps  Kraken  remain SLO compliant while using minimum resources. 5 Implementation and Evaluation We have implemented a prototype of  Kraken  using open- source tools for evaluation with synthetic and real-world traces. The details are described below.",
      "type": "sliding_window",
      "tokens": 75
    },
    {
      "text": "The details are described below. 5.1 Prototype Implementation Kraken  is implemented primarily using Python and Go on top of  OpenFaaS  [ 11 ], an open-source serverless platform. Algorithm 2  Reactive Scaling \n1:  for  Every Monitor_Interval= DR  do 2: Reactive_Resource_Manager ( ∀ 𝑓𝑢𝑛𝑐𝑡𝑖𝑜𝑛𝑠 ) 3:  procedure  Reactive_Resource_Manager( func ) 4: cl  ← 𝐶𝑢𝑟𝑟𝑒𝑛𝑡 _ 𝐿𝑜𝑎𝑑 ( 𝑓𝑢𝑛𝑐 ) 5: func.existing_con  ← 𝐶𝑢𝑟𝑟𝑒𝑛𝑡 _ 𝑅𝑒𝑝𝑙𝑖𝑐𝑎𝑠 ( 𝑓𝑢𝑛𝑐 ) 6: if l c 𝑙 f 𝑢𝑛𝑐.𝑏𝑎𝑡𝑐ℎ _ 𝑠𝑖𝑧𝑒 m ≤ func.existing_con  then  a \n7: reqd_con  ← l c 𝑙 f 𝑢𝑛𝑐.𝑏𝑎𝑡𝑐ℎ _ 𝑠𝑖𝑧𝑒 m \n8: else 9: #_delayed_requests  ← Delay_Estimator ( 𝑓𝑢𝑛𝑐 )  b 10: extra_con  ← l  #_delayed_requests \nf 𝑢𝑛𝑐.𝑏𝑎𝑡𝑐ℎ _ 𝑠𝑖𝑧𝑒 m \nc 11: reqd_con  ← func.existing_con + extra_con 12: Scale_Containers ( 𝑓𝑢𝑛𝑐,𝑟𝑒𝑞𝑑 _ 𝑐𝑜𝑛 ) \nOpenFaaS  is deployed on top of  Kubernetes  [ 9 ], which acts as the chief container orchestrator.",
      "type": "sliding_window",
      "tokens": 330
    },
    {
      "text": "Algorithm 2  Reactive Scaling \n1:  for  Every Monitor_Interval= DR  do 2: Reactive_Resource_Manager ( ∀ 𝑓𝑢𝑛𝑐𝑡𝑖𝑜𝑛𝑠 ) 3:  procedure  Reactive_Resource_Manager( func ) 4: cl  ← 𝐶𝑢𝑟𝑟𝑒𝑛𝑡 _ 𝐿𝑜𝑎𝑑 ( 𝑓𝑢𝑛𝑐 ) 5: func.existing_con  ← 𝐶𝑢𝑟𝑟𝑒𝑛𝑡 _ 𝑅𝑒𝑝𝑙𝑖𝑐𝑎𝑠 ( 𝑓𝑢𝑛𝑐 ) 6: if l c 𝑙 f 𝑢𝑛𝑐.𝑏𝑎𝑡𝑐ℎ _ 𝑠𝑖𝑧𝑒 m ≤ func.existing_con  then  a \n7: reqd_con  ← l c 𝑙 f 𝑢𝑛𝑐.𝑏𝑎𝑡𝑐ℎ _ 𝑠𝑖𝑧𝑒 m \n8: else 9: #_delayed_requests  ← Delay_Estimator ( 𝑓𝑢𝑛𝑐 )  b 10: extra_con  ← l  #_delayed_requests \nf 𝑢𝑛𝑐.𝑏𝑎𝑡𝑐ℎ _ 𝑠𝑖𝑧𝑒 m \nc 11: reqd_con  ← func.existing_con + extra_con 12: Scale_Containers ( 𝑓𝑢𝑛𝑐,𝑟𝑒𝑞𝑑 _ 𝑐𝑜𝑛 ) \nOpenFaaS  is deployed on top of  Kubernetes  [ 9 ], which acts as the chief container orchestrator. OpenFaaS , by default, comes packaged with an Alert Manager module which is re- sponsible for alerting the underlying orchestrator of request surges by using metrics scraped by  Prometheus , which is an open-source systems monitoring toolkit [ 12 ]. This, in turn, triggers autoscaling to provision extra containers to service the load surge.",
      "type": "sliding_window",
      "tokens": 375
    },
    {
      "text": "This, in turn, triggers autoscaling to provision extra containers to service the load surge. We disable this Alert Manager and deploy the Proactive Weighted Scaler (PWS) and Reactive Scaler (RS) to carry out our container provisioning policies. Both the PWS and RS collect metrics, such as the current container count, load history and request rate for a function for a given time window, from  Prometheus  and the  Kubernetes system log, using the Replica Tracker and Load Monitor mod- ules.",
      "type": "sliding_window",
      "tokens": 125
    },
    {
      "text": "Both the PWS and RS collect metrics, such as the current container count, load history and request rate for a function for a given time window, from  Prometheus  and the  Kubernetes system log, using the Replica Tracker and Load Monitor mod- ules. Although fetching function metrics incurs a latency in the order of tens of milliseconds, it is performed in the back- ground (during autoscaling) and hence, does not affect the critical path. The load to each function within each applica- tion is calculated separately using the collected information.",
      "type": "sliding_window",
      "tokens": 139
    },
    {
      "text": "The load to each function within each applica- tion is calculated separately using the collected information. This prevents other applications from interfering with the probability calculation of shared functions. Additionally, the PWS uses a DAG descriptor, which is a file that contains a python dictionary that specifies the connectivity among functions.",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "Additionally, the PWS uses a DAG descriptor, which is a file that contains a python dictionary that specifies the connectivity among functions. Although constructing this is a one-time effort, automating this process through offline DAG profiling can be explored in future work. Table 4 gives an overview of Kraken ’s policies and their implementation details.",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "Table 4 gives an overview of Kraken ’s policies and their implementation details. 160 \nKraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC ’21, November 1–4, 2021, Seattle, WA, USA \nPolicy Component Implemented using/as \nPWS \nProbability System log info, Sparse Data Structures Commonality  &  Connectivity DAG Descriptor Load Predictor Pluggable model (EWMA) Batching Function containers persisted in memory \nRS Load Monitor Metrics from Prometheus & System logs Replica Tracker Table 4: Implementation details of  Kraken ’s policies. 5.2 Evaluation Methodology We evaluate the  Kraken  prototype on a 5 node  Kuber- netes  cluster with a dedicated manager node.",
      "type": "sliding_window",
      "tokens": 189
    },
    {
      "text": "5.2 Evaluation Methodology We evaluate the  Kraken  prototype on a 5 node  Kuber- netes  cluster with a dedicated manager node. Each node is equipped with, 32 cores (Intel CascadeLake), 256GB of RAM, 1 TB of storage and a 10 Gigabit Ethernet interconnect [ 35 ]. For energy measurements, we use an open-source version of Intel Power Gadget [ 16 ] that measures the energy consumed by all sockets in a node.",
      "type": "sliding_window",
      "tokens": 114
    },
    {
      "text": "For energy measurements, we use an open-source version of Intel Power Gadget [ 16 ] that measures the energy consumed by all sockets in a node. Load Generator:  We provide different traces as inputs to a load generator, which is based on Hey, an HTTP Load generator tool [ 7 ]. First, we use a synthetic Poisson-based request arrival rate with an average rate  𝜇 =  100.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "First, we use a synthetic Poisson-based request arrival rate with an average rate  𝜇 =  100. Second, we use real-world request arrival traces from Wiki [ 49 ] and Twitter [ 1 ] by running each experiment for about an hour. The Twitter trace has a large variation in peaks (average = 3332 rps, peak= 6978 rps) when compared to the Wiki trace (average = 284 rps, peak = 331 rps).",
      "type": "sliding_window",
      "tokens": 114
    },
    {
      "text": "The Twitter trace has a large variation in peaks (average = 3332 rps, peak= 6978 rps) when compared to the Wiki trace (average = 284 rps, peak = 331 rps). Applications:  Each request is modeled after a query to one of the three applications (DDAs) we consider from the 𝐷𝑒𝑎𝑡ℎ𝑆𝑡𝑎𝑟 benchmark suite [ 29 ]. We implement each ap- plication as a workflow of chained functions in  OpenFaaS .",
      "type": "sliding_window",
      "tokens": 118
    },
    {
      "text": "We implement each ap- plication as a workflow of chained functions in  OpenFaaS . To model the characteristics of the original functions, we invoke sleep timers within our functions to emulate their execution times (including the time for state recovery, if any). Transitions between functions are done using function calls on the basis of pre-assigned inter-function transition proba- bilities.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "Transitions between functions are done using function calls on the basis of pre-assigned inter-function transition proba- bilities. The probabilities vary by approximately  ± 20% of a seed. Note that these probabilities are not visible to  Kraken , but are only used to model function invocation patterns.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "Note that these probabilities are not visible to  Kraken , but are only used to model function invocation patterns. Metrics and Resource Management Policies:  We use the following metrics for evaluation: (i) average number of containers spawned, (ii) percentage of requests satisfy- ing the SLO (SLO guarantees), (iii) average application re- sponse times, (iv) end-to-end request latency percentiles, (v) container utilization, and (vi) cluster-wide energy sav- ings. We set the SLO at 1000ms.",
      "type": "sliding_window",
      "tokens": 138
    },
    {
      "text": "We set the SLO at 1000ms. We compare these metrics for  Kraken  against the container provisioning policies of Archipelago [ 44 ],  Fifer  [ 32 ] and  Xanadu  [ 27 ], which we will, henceforth, refer to as  Arch ,  Fifer  and  Xanadu , respectively. Additionally, we compare  Kraken  against policies with (a) statically assigned function probabilities ( SProb ) and (b) func- tion probabilities that dynamically adapt to changing invoca- tion patterns ( DProb ).",
      "type": "sliding_window",
      "tokens": 129
    },
    {
      "text": "Additionally, we compare  Kraken  against policies with (a) statically assigned function probabilities ( SProb ) and (b) func- tion probabilities that dynamically adapt to changing invoca- tion patterns ( DProb ). These policies use all the components of  Kraken  except  Commonality  and  Connectivity . 5.3 Large Scale Simulation To evaluate the effectiveness of  Kraken  in large-scale sys- tems, we built a high fidelity, multi-threaded simulator in Python using container cold start latencies and function execution times profiled from our real-system counterpart.",
      "type": "sliding_window",
      "tokens": 133
    },
    {
      "text": "5.3 Large Scale Simulation To evaluate the effectiveness of  Kraken  in large-scale sys- tems, we built a high fidelity, multi-threaded simulator in Python using container cold start latencies and function execution times profiled from our real-system counterpart. It simulates the working of DDAs running on a serverless framework that are subjected to both real-world (Twitter and Wiki) and synthetic (Poisson-based) traces. We have validated its correctness by correlating various metrics of interest generated from experiments run on the real system with scaled-down versions of the same traces (average ar- rival rate of  ∼ 100rps).",
      "type": "sliding_window",
      "tokens": 156
    },
    {
      "text": "We have validated its correctness by correlating various metrics of interest generated from experiments run on the real system with scaled-down versions of the same traces (average ar- rival rate of  ∼ 100rps). Therefore, the simulator allows us to evaluate our model for a larger setup, where we mimic an 11k core cluster which can handle up to 7000 requests (70 × more than the real system). Additionally, it helps compare the resource footprint of  Kraken  against a clairvoyant policy (Oracle) that has 100% load prediction accuracy.",
      "type": "sliding_window",
      "tokens": 121
    },
    {
      "text": "Additionally, it helps compare the resource footprint of  Kraken  against a clairvoyant policy (Oracle) that has 100% load prediction accuracy. 6 Analysis of Results This section presents experimental results for single ap- plications run in isolation for all schemes on the real system and simulation platform. We have also verified that  Kraken (as well as the other schemes) yield similar results (within 2%) when multiple applications are run concurrently.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "We have also verified that  Kraken (as well as the other schemes) yield similar results (within 2%) when multiple applications are run concurrently. 6.1 Real System Results 6.1.1 Containers Spawned : Figure 8 depicts the function- wise breakdown of the number of containers provisioned across all policies for individual applications. This repre- sents  𝑁𝐶 𝑑 𝑡 (Section 3) for all possible depths,  𝑑 .",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "This repre- sents  𝑁𝐶 𝑑 𝑡 (Section 3) for all possible depths,  𝑑 . It can be ob- served that, existing policies, namely,  Arch ,  Fifer  and  Xanadu spawn, respectively, 2.41x, 76% and 30% more containers than  Kraken , on average, across all applications. Overallo- cation of containers in case of  Arch  is due to two reasons: (i) it assumes that all functions in the application will be invoked at runtime; and (ii) it spawns one container per in- vocation request.",
      "type": "sliding_window",
      "tokens": 138
    },
    {
      "text": "Overallo- cation of containers in case of  Arch  is due to two reasons: (i) it assumes that all functions in the application will be invoked at runtime; and (ii) it spawns one container per in- vocation request. On the other hand,  Fifer  improves upon this by reducing the total number of containers spawned using request batching. However, it does not take workflow activation patterns into consideration while spawning con- tainers, leading to container overprovisioning.",
      "type": "sliding_window",
      "tokens": 116
    },
    {
      "text": "However, it does not take workflow activation patterns into consideration while spawning con- tainers, leading to container overprovisioning. The recently proposed scheme,  Xanadu , is based on a workflow-aware container deployment mechanism, but does not employ re- quest batching, leading to extra containers being deployed in comparison to  Kraken . Furthermore, it can be seen that Xanadu  provisions a relatively high number of containers for a particular group of functions as compared to the rest.",
      "type": "sliding_window",
      "tokens": 116
    },
    {
      "text": "Furthermore, it can be seen that Xanadu  provisions a relatively high number of containers for a particular group of functions as compared to the rest. This is because it allocates containers to serve the predicted load along only the Most Likely Path (MLP) of a request. The rest of the containers are a result of  reactive scaling  that follows from MLP mispredictions, which accounts for 34% of the total number of containers spawned.",
      "type": "sliding_window",
      "tokens": 101
    },
    {
      "text": "The rest of the containers are a result of  reactive scaling  that follows from MLP mispredictions, which accounts for 34% of the total number of containers spawned. 161 \nSoCC ’21, November 1–4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al. 0 500 1000 1500 \nArch \nFifer \nDProb \nKraken \nSProb \nXanadu \n# Containers \nNGINX Search Make_Post Text Media User_Tag URL_Shortener Compose_Post Post_Storage Read_Timeline Follow \n(a) Social Network.",
      "type": "sliding_window",
      "tokens": 142
    },
    {
      "text": "0 500 1000 1500 \nArch \nFifer \nDProb \nKraken \nSProb \nXanadu \n# Containers \nNGINX Search Make_Post Text Media User_Tag URL_Shortener Compose_Post Post_Storage Read_Timeline Follow \n(a) Social Network. 0 500 1000 1500 \nArch \nFifer \nDProb \nKraken \nSProb \nXanadu \n# Containers NGINX ID Movie_ID Text User_Service Rating Compose_Review Movie_Review User_Review Review_Storage \n(b) Media Service. 0 200 400 600 \nArch \nFifer \nDProb \nKraken \nSProb \nXanadu \n# Containers \nNGINX Check_Reservation \nGet_Profiles Search \nMake_Reservation \n(c) Hotel Reservation.",
      "type": "sliding_window",
      "tokens": 171
    },
    {
      "text": "0 200 400 600 \nArch \nFifer \nDProb \nKraken \nSProb \nXanadu \n# Containers \nNGINX Check_Reservation \nGet_Profiles Search \nMake_Reservation \n(c) Hotel Reservation. Figure 8: Real System: Stage-wise Breakdown of Containers spawned by each policy. The reduction in the number of containers spawned by Kraken  in comparison to other policies is roughly propor- tional to the total number of application workflows and the slack available for each function within a workflow (see Ta- ble 2 and Figure 7).",
      "type": "sliding_window",
      "tokens": 131
    },
    {
      "text": "The reduction in the number of containers spawned by Kraken  in comparison to other policies is roughly propor- tional to the total number of application workflows and the slack available for each function within a workflow (see Ta- ble 2 and Figure 7). For instance, Figure 8 indicates that the Social Network ,  Media Service  and  Hotel Reservation  applica- tions show the highest (73%, 53% and 36%), moderate (40%, 28% and 7%) and least (at most 33%) reductions in the number of containers spawned with respect to existing policies,  Arch , Fifer  and  Xanadu , respectively. Both  Social Network  and  Me- dia Service  have a high number of workflows, but the former has more functions with higher slack, leading to increased batching, thereby resulting in the most reduction in con- tainers spawned.",
      "type": "sliding_window",
      "tokens": 202
    },
    {
      "text": "Both  Social Network  and  Me- dia Service  have a high number of workflows, but the former has more functions with higher slack, leading to increased batching, thereby resulting in the most reduction in con- tainers spawned. Hotel Reservation  has the least number of workflows as well as the lowest overall slack for all functions, resulting in the least reduction in the number of containers. On the other hand,  DProb  and  SProb  spawn fewer containers than  Kraken  as a consequence of not using  Commonality  and Connectivity  to augment function weights, while making container allocation decisions.",
      "type": "sliding_window",
      "tokens": 136
    },
    {
      "text": "On the other hand,  DProb  and  SProb  spawn fewer containers than  Kraken  as a consequence of not using  Commonality  and Connectivity  to augment function weights, while making container allocation decisions. As a result,  Kraken  provisions up to 21% more containers than both  DProb  and  SProb  for the three applications. Note that, these additional containers are necessary to reduce SLO violations.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "Note that, these additional containers are necessary to reduce SLO violations. 6.1.2 End-to-End Response Times and SLO Compli- ance : Figure 9 shows the breakdown of the average end-to- end response times and Figure 10 juxtaposes the total number of containers provisioned against the SLO Guarantees for all policies and applications, averaged across all traces. From these graphs, it is evident that  Kraken  exhibits comparable performance to existing policies while having a minimal re- source footprint.",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": "From these graphs, it is evident that  Kraken  exhibits comparable performance to existing policies while having a minimal re- source footprint. For the  Social Network  application,  Kraken remains within 60 ms of the end-to-end response time of Arch  (Figure 9a), which performs the best out of all policies with respect to these metrics, while ensuring 99.94% SLO guarantees (Figure 10a) . However,  Arch  uses 4x the number of containers used by  Kraken  (Figure 10a).",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "However,  Arch  uses 4x the number of containers used by  Kraken  (Figure 10a). Kraken  also performs similar to  Fifer , while using 58% reduced containers for  Social Network . From Figures 9 and 10, it can be seen that  Xanadu  has similar (or worse) end- to-end response times than  Kraken  (up to 50 ms more), but \nspawns more containers as well (up to 70% more) and satisfies fewer SLOs on average (0.2% lesser).",
      "type": "sliding_window",
      "tokens": 115
    },
    {
      "text": "From Figures 9 and 10, it can be seen that  Xanadu  has similar (or worse) end- to-end response times than  Kraken  (up to 50 ms more), but \nspawns more containers as well (up to 70% more) and satisfies fewer SLOs on average (0.2% lesser). This can be attributed to  Xanadu ’s container pre-deployment policy which causes reactive scale outs as a result of MLP mispredictions. This ef- fect is highlighted in applications such as  Social Network  and Media Service  which have relatively high MLP misprediction rates (80% and 50%, respectively 2 )) due to the presence of multiple possible paths (Table 2).",
      "type": "sliding_window",
      "tokens": 164
    },
    {
      "text": "This ef- fect is highlighted in applications such as  Social Network  and Media Service  which have relatively high MLP misprediction rates (80% and 50%, respectively 2 )) due to the presence of multiple possible paths (Table 2). Media Service  suffers from higher end-to-end response times, further exacerbating this effect. Xanadu  has only a 34% misprediction rate for  Hotel Reservation , due to the lower number of workflows, and is seen to match  Kraken  in terms of SLOs satisfied (99.87%).",
      "type": "sliding_window",
      "tokens": 123
    },
    {
      "text": "Xanadu  has only a 34% misprediction rate for  Hotel Reservation , due to the lower number of workflows, and is seen to match  Kraken  in terms of SLOs satisfied (99.87%). The breakdown of the average response times in Figure 9 shows that both  Arch  and  Xanadu  do not suffer from queue- ing delays. This is because both policies spawn a container per request, resulting in almost zero queueing.",
      "type": "sliding_window",
      "tokens": 101
    },
    {
      "text": "This is because both policies spawn a container per request, resulting in almost zero queueing. The relatively high cold start-induced delay experienced by  Xanadu  can be attributed to the reactive scaling it uses to cope with MLP mispredictions. Kraken  exhibits delay characteristics simi- lar to  Fifer  owing to both policies having batching and a similar container pre-deployment policy.",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "Kraken  exhibits delay characteristics simi- lar to  Fifer  owing to both policies having batching and a similar container pre-deployment policy. However,  Kraken allocates fewer containers (57% lesser, on average across all applications) along each workflow compared to  Fifer . DProb and  SProb  exhibit higher overall end-to-end response times compared to  Kraken , with  SProb  experiencing a dispropor- tionately high queueing delay compared to its cold start delay.",
      "type": "sliding_window",
      "tokens": 115
    },
    {
      "text": "DProb and  SProb  exhibit higher overall end-to-end response times compared to  Kraken , with  SProb  experiencing a dispropor- tionately high queueing delay compared to its cold start delay. This is because it uses statically assigned function weights, which prevents it from being able to proactively spawn con- tainers according to the varying user input. This results in the majority of requests getting queued at the containers.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "This results in the majority of requests getting queued at the containers. 6.1.3 Analysis of Key Improvements : This subsection fo- cuses on the key improvements offered by  Kraken  in terms of Container Utilization, Response Latency Distribution and Energy Efficiency. Although we use specific combinations of applications and traces to highlight the improvements, the results are similar for other workload mixes as well.",
      "type": "sliding_window",
      "tokens": 79
    },
    {
      "text": "Although we use specific combinations of applications and traces to highlight the improvements, the results are similar for other workload mixes as well. Container Utilization:  Figure 11 plots the average num- ber of requests executed per container (Jobs per container) \n2 MLP misprediction rates are not shown in any Figure \n162 \nKraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC ’21, November 1–4, 2021, Seattle, WA, USA \n0 \n100 \n200 \n300 \nArch Fifer DProb Kraken SProb Xanadu \nResponse Time (ms) \nQueueing Cold Start Execution Time \n(a) Social Network. 0 \n150 \n300 \n450 \n600 \nArch Fifer DProb Kraken SProb Xanadu \nResponse Time (ms) \nQueueing Cold Start Execution Time \n(b) Media Service.",
      "type": "sliding_window",
      "tokens": 198
    },
    {
      "text": "0 \n150 \n300 \n450 \n600 \nArch Fifer DProb Kraken SProb Xanadu \nResponse Time (ms) \nQueueing Cold Start Execution Time \n(b) Media Service. 0 \n150 \n300 \n450 \nArch Fifer DProb Kraken SProb Xanadu \nResponse Time (ms) \nQueueing Cold Start Execution Time \n(c) Hotel Reservation. Figure 9: Real System: Breakdown of Average End-to-End Response Times in terms of queueing delay, cold start delay and execution time.",
      "type": "sliding_window",
      "tokens": 120
    },
    {
      "text": "Figure 9: Real System: Breakdown of Average End-to-End Response Times in terms of queueing delay, cold start delay and execution time. 99.40% \n99.55% \n99.70% \n99.85% \n100.00% \n0 \n300 \n600 \n900 \n1200 \nArch Fifer Dprob Kraken Sprob Xanadu \nPercentage \n# Containers \n# Containers SLO Guarantees \n(a) Social Network. 99.00% \n99.25% \n99.50% \n99.75% \n100.00% \n0 \n300 \n600 \n900 \n1200 \nArch Fifer DProb Kraken SProb Xanadu \nPercentage \n# Containers \n# Containers SLO Guarantees \n(b) Media Service.",
      "type": "sliding_window",
      "tokens": 151
    },
    {
      "text": "99.00% \n99.25% \n99.50% \n99.75% \n100.00% \n0 \n300 \n600 \n900 \n1200 \nArch Fifer DProb Kraken SProb Xanadu \nPercentage \n# Containers \n# Containers SLO Guarantees \n(b) Media Service. 98.50% \n99.00% \n99.50% \n100.00% \n0 \n200 \n400 \n600 \nArch Fifer DProb Kraken SProb Xanadu \nPercentage \n# Containers \n# Containers SLO Guarantees \n(c) Hotel Reservation. Figure 10: Real System: Comparison of Total Number of Containers spawned VS SLOs satisfied by each policy.",
      "type": "sliding_window",
      "tokens": 138
    },
    {
      "text": "Figure 10: Real System: Comparison of Total Number of Containers spawned VS SLOs satisfied by each policy. The Primary Y-Axis denotes the number of containers spawned, The secondary Y-axis indicates the percentage of SLOs met and the X-axis represents each policy. 0 \n300 \n600 \n900 \n1200 \n1500 \nArch Fifer DProb Kraken SProb Xanadu \nJobs per Container \nFigure 11: Real System: Comparison of Container Utilization (a.k.a.",
      "type": "sliding_window",
      "tokens": 119
    },
    {
      "text": "0 \n300 \n600 \n900 \n1200 \n1500 \nArch Fifer DProb Kraken SProb Xanadu \nJobs per Container \nFigure 11: Real System: Comparison of Container Utilization (a.k.a. average #jobs executed per Container). 0 \n300 \n600 \n900 \n1200 \n0.25 0.5 0.75 0.98 0.99 \nResponse Time (ms) \nCDF Archipelago Fifer DProb Kraken SProb SLO Xanadu \nFigure 12: Real System: Response Time Distribution.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "0 \n300 \n600 \n900 \n1200 \n0.25 0.5 0.75 0.98 0.99 \nResponse Time (ms) \nCDF Archipelago Fifer DProb Kraken SProb SLO Xanadu \nFigure 12: Real System: Response Time Distribution. across all functions in  Social Network  for the Poisson trace. An ideal scheme would focus on packing more number of requests per container to improve utilization without caus- ing SLO violations.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "An ideal scheme would focus on packing more number of requests per container to improve utilization without caus- ing SLO violations. Kraken  shows 4x, 2.16x and 2.06x more container utilization compared to  Arch ,  Fifer , and  Xanadu respectively. This is because  Kraken  limits the number of containers spawned through function weight assignment and request batching.",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "This is because  Kraken  limits the number of containers spawned through function weight assignment and request batching. DProb  and  SProb  both exhibit higher utilization compared to  Kraken  (15%) as a result of spawning fewer containers overall, owing to not accounting for  crit- ical  and  common  functions while provisioning containers. Consequently, they exhibit up to 0.24% more SLO Violations compared to  Kraken , for this workload mix.",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "Consequently, they exhibit up to 0.24% more SLO Violations compared to  Kraken , for this workload mix. Latency Distribution:  The end-to-end latency distribution for all policies for the  Social Network  application with the Twitter trace is plotted in Figure 12. In particular,  Arch ,  Fifer and  Kraken  show comparable latencies, with P99 values re- maining well within the SLO of 1000ms.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "In particular,  Arch ,  Fifer and  Kraken  show comparable latencies, with P99 values re- maining well within the SLO of 1000ms. However,  Arch  and Fifer  use 3.51x and 2.1x more containers than  Kraken  to \n0 \n0.25 \n0.5 \n0.75 \n1 \nArch Fifer DProb Kraken SProb Xanadu \nEnergy Consumption Rate \n(a) Energy Consumption Rate. 0 \n300 \n600 \n900 \n1200 \n0.25 0.5 0.75 0.98 0.99 \nLatency (ms) \nCDF Kraken Comm Only \nConn Only SLO \n(b) Response Time Distribution.",
      "type": "sliding_window",
      "tokens": 138
    },
    {
      "text": "0 \n300 \n600 \n900 \n1200 \n0.25 0.5 0.75 0.98 0.99 \nLatency (ms) \nCDF Kraken Comm Only \nConn Only SLO \n(b) Response Time Distribution. Figure 13: Real System: Normalized Energy Consumption of all Schemes and Response Time Distribution of  Kraken ,  Comm Only and  Conn Only achieve this. The tail latency (measured at P99) for  DProb almost exceeds the SLO, whereas it does so for  SProb .",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "The tail latency (measured at P99) for  DProb almost exceeds the SLO, whereas it does so for  SProb . Kraken manages to avoid high tail latency by assigning augmented weights to key functions, thus, helping it tolerate incorrect load/probability estimations. SProb  does worse than  DProb  at the tail because of its lack of adaptive probability estimation.",
      "type": "sliding_window",
      "tokens": 88
    },
    {
      "text": "SProb  does worse than  DProb  at the tail because of its lack of adaptive probability estimation. Kraken  makes use of 21% more containers to achieve the improved latencies. Xanadu  experiences a sudden rise in tail latency, with it being 100ms more than that of  Kraken , while using 96% more containers.",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "Xanadu  experiences a sudden rise in tail latency, with it being 100ms more than that of  Kraken , while using 96% more containers. This is due to  Xanadu ’s MLP misprediction and the resultant container over-provisioning. Energy Efficiency:  We measure the energy-consumption as total Energy consumed divided over total time.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "Energy Efficiency:  We measure the energy-consumption as total Energy consumed divided over total time. Kraken achieves one of the lowest energy consumption rates among all the policies considered, with it bettering existing policies, namely,  Arch ,  Fifer  and  Xanadu  by 26%, 14% and 3% respec- tively (for the workload mix of  Media Service  application with Wiki trace) as depicted in Figure 13a. These savings can go up to 48% compared to  Arch  for applications like  Social Network .",
      "type": "sliding_window",
      "tokens": 118
    },
    {
      "text": "These savings can go up to 48% compared to  Arch  for applications like  Social Network . The resultant energy savings of  Kraken  are a direct consequence of the savings in computation and memory usage from the fewer containers spawned. Only  DProb  and SProb  consume lesser energy than  Kraken  (4% lesser), due to their more aggressive container reduction approach.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "Only  DProb  and SProb  consume lesser energy than  Kraken  (4% lesser), due to their more aggressive container reduction approach. 6.1.4 Ablation Study : This subsection conducts a brick-by- brick evaluation of  Kraken  using  Conn Only  and  Comm Only , \n163 \nSoCC ’21, November 1–4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al. Application Kraken Comm Only Conn Only Social Network (99.94%, 284) (99.91%, 276) (99.89%, 256) Media Service (99.73%, 572) (99.66%, 561) (99.64%, 552) Hotel Reservation (99.87%, 316) (99.77%, 290) (99.74%, 282) Table 5: Real System: Comparing (SLO Guarantees,#Containers Spawned) against  Comm Only  and  Conn Only .",
      "type": "sliding_window",
      "tokens": 226
    },
    {
      "text": "Application Kraken Comm Only Conn Only Social Network (99.94%, 284) (99.91%, 276) (99.89%, 256) Media Service (99.73%, 572) (99.66%, 561) (99.64%, 552) Hotel Reservation (99.87%, 316) (99.77%, 290) (99.74%, 282) Table 5: Real System: Comparing (SLO Guarantees,#Containers Spawned) against  Comm Only  and  Conn Only . schemes that exclude  Commonality  and  Connectivity  com- ponents from  Kraken , respectively. From Table 5, it can be seen that  Comm Only  spawns 8% more containers than  Conn Only  for  Social Network .",
      "type": "sliding_window",
      "tokens": 175
    },
    {
      "text": "From Table 5, it can be seen that  Comm Only  spawns 8% more containers than  Conn Only  for  Social Network . This difference is lesser for the other applications. Upon closer examination, we see that this is due to functions having different degrees of  Commonality and  Connectivity .",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "Upon closer examination, we see that this is due to functions having different degrees of  Commonality and  Connectivity . Moreover, the majority of functions whose Commonality  and  Connectivity  differ, have a high batch size, thereby reducing the variation in the number of containers spawned. Following this, we observe that the variation in the number of containers in  Social Network  is mainly due to the significant difference in the  Commonality  and  Connectivity of the  Compose Post  function whose batch size is only one.",
      "type": "sliding_window",
      "tokens": 110
    },
    {
      "text": "Following this, we observe that the variation in the number of containers in  Social Network  is mainly due to the significant difference in the  Commonality  and  Connectivity of the  Compose Post  function whose batch size is only one. There is lesser difference in containers spawned by  Comm Only ,  Conn Only  and  Kraken  for  Media Service  because we have implemented  Kraken  with a cap on the additional con- tainers spawned due to  Commonality  and  Connectivity  when the sum of their values exceeds a threshold. This threshold is exceeded in  Media Service  for the majority of functions.",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "This threshold is exceeded in  Media Service  for the majority of functions. Due to the difference in container provisioning, the difference in response times between the three schemes is evident at the tail of the response time distribution (Figure 13b). Comm Only  and  Conn Only  are seen to exceed the target SLO at the 99th percentile.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "Comm Only  and  Conn Only  are seen to exceed the target SLO at the 99th percentile. The tail latency of  Kraken , in comparison, grows slower and remains within the target SLO. 6.2 Simulator Results Since the real-system is limited to a 160-core cluster, we use our in-house simulator, which can simulate an 11k-core cluster, to study the scalability of  Kraken .",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "6.2 Simulator Results Since the real-system is limited to a 160-core cluster, we use our in-house simulator, which can simulate an 11k-core cluster, to study the scalability of  Kraken . We mimic a large scale Poisson arrival trace ( 𝜇 = 1000rps), Wiki ( 𝜇 = 284 rps) and Twitter ( 𝜇 = 3332 rps) traces. Figure 14 plots the con- tainers spawned versus the SLO guarantees for each appli- cation for all traces.",
      "type": "sliding_window",
      "tokens": 134
    },
    {
      "text": "Figure 14 plots the con- tainers spawned versus the SLO guarantees for each appli- cation for all traces. The simulator results closely correlate to those of the real system. Kraken  is seen to reduce con- tainer overprovisioning when applications have numerous possible workflows and enough slack per function to exploit.",
      "type": "sliding_window",
      "tokens": 80
    },
    {
      "text": "Kraken  is seen to reduce con- tainer overprovisioning when applications have numerous possible workflows and enough slack per function to exploit. Notably,  Kraken  spawns nearly 80% less containers for  Social Network  in comparison to  Arch . Container overprovisioning is inflated 15% more than the corresponding real system re- sult, due to the large-scale traces.",
      "type": "sliding_window",
      "tokens": 89
    },
    {
      "text": "Container overprovisioning is inflated 15% more than the corresponding real system re- sult, due to the large-scale traces. Table 6 shows the median and tail latencies of each policy averaged across all appli- cations for the three traces. The trend we observe is that traces with higher variability, such as the Twitter trace, af- fect the tail latencies of policies more harshly than the other, more predictable, traces.",
      "type": "sliding_window",
      "tokens": 107
    },
    {
      "text": "The trend we observe is that traces with higher variability, such as the Twitter trace, af- fect the tail latencies of policies more harshly than the other, more predictable, traces. Nevertheless,  Kraken  is resilient to \nPolicy Poisson Wiki Twitter Med Tail Med Tail Med Tail Arch 336 568 336 568 336 599 Fifer 362 612 360 611 373 833 DProb 371 746 368 753 381 1549 Kraken 366 634 358 633 371 974 SProb 395 1101 382 1073 395 1610 Xanadu 343 723 340 774 340 1244 Table 6: Simulator: Median and tail latencies (in ms) averaged across all applications for the three traces \nunpredictable loads as well, with tail latencies always remain- ing within the SLO (1000 ms). However, the tail latencies of  DProb  and  SProb  sometimes exceeds the SLO, since they don’t use  Commonality  and  Connectivity .",
      "type": "sliding_window",
      "tokens": 240
    },
    {
      "text": "However, the tail latencies of  DProb  and  SProb  sometimes exceeds the SLO, since they don’t use  Commonality  and  Connectivity . It is observed that Xanadu  also violates the SLO for the Twitter trace, owing to the reactive scale-outs resulting from MLP mispredictions. 6.2.1 Sensitivity Study : This subsection compares  Kraken against  Oracle , which is an ideal policy that is assumed to be able to predict future load and all path probabilities with 100% accuracy and also has request batching.",
      "type": "sliding_window",
      "tokens": 124
    },
    {
      "text": "6.2.1 Sensitivity Study : This subsection compares  Kraken against  Oracle , which is an ideal policy that is assumed to be able to predict future load and all path probabilities with 100% accuracy and also has request batching. Consequently, Oracle  does not suffer from cold starts and minimizes con- tainers spawned. Figure 15 shows the breakdown of total number of containers spawned for each application, aver- aged across all realistic large-scale traces using the simulator.",
      "type": "sliding_window",
      "tokens": 110
    },
    {
      "text": "Figure 15 shows the breakdown of total number of containers spawned for each application, aver- aged across all realistic large-scale traces using the simulator. It is observed that  Kraken  spawns more containers ( 7%) than  Oracle , on average. This is due to  Kraken ’s load/path probability miscalculations and the usage of  Commonality and  Connectivity  to cope with this.",
      "type": "sliding_window",
      "tokens": 89
    },
    {
      "text": "This is due to  Kraken ’s load/path probability miscalculations and the usage of  Commonality and  Connectivity  to cope with this. It is seen that  Kraken spawns 10% more containers for  Media Service  and 6% more for  Hotel Reservation  and  Social Network . This may be due to Media Service  having higher path unpredictability than  Hotel Reservation  (Table 2) as well as lower slack per function than Social Network  (Figure 7).",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "This may be due to Media Service  having higher path unpredictability than  Hotel Reservation  (Table 2) as well as lower slack per function than Social Network  (Figure 7). From Figure 16b, it is observed that  Oracle , being clairvoyant, spawns containers in accor- dance with the peaks and valleys of the request arrival trace. Kraken , while spawning more containers, also is seen to lag behind the trend of the trace due to load prediction errors.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "Kraken , while spawning more containers, also is seen to lag behind the trend of the trace due to load prediction errors. Performance under Sparse Load:  Analysis of logs col- lected from the Azure cloud platform [ 42 ] shows request volumes that are much lighter (average of 2 requests/hour) than those of the traces we have considered. Moreover, more than 40% of requests show significant variability in inter- arrival times.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "Moreover, more than 40% of requests show significant variability in inter- arrival times. To deal with such traces, we modified  Kraken ’s load prediction model to predict future request arrival times, owing to the sparse nature of the trace. We also spawn con- tainers much more in advance than the predicted arrival time and also keep them alive for at least a minute before evicting them from memory, to account for arrival unpredictability.",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "We also spawn con- tainers much more in advance than the predicted arrival time and also keep them alive for at least a minute before evicting them from memory, to account for arrival unpredictability. It is seen that  Kraken  meets the SLOs for all requests from the lightly-loaded trace over 18 hours while averaging 0.85 memory-resident containers at any given second 3 . Other \n3 These results are not shown in any graph.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "Other \n3 These results are not shown in any graph. 164 \nKraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC ’21, November 1–4, 2021, Seattle, WA, USA \n98.80% \n99.20% \n99.60% \n100.00% \n0 \n10000 \n20000 \n30000 \nArch Fifer DProb Kraken SProb Xanadu \nPercentage \n# Containers \n# Containers SLO Guarantees \n(a) Social Network. 98.50% \n99.00% \n99.50% \n100.00% \n0 \n10000 \n20000 \n30000 \nArch Fifer DProb Kraken SProb Xanadu \nPercentage \n# Containers \n# Containers SLO Guarantees \n(b) Media Service.",
      "type": "sliding_window",
      "tokens": 167
    },
    {
      "text": "98.50% \n99.00% \n99.50% \n100.00% \n0 \n10000 \n20000 \n30000 \nArch Fifer DProb Kraken SProb Xanadu \nPercentage \n# Containers \n# Containers SLO Guarantees \n(b) Media Service. 99.00% \n99.25% \n99.50% \n99.75% \n100.00% \n0 \n3000 \n6000 \n9000 \n12000 \nArch Fifer DProb Kraken SProb Xanadu \nPercentage \n# Containers \n# Containers SLO Guarantees \n(c) Hotel Reservation. Figure 14: Simulator: Comparison of Total Number of Containers spawned VS SLOs satisfied by each policy.",
      "type": "sliding_window",
      "tokens": 144
    },
    {
      "text": "Figure 14: Simulator: Comparison of Total Number of Containers spawned VS SLOs satisfied by each policy. The Primary Y-Axis denotes the number of containers spawned, The secondary Y-axis indicates the percentage of SLOs met and the X-axis represents each policy. 0 \n2000 \n4000 \n6000 \nOracle Kraken \n# Containers \nNGINX Search Make_Post Text Media User_Tag URL_Shortener Compose_Post Post_Storage Read_Timeline Follow \n(a) Social Network.",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "0 \n2000 \n4000 \n6000 \nOracle Kraken \n# Containers \nNGINX Search Make_Post Text Media User_Tag URL_Shortener Compose_Post Post_Storage Read_Timeline Follow \n(a) Social Network. 0 \n4000 \n8000 \n12000 \nOracle Kraken \n# Containers \nNGINX ID Movie_ID Text User_Service Rating Compose_Review Movie_Review User_Review Review_Storage \n(b) Media Service. 0 \n2000 \n4000 \n6000 \nOracle Kraken \n# Containers \nNGINX Check_Reservation Get_Profiles Search Make_Reservation \n(c) Hotel Reserva- tion.",
      "type": "sliding_window",
      "tokens": 147
    },
    {
      "text": "0 \n2000 \n4000 \n6000 \nOracle Kraken \n# Containers \nNGINX Check_Reservation Get_Profiles Search Make_Reservation \n(c) Hotel Reserva- tion. Figure 15: Simulator: Comparison of Function-wise Breakdown of Containers spawned by  Kraken  and  Oracle . 0 \n150 \n300 \n450 \n600 \nOracle Kraken Oracle Kraken Oracle Kraken \nSocial Network Media Service Hotel Reservation \nResponse Time (ms) \nQueueing Cold Start Execution Time \n(a) E2E Response Time Break- down.",
      "type": "sliding_window",
      "tokens": 122
    },
    {
      "text": "0 \n150 \n300 \n450 \n600 \nOracle Kraken Oracle Kraken Oracle Kraken \nSocial Network Media Service Hotel Reservation \nResponse Time (ms) \nQueueing Cold Start Execution Time \n(a) E2E Response Time Break- down. 275 \n290 \n305 \n320 \n600 \n700 \n800 \n1 10 19 28 37 46 55 \nRequests/second \n# Containers \nSampling interval (minutes) \nOracle Kraken Trace \n(b) Containers spawned over time. Figure 16: Simulator: Comparison of End-to-End (E2E) Response Times and Containers Spawned Over Time (60 minutes) of  Kraken and  Oracle .",
      "type": "sliding_window",
      "tokens": 144
    },
    {
      "text": "Figure 16: Simulator: Comparison of End-to-End (E2E) Response Times and Containers Spawned Over Time (60 minutes) of  Kraken and  Oracle . Trace Arch Fifer Kraken Xanadu Comm Only Conn Only Wiki (99.91%, 2737) (99.90%, 2092) (99.86%, 1396) (99.66%, 1737) (99.78%, ) (99.75%, ) Twitter (99.72%, 45,107) (99.63%, 34,210) (99.50%, 22,377) (99.10%, 25,132) (99.22%, ) (99.15%, ) Table 7: Simulator: Comparing (% SLO met,# Containers Spawned) against Existing Policies after Varying the Target SLOs. existing policies such as  Arch  and  Fifer  exhibit similar perfor- mance and resource usage when their prediction models and keep-alive times are similarly adjusted.",
      "type": "sliding_window",
      "tokens": 239
    },
    {
      "text": "existing policies such as  Arch  and  Fifer  exhibit similar perfor- mance and resource usage when their prediction models and keep-alive times are similarly adjusted. Xanadu , on the other hand, while having 0.74 memory-resident containers per sec- ond, suffers from 55% SLO Violations on average across all applications as a result of MLP mispredictions whose effects are exacerbated in this scenario, due to low request volume. Varying SLO:  Table 7 shows the SLO guarantees and num- ber of containers spawned for existing policies as well as Comm Only  and  Conn Only , when the SLO is reduced from 1000ms to a value 30% higher than the response time of the slowest workflow in each application.",
      "type": "sliding_window",
      "tokens": 172
    },
    {
      "text": "Varying SLO:  Table 7 shows the SLO guarantees and num- ber of containers spawned for existing policies as well as Comm Only  and  Conn Only , when the SLO is reduced from 1000ms to a value 30% higher than the response time of the slowest workflow in each application. The resultant SLOs are 500ms, 910ms and 809ms for  Social Network ,  Media Ser- vice  and  Hotel Reservation  respectively. Reducing the SLO, in turn, can potentially reduce the batch sizes of functions as well.",
      "type": "sliding_window",
      "tokens": 124
    },
    {
      "text": "Reducing the SLO, in turn, can potentially reduce the batch sizes of functions as well. Moreover, the reduced SLO target results in increased SLO violations across all policies. However,  Kraken  is able to \nmaintain at least 99.5% SLO guarantee and spawns 50%, 34% and 15% less containers compared to  Arch ,  Fifer  and  Xanadu , respectively.",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "However,  Kraken  is able to \nmaintain at least 99.5% SLO guarantee and spawns 50%, 34% and 15% less containers compared to  Arch ,  Fifer  and  Xanadu , respectively. It can be seen that the difference in SLO compli- ance between  Kraken ,  Comm Only , and  Conn Only  increases due to the reduced target SLO. This difference, in terms of percent of SLO violations, changes from being at most 0.1% to being between 0.1 to 0.35%.",
      "type": "sliding_window",
      "tokens": 116
    },
    {
      "text": "This difference, in terms of percent of SLO violations, changes from being at most 0.1% to being between 0.1 to 0.35%. This is a result of  Kraken  being more resilient at the tail of the response time distribution as it uses both  Commonality  and  Connectivity  while spawning containers. In comparison,  Comm Only  and  Conn Only  fail to spawn enough containers for each important function as they do not consider both these parameters, resulting in increased tail latency and exacerbates the SLO violations.",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "In comparison,  Comm Only  and  Conn Only  fail to spawn enough containers for each important function as they do not consider both these parameters, resulting in increased tail latency and exacerbates the SLO violations. 7 Concluding Remarks Adopting serverless functions for executing microservice- based applications introduces critical inefficiencies in terms of scheduling and resource management for the cloud provider, especially when deploying Dynamic DAG Applications. To- wards addressing these challenges, we design and evalu- ate  Kraken , a DAG workflow-aware resource management framework, for efficiently running such applications by uti- lizing minimum resources, while remaining SLO-compliant.",
      "type": "sliding_window",
      "tokens": 156
    },
    {
      "text": "To- wards addressing these challenges, we design and evalu- ate  Kraken , a DAG workflow-aware resource management framework, for efficiently running such applications by uti- lizing minimum resources, while remaining SLO-compliant. Kraken  employs proactive weighted scaling of functions, where the weights are calculated using function invocation probabilities and other parameters pertaining to the appli- cation’s DAG structure. Our experimental evaluation on a 160-core cluster using  Deathstarbench  workload suite and real-world traces demonstrate that  Kraken  spawns up to 76% fewer containers, thereby improving container utilization and cluster-wide energy savings by up to 4 ×  and 48%, respec- tively, compared to state-of-the art schedulers employed in serverless platforms.",
      "type": "sliding_window",
      "tokens": 189
    },
    {
      "text": "Our experimental evaluation on a 160-core cluster using  Deathstarbench  workload suite and real-world traces demonstrate that  Kraken  spawns up to 76% fewer containers, thereby improving container utilization and cluster-wide energy savings by up to 4 ×  and 48%, respec- tively, compared to state-of-the art schedulers employed in serverless platforms. 8 Acknowledgement We are indebted to the anonymous reviewers for their in- sightful comments. This research was partially supported by NSF grants #1931531, #1955815, #1763681, #2116962, #2122155 and #2028929.",
      "type": "sliding_window",
      "tokens": 146
    },
    {
      "text": "This research was partially supported by NSF grants #1931531, #1955815, #1763681, #2116962, #2122155 and #2028929. We also thank the NSF Chameleon Cloud project CH-819640 for their generous compute grant. All product names used here are for identification purposes only and may be trademarks of their respective companies.",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "All product names used here are for identification purposes only and may be trademarks of their respective companies. 165 \nSoCC ’21, November 1–4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al. References \n[1]  [n.d.].",
      "type": "sliding_window",
      "tokens": 73
    },
    {
      "text": "References \n[1]  [n.d.]. Twitter Stream traces. https://archive.org/details/twitterstream.",
      "type": "sliding_window",
      "tokens": 34
    },
    {
      "text": "https://archive.org/details/twitterstream. Accessed: 2020-05-07. [2]  2019.",
      "type": "sliding_window",
      "tokens": 29
    },
    {
      "text": "[2]  2019. Airbnb AWS Case Study. https://aws.amazon.com/solutions/ case-studies/airbnb/.",
      "type": "sliding_window",
      "tokens": 37
    },
    {
      "text": "https://aws.amazon.com/solutions/ case-studies/airbnb/. [3]  2019. Provisioned Concurrency.",
      "type": "sliding_window",
      "tokens": 39
    },
    {
      "text": "Provisioned Concurrency. https://docs.aws.amazon.com/ lambda/latest/dg/configuration-concurrency.html. [4]  2020.",
      "type": "sliding_window",
      "tokens": 49
    },
    {
      "text": "[4]  2020. Amazon States Language. https://docs.aws.amazon.com/step- functions/latest/dg/concepts-amazon-states-language.html.",
      "type": "sliding_window",
      "tokens": 50
    },
    {
      "text": "https://docs.aws.amazon.com/step- functions/latest/dg/concepts-amazon-states-language.html. [5]  2020. AWS Lambda.",
      "type": "sliding_window",
      "tokens": 52
    },
    {
      "text": "AWS Lambda. Serverless Functions. https://aws.amazon.com/ lambda/.",
      "type": "sliding_window",
      "tokens": 29
    },
    {
      "text": "https://aws.amazon.com/ lambda/. [6]  2020. Azure Durable Functions.",
      "type": "sliding_window",
      "tokens": 29
    },
    {
      "text": "Azure Durable Functions. https://docs.microsoft.com/en- us/azure/azure-functions/durable. [7] 2020. hey HTTP Load Testing Tool.",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "[7] 2020. hey HTTP Load Testing Tool. https://github.com/rakyll/hey. [8]  2020.",
      "type": "sliding_window",
      "tokens": 33
    },
    {
      "text": "[8]  2020. IBM-Composer. https://cloud.ibm.com/docs/openwhisk?topic= cloud-functions-pkg_composer.",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "https://cloud.ibm.com/docs/openwhisk?topic= cloud-functions-pkg_composer. [9] 2020. Kubernetes.",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "[10]  2020. Microsoft Azure Serverless Functions. https://azure.microsoft.",
      "type": "sliding_window",
      "tokens": 21
    },
    {
      "text": "https://azure.microsoft. com/en-us/services/functions/. [11] 2020.",
      "type": "sliding_window",
      "tokens": 28
    },
    {
      "text": "[13]  2021. AWS Lambda Cold Starts. https://mikhail.io/serverless/ coldstarts/aws/.",
      "type": "sliding_window",
      "tokens": 37
    },
    {
      "text": "https://mikhail.io/serverless/ coldstarts/aws/. [14]  2021. Azure Functions Cold Starts.",
      "type": "sliding_window",
      "tokens": 35
    },
    {
      "text": "Azure Functions Cold Starts. https://mikhail.io/serverless/ coldstarts/azure/. [15]  2021.",
      "type": "sliding_window",
      "tokens": 35
    },
    {
      "text": "[15]  2021. Expedia Case Study - Amazon AWS. https://mikhail.io/ serverless/coldstarts/azure/.",
      "type": "sliding_window",
      "tokens": 39
    },
    {
      "text": "https://mikhail.io/ serverless/coldstarts/azure/. [16]  Feb 24, 2020. Intel Power Gadget.",
      "type": "sliding_window",
      "tokens": 36
    },
    {
      "text": "Intel Power Gadget. https://github.com/sosy-lab/cpu- energy-meter. [17]  February 2018.",
      "type": "sliding_window",
      "tokens": 33
    },
    {
      "text": "[17]  February 2018. Google Cloud Functions. https://cloud.google.com/ functions/docs/.",
      "type": "sliding_window",
      "tokens": 27
    },
    {
      "text": "https://cloud.google.com/ functions/docs/. [18]  Istemi Ekin Akkus et al . 2018.",
      "type": "sliding_window",
      "tokens": 36
    },
    {
      "text": "2018. SAND: Towards High-Performance Serverless Computing. In  ATC .",
      "type": "sliding_window",
      "tokens": 22
    },
    {
      "text": "In  ATC . [19]  Mamoun Awad, Latifur Khan, and Bhavani Thuraisingham. 2008.",
      "type": "sliding_window",
      "tokens": 33
    },
    {
      "text": "2008. Pre- dicting WWW surfing using multiple evidence combination. The VLDB Journal  17, 3 (2008), 401–417.",
      "type": "sliding_window",
      "tokens": 30
    },
    {
      "text": "The VLDB Journal  17, 3 (2008), 401–417. [20]  M. A. Awad and I. Khalil. 2012.",
      "type": "sliding_window",
      "tokens": 35
    },
    {
      "text": "2012. Prediction of User’s Web-Browsing Behavior: Application of Markov Model. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)  42, 4 (2012), 1131–1142.",
      "type": "sliding_window",
      "tokens": 56
    },
    {
      "text": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)  42, 4 (2012), 1131–1142. https://doi.org/10.1109/TSMCB.2012.2187441 [21]  Ron Begleiter, Ran El-Yaniv, and Golan Yona. 2004.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "2004. On Prediction Using Variable Order Markov Models. Journal of Artificial Intelligence Research  22 (2004), 385–421.",
      "type": "sliding_window",
      "tokens": 31
    },
    {
      "text": "Journal of Artificial Intelligence Research  22 (2004), 385–421. [22]  Marc Brooker, Andreea Florescu, Diana-Maria Popa, Rolf Neugebauer, Alexandru Agache, Alexandra Iordache, Anthony Liguori, and Phil Piwonka. 2020.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "2020. Firecracker: Lightweight Virtualization for Serverless Applications. In  NSDI .",
      "type": "sliding_window",
      "tokens": 22
    },
    {
      "text": "In  NSDI . [23]  Jyothi Prasad Buddha and Reshma Beesetty. 2019.",
      "type": "sliding_window",
      "tokens": 29
    },
    {
      "text": "2019. Step Functions. In The Definitive Guide to AWS Application Integration .",
      "type": "sliding_window",
      "tokens": 19
    },
    {
      "text": "In The Definitive Guide to AWS Application Integration . Springer. [24]  James Cadden, Thomas Unger, Yara Awad, Han Dong, Orran Krieger, and Jonathan Appavoo.",
      "type": "sliding_window",
      "tokens": 52
    },
    {
      "text": "[24]  James Cadden, Thomas Unger, Yara Awad, Han Dong, Orran Krieger, and Jonathan Appavoo. 2020. SEUSS: skip redundant paths to make serverless fast.",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": "SEUSS: skip redundant paths to make serverless fast. In  Proceedings of the Fifteenth European Conference on Computer Systems . 1–15.",
      "type": "sliding_window",
      "tokens": 33
    },
    {
      "text": "1–15. [25]  Joao Carreira, Pedro Fonseca, Alexey Tumanov, Andrew Zhang, and Randy Katz. 2019.",
      "type": "sliding_window",
      "tokens": 36
    },
    {
      "text": "2019. Cirrus: A Serverless Framework for End-to-End ML Workflows. In  Proceedings of the ACM Symposium on Cloud Computing (Santa Cruz, CA, USA)  (SoCC ’19) .",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": "In  Proceedings of the ACM Symposium on Cloud Computing (Santa Cruz, CA, USA)  (SoCC ’19) . Association for Computing Ma- chinery, New York, NY, USA, 13–24. https://doi.org/10.1145/3357223.",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "https://doi.org/10.1145/3357223. 3362711 [26]  Benjamin Carver, Jingyuan Zhang, Ao Wang, and Yue Cheng. 2019.",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "2019. In search of a fast and efficient serverless dag engine. In  2019 IEEE/ACM Fourth International Parallel Data Systems Workshop (PDSW) .",
      "type": "sliding_window",
      "tokens": 34
    },
    {
      "text": "In  2019 IEEE/ACM Fourth International Parallel Data Systems Workshop (PDSW) . IEEE, \n1–10. [27]  Nilanjan Daw, Umesh Bellur, and Purushottam Kulkarni.",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": "[27]  Nilanjan Daw, Umesh Bellur, and Purushottam Kulkarni. 2020. Xanadu: Mitigating cascading cold starts in serverless function chain deploy- ments.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "Xanadu: Mitigating cascading cold starts in serverless function chain deploy- ments. In  Proceedings of the 21st International Middleware Conference . 356–370.",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "2017. Markov chains: From Theory to Implementation and Experimentation . John Wiley & Sons.",
      "type": "sliding_window",
      "tokens": 26
    },
    {
      "text": "John Wiley & Sons. [29]  Yu Gan, Yanqi Zhang, Dailun Cheng, Ankitha Shetty, Priyal Rathi, Nayan Katarki, Ariana Bruno, Justin Hu, Brian Ritchken, Brendon Jackson, et al . 2019.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "2019. An open-source benchmark suite for microservices and their hardware-software implications for cloud & edge systems. In  Proceedings of the Twenty-Fourth International Conference on Archi- tectural Support for Programming Languages and Operating Systems .",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "In  Proceedings of the Twenty-Fourth International Conference on Archi- tectural Support for Programming Languages and Operating Systems . 3–18. [30]  Arpan Gujarati, Sameh Elnikety, Yuxiong He, Kathryn S. McKinley, and Björn B. Brandenburg.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "[30]  Arpan Gujarati, Sameh Elnikety, Yuxiong He, Kathryn S. McKinley, and Björn B. Brandenburg. 2017. Swayam: Distributed Autoscaling to Meet SLAs of Machine Learning Inference Services with Resource Efficiency.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "Swayam: Distributed Autoscaling to Meet SLAs of Machine Learning Inference Services with Resource Efficiency. In  USENIX Middleware Conference . [31]  Jashwant Raj Gunasekaran, Prashanth Thinakaran, Mahmut Tay- lan Kandemir, Bhuvan Urgaonkar, George Kesidis, and Chita Das.",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "[31]  Jashwant Raj Gunasekaran, Prashanth Thinakaran, Mahmut Tay- lan Kandemir, Bhuvan Urgaonkar, George Kesidis, and Chita Das. 2019. Spock: Exploiting Serverless Functions for SLO and Cost Aware Resource Procurement in Public Cloud.",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "Spock: Exploiting Serverless Functions for SLO and Cost Aware Resource Procurement in Public Cloud. In  2019 IEEE 12th Interna- tional Conference on Cloud Computing (CLOUD) . 199–208.",
      "type": "sliding_window",
      "tokens": 54
    },
    {
      "text": "199–208. https: //doi.org/10.1109/CLOUD.2019.00043 [32]  Jashwant Raj Gunasekaran, Prashanth Thinakaran, Nachiappan C Nachiappan, Mahmut Taylan Kandemir, and Chita R Das. 2020.",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "2020. Fifer: Tackling Resource Underutilization in the Serverless Era. In  Proceedings of the 21st International Middleware Conference .",
      "type": "sliding_window",
      "tokens": 33
    },
    {
      "text": "In  Proceedings of the 21st International Middleware Conference . 280–295. [33]  Eric Jonas, Johann Schleier-Smith, Vikram Sreekanti, Chia-Che Tsai, Anurag Khandelwal, Qifan Pu, Vaishaal Shankar, Joao Carreira, Karl Krauth, Neeraja Yadwadkar, et al .",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "[33]  Eric Jonas, Johann Schleier-Smith, Vikram Sreekanti, Chia-Che Tsai, Anurag Khandelwal, Qifan Pu, Vaishaal Shankar, Joao Carreira, Karl Krauth, Neeraja Yadwadkar, et al . 2019. Cloud programming sim- plified: A berkeley view on serverless computing.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "Cloud programming sim- plified: A berkeley view on serverless computing. arXiv preprint arXiv:1902.03383  (2019). [34]  Ram Srivatsa Kannan, Lavanya Subramanian, Ashwin Raju, Jeongseob Ahn, Jason Mars, and Lingjia Tang.",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "[34]  Ram Srivatsa Kannan, Lavanya Subramanian, Ashwin Raju, Jeongseob Ahn, Jason Mars, and Lingjia Tang. 2019. GrandSLAm: Guaranteeing SLAs for Jobs in Microservices Execution Frameworks.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "GrandSLAm: Guaranteeing SLAs for Jobs in Microservices Execution Frameworks. In  EuroSys . [35]  Kate Keahey, Jason Anderson, Zhuo Zhen, Pierre Riteau, Paul Ruth, Dan Stanzione, Mert Cevik, Jacob Colleran, Haryadi S. Gunawi, Cody Hammock, Joe Mambretti, Alexander Barnes, François Halbach, Alex Rocha, and Joe Stubbs.",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "[35]  Kate Keahey, Jason Anderson, Zhuo Zhen, Pierre Riteau, Paul Ruth, Dan Stanzione, Mert Cevik, Jacob Colleran, Haryadi S. Gunawi, Cody Hammock, Joe Mambretti, Alexander Barnes, François Halbach, Alex Rocha, and Joe Stubbs. 2020. Lessons Learned from the Chameleon Testbed.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "Lessons Learned from the Chameleon Testbed. In  Proceedings of the 2020 USENIX Annual Technical Conference (USENIX ATC ’20) . USENIX Association.",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "USENIX Association. [36]  Bernhard Korte and Jens Vygen. 2018.",
      "type": "sliding_window",
      "tokens": 23
    },
    {
      "text": "2018. Bin-Packing. In  Combinatorial Optimization .",
      "type": "sliding_window",
      "tokens": 14
    },
    {
      "text": "In  Combinatorial Optimization . Springer, 489–507. [37]  Jörn Kuhlenkamp, Sebastian Werner, and Stefan Tai.",
      "type": "sliding_window",
      "tokens": 35
    },
    {
      "text": "[37]  Jörn Kuhlenkamp, Sebastian Werner, and Stefan Tai. 2020. The ifs and buts of less is more: a serverless computing reality check.",
      "type": "sliding_window",
      "tokens": 42
    },
    {
      "text": "The ifs and buts of less is more: a serverless computing reality check. In  2020 IEEE International Conference on Cloud Engineering (IC2E) . IEEE, 154–161.",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "IEEE, 154–161. [38]  Anup Mohan, Harshad Sane, Kshitij Doshi, Saikrishna Edupuganti, Naren Nayak, and Vadim Sukhomlinov. 2019.",
      "type": "sliding_window",
      "tokens": 60
    },
    {
      "text": "2019. Agile cold starts for scalable serverless. In  11th  { USENIX }  Workshop on Hot Topics in Cloud Computing (HotCloud 19) .",
      "type": "sliding_window",
      "tokens": 38
    },
    {
      "text": "In  11th  { USENIX }  Workshop on Hot Topics in Cloud Computing (HotCloud 19) . [39]  Edward Oakes, Leon Yang, Dennis Zhou, Kevin Houck, Tyler Harter, Andrea Arpaci-Dusseau, and Remzi Arpaci-Dusseau. 2018.",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "2018. SOCK: Rapid Task Provisioning with Serverless-Optimized Containers. In USENIX ATC .",
      "type": "sliding_window",
      "tokens": 28
    },
    {
      "text": "In USENIX ATC . [40]  Haoran Qiu, Subho S Banerjee, Saurabh Jha, Zbigniew T Kalbarczyk, and Ravishankar K Iyer. 2020.",
      "type": "sliding_window",
      "tokens": 58
    },
    {
      "text": "2020. { FIRM } : An Intelligent Fine-grained Resource Management Framework for SLO-Oriented Microservices. In  14th  { USENIX }  Symposium on Operating Systems Design and Imple- mentation ( { OSDI }  20) .",
      "type": "sliding_window",
      "tokens": 67
    },
    {
      "text": "In  14th  { USENIX }  Symposium on Operating Systems Design and Imple- mentation ( { OSDI }  20) . 805–825. 166 \nKraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC ’21, November 1–4, 2021, Seattle, WA, USA \n[41]  Mohammad Shahrad, Jonathan Balkind, and David Wentzlaff.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "166 \nKraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC ’21, November 1–4, 2021, Seattle, WA, USA \n[41]  Mohammad Shahrad, Jonathan Balkind, and David Wentzlaff. 2019. Architectural implications of function-as-a-service computing.",
      "type": "sliding_window",
      "tokens": 79
    },
    {
      "text": "Architectural implications of function-as-a-service computing. In  Pro- ceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture . 1063–1075.",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "1063–1075. [42]  Mohammad Shahrad, Rodrigo Fonseca, Íñigo Goiri, Gohar Chaudhry, Paul Batum, Jason Cooke, Eduardo Laureano, Colby Tresness, Mark Russinovich, and Ricardo Bianchini. 2020.",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "2020. In  2020  { USENIX }  Annual Technical Conference ( { USENIX }{ ATC }  20) . 205–218.",
      "type": "sliding_window",
      "tokens": 39
    },
    {
      "text": "205–218. [43]  Paulo Silva, Daniel Fireman, and Thiago Emmanuel Pereira. 2020.",
      "type": "sliding_window",
      "tokens": 29
    },
    {
      "text": "2020. Prebaking Functions to Warm the Serverless Cold Start. In  Proceedings of the 21st International Middleware Conference .",
      "type": "sliding_window",
      "tokens": 29
    },
    {
      "text": "In  Proceedings of the 21st International Middleware Conference . 1–13. [44]  Arjun Singhvi, Kevin Houck, Arjun Balasubramanian, Mo- hammed Danish Shaikh, Shivaram Venkataraman, and Aditya Akella.",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "[44]  Arjun Singhvi, Kevin Houck, Arjun Balasubramanian, Mo- hammed Danish Shaikh, Shivaram Venkataraman, and Aditya Akella. 2019. Archipelago: A scalable low-latency serverless platform.",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "Archipelago: A scalable low-latency serverless platform. arXiv preprint arXiv:1911.09849  (2019). [45]  Davide Taibi, Nabil El Ioini, Claus Pahl, and Jan Raphael Schmid Niederkofler.",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "[45]  Davide Taibi, Nabil El Ioini, Claus Pahl, and Jan Raphael Schmid Niederkofler. 2020. Patterns for Serverless Functions (Function-as- a-Service): A Multivocal Literature Review..",
      "type": "sliding_window",
      "tokens": 67
    },
    {
      "text": "Patterns for Serverless Functions (Function-as- a-Service): A Multivocal Literature Review.. In  CLOSER . 181–192.",
      "type": "sliding_window",
      "tokens": 42
    },
    {
      "text": "181–192. [46]  Ali Tariq, Austin Pahl, Sharat Nimmagadda, Eric Rozner, and Siddharth Lanka. 2020.",
      "type": "sliding_window",
      "tokens": 42
    },
    {
      "text": "2020. Sequoia: Enabling quality-of-service in serverless com- puting. In  Proceedings of the 11th ACM Symposium on Cloud Computing .",
      "type": "sliding_window",
      "tokens": 39
    },
    {
      "text": "In  Proceedings of the 11th ACM Symposium on Cloud Computing . 311–327. [47]  Prashanth Thinakaran, Jashwant Raj Gunasekaran, Bikash Sharma, Mahmut Taylan Kandemir, and Chita R. Das.",
      "type": "sliding_window",
      "tokens": 67
    },
    {
      "text": "[47]  Prashanth Thinakaran, Jashwant Raj Gunasekaran, Bikash Sharma, Mahmut Taylan Kandemir, and Chita R. Das. 2017. Phoenix: A \nConstraint-Aware Scheduler for Heterogeneous Datacenters.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "Phoenix: A \nConstraint-Aware Scheduler for Heterogeneous Datacenters. In  2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS) . 977–987.",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": "977–987. https://doi.org/10.1109/ICDCS.2017.262 [48]  Prashanth Thinakaran, Jashwant Raj Gunasekaran, Bikash Sharma, Mahmut Taylan Kandemir, and Chita R. Das. 2019.",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "2019. Kube-Knots: Re- source Harvesting through Dynamic Container Orchestration in GPU- based Datacenters. In  2019 IEEE International Conference on Cluster Computing (CLUSTER) .",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "In  2019 IEEE International Conference on Cluster Computing (CLUSTER) . 1–13. https://doi.org/10.1109/CLUSTER.2019.",
      "type": "sliding_window",
      "tokens": 35
    },
    {
      "text": "https://doi.org/10.1109/CLUSTER.2019. 8891040 [49]  Guido Urdaneta, Guillaume Pierre, and Maarten Van Steen. 2009.",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "2009. Wikipedia workload analysis for decentralized hosting. Computer Networks  (2009).",
      "type": "sliding_window",
      "tokens": 16
    },
    {
      "text": "Computer Networks  (2009). [50]  Liang Wang, Mengyuan Li, Yinqian Zhang, Thomas Ristenpart, and Michael Swift. 2018.",
      "type": "sliding_window",
      "tokens": 39
    },
    {
      "text": "2018. Peeking Behind the Curtains of Serverless Plat- forms. In  ATC .",
      "type": "sliding_window",
      "tokens": 22
    },
    {
      "text": "In  ATC . [51]  Hailong Yang, Quan Chen, Moeiz Riaz, Zhongzhi Luan, Lingjia Tang, and Jason Mars. 2017.",
      "type": "sliding_window",
      "tokens": 45
    },
    {
      "text": "2017. PowerChief: Intelligent power allocation for multi-stage applications to improve responsiveness on power con- strained CMP. In  Computer Architecture News .",
      "type": "sliding_window",
      "tokens": 35
    },
    {
      "text": "In  Computer Architecture News . [52]  Yiming Zhang, Jon Crowcroft, Dongsheng Li, Chengfen Zhang, Huiba Li, Yaozheng Wang, Kai Yu, Yongqiang Xiong, and Guihai Chen. 2018.",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "2018. KylinX: a dynamic library operating system for simplified and efficient cloud virtualization. In  2018 USENIX Annual Technical Conference .",
      "type": "sliding_window",
      "tokens": 32
    },
    {
      "text": "In  2018 USENIX Annual Technical Conference . 173– 186. 167",
      "type": "sliding_window",
      "tokens": 20
    },
    {
      "text": "The vast majority of these applications are user-facing, and hence, have stringent SLO requirements. Abstract The growing popularity of microservices has led to the pro- liferation of online cloud service-based applications, which are typically modelled as Directed Acyclic Graphs (DAGs) comprising of tens to hundreds of microservices. Serverless functions, hav- ing short resource provisioning times and instant scalability, are suitable candidates for developing such latency-critical applications.",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "However, existing serverless providers are un- aware of the workflow characteristics of application DAGs, leading to container over-provisioning in many cases. This is further exacerbated in the case of dynamic DAGs, where the function chain for an application is not known a pri- ori. Serverless functions, hav- ing short resource provisioning times and instant scalability, are suitable candidates for developing such latency-critical applications.",
      "type": "sliding_window_shuffled",
      "tokens": 98,
      "augmented": true
    },
    {
      "text": "Motivated by these observations, we propose  Kraken , a workflow-aware resource management framework that minimizes the number of containers provisioned for an ap- plication DAG while ensuring SLO-compliance. We design and implement  Kraken  on  OpenFaaS  and evaluate it on a multi-node  Kubernetes -managed cluster. This is further exacerbated in the case of dynamic DAGs, where the function chain for an application is not known a pri- ori.",
      "type": "sliding_window_shuffled",
      "tokens": 118,
      "augmented": true
    },
    {
      "text": "Our extensive ex- perimental evaluation using  DeathStarbench  workload suite and real-world traces demonstrates that  Kraken  spawns up to 76% fewer containers, thereby improving container uti- lization and saving cluster-wide energy by up to 4 ×  and 48%, respectively, when compared to state-of-the art schedulers employed in serverless platforms. We design and implement  Kraken  on  OpenFaaS  and evaluate it on a multi-node  Kubernetes -managed cluster. CCS Concepts •  Computer systems organization  → Cloud Comput- ing ;  Resource-Management ; Scheduling.",
      "type": "sliding_window_shuffled",
      "tokens": 148,
      "augmented": true
    },
    {
      "text": "Copyrights for components of this work owned by others than ACM must be honored. CCS Concepts •  Computer systems organization  → Cloud Comput- ing ;  Resource-Management ; Scheduling. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyrights for components of this work owned by others than ACM must be honored.",
      "type": "sliding_window_shuffled",
      "tokens": 60,
      "augmented": true
    },
    {
      "text": "To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SoCC ’21, November 1–4, 2021, Seattle, WA, USA © 2021 Association for Computing Machinery. Request permissions from permissions@acm.org.",
      "type": "sliding_window_shuffled",
      "tokens": 75,
      "augmented": true
    },
    {
      "text": "2021. SoCC ’21, November 1–4, 2021, Seattle, WA, USA © 2021 Association for Computing Machinery. ACM ISBN 978-1-4503-8638-8/21/11...$15.00 https://doi.org/10.1145/3472883.3486992 \nKeywords serverless, resource-management, scheduling, queuing \nACM Reference Format: Vivek M. Bhasi, Jashwant Raj Gunasekaran, Prashanth Thinakaran, Cyan Subhra Mishra, Mahmut Taylan Kandemir, and Chita Das.",
      "type": "sliding_window_shuffled",
      "tokens": 142,
      "augmented": true
    },
    {
      "text": "2021. Kraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms. In  ACM Symposium on Cloud Computing (SoCC ’21), November 1–4, 2021, Seattle, WA, USA.",
      "type": "sliding_window_shuffled",
      "tokens": 56,
      "augmented": true
    },
    {
      "text": "In  ACM Symposium on Cloud Computing (SoCC ’21), November 1–4, 2021, Seattle, WA, USA. https://doi.org/10.1145/3472883. ACM, New York, NY, USA, 15 pages.",
      "type": "sliding_window_shuffled",
      "tokens": 54,
      "augmented": true
    },
    {
      "text": "https://doi.org/10.1145/3472883. Many of these real-world services often comprise of tens or even hundreds of loosely-coupled microservices [ 42 ] (e.g. 3486992 \n1 Introduction Cloud applications are embracing microservices as a pre- mier application model, owing to their advantages in terms of simplified development and ease of scalability [ 29 ,  40 ].",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "Typically, these online service ap- plications are user-facing and hence, are administered under strict Service Level Objectives (SLOs) [ 47 ,  48 ] and response latency requirements. Ex- pedia [ 15 ] and Airbnb [ 2 ]). Many of these real-world services often comprise of tens or even hundreds of loosely-coupled microservices [ 42 ] (e.g.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "Typically, these online service ap- plications are user-facing and hence, are administered under strict Service Level Objectives (SLOs) [ 47 ,  48 ] and response latency requirements. Therefore, choosing the underlying resources (virtual machines or containers) from a plethora of public cloud resource offerings [ 31 ,  33 ,  37 ,  41 ,  45 ,  50 ] becomes crucial due to their characteristics (such as provisioning la- tency) that determine the response latency. Serverless com- puting (FaaS) has recently emerged as a first-class platform to deploy latency-critical user facing applications as it miti- gates resource management overheads for developers while simultaneously offering instantaneous scalability.",
      "type": "sliding_window_shuffled",
      "tokens": 173,
      "augmented": true
    },
    {
      "text": "First, due to the stateless nature of FaaS, individual mi- croservices have to be designed as functions and explicitly chained together using tools to compose the entire appli- cation, thus forming a Directed Acyclic Graph (DAG) [ 33 ]. Serverless com- puting (FaaS) has recently emerged as a first-class platform to deploy latency-critical user facing applications as it miti- gates resource management overheads for developers while simultaneously offering instantaneous scalability. However, deploying complex microservice-based applications on FaaS has unique challenges owing to its design limitations.",
      "type": "sliding_window_shuffled",
      "tokens": 147,
      "augmented": true
    },
    {
      "text": "Second, the state management between dependent functions has to be explicitly handled using a predefined state ma- chine and made available to the cloud provider [ 6 ,  23 ]. First, due to the stateless nature of FaaS, individual mi- croservices have to be designed as functions and explicitly chained together using tools to compose the entire appli- cation, thus forming a Directed Acyclic Graph (DAG) [ 33 ]. Third, the presence of conditional branches in some DAGs can lead to uncertainties in determining which functions will \n153 \nSoCC ’21, November 1–4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al.",
      "type": "sliding_window_shuffled",
      "tokens": 169,
      "augmented": true
    },
    {
      "text": "Third, the presence of conditional branches in some DAGs can lead to uncertainties in determining which functions will \n153 \nSoCC ’21, November 1–4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al. be invoked by different requests to the same application. For instance, in a train-ticket application [ 40 ], actions like make_reservation  can trigger different paths/workflows (sub- set of functions) within the application.",
      "type": "sliding_window_shuffled",
      "tokens": 116,
      "augmented": true
    },
    {
      "text": "These design chal- lenges, when combined with the scheduling and container provisioning policies of current serverless platforms, result in crucial inefficiencies with respect to application performance and provider-side resource utilization. For instance, in a train-ticket application [ 40 ], actions like make_reservation  can trigger different paths/workflows (sub- set of functions) within the application. Two such inefficien- cies are described below: •  The majority of serverless platforms [ 32 ,  44 ,  46 ,  50 ] assume that DAGs in applications are static, implying that all com- posite functions will be invoked by a single request to the application.",
      "type": "sliding_window_shuffled",
      "tokens": 151,
      "augmented": true
    },
    {
      "text": "•  Dynamic DAGs, where only a subset of functions within each DAG are invoked per request type, necessitate the ap- portioning of containers to each function. This assumption leads to the spawning of equal number of containers for all functions in proportion to the application load, resulting in container over-provisioning. Two such inefficien- cies are described below: •  The majority of serverless platforms [ 32 ,  44 ,  46 ,  50 ] assume that DAGs in applications are static, implying that all com- posite functions will be invoked by a single request to the application.",
      "type": "sliding_window_shuffled",
      "tokens": 146,
      "augmented": true
    },
    {
      "text": "•  Dynamic DAGs, where only a subset of functions within each DAG are invoked per request type, necessitate the ap- portioning of containers to each function. This results in container provisioning along a single function chain. Recent frame- works like Xanadu [ 27 ], predict the most likely functions to be used in the DAG.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "To address these challenges, we propose  Kraken , a DAG workflow-aware resource management framework specifi- cally catered to dynamic DAGs, that minimizes resource con- sumption, while remaining SLO compliant. However, not proportionately allocating containers to all functions in the application can lead to under-provisioning containers for some functions when requests deviate from the predicted path. This results in container provisioning along a single function chain.",
      "type": "sliding_window_shuffled",
      "tokens": 104,
      "augmented": true
    },
    {
      "text": "The key compo- nents of  Kraken  are (i)  Kraken  employs a Proactive Weighted Scaler (PWS) which deploys containers for functions in ad- vance by utilizing a request arrival estimation model. The number of containers to be deployed is jointly determined by the estimation model and function weights. To address these challenges, we propose  Kraken , a DAG workflow-aware resource management framework specifi- cally catered to dynamic DAGs, that minimizes resource con- sumption, while remaining SLO compliant.",
      "type": "sliding_window_shuffled",
      "tokens": 130,
      "augmented": true
    },
    {
      "text": "These weights are assigned by the PWS by taking into account function invocation probabilities and parameters pertaining to the DAG structure, namely,  Commonality  (functions common to multiple workflows) and  Connectivity  (number of descen- dant functions), (ii) In addition to the PWS,  Kraken  employs a Reactive Scaler (RS) to scale containers appropriately to re- cover from potential resource mismanagement by the PWS, (iii) Further, we batch multiple requests to each container in order to minimize resource consumption. The number of containers to be deployed is jointly determined by the estimation model and function weights. We have developed a prototype of  Kraken  using  OpenFaaS , an open source serverless framework [ 11 ], and extensively evaluated it using real-world datacenter traces on a 160 core Kubernetes  cluster.",
      "type": "sliding_window_shuffled",
      "tokens": 193,
      "augmented": true
    },
    {
      "text": "We have developed a prototype of  Kraken  using  OpenFaaS , an open source serverless framework [ 11 ], and extensively evaluated it using real-world datacenter traces on a 160 core Kubernetes  cluster. Our results show that  Kraken  spawns up to 76% fewer containers on average, thereby improving container utilization and cluster-wide energy savings by up to 4 ×  and 48%, respectively, when compared to state-of-the art serverless schedulers. Furthermore,  Kraken  guarantees SLO requirements for up to 99.97% of requests.",
      "type": "sliding_window_shuffled",
      "tokens": 128,
      "augmented": true
    },
    {
      "text": "2.1 Serverless Function Chains (DAGs) Many applications are modeled as function chains and typically administered under strict SLOs (hundreds of mil- liseconds) [ 30 ]. Furthermore,  Kraken  guarantees SLO requirements for up to 99.97% of requests. 2 Background and Motivation We start with providing an overview of serverless DAGs along with related work (Table 1) and discuss the challenges which motivate the need for  Kraken .",
      "type": "sliding_window_shuffled",
      "tokens": 101,
      "augmented": true
    },
    {
      "text": "Serverless function chains are formed by stitching together various individual serverless functions using some form of synchronization to provide the func- tionality of a full-fledged application. Function chains are supported in commercial serverless platforms such as AWS Step Functions [4, 23], IBM Cloud Functions [8], and Azure Durable functions [ 6 ]. 2.1 Serverless Function Chains (DAGs) Many applications are modeled as function chains and typically administered under strict SLOs (hundreds of mil- liseconds) [ 30 ].",
      "type": "sliding_window_shuffled",
      "tokens": 124,
      "augmented": true
    },
    {
      "text": "By characterizing production appli- cation traces from Azure, Shahrad et.al [ 42 ] have elucidated that 46% of applications have 2-10 functions. Excluding the most general (and rare) cases where applications can have loops/cycles within a function chain [ 27 ], applications can be modeled as a  Directed Acyclic Graph  (DAG) where each ver- tex/stage is a function [ 26 ] Henceforth, we will use the terms ‘function’ and ‘stage’ interchangeably. Function chains are supported in commercial serverless platforms such as AWS Step Functions [4, 23], IBM Cloud Functions [8], and Azure Durable functions [ 6 ].",
      "type": "sliding_window_shuffled",
      "tokens": 164,
      "augmented": true
    },
    {
      "text": "Excluding the most general (and rare) cases where applications can have loops/cycles within a function chain [ 27 ], applications can be modeled as a  Directed Acyclic Graph  (DAG) where each ver- tex/stage is a function [ 26 ] Henceforth, we will use the terms ‘function’ and ‘stage’ interchangeably. An application invokes functions in the sequence as specified by the path in the DAG. We define a  workflow or  path  within an application as a sequence of vertices and the edges that connect them, starting from the first vertex (or vertices) and ending at the last vertex (or vertices).",
      "type": "sliding_window_shuffled",
      "tokens": 158,
      "augmented": true
    },
    {
      "text": "Based on the nature of the workflow, function chains can be classified as Static or Dynamic. 2.1.1 Static DAGs : In static function chains (or DAGs), the workflows are specified in advance by the developer (using a schema), which is then orchestrated by the provider. An application invokes functions in the sequence as specified by the path in the DAG.",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "For example, in  Hotel Reservation (Figure 1c), if only one path (say,  NGINX - Make_Reservation ) is always chosen, it represents a static function chain. 2.1.1 Static DAGs : In static function chains (or DAGs), the workflows are specified in advance by the developer (using a schema), which is then orchestrated by the provider. This re- sults in a predetermined path being traversed in the event of an application invocation.",
      "type": "sliding_window_shuffled",
      "tokens": 116,
      "augmented": true
    },
    {
      "text": "Hence- forth, we refer to static function chains as Static DAG Ap- plications (SDAs). Clearly, having prior knowledge of what functions will be invoked for an application makes container provisioning easier for SDAs. For example, in  Hotel Reservation (Figure 1c), if only one path (say,  NGINX - Make_Reservation ) is always chosen, it represents a static function chain.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "We refer to such functions as Dynamic Branch Points (DBPs), and the chains they are a part of as Dynamic Function Chains. Clearly, having prior knowledge of what functions will be invoked for an application makes container provisioning easier for SDAs. 2.1.2 Dynamic DAGs : Although the application DAG con- sists of multiple functions that may be invoked, there are cases where the functions can themselves invoke other func- tions depending on the inputs they receive.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "We refer to such functions as Dynamic Branch Points (DBPs), and the chains they are a part of as Dynamic Function Chains. Figure 1 shows the DAGs for three Dynamic Function Chains. In such cases, deploying containers without prior knowledge about the possible paths in the workflow leads to sub-optimal con- tainer provisioning for individual functions.",
      "type": "sliding_window_shuffled",
      "tokens": 82,
      "augmented": true
    },
    {
      "text": "For instance, from \n154 \nKraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC ’21, November 1–4, 2021, Seattle, WA, USA \nFeatures \nArchipelago [44] \nPower-chief [51] \nFifer [32] \nXanadu [27] \nGrandSLAm [34] \nSequoia [46] \nHybrid Histogram [42] \nCirrus [25] \nKraken \nSLO Guarantees ✓ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Dynamic DAG Applications ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ Slack-aware batching ✗ ✗ ✓ ✗ ✓ ✗ ✗ ✗ ✓ Cold Start Spillover Prevention ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓ Function Weight Apportioning ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓ Energy Efficieny ✗ ✓ ✓ ✓ ✗ ✗ ✓ ✓ ✓ Request Arrival Prediction ✓ ✗ ✓ ✓ ✓ ✗ ✓ ✗ ✓ Satisfactory Tail Latency ✓ ✗ ✓ ✗ ✓ ✓ ✓ ✓ ✓ \nTable 1: Comparing the features of  Kraken  with other state-of-the- art resource management frameworks. Figure 1 shows the DAGs for three Dynamic Function Chains. Social Net- work  (Figure 1a), for example, is one such chain that has 11 functions in total, with each subset of functions contribut- ing to multiple paths (7 paths in total).",
      "type": "sliding_window_shuffled",
      "tokens": 377,
      "augmented": true
    },
    {
      "text": "App DBP Total Fanout Possible Paths Max Depth Social Network 2 8 7 5 Media Service 3 7 5 6 Hotel Reservation 1 2 2 4 Table 2: Analyzing Variability in Application Workflows. For instance, from \n154 \nKraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC ’21, November 1–4, 2021, Seattle, WA, USA \nFeatures \nArchipelago [44] \nPower-chief [51] \nFifer [32] \nXanadu [27] \nGrandSLAm [34] \nSequoia [46] \nHybrid Histogram [42] \nCirrus [25] \nKraken \nSLO Guarantees ✓ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Dynamic DAG Applications ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✓ Slack-aware batching ✗ ✗ ✓ ✗ ✓ ✗ ✗ ✗ ✓ Cold Start Spillover Prevention ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓ Function Weight Apportioning ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓ Energy Efficieny ✗ ✓ ✓ ✓ ✗ ✗ ✓ ✓ ✓ Request Arrival Prediction ✓ ✗ ✓ ✓ ✓ ✗ ✓ ✗ ✓ Satisfactory Tail Latency ✓ ✗ ✓ ✗ ✓ ✓ ✓ ✓ ✓ \nTable 1: Comparing the features of  Kraken  with other state-of-the- art resource management frameworks. the start function  NGINX , any one of  Search ,  Make_Post , Read_Timeline  and  Follow  can be taken.",
      "type": "sliding_window_shuffled",
      "tokens": 393,
      "augmented": true
    },
    {
      "text": "Henceforth, we refer to such Dynamic DAG Applications as DDAs. 2.2 Motivation Two specific challenges in the context of DDAs along with potential opportunities to resolve them are described below: Challenge 1: Path Prediction in DDAs. the start function  NGINX , any one of  Search ,  Make_Post , Read_Timeline  and  Follow  can be taken.",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "2.2 Motivation Two specific challenges in the context of DDAs along with potential opportunities to resolve them are described below: Challenge 1: Path Prediction in DDAs. DDAs will only have a subset of their functions invoked for an incoming request to the application due to the presence of conditional paths within their DAGs. Figure 1 depicts the DAGs of three such applications from the  𝐷𝑒𝑎𝑡ℎ𝑆𝑡𝑎𝑟 benchmark suite [ 29 ], and Table 2 summarizes the various workflows that can be triggered by an incoming request to them.",
      "type": "sliding_window_shuffled",
      "tokens": 117,
      "augmented": true
    },
    {
      "text": "Note that each func- tion triggers only one other function in the application at a time. ‘Total fan-out’ and ‘Max Depth’ denotes the total number of outgoing branches and maximum distance between the start function and any other function in a DAG, respectively. Figure 1 depicts the DAGs of three such applications from the  𝐷𝑒𝑎𝑡ℎ𝑆𝑡𝑎𝑟 benchmark suite [ 29 ], and Table 2 summarizes the various workflows that can be triggered by an incoming request to them.",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "Note that each func- tion triggers only one other function in the application at a time. Therefore, there is considerable variation in the functions that can be invoked in DDAs, thus, negating the inherent assumption in many frameworks [ 32 ,  42 ,  44 ,  50 ] that all functions will be invoked with the same frequency as the application. The decision to trigger the next function typically depends on the input to the current function, although there are cases like  Media Service  where this decision may de- pend on previous function inputs as well.",
      "type": "sliding_window_shuffled",
      "tokens": 121,
      "augmented": true
    },
    {
      "text": "Therefore, there is considerable variation in the functions that can be invoked in DDAs, thus, negating the inherent assumption in many frameworks [ 32 ,  42 ,  44 ,  50 ] that all functions will be invoked with the same frequency as the application. Opportunity 1:  In order to reduce overprovisioning of contain- ers, it is vital to design a workflow-aware resource management (RM) framework that can dynamically scale containers for each function, as opposed to uniformly scaling for all functions. This discrepancy can lead to substantial container overprovisioning.",
      "type": "sliding_window_shuffled",
      "tokens": 128,
      "augmented": true
    },
    {
      "text": "We introduce weights to estimate the appropriate number of containers to be spawned for each function. To design such a policy, the RM framework needs to know each function’s invocation frequency, which is a good estimator of its relative popularity. Opportunity 1:  In order to reduce overprovisioning of contain- ers, it is vital to design a workflow-aware resource management (RM) framework that can dynamically scale containers for each function, as opposed to uniformly scaling for all functions.",
      "type": "sliding_window_shuffled",
      "tokens": 110,
      "augmented": true
    },
    {
      "text": "The relative invocation frequency of a function is measured with respect to the application it consti- tutes. We introduce weights to estimate the appropriate number of containers to be spawned for each function. A function’s weight is calculated using the relative invocation frequency of a function along with other DAG-specific parameters  (explained in the next section).",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "The same function belonging to multiple applications can, therefore, have distinct weights in each application. To analyze the benefits of using invocation frequency, we designed a probability-based policy that employs weighted container scaling. The relative invocation frequency of a function is measured with respect to the application it consti- tutes.",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "For the purposes of this experiment, we base our function weights only on invocation frequencies that are periodically calculated at the beginning of each scaling window. Figure 2 depicts the number of containers provisioned per function for three container provisioning policies subject to a Poisson arrival trace ( 𝜇 = 25 requests per second (rps)) for three applications. To analyze the benefits of using invocation frequency, we designed a probability-based policy that employs weighted container scaling.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "Xanadu  [ 27 ] represents the policy that scales containers only along the Most Likely Path (MLP), which is the request’s expected path. Figure 2 depicts the number of containers provisioned per function for three container provisioning policies subject to a Poisson arrival trace ( 𝜇 = 25 requests per second (rps)) for three applications. The static provision- ing policy is representative of current platforms [ 50 ] which spawn containers for functions in a workflow-agnostic fash- ion.",
      "type": "sliding_window_shuffled",
      "tokens": 121,
      "augmented": true
    },
    {
      "text": "If the request takes a different path,  Xanadu provisions containers along the path actually taken, in a reactive  fashion, and scales down the containers it provi- sioned along the MLP. Consequently,  Xanadu , when subject to moderate/heavy load, over-provisions containers by 32% compared to the Probability-based policy (from Figure 2) as a result of being locked into provisioning containers for the MLP until it is able to recalculate it. Xanadu  [ 27 ] represents the policy that scales containers only along the Most Likely Path (MLP), which is the request’s expected path.",
      "type": "sliding_window_shuffled",
      "tokens": 151,
      "augmented": true
    },
    {
      "text": "Our probability-based policy, on the other hand, provisions containers for func- tions along  every possible path in proportion to their assigned weights . Note that variability in application usage patterns can lead to changes in function probabilities within each DDA, which the policy will have to account for. Consequently,  Xanadu , when subject to moderate/heavy load, over-provisions containers by 32% compared to the Probability-based policy (from Figure 2) as a result of being locked into provisioning containers for the MLP until it is able to recalculate it.",
      "type": "sliding_window_shuffled",
      "tokens": 130,
      "augmented": true
    },
    {
      "text": "While probability-based container provisioning can significantly reduce the number of containers, the presence of container cold-starts leads to SLO violations (requests not meeting their expected response latency). Note that variability in application usage patterns can lead to changes in function probabilities within each DDA, which the policy will have to account for. Challenge 2: Adaptive Container Provisioning.",
      "type": "sliding_window_shuffled",
      "tokens": 80,
      "augmented": true
    },
    {
      "text": "A significant amount of research [ 18 ,  22 ,  24 ,  38 ,  39 ,  43 ,  52 ] has been focused to- wards reducing cold-start overheads (in particular, proactive container provisioning [ 3 ,  32 ,  44 ,  46 ]). While probability-based container provisioning can significantly reduce the number of containers, the presence of container cold-starts leads to SLO violations (requests not meeting their expected response latency). This is because cold starts can take up a significant proportion of a function’s response time (up to 10s of seconds [ 13 ,  14 ]).",
      "type": "sliding_window_shuffled",
      "tokens": 140,
      "augmented": true
    },
    {
      "text": "We identify two interlinked factors, in the context of DDAs, that need to be accounted for when making container scaling decisions. A significant amount of research [ 18 ,  22 ,  24 ,  38 ,  39 ,  43 ,  52 ] has been focused to- wards reducing cold-start overheads (in particular, proactive container provisioning [ 3 ,  32 ,  44 ,  46 ]). However, in the case of DDAs, DBPs make it unclear as to how many containers should be provisioned in advance for the functions along each path in the DAG.",
      "type": "sliding_window_shuffled",
      "tokens": 132,
      "augmented": true
    },
    {
      "text": "These are functions within a DAG that have a high number of descendant functions that are linked to it and we use the \n155 \nSoCC ’21, November 1–4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al. We identify two interlinked factors, in the context of DDAs, that need to be accounted for when making container scaling decisions. The first, is what we call  critical functions .",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "These are functions within a DAG that have a high number of descendant functions that are linked to it and we use the \n155 \nSoCC ’21, November 1–4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al. SEARCH \nNGINX \nMAKE_POST \nREAD_TIMELINE \nFOLLOW \nTEXT \nMEDIA \nUSER_TAG \nURL_SHORTENER \nCOMPOSE_POST \nPOST_STORAGE \n(a) Social Network. NGINX ID \nMOVIE_ID \nTEXT_SERVICE \nUSER_SERVICE \nRATING \nCOMPOSE_REVIEW \nMOVIE_REVIEW \nUSER_REVIEW \nREVIEW_STORAGE \n(b) Media Service.",
      "type": "sliding_window_shuffled",
      "tokens": 175,
      "augmented": true
    },
    {
      "text": "Figure 1: DAGs of Dynamic Function Chains. NGINX ID \nMOVIE_ID \nTEXT_SERVICE \nUSER_SERVICE \nRATING \nCOMPOSE_REVIEW \nMOVIE_REVIEW \nUSER_REVIEW \nREVIEW_STORAGE \n(b) Media Service. NGINX \nCHECK_RESERVATION GET_PROFILES SEARCH \nMAKE_RESERVATION \n(c) Hotel Reservation.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "0 100 200 300 \nStatic Provisioning \nProbability-based \nXanadu \n# Containers NGINX Search Make_Post Text Media User_Tag URL_Shortener Compose_Post Post_Storage Read_Timeline Follow \n(a) Social Network. 0 100 200 300 \nStatic Provisioning \nProbability-based \nXanadu \n# Containers NGINX ID Movie_ID Text User_Service Rating Compose_Review Movie_Review User_Review Review_Storage \n(b) Media Service. Figure 1: DAGs of Dynamic Function Chains.",
      "type": "sliding_window_shuffled",
      "tokens": 132,
      "augmented": true
    },
    {
      "text": "Figure 2: Function-wise Breakdown of Container Provisioning across Applications. 0 50 100 150 \nStatic Provisioning \nProbability-based \nXanadu \n# Containers \nNGINX Check_Reservation Get_Profiles Search Make_Reservation \n(c) Hotel Reservation. 0 100 200 300 \nStatic Provisioning \nProbability-based \nXanadu \n# Containers NGINX ID Movie_ID Text User_Service Rating Compose_Review Movie_Review User_Review Review_Storage \n(b) Media Service.",
      "type": "sliding_window_shuffled",
      "tokens": 120,
      "augmented": true
    },
    {
      "text": "The Primary Y-axis denotes the Av- erage End-to-End Response Time, the Secondary Y-axis represents the percentage of SLOs satisfied and the X-axis indicates the Appli- cation under consideration. Figure 2: Function-wise Breakdown of Container Provisioning across Applications. 98.10% \n98.55% \n99.00% \n99.45% \n99.90% \n0 \n200 \n400 \n600 \n800 \nCritical Non-Critical Critical Non-Critical Critical Non-Critical \nSocial Network Media Service Hotel Reservation \nPercentage \nResponse Time (ms) \nEnd-to-End Response Time SLO Guarantee \nFigure 3: Performance Deterioration resulting from Container De- ficiency at Critical Functions.",
      "type": "sliding_window_shuffled",
      "tokens": 164,
      "augmented": true
    },
    {
      "text": "Inadequately provisioning containers for such functions causes requests to queue up as containers are spawned in the background. The Primary Y-axis denotes the Av- erage End-to-End Response Time, the Secondary Y-axis represents the percentage of SLOs satisfied and the X-axis indicates the Appli- cation under consideration. term  Connectivity  to denote the ratio of number of descen- dant functions to the total number of functions.",
      "type": "sliding_window_shuffled",
      "tokens": 114,
      "augmented": true
    },
    {
      "text": "Moreover, this additional request load trickles down to all the descendants, adversely affecting their response times as well. We refer to this effect as  Cold Start Spillover . Inadequately provisioning containers for such functions causes requests to queue up as containers are spawned in the background.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "We refer to this effect as  Cold Start Spillover . Fig- ure 3 compares the performance degradation resulting from underprovisioning both Critical and Non-Critical functions. The (Critical, Non-Critical) function pairs chosen for this experiment were ( Make_Post ,  Text ), ( ID ,  Rating ) and ( NGINX , Search ) for  Social Network ,  Media Service  and  Hotel Reserva- tion , respectively.",
      "type": "sliding_window_shuffled",
      "tokens": 106,
      "augmented": true
    },
    {
      "text": "It can be observed that underprovisioning containers for just one Critical function has a greater im- pact on application performance than doing so for a single Non-Critical function, with the end-to-end response time and SLO guarantees becoming 24ms and 0.25% worse on average. This effect can worsen if the same were to happen with multiple critical functions. The (Critical, Non-Critical) function pairs chosen for this experiment were ( Make_Post ,  Text ), ( ID ,  Rating ) and ( NGINX , Search ) for  Social Network ,  Media Service  and  Hotel Reserva- tion , respectively.",
      "type": "sliding_window_shuffled",
      "tokens": 148,
      "augmented": true
    },
    {
      "text": "Common func- tions refer to those which are a part of two or more paths within an application DAG. In addition to critical functions, it is also crucial to assign higher weights to common functions as well. This effect can worsen if the same were to happen with multiple critical functions.",
      "type": "sliding_window_shuffled",
      "tokens": 64,
      "augmented": true
    },
    {
      "text": "It can be seen that functions which are common to a larger number of paths are invoked at a higher rate by such a request arrival pattern. Common func- tions refer to those which are a part of two or more paths within an application DAG. Figure 4 shows the ‘hit rate’ of \nfunctions within an application that is subject to a constant load where any path in the application is equally likely to be picked.",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "Therefore, common functions have a higher chance of experiencing increased load due to be- ing present in multiple paths. It can be seen that functions which are common to a larger number of paths are invoked at a higher rate by such a request arrival pattern. Consequently, higher weights have to be assigned to such functions to ensure resilience in the presence of varying application usage patterns.",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "Consequently, higher weights have to be assigned to such functions to ensure resilience in the presence of varying application usage patterns. Hence, rather than simply measuring the weights only in terms of function invocation frequency, we also need to account for DAG specific factors like  Commonality  and  Con- nectivity . Opportunity 2:  Although proactive provisioning combined with probability-based scaling is useful, it is essential to iden- tify critical and common functions in each DDA and assign them higher weights in comparison to standard functions.",
      "type": "sliding_window_shuffled",
      "tokens": 114,
      "augmented": true
    },
    {
      "text": "The above discourse motivates us to rethink the design of serverless RM frameworks to cater to DDAs as well. Hence, rather than simply measuring the weights only in terms of function invocation frequency, we also need to account for DAG specific factors like  Commonality  and  Con- nectivity . One key driver for the design lies in a  Probability Estimation Model  for individual functions, which is explained below.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "In this section, we model the function probability estimation problem using a Variable Order Markov Model (VOMM) [ 21 ]. One key driver for the design lies in a  Probability Estimation Model  for individual functions, which is explained below. 3 Function Probability Estimation Model As elucidated in  Opportunity-1 , to specifically address the container over-provisioning problem for DDAs, we need to estimate the weights to be assigned to their composite func- tions, a key component of which is the function invocation probability.",
      "type": "sliding_window_shuffled",
      "tokens": 120,
      "augmented": true
    },
    {
      "text": "This aids us in the calculation of function invocation probabilities. In this section, we model the function probability estimation problem using a Variable Order Markov Model (VOMM) [ 21 ]. VOMMs are effective in capturing the invo- cation patterns of functions within each application while simultaneously isolating the effects of other applications that share them.",
      "type": "sliding_window_shuffled",
      "tokens": 80,
      "augmented": true
    },
    {
      "text": "Wherever appropriate, we draw in- spiration from related works that model user web surfing \n156 \nKraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC ’21, November 1–4, 2021, Seattle, WA, USA \n0 \n0.25 \n0.5 \n0.75 \n1 \nHit Rate \n(a) Social Network. This aids us in the calculation of function invocation probabilities. 0 \n0.25 \n0.5 \n0.75 \n1 \nHit Rate \n(b) Media Service.",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "0 \n0.25 \n0.5 \n0.75 \n1 \nHit Rate \n(b) Media Service. Figure 4: Function Hit Rate for an Evenly Distributed Load across all Paths in each Application. 0 \n0.25 \n0.5 \n0.75 \n1 \nHit Rate \n(c) Hotel Reservation.",
      "type": "sliding_window_shuffled",
      "tokens": 58,
      "augmented": true
    },
    {
      "text": "Figure 4: Function Hit Rate for an Evenly Distributed Load across all Paths in each Application. NGINX \nSearch \nMake_Post \nRead_Timeline \nFollow \nText \nMedia \nUser_tag \nURL_Shortener \nCompose_Post \nPost_Storage User_Tag URL Compose_Post \nFollow Text \nend \nSearch Make_Post Read_Timeline \nNGINX \nPost_Storage \n0.08 \n0.4 \n0.32 \n0.2 \n0.5 \n0.3 \n0.1 \n0.1 \n1 \n1 \n1 \n1 1 \n1 \nFigure 5: Transforming the Social Network DAG into a Transition Matrix. behavior [ 19 ,  20 ].",
      "type": "sliding_window_shuffled",
      "tokens": 133,
      "augmented": true
    },
    {
      "text": "VOMMs are an extension of Markov Mod- els [ 28 ], where the transition probability from the current state to the next state depends not only on the current state, but possibly on its predecessors (which we refer to as the ‘context’ of the state). Such behavior is seen in some of our workloads such as  𝑀𝑒𝑑𝑖𝑎𝑆𝑒𝑟𝑣𝑖𝑐𝑒 . behavior [ 19 ,  20 ].",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "The order of the VOMM denotes the number of predecessors that influence the tran- sition decision. Such behavior is seen in some of our workloads such as  𝑀𝑒𝑑𝑖𝑎𝑆𝑒𝑟𝑣𝑖𝑐𝑒 . An application DAG can map neatly onto a Markov model wherein the functions within the application DAG are mod- eled as states of the VOMM.",
      "type": "sliding_window_shuffled",
      "tokens": 80,
      "augmented": true
    },
    {
      "text": "The weight for each function corresponds to the state transition probability from the start state to the current one (note that this may require possibly transitioning through a number of intermediate states). An application DAG can map neatly onto a Markov model wherein the functions within the application DAG are mod- eled as states of the VOMM. The process of one function invoking another function corresponds to a transition from the caller function state to the callee function state.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "Thus, for a DAG with  𝑛 functions, the transition probabil- ity matrix,  𝑇 , is an  𝑛 ×  𝑛 matrix, where  𝑛 is the total number of states and each entry,  𝑡 𝑗𝑖 , is the transition probability from the state corresponding to the function along the col- umn j, ( 𝑓 𝑗 ), to that of the function along the row i, ( 𝑓 𝑖 ). An example of a Transition Matrix for the  Social Network , with 11 functions, is depicted in Figure 5. The weight for each function corresponds to the state transition probability from the start state to the current one (note that this may require possibly transitioning through a number of intermediate states).",
      "type": "sliding_window_shuffled",
      "tokens": 163,
      "augmented": true
    },
    {
      "text": "In Figure 5, as- suming both column and row indices of 𝑇 start at 0, an entry 𝑡 0 4  represents the transition probability from  NGINX ’s state to  Follow ’s state and is equal to 0.2. An additional state,  end , is added to represent the state the model transitions to after a path in the DAG is completely executed. An example of a Transition Matrix for the  Social Network , with 11 functions, is depicted in Figure 5.",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "In general, this transition probability,  𝑡 𝑗𝑖 , is calculated as the number of requests from 𝑓 𝑗 to  𝑓 𝑖 divided by the number of incoming requests to  𝑓 𝑖 in the context of the application being considered. In Figure 5, as- suming both column and row indices of 𝑇 start at 0, an entry 𝑡 0 4  represents the transition probability from  NGINX ’s state to  Follow ’s state and is equal to 0.2. The Probability Vector is an  𝑛 × 1 column vector that cap- tures the probabilities of the model being in different states after a number of time steps have elapsed, given that the model was initialized at a known state.",
      "type": "sliding_window_shuffled",
      "tokens": 159,
      "augmented": true
    },
    {
      "text": "For practical purposes, we fix it to be the execution time of the slowest function at the current function depth. A ‘time step’ refers to a unit of measuring state change in the Markov Model. The Probability Vector is an  𝑛 × 1 column vector that cap- tures the probabilities of the model being in different states after a number of time steps have elapsed, given that the model was initialized at a known state.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "The ‘depth’ of a function, in this context, is defined as the distance, in terms of the number of edges in the DAG, from the start state to the current state. For practical purposes, we fix it to be the execution time of the slowest function at the current function depth. The Probability Vector after  𝑑 number of time steps can be represented as  𝑃 𝑑 .",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "This equation infers that the Proba- bility Vector at the next time step is obtained by performing a transition operation across all possible current states. The Probability Vector after  𝑑 number of time steps can be represented as  𝑃 𝑑 . Then, the Probability Vector for the next time step,  𝑑 +  1, is given by the  transition equation ,  𝑃 𝑡 + 1  =  𝑇 ·  𝑃 𝑡 .",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "Repeatedly carrying out this transition process, starting from the initial Probability Vector, enables the estimation of probabilities of each function along all possible workflows. This equation infers that the Proba- bility Vector at the next time step is obtained by performing a transition operation across all possible current states. Iterating this process for  𝑑 time steps would yield the proba- bilities of functions at a depth of  𝑑 from the start function, given by  𝑃 𝑑 =  𝑇 𝑑 ·  𝑃 0 .",
      "type": "sliding_window_shuffled",
      "tokens": 118,
      "augmented": true
    },
    {
      "text": "Thus, we can compute the probability of any function in the DAG by varying the depth,  𝑑 , using this equation. In order to apply this to proactive container allocation decisions, we can adopt the following procedure. Iterating this process for  𝑑 time steps would yield the proba- bilities of functions at a depth of  𝑑 from the start function, given by  𝑃 𝑑 =  𝑇 𝑑 ·  𝑃 0 .",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "Assuming each request to a function within the appli- cation spawns one container for that function, the number of containers to be provisioned in advance for functions at depth  𝑑 is given by: \n𝑁𝐶 𝑑 𝑡 =  ⌈ PL  𝑡 · ( 𝑇 𝑑 ·  𝑃 0 )⌉ \nHere,  𝑁𝐶 𝑑 𝑡 is a column vector of 𝑛 elements, each correspond- ing to the number of elements required to be provisioned for functions at a depth,  𝑑 , from the start function. In order to apply this to proactive container allocation decisions, we can adopt the following procedure. The incoming load to the application at time stamp,  𝑡 , is denoted as  𝑃𝐿 𝑡 and can be predicted using a load estimation model.",
      "type": "sliding_window_shuffled",
      "tokens": 176,
      "augmented": true
    },
    {
      "text": "Assuming each request to a function within the appli- cation spawns one container for that function, the number of containers to be provisioned in advance for functions at depth  𝑑 is given by: \n𝑁𝐶 𝑑 𝑡 =  ⌈ PL  𝑡 · ( 𝑇 𝑑 ·  𝑃 0 )⌉ \nHere,  𝑁𝐶 𝑑 𝑡 is a column vector of 𝑛 elements, each correspond- ing to the number of elements required to be provisioned for functions at a depth,  𝑑 , from the start function. For example, if  𝑃𝐿 𝑡 is estimated to be 25 requests, then from Figure 5, we obtain the number of containers needed for functions at depth,  𝑑 =  1, by multiplying 25 with  𝑃 1  (which is  𝑇 1 · P 0 ). Provisioning these containers at a fixed time window in advance from  𝑡 prevents cold starts from affecting the end-user experience.",
      "type": "sliding_window_shuffled",
      "tokens": 206,
      "augmented": true
    },
    {
      "text": "For example, if  𝑃𝐿 𝑡 is estimated to be 25 requests, then from Figure 5, we obtain the number of containers needed for functions at depth,  𝑑 =  1, by multiplying 25 with  𝑃 1  (which is  𝑇 1 · P 0 ). Consequently, the total number of containers re- quired for each function in the application can be computed \n157 \nSoCC ’21, November 1–4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al. Notation Meaning T Transition Matrix P 𝑑 Probability Vector for functions at depth,  d n # functions in application or # states in model f  𝑖 ,  f 𝑗 functions along row,  i  or column,  j  in  T t  𝑗𝑖 Transition probability from  f  𝑗 𝑡𝑜 f 𝑖 W 𝑝 Probability calculation time window t Request arrival time d # time steps for which transitions are done PL 𝑡 Scalar that represents the anticipated # requests at time,  t NC 𝑑 𝑡 # containers needed for functions at depth  d , at time  t Table 3: Notations used in Equations.",
      "type": "sliding_window_shuffled",
      "tokens": 263,
      "augmented": true
    },
    {
      "text": "We can now transform our previously-assumed Markov Model into a VOMM by splitting up context-dependent states into multiple context-independent states (the number of which is dependent on the DAG structure and the order of the VOMM). by performing a summation of  𝑁𝐶 𝑑 𝑡 across all possible depths, 𝑑 , from the start function. Notation Meaning T Transition Matrix P 𝑑 Probability Vector for functions at depth,  d n # functions in application or # states in model f  𝑖 ,  f 𝑗 functions along row,  i  or column,  j  in  T t  𝑗𝑖 Transition probability from  f  𝑗 𝑡𝑜 f 𝑖 W 𝑝 Probability calculation time window t Request arrival time d # time steps for which transitions are done PL 𝑡 Scalar that represents the anticipated # requests at time,  t NC 𝑑 𝑡 # containers needed for functions at depth  d , at time  t Table 3: Notations used in Equations.",
      "type": "sliding_window_shuffled",
      "tokens": 228,
      "augmented": true
    },
    {
      "text": "For example, in Figure 5, if the transition from  Com- pose_Post  to  Post_Storage  depended on the immediate prede- cessors of  Compose_Post , the  Compose_Post  state would be context-dependent and would therefore, be split into context- independent states, namely,  𝐶𝑜𝑚𝑝𝑜𝑠𝑒 _ 𝑃𝑜𝑠𝑡 | 𝑇𝑒𝑥𝑡 ( Compose Post  given 𝑇𝑒𝑥𝑡 was already invoked), 𝐶𝑜𝑚𝑝𝑜𝑠𝑒 _ 𝑃𝑜𝑠𝑡 | 𝑀𝑒𝑑𝑖𝑎 etc. We can now transform our previously-assumed Markov Model into a VOMM by splitting up context-dependent states into multiple context-independent states (the number of which is dependent on the DAG structure and the order of the VOMM). for the previous equations to hold.",
      "type": "sliding_window_shuffled",
      "tokens": 165,
      "augmented": true
    },
    {
      "text": "This changes the to- tal number of states from  𝑛 to  𝑁 , the number of extended states, resulting in a larger Transition Matrix and Probability Vector. To calculate the required number of containers for a single function that has multiple context-independent states associated with it, we take the sum of the calculated values for all of those states. for the previous equations to hold.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "To calculate the required number of containers for a single function that has multiple context-independent states associated with it, we take the sum of the calculated values for all of those states. Users submit requests in the form of invocation triggers to applications  1  hosted on a Serverless platform. 4 Overall Design of Kraken \nKraken 1   leverages the function weight estimation model from the above section along with several other design choices as outlined in this section (Figure 6).",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "Users submit requests in the form of invocation triggers to applications  1  hosted on a Serverless platform. In  Kraken , containers are provisioned in advance by the Proactive Weighted Scaler (PWS)  2  to serve these incoming requests by avoiding cold starts. To achieve this, the PWS  2  first fetches relevant system metrics (using a monitoring tool  3  and orchestrator logs).",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": "To achieve this, the PWS  2  first fetches relevant system metrics (using a monitoring tool  3  and orchestrator logs). These metrics, in addition to a developer-provided DAG Descriptor  4  , are then used by the Weight Estimation module  2a  of PWS  2  to assign weights to functions on the basis of their invocation probabil- ities. Commonality  and  Connectivity  (parameters in  2a  ) are additional parameters used in weight estimation to account for critical and common functions.",
      "type": "sliding_window_shuffled",
      "tokens": 113,
      "augmented": true
    },
    {
      "text": "Additionally, a Load Pre- dictor module  2b  makes use of the system metrics to predict \n1 Kraken is a legendary sea monster with tentacles akin to multiple paths/chains in a Serverless DAG. Commonality  and  Connectivity  (parameters in  2a  ) are additional parameters used in weight estimation to account for critical and common functions. Containers \nRequest  \nQueue \nFunction 1 \nFunction 2 \nFunction n \n.",
      "type": "sliding_window_shuffled",
      "tokens": 98,
      "augmented": true
    },
    {
      "text": "Containers \nRequest  \nQueue \nFunction 1 \nFunction 2 \nFunction n \n. . .",
      "type": "sliding_window_shuffled",
      "tokens": 20,
      "augmented": true
    },
    {
      "text": "REPLICA TRACKER \nLOAD MONITOR \nOVERLOAD DETECTOR \nFUNCTION \nIDLER \nPROACTIVE WEIGHTED SCALER \nREACTIVE SCALER \nWEIGHT ESTIMATOR \nLOAD PREDICTOR \nDev-Provided  DAG Descriptor \nScrape Metrics \nAPPLICATIONS \nDECISION \nSCALE \n2a \n2 \n7 \n7a 7b \n1 \n3 \n2b \n4 \n5 \n3a \n3b \n6 \nPROBABILITY CONNECTIVITY COMMONALITY \nKRAKEN \nFigure 6: High-level View of Kraken Architecture incoming load and uses this in conjunction with the calcu- lated function weights to determine the number of function containers to be spawned by the underlying resource orches- trator  6  . However, only a fraction of these containers are actually spawned, as determined by the function’s batch size. .",
      "type": "sliding_window_shuffled",
      "tokens": 200,
      "augmented": true
    },
    {
      "text": "The batch size denotes the number of requests per function each container can simultaneously serve without exceed- ing the SLO. However, only a fraction of these containers are actually spawned, as determined by the function’s batch size. In order to effectively handle mis-predictions in load,  Kraken  also employs a Reactive Scaler (RS)  7  that consists of two major components.",
      "type": "sliding_window_shuffled",
      "tokens": 89,
      "augmented": true
    },
    {
      "text": "In order to effectively handle mis-predictions in load,  Kraken  also employs a Reactive Scaler (RS)  7  that consists of two major components. Subsequently, it triggers container scaling  6  by calculating the additional containers needed to mitigate the delay. First, is an Overload De- tector  7a  that keeps track of request overloading at functions by monitoring queuing delays at containers.",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": "Thus,  Kraken  makes use of PWS and RS to scale containers to meet the target SLOs while simul- taneously minimizing the number of containers by making use of function invocation probabilities, function batching, and container eviction, where appropriate. Subsequently, it triggers container scaling  6  by calculating the additional containers needed to mitigate the delay. Second, a Function Idler component  7b  evicts containers from memory  6  when an excess is detected.",
      "type": "sliding_window_shuffled",
      "tokens": 110,
      "augmented": true
    },
    {
      "text": "Thus,  Kraken  makes use of PWS and RS to scale containers to meet the target SLOs while simul- taneously minimizing the number of containers by making use of function invocation probabilities, function batching, and container eviction, where appropriate. 4.1 Proactive Weighted Scaler We describe in detail the components of PWS below. 4.1.1 Estimating function weights : Since workflows in SDAs are pre-determined, pre-deploying resources for them is straightforward in comparison to DDAs, whose workflow activation patterns are not known a priori.",
      "type": "sliding_window_shuffled",
      "tokens": 131,
      "augmented": true
    },
    {
      "text": "4.1.1 Estimating function weights : Since workflows in SDAs are pre-determined, pre-deploying resources for them is straightforward in comparison to DDAs, whose workflow activation patterns are not known a priori. To address this, we design a Weight Estimator  2a  to assign weights to all functions so as to allocate resources in propor- tion to them. For DDAs, de- ploying containers for each function in proportion to the application load will inevitably lead to resource wastage.",
      "type": "sliding_window_shuffled",
      "tokens": 114,
      "augmented": true
    },
    {
      "text": "Explained below is the working of the proce- dure  𝐸𝑠𝑡𝑖𝑚𝑎𝑡𝑒 _ 𝐶𝑜𝑛𝑡𝑎𝑖𝑛𝑒𝑟𝑠 in Algorithm 1 which is used to estimate function weights. To address this, we design a Weight Estimator  2a  to assign weights to all functions so as to allocate resources in propor- tion to them. Probability:  As alluded to in Section 2, one of the factors used in function weight estimation is its invocation probabil- ity.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "Probability:  As alluded to in Section 2, one of the factors used in function weight estimation is its invocation probabil- ity. The procedure in Section 3 describes how the transition probabilities of the states associated with functions are com- puted through repeated matrix multiplications of the Transi- tion Matrix, 𝑇 with the Probability Vector,  𝑃 . 𝐶𝑜𝑚𝑝𝑢𝑡𝑒 _ 𝑃𝑟𝑜𝑏 , \n158 \nKraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC ’21, November 1–4, 2021, Seattle, WA, USA \nin Algorithm 1, first estimates the invocation probabilities of a function’s immediate predecessors and uses it along with system log information and load measurements of the function to calculate its invocation probability.",
      "type": "sliding_window_shuffled",
      "tokens": 176,
      "augmented": true
    },
    {
      "text": "Connectivity:  In addition to function invocation probabil- ities, it is necessary to also account for the effects of cold starts on DDAs while estimating function weights. Cold start spillovers (that often occur due to container underprovision- ing), as described in Section 2, can impact the response la- tency of applications harshly. 𝐶𝑜𝑚𝑝𝑢𝑡𝑒 _ 𝑃𝑟𝑜𝑏 , \n158 \nKraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC ’21, November 1–4, 2021, Seattle, WA, USA \nin Algorithm 1, first estimates the invocation probabilities of a function’s immediate predecessors and uses it along with system log information and load measurements of the function to calculate its invocation probability.",
      "type": "sliding_window_shuffled",
      "tokens": 171,
      "augmented": true
    },
    {
      "text": "Cold start spillovers (that often occur due to container underprovision- ing), as described in Section 2, can impact the response la- tency of applications harshly. Provisioning critical functions with more containers helps throttle this at the source. To this end,  Kraken  makes use of a parameter called  Connec- tivity , while assigning function weights.",
      "type": "sliding_window_shuffled",
      "tokens": 82,
      "augmented": true
    },
    {
      "text": "The  𝐶𝑜𝑛𝑛 procedure in Algorithm 1 makes use of this formula. To this end,  Kraken  makes use of a parameter called  Connec- tivity , while assigning function weights. The  Connectivity of a function is defined as the ratio of number of its descen- dant functions to the total number of functions.",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "Bringing  Connectivity  into the weight estimation process helps  Kraken  assign a higher weight to critical func- tions, in turn, ensuring that more containers are assigned to them, resulting in improved response times for the functions themselves, as well as their descendants. The  𝐶𝑜𝑛𝑛 procedure in Algorithm 1 makes use of this formula. For ex- ample, in Figure 1c, the  Connectivity  of  𝐶ℎ𝑒𝑐𝑘 _ 𝑅𝑒𝑠𝑒𝑟𝑣𝑎𝑡𝑖𝑜𝑛 is   2 \n5   since it has two descendants and there is a total of five functions.",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "This may be due to change in user behavior manifesting itself as variable function input patterns. Commonality:  As described in Section 2, in addition to cold start spillovers, incorrect probability estimations may arise due to variability in workflow activation patterns. Bringing  Connectivity  into the weight estimation process helps  Kraken  assign a higher weight to critical func- tions, in turn, ensuring that more containers are assigned to them, resulting in improved response times for the functions themselves, as well as their descendants.",
      "type": "sliding_window_shuffled",
      "tokens": 106,
      "augmented": true
    },
    {
      "text": "To cope with this, we introduce a parameter called  Commonality , which is defined as the fraction of number of unique paths that the function can be a part of with respect to the total number of unique paths. Such errors can lead to sub- optimal container allocation to DAG stages in proportion to the wrongly-calculated function weights. This may be due to change in user behavior manifesting itself as variable function input patterns.",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "For example, in Figure 1a, the Commonality  of the function  𝐶𝑜𝑚𝑝𝑜𝑠𝑒 _ 𝑃𝑜𝑠𝑡 in the  Social Network  application is given by the fraction   4 \n7   as it is present in four out of the seven possible paths in the DAG. To cope with this, we introduce a parameter called  Commonality , which is defined as the fraction of number of unique paths that the function can be a part of with respect to the total number of unique paths. This is how the procedure  𝐶𝑜𝑚𝑚 calculates Commonality  in Algorithm 1.",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "Using Commonality  in the weight estimation process allows  Kraken to tolerate function probability miscalculations by assigning higher weights to those functions that are statistically more likely to experience rise in usage because of their presence in a larger number of workflows. For example, in Figure 1a, the Commonality  of the function  𝐶𝑜𝑚𝑝𝑜𝑠𝑒 _ 𝑃𝑜𝑠𝑡 in the  Social Network  application is given by the fraction   4 \n7   as it is present in four out of the seven possible paths in the DAG. Note that we deal with the possibility of container overprovisioning due to the in- creased function weights by allowing both  Connectivity  and Commonality  to be capped at a certain value.",
      "type": "sliding_window_shuffled",
      "tokens": 146,
      "augmented": true
    },
    {
      "text": "4.1.2 Proactive Container Provisioning : Once function weights are assigned by considering the above factors, they are employed in estimating the number of containers needed per DAG stage ( Estimate_Containers  in Algorithm 1). These containers have to be provisioned in advance to service fu- ture load to shield the end user from the effects of cold starts \nand thereby meet the SLO. Note that we deal with the possibility of container overprovisioning due to the in- creased function weights by allowing both  Connectivity  and Commonality  to be capped at a certain value.",
      "type": "sliding_window_shuffled",
      "tokens": 131,
      "augmented": true
    },
    {
      "text": "This load will have to be predicted in order to make timely container provisioning decisions. These containers have to be provisioned in advance to service fu- ture load to shield the end user from the effects of cold starts \nand thereby meet the SLO. Algorithm 1  Proactive Scaling with weight estimation \n1:  for  Every Monitor_Interval= PW  do 2: Proactive_Weighted_Scaler ( ∀ 𝑓𝑢𝑛𝑐𝑡𝑖𝑜𝑛𝑠 ) 3:  procedure  Proactive_Weighted_Scaler( func ) 4: cl  ← 𝐶𝑢𝑟𝑟𝑒𝑛𝑡 _ 𝐿𝑜𝑎𝑑 ( 𝑓𝑢𝑛𝑐 ) 5: 𝑝𝑙 𝑡 + 𝑃𝑊 ← Load_Predictor ( 𝑐𝑙, 𝑝𝑙 𝑡 )  a 6: batches  ← l p 𝑙𝑡 + 𝑃𝑊 f 𝑢𝑛𝑐.𝑏𝑎𝑡𝑐ℎ _ 𝑠𝑖𝑧𝑒 m \nb 7: total_con  ← Estimate_Containers ( 𝑏𝑎𝑡𝑐ℎ𝑒𝑠, 𝑓𝑢𝑛𝑐 ) 8: reqd_con  ← 𝑚𝑎𝑥 ( 𝑚𝑖𝑛 _ 𝑐𝑜𝑛,𝑡𝑜𝑡𝑎𝑙 _ 𝑐𝑜𝑛 ) 9: Scale_Containers ( 𝑓𝑢𝑛𝑐,𝑟𝑒𝑞𝑑 _ 𝑐𝑜𝑛 ) 10:  procedure  estimate_containers( load, func ) ⊲ Output:  𝑟𝑒𝑞𝑑 _ 𝑐𝑜𝑛 11: func.prob  ← Compute_Prob (func) 12: reqd_con  ←⌈ 𝑙𝑜𝑎𝑑 ∗ 𝑓𝑢𝑛𝑐.𝑝𝑟𝑜𝑏 ⌉ 13: extra  ←⌈( Comm ( 𝑓𝑢𝑛𝑐 ) +  Conn ( 𝑓𝑢𝑛𝑐 )) ∗ 𝑟𝑒𝑞𝑑 _ 𝑐𝑜𝑛 ⌉ 14: reqd_con  ← reqd_con + extra \nKraken  makes use of a Load Predictor  2b  (Algorithm 1  a ) which uses the EWMA model to predict the incoming load at the end of a fixed time window,  𝑃𝑊 .",
      "type": "sliding_window_shuffled",
      "tokens": 427,
      "augmented": true
    },
    {
      "text": "Algorithm 1  Proactive Scaling with weight estimation \n1:  for  Every Monitor_Interval= PW  do 2: Proactive_Weighted_Scaler ( ∀ 𝑓𝑢𝑛𝑐𝑡𝑖𝑜𝑛𝑠 ) 3:  procedure  Proactive_Weighted_Scaler( func ) 4: cl  ← 𝐶𝑢𝑟𝑟𝑒𝑛𝑡 _ 𝐿𝑜𝑎𝑑 ( 𝑓𝑢𝑛𝑐 ) 5: 𝑝𝑙 𝑡 + 𝑃𝑊 ← Load_Predictor ( 𝑐𝑙, 𝑝𝑙 𝑡 )  a 6: batches  ← l p 𝑙𝑡 + 𝑃𝑊 f 𝑢𝑛𝑐.𝑏𝑎𝑡𝑐ℎ _ 𝑠𝑖𝑧𝑒 m \nb 7: total_con  ← Estimate_Containers ( 𝑏𝑎𝑡𝑐ℎ𝑒𝑠, 𝑓𝑢𝑛𝑐 ) 8: reqd_con  ← 𝑚𝑎𝑥 ( 𝑚𝑖𝑛 _ 𝑐𝑜𝑛,𝑡𝑜𝑡𝑎𝑙 _ 𝑐𝑜𝑛 ) 9: Scale_Containers ( 𝑓𝑢𝑛𝑐,𝑟𝑒𝑞𝑑 _ 𝑐𝑜𝑛 ) 10:  procedure  estimate_containers( load, func ) ⊲ Output:  𝑟𝑒𝑞𝑑 _ 𝑐𝑜𝑛 11: func.prob  ← Compute_Prob (func) 12: reqd_con  ←⌈ 𝑙𝑜𝑎𝑑 ∗ 𝑓𝑢𝑛𝑐.𝑝𝑟𝑜𝑏 ⌉ 13: extra  ←⌈( Comm ( 𝑓𝑢𝑛𝑐 ) +  Conn ( 𝑓𝑢𝑛𝑐 )) ∗ 𝑟𝑒𝑞𝑑 _ 𝑐𝑜𝑛 ⌉ 14: reqd_con  ← reqd_con + extra \nKraken  makes use of a Load Predictor  2b  (Algorithm 1  a ) which uses the EWMA model to predict the incoming load at the end of a fixed time window,  𝑃𝑊 . This time window is chosen according to the time taken to scale all functions in the respective application. Note that  𝑡 in the algorithm refers to the current time.",
      "type": "sliding_window_shuffled",
      "tokens": 407,
      "augmented": true
    },
    {
      "text": "This Load Predictor  2b  can be used in conjunction with the afore- mentioned Weight Estimator  2a  to calculate the fraction of application load each function will receive. Note that  𝑡 in the algorithm refers to the current time. We choose this model so as to have a light-weight load prediction mechanism that has min- imal impact on the end-to-end latency ( ∼ 10 − 3   ms).",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "Kraken  uses this load distribution to pre-provision the requisite number of containers for all functions in the application. This Load Predictor  2b  can be used in conjunction with the afore- mentioned Weight Estimator  2a  to calculate the fraction of application load each function will receive. 4.2 Request Batching Many serverless frameworks [ 5 ,  10 ,  17 ,  27 ,  44 ,  46 ,  50 ] spawn a single container to serve each incoming request to a function.",
      "type": "sliding_window_shuffled",
      "tokens": 113,
      "augmented": true
    },
    {
      "text": "While this approach is beneficial to minimize SLO violations, comparable performance can be achieved by using fewer containers by leveraging the notion of slack [ 32 ,  34 ]. Slack refers to the difference in expected response time and actual execution time of functions within a function chain. 4.2 Request Batching Many serverless frameworks [ 5 ,  10 ,  17 ,  27 ,  44 ,  46 ,  50 ] spawn a single container to serve each incoming request to a function.",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "Allotting stage-wise SLOs to each function in a chain in proportion to their execution times reveals that there are cases where there is significant difference (slack) between the function’s expected SLO and its run-time. Functions in a chain can have widely varying execution times. Slack refers to the difference in expected response time and actual execution time of functions within a function chain.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "This slack is leveraged by  Kraken  by batching multiple requests to the functions by queueing requests at their con- tainers. Figure 7 depicts this slack for all functions in the applications considered. Allotting stage-wise SLOs to each function in a chain in proportion to their execution times reveals that there are cases where there is significant difference (slack) between the function’s expected SLO and its run-time.",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "Batching reduces the number of containers spawned for each function by a factor of its batch size (Algorithm 1  b  ). Requests are batched onto containers in a fashion similar to the First Fit Bin Packing algorithm [ 36 ]. This slack is leveraged by  Kraken  by batching multiple requests to the functions by queueing requests at their con- tainers.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "The batch size \nfor a function,  𝑓 , is defined as  BatchSize  ( 𝑓 )  = j StageSLO  ( 𝑓 ) ExecTime  ( 𝑓 ) k \n(Algorithm 1  b  ). Batching reduces the number of containers spawned for each function by a factor of its batch size (Algorithm 1  b  ). Note that  ExecTime (f)  is estimated by aver- aging the execution times of the function obtained through \n159 \nSoCC ’21, November 1–4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al.",
      "type": "sliding_window_shuffled",
      "tokens": 158,
      "augmented": true
    },
    {
      "text": "Note that  ExecTime (f)  is estimated by aver- aging the execution times of the function obtained through \n159 \nSoCC ’21, November 1–4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al. 0 \n200 \n400 \n600 \nTime (ms) \nExec Time(ms) Stage-wise SLO(ms) \n(a) Social Network. 0 \nTime (ms) \nExec Time (ms) Stage-wise SLO (ms) \n600 \n450 \n300 \n150 \n(b) Media Service.",
      "type": "sliding_window_shuffled",
      "tokens": 136,
      "augmented": true
    },
    {
      "text": "0 \nTime (ms) \nExec Time (ms) Stage-wise SLO (ms) \n400 \n300 \n200 \n100 \n(c) Hotel Reservation. Figure 7: Slack for various Functions in each Application. 0 \nTime (ms) \nExec Time (ms) Stage-wise SLO (ms) \n600 \n450 \n300 \n150 \n(b) Media Service.",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "Figure 7: Slack for various Functions in each Application. The batch size represents the number of requests that can be served by a function without violating the allotted stage-wise SLO. offline profiling and  StageSLO (f)  is allotted in proportion to it.",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "4.3 Reactive Scaler (RS) Though the introduction of Request Batching  5  allows Kraken  to reduce the containers provisioned, load mispredic- tions and probability miscalculations can still occur, leading to resource mismanagement, which could potentially affect the SLO compliance. The batch size represents the number of requests that can be served by a function without violating the allotted stage-wise SLO. To deal with this,  Kraken  also employs the RS  7  to scale containers up or down in response to re- quest overloading at containers (due to under-provisioning) and container over-provisioning, respectively.",
      "type": "sliding_window_shuffled",
      "tokens": 141,
      "augmented": true
    },
    {
      "text": "If it detects requests whose wait times exceed the cost of spawning a new container (the cold start of the function), overloading is said to have occurred at the stage. To deal with this,  Kraken  also employs the RS  7  to scale containers up or down in response to re- quest overloading at containers (due to under-provisioning) and container over-provisioning, respectively. In case of inadequate container provisioning, the Overload Detector  7a  in the RS  7  detects the number of allocated con- tainers for each DAG stage and calculates the estimated wait times of their queued requests (Algorithm 2  b  ).",
      "type": "sliding_window_shuffled",
      "tokens": 152,
      "augmented": true
    },
    {
      "text": "If it detects requests whose wait times exceed the cost of spawning a new container (the cold start of the function), overloading is said to have occurred at the stage. This is because requests that have to wait longer than the cold start would be served faster at a newly created container than by waiting at an overloaded container. In such a scenario,  Kraken batches these requests (# _ 𝑑𝑒𝑙𝑎𝑦𝑒𝑑 _ 𝑟𝑒𝑞𝑢𝑒𝑠𝑡𝑠 in Algorithm 2) onto a newly-spawned container(s) (Algorithm 2  c  ).",
      "type": "sliding_window_shuffled",
      "tokens": 120,
      "augmented": true
    },
    {
      "text": "This is because requests that have to wait longer than the cold start would be served faster at a newly created container than by waiting at an overloaded container. Thus, the RS  7  , in combination with the PWS  2  and re- quest batching  5  , helps  Kraken  remain SLO compliant while using minimum resources. Similarly, for stages where container overprovisioning has occurred, the RS  7  gradually scales down its allocated containers to the appropriate number, if its Function Idler module  7b  detects excess containers for serving the current load (Algorithm 2  a ).",
      "type": "sliding_window_shuffled",
      "tokens": 130,
      "augmented": true
    },
    {
      "text": "5 Implementation and Evaluation We have implemented a prototype of  Kraken  using open- source tools for evaluation with synthetic and real-world traces. Thus, the RS  7  , in combination with the PWS  2  and re- quest batching  5  , helps  Kraken  remain SLO compliant while using minimum resources. The details are described below.",
      "type": "sliding_window_shuffled",
      "tokens": 75,
      "augmented": true
    },
    {
      "text": "The details are described below. Algorithm 2  Reactive Scaling \n1:  for  Every Monitor_Interval= DR  do 2: Reactive_Resource_Manager ( ∀ 𝑓𝑢𝑛𝑐𝑡𝑖𝑜𝑛𝑠 ) 3:  procedure  Reactive_Resource_Manager( func ) 4: cl  ← 𝐶𝑢𝑟𝑟𝑒𝑛𝑡 _ 𝐿𝑜𝑎𝑑 ( 𝑓𝑢𝑛𝑐 ) 5: func.existing_con  ← 𝐶𝑢𝑟𝑟𝑒𝑛𝑡 _ 𝑅𝑒𝑝𝑙𝑖𝑐𝑎𝑠 ( 𝑓𝑢𝑛𝑐 ) 6: if l c 𝑙 f 𝑢𝑛𝑐.𝑏𝑎𝑡𝑐ℎ _ 𝑠𝑖𝑧𝑒 m ≤ func.existing_con  then  a \n7: reqd_con  ← l c 𝑙 f 𝑢𝑛𝑐.𝑏𝑎𝑡𝑐ℎ _ 𝑠𝑖𝑧𝑒 m \n8: else 9: #_delayed_requests  ← Delay_Estimator ( 𝑓𝑢𝑛𝑐 )  b 10: extra_con  ← l  #_delayed_requests \nf 𝑢𝑛𝑐.𝑏𝑎𝑡𝑐ℎ _ 𝑠𝑖𝑧𝑒 m \nc 11: reqd_con  ← func.existing_con + extra_con 12: Scale_Containers ( 𝑓𝑢𝑛𝑐,𝑟𝑒𝑞𝑑 _ 𝑐𝑜𝑛 ) \nOpenFaaS  is deployed on top of  Kubernetes  [ 9 ], which acts as the chief container orchestrator. 5.1 Prototype Implementation Kraken  is implemented primarily using Python and Go on top of  OpenFaaS  [ 11 ], an open-source serverless platform.",
      "type": "sliding_window_shuffled",
      "tokens": 330,
      "augmented": true
    },
    {
      "text": "This, in turn, triggers autoscaling to provision extra containers to service the load surge. Algorithm 2  Reactive Scaling \n1:  for  Every Monitor_Interval= DR  do 2: Reactive_Resource_Manager ( ∀ 𝑓𝑢𝑛𝑐𝑡𝑖𝑜𝑛𝑠 ) 3:  procedure  Reactive_Resource_Manager( func ) 4: cl  ← 𝐶𝑢𝑟𝑟𝑒𝑛𝑡 _ 𝐿𝑜𝑎𝑑 ( 𝑓𝑢𝑛𝑐 ) 5: func.existing_con  ← 𝐶𝑢𝑟𝑟𝑒𝑛𝑡 _ 𝑅𝑒𝑝𝑙𝑖𝑐𝑎𝑠 ( 𝑓𝑢𝑛𝑐 ) 6: if l c 𝑙 f 𝑢𝑛𝑐.𝑏𝑎𝑡𝑐ℎ _ 𝑠𝑖𝑧𝑒 m ≤ func.existing_con  then  a \n7: reqd_con  ← l c 𝑙 f 𝑢𝑛𝑐.𝑏𝑎𝑡𝑐ℎ _ 𝑠𝑖𝑧𝑒 m \n8: else 9: #_delayed_requests  ← Delay_Estimator ( 𝑓𝑢𝑛𝑐 )  b 10: extra_con  ← l  #_delayed_requests \nf 𝑢𝑛𝑐.𝑏𝑎𝑡𝑐ℎ _ 𝑠𝑖𝑧𝑒 m \nc 11: reqd_con  ← func.existing_con + extra_con 12: Scale_Containers ( 𝑓𝑢𝑛𝑐,𝑟𝑒𝑞𝑑 _ 𝑐𝑜𝑛 ) \nOpenFaaS  is deployed on top of  Kubernetes  [ 9 ], which acts as the chief container orchestrator. OpenFaaS , by default, comes packaged with an Alert Manager module which is re- sponsible for alerting the underlying orchestrator of request surges by using metrics scraped by  Prometheus , which is an open-source systems monitoring toolkit [ 12 ].",
      "type": "sliding_window_shuffled",
      "tokens": 375,
      "augmented": true
    },
    {
      "text": "Both the PWS and RS collect metrics, such as the current container count, load history and request rate for a function for a given time window, from  Prometheus  and the  Kubernetes system log, using the Replica Tracker and Load Monitor mod- ules. We disable this Alert Manager and deploy the Proactive Weighted Scaler (PWS) and Reactive Scaler (RS) to carry out our container provisioning policies. This, in turn, triggers autoscaling to provision extra containers to service the load surge.",
      "type": "sliding_window_shuffled",
      "tokens": 125,
      "augmented": true
    },
    {
      "text": "Both the PWS and RS collect metrics, such as the current container count, load history and request rate for a function for a given time window, from  Prometheus  and the  Kubernetes system log, using the Replica Tracker and Load Monitor mod- ules. The load to each function within each applica- tion is calculated separately using the collected information. Although fetching function metrics incurs a latency in the order of tens of milliseconds, it is performed in the back- ground (during autoscaling) and hence, does not affect the critical path.",
      "type": "sliding_window_shuffled",
      "tokens": 139,
      "augmented": true
    },
    {
      "text": "The load to each function within each applica- tion is calculated separately using the collected information. This prevents other applications from interfering with the probability calculation of shared functions. Additionally, the PWS uses a DAG descriptor, which is a file that contains a python dictionary that specifies the connectivity among functions.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "Additionally, the PWS uses a DAG descriptor, which is a file that contains a python dictionary that specifies the connectivity among functions. Although constructing this is a one-time effort, automating this process through offline DAG profiling can be explored in future work. Table 4 gives an overview of Kraken ’s policies and their implementation details.",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "Table 4 gives an overview of Kraken ’s policies and their implementation details. 5.2 Evaluation Methodology We evaluate the  Kraken  prototype on a 5 node  Kuber- netes  cluster with a dedicated manager node. 160 \nKraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC ’21, November 1–4, 2021, Seattle, WA, USA \nPolicy Component Implemented using/as \nPWS \nProbability System log info, Sparse Data Structures Commonality  &  Connectivity DAG Descriptor Load Predictor Pluggable model (EWMA) Batching Function containers persisted in memory \nRS Load Monitor Metrics from Prometheus & System logs Replica Tracker Table 4: Implementation details of  Kraken ’s policies.",
      "type": "sliding_window_shuffled",
      "tokens": 189,
      "augmented": true
    },
    {
      "text": "5.2 Evaluation Methodology We evaluate the  Kraken  prototype on a 5 node  Kuber- netes  cluster with a dedicated manager node. Each node is equipped with, 32 cores (Intel CascadeLake), 256GB of RAM, 1 TB of storage and a 10 Gigabit Ethernet interconnect [ 35 ]. For energy measurements, we use an open-source version of Intel Power Gadget [ 16 ] that measures the energy consumed by all sockets in a node.",
      "type": "sliding_window_shuffled",
      "tokens": 114,
      "augmented": true
    },
    {
      "text": "First, we use a synthetic Poisson-based request arrival rate with an average rate  𝜇 =  100. Load Generator:  We provide different traces as inputs to a load generator, which is based on Hey, an HTTP Load generator tool [ 7 ]. For energy measurements, we use an open-source version of Intel Power Gadget [ 16 ] that measures the energy consumed by all sockets in a node.",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "First, we use a synthetic Poisson-based request arrival rate with an average rate  𝜇 =  100. The Twitter trace has a large variation in peaks (average = 3332 rps, peak= 6978 rps) when compared to the Wiki trace (average = 284 rps, peak = 331 rps). Second, we use real-world request arrival traces from Wiki [ 49 ] and Twitter [ 1 ] by running each experiment for about an hour.",
      "type": "sliding_window_shuffled",
      "tokens": 114,
      "augmented": true
    },
    {
      "text": "We implement each ap- plication as a workflow of chained functions in  OpenFaaS . Applications:  Each request is modeled after a query to one of the three applications (DDAs) we consider from the 𝐷𝑒𝑎𝑡ℎ𝑆𝑡𝑎𝑟 benchmark suite [ 29 ]. The Twitter trace has a large variation in peaks (average = 3332 rps, peak= 6978 rps) when compared to the Wiki trace (average = 284 rps, peak = 331 rps).",
      "type": "sliding_window_shuffled",
      "tokens": 118,
      "augmented": true
    },
    {
      "text": "To model the characteristics of the original functions, we invoke sleep timers within our functions to emulate their execution times (including the time for state recovery, if any). We implement each ap- plication as a workflow of chained functions in  OpenFaaS . Transitions between functions are done using function calls on the basis of pre-assigned inter-function transition proba- bilities.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "The probabilities vary by approximately  ± 20% of a seed. Note that these probabilities are not visible to  Kraken , but are only used to model function invocation patterns. Transitions between functions are done using function calls on the basis of pre-assigned inter-function transition proba- bilities.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "Note that these probabilities are not visible to  Kraken , but are only used to model function invocation patterns. We set the SLO at 1000ms. Metrics and Resource Management Policies:  We use the following metrics for evaluation: (i) average number of containers spawned, (ii) percentage of requests satisfy- ing the SLO (SLO guarantees), (iii) average application re- sponse times, (iv) end-to-end request latency percentiles, (v) container utilization, and (vi) cluster-wide energy sav- ings.",
      "type": "sliding_window_shuffled",
      "tokens": 138,
      "augmented": true
    },
    {
      "text": "We compare these metrics for  Kraken  against the container provisioning policies of Archipelago [ 44 ],  Fifer  [ 32 ] and  Xanadu  [ 27 ], which we will, henceforth, refer to as  Arch ,  Fifer  and  Xanadu , respectively. We set the SLO at 1000ms. Additionally, we compare  Kraken  against policies with (a) statically assigned function probabilities ( SProb ) and (b) func- tion probabilities that dynamically adapt to changing invoca- tion patterns ( DProb ).",
      "type": "sliding_window_shuffled",
      "tokens": 129,
      "augmented": true
    },
    {
      "text": "These policies use all the components of  Kraken  except  Commonality  and  Connectivity . Additionally, we compare  Kraken  against policies with (a) statically assigned function probabilities ( SProb ) and (b) func- tion probabilities that dynamically adapt to changing invoca- tion patterns ( DProb ). 5.3 Large Scale Simulation To evaluate the effectiveness of  Kraken  in large-scale sys- tems, we built a high fidelity, multi-threaded simulator in Python using container cold start latencies and function execution times profiled from our real-system counterpart.",
      "type": "sliding_window_shuffled",
      "tokens": 133,
      "augmented": true
    },
    {
      "text": "It simulates the working of DDAs running on a serverless framework that are subjected to both real-world (Twitter and Wiki) and synthetic (Poisson-based) traces. We have validated its correctness by correlating various metrics of interest generated from experiments run on the real system with scaled-down versions of the same traces (average ar- rival rate of  ∼ 100rps). 5.3 Large Scale Simulation To evaluate the effectiveness of  Kraken  in large-scale sys- tems, we built a high fidelity, multi-threaded simulator in Python using container cold start latencies and function execution times profiled from our real-system counterpart.",
      "type": "sliding_window_shuffled",
      "tokens": 156,
      "augmented": true
    },
    {
      "text": "We have validated its correctness by correlating various metrics of interest generated from experiments run on the real system with scaled-down versions of the same traces (average ar- rival rate of  ∼ 100rps). Therefore, the simulator allows us to evaluate our model for a larger setup, where we mimic an 11k core cluster which can handle up to 7000 requests (70 × more than the real system). Additionally, it helps compare the resource footprint of  Kraken  against a clairvoyant policy (Oracle) that has 100% load prediction accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 121,
      "augmented": true
    },
    {
      "text": "6 Analysis of Results This section presents experimental results for single ap- plications run in isolation for all schemes on the real system and simulation platform. We have also verified that  Kraken (as well as the other schemes) yield similar results (within 2%) when multiple applications are run concurrently. Additionally, it helps compare the resource footprint of  Kraken  against a clairvoyant policy (Oracle) that has 100% load prediction accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "This repre- sents  𝑁𝐶 𝑑 𝑡 (Section 3) for all possible depths,  𝑑 . We have also verified that  Kraken (as well as the other schemes) yield similar results (within 2%) when multiple applications are run concurrently. 6.1 Real System Results 6.1.1 Containers Spawned : Figure 8 depicts the function- wise breakdown of the number of containers provisioned across all policies for individual applications.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "It can be ob- served that, existing policies, namely,  Arch ,  Fifer  and  Xanadu spawn, respectively, 2.41x, 76% and 30% more containers than  Kraken , on average, across all applications. Overallo- cation of containers in case of  Arch  is due to two reasons: (i) it assumes that all functions in the application will be invoked at runtime; and (ii) it spawns one container per in- vocation request. This repre- sents  𝑁𝐶 𝑑 𝑡 (Section 3) for all possible depths,  𝑑 .",
      "type": "sliding_window_shuffled",
      "tokens": 138,
      "augmented": true
    },
    {
      "text": "Overallo- cation of containers in case of  Arch  is due to two reasons: (i) it assumes that all functions in the application will be invoked at runtime; and (ii) it spawns one container per in- vocation request. On the other hand,  Fifer  improves upon this by reducing the total number of containers spawned using request batching. However, it does not take workflow activation patterns into consideration while spawning con- tainers, leading to container overprovisioning.",
      "type": "sliding_window_shuffled",
      "tokens": 116,
      "augmented": true
    },
    {
      "text": "Furthermore, it can be seen that Xanadu  provisions a relatively high number of containers for a particular group of functions as compared to the rest. However, it does not take workflow activation patterns into consideration while spawning con- tainers, leading to container overprovisioning. The recently proposed scheme,  Xanadu , is based on a workflow-aware container deployment mechanism, but does not employ re- quest batching, leading to extra containers being deployed in comparison to  Kraken .",
      "type": "sliding_window_shuffled",
      "tokens": 116,
      "augmented": true
    },
    {
      "text": "This is because it allocates containers to serve the predicted load along only the Most Likely Path (MLP) of a request. Furthermore, it can be seen that Xanadu  provisions a relatively high number of containers for a particular group of functions as compared to the rest. The rest of the containers are a result of  reactive scaling  that follows from MLP mispredictions, which accounts for 34% of the total number of containers spawned.",
      "type": "sliding_window_shuffled",
      "tokens": 101,
      "augmented": true
    },
    {
      "text": "0 500 1000 1500 \nArch \nFifer \nDProb \nKraken \nSProb \nXanadu \n# Containers \nNGINX Search Make_Post Text Media User_Tag URL_Shortener Compose_Post Post_Storage Read_Timeline Follow \n(a) Social Network. The rest of the containers are a result of  reactive scaling  that follows from MLP mispredictions, which accounts for 34% of the total number of containers spawned. 161 \nSoCC ’21, November 1–4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al.",
      "type": "sliding_window_shuffled",
      "tokens": 142,
      "augmented": true
    },
    {
      "text": "0 500 1000 1500 \nArch \nFifer \nDProb \nKraken \nSProb \nXanadu \n# Containers \nNGINX Search Make_Post Text Media User_Tag URL_Shortener Compose_Post Post_Storage Read_Timeline Follow \n(a) Social Network. 0 500 1000 1500 \nArch \nFifer \nDProb \nKraken \nSProb \nXanadu \n# Containers NGINX ID Movie_ID Text User_Service Rating Compose_Review Movie_Review User_Review Review_Storage \n(b) Media Service. 0 200 400 600 \nArch \nFifer \nDProb \nKraken \nSProb \nXanadu \n# Containers \nNGINX Check_Reservation \nGet_Profiles Search \nMake_Reservation \n(c) Hotel Reservation.",
      "type": "sliding_window_shuffled",
      "tokens": 171,
      "augmented": true
    },
    {
      "text": "0 200 400 600 \nArch \nFifer \nDProb \nKraken \nSProb \nXanadu \n# Containers \nNGINX Check_Reservation \nGet_Profiles Search \nMake_Reservation \n(c) Hotel Reservation. The reduction in the number of containers spawned by Kraken  in comparison to other policies is roughly propor- tional to the total number of application workflows and the slack available for each function within a workflow (see Ta- ble 2 and Figure 7). Figure 8: Real System: Stage-wise Breakdown of Containers spawned by each policy.",
      "type": "sliding_window_shuffled",
      "tokens": 131,
      "augmented": true
    },
    {
      "text": "For instance, Figure 8 indicates that the Social Network ,  Media Service  and  Hotel Reservation  applica- tions show the highest (73%, 53% and 36%), moderate (40%, 28% and 7%) and least (at most 33%) reductions in the number of containers spawned with respect to existing policies,  Arch , Fifer  and  Xanadu , respectively. Both  Social Network  and  Me- dia Service  have a high number of workflows, but the former has more functions with higher slack, leading to increased batching, thereby resulting in the most reduction in con- tainers spawned. The reduction in the number of containers spawned by Kraken  in comparison to other policies is roughly propor- tional to the total number of application workflows and the slack available for each function within a workflow (see Ta- ble 2 and Figure 7).",
      "type": "sliding_window_shuffled",
      "tokens": 202,
      "augmented": true
    },
    {
      "text": "Both  Social Network  and  Me- dia Service  have a high number of workflows, but the former has more functions with higher slack, leading to increased batching, thereby resulting in the most reduction in con- tainers spawned. On the other hand,  DProb  and  SProb  spawn fewer containers than  Kraken  as a consequence of not using  Commonality  and Connectivity  to augment function weights, while making container allocation decisions. Hotel Reservation  has the least number of workflows as well as the lowest overall slack for all functions, resulting in the least reduction in the number of containers.",
      "type": "sliding_window_shuffled",
      "tokens": 136,
      "augmented": true
    },
    {
      "text": "On the other hand,  DProb  and  SProb  spawn fewer containers than  Kraken  as a consequence of not using  Commonality  and Connectivity  to augment function weights, while making container allocation decisions. Note that, these additional containers are necessary to reduce SLO violations. As a result,  Kraken  provisions up to 21% more containers than both  DProb  and  SProb  for the three applications.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "From these graphs, it is evident that  Kraken  exhibits comparable performance to existing policies while having a minimal re- source footprint. 6.1.2 End-to-End Response Times and SLO Compli- ance : Figure 9 shows the breakdown of the average end-to- end response times and Figure 10 juxtaposes the total number of containers provisioned against the SLO Guarantees for all policies and applications, averaged across all traces. Note that, these additional containers are necessary to reduce SLO violations.",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "For the  Social Network  application,  Kraken remains within 60 ms of the end-to-end response time of Arch  (Figure 9a), which performs the best out of all policies with respect to these metrics, while ensuring 99.94% SLO guarantees (Figure 10a) . From these graphs, it is evident that  Kraken  exhibits comparable performance to existing policies while having a minimal re- source footprint. However,  Arch  uses 4x the number of containers used by  Kraken  (Figure 10a).",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "Kraken  also performs similar to  Fifer , while using 58% reduced containers for  Social Network . From Figures 9 and 10, it can be seen that  Xanadu  has similar (or worse) end- to-end response times than  Kraken  (up to 50 ms more), but \nspawns more containers as well (up to 70% more) and satisfies fewer SLOs on average (0.2% lesser). However,  Arch  uses 4x the number of containers used by  Kraken  (Figure 10a).",
      "type": "sliding_window_shuffled",
      "tokens": 115,
      "augmented": true
    },
    {
      "text": "This ef- fect is highlighted in applications such as  Social Network  and Media Service  which have relatively high MLP misprediction rates (80% and 50%, respectively 2 )) due to the presence of multiple possible paths (Table 2). This can be attributed to  Xanadu ’s container pre-deployment policy which causes reactive scale outs as a result of MLP mispredictions. From Figures 9 and 10, it can be seen that  Xanadu  has similar (or worse) end- to-end response times than  Kraken  (up to 50 ms more), but \nspawns more containers as well (up to 70% more) and satisfies fewer SLOs on average (0.2% lesser).",
      "type": "sliding_window_shuffled",
      "tokens": 164,
      "augmented": true
    },
    {
      "text": "Media Service  suffers from higher end-to-end response times, further exacerbating this effect. This ef- fect is highlighted in applications such as  Social Network  and Media Service  which have relatively high MLP misprediction rates (80% and 50%, respectively 2 )) due to the presence of multiple possible paths (Table 2). Xanadu  has only a 34% misprediction rate for  Hotel Reservation , due to the lower number of workflows, and is seen to match  Kraken  in terms of SLOs satisfied (99.87%).",
      "type": "sliding_window_shuffled",
      "tokens": 123,
      "augmented": true
    },
    {
      "text": "The breakdown of the average response times in Figure 9 shows that both  Arch  and  Xanadu  do not suffer from queue- ing delays. This is because both policies spawn a container per request, resulting in almost zero queueing. Xanadu  has only a 34% misprediction rate for  Hotel Reservation , due to the lower number of workflows, and is seen to match  Kraken  in terms of SLOs satisfied (99.87%).",
      "type": "sliding_window_shuffled",
      "tokens": 101,
      "augmented": true
    },
    {
      "text": "The relatively high cold start-induced delay experienced by  Xanadu  can be attributed to the reactive scaling it uses to cope with MLP mispredictions. This is because both policies spawn a container per request, resulting in almost zero queueing. Kraken  exhibits delay characteristics simi- lar to  Fifer  owing to both policies having batching and a similar container pre-deployment policy.",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": "Kraken  exhibits delay characteristics simi- lar to  Fifer  owing to both policies having batching and a similar container pre-deployment policy. DProb and  SProb  exhibit higher overall end-to-end response times compared to  Kraken , with  SProb  experiencing a dispropor- tionately high queueing delay compared to its cold start delay. However,  Kraken allocates fewer containers (57% lesser, on average across all applications) along each workflow compared to  Fifer .",
      "type": "sliding_window_shuffled",
      "tokens": 115,
      "augmented": true
    },
    {
      "text": "This is because it uses statically assigned function weights, which prevents it from being able to proactively spawn con- tainers according to the varying user input. DProb and  SProb  exhibit higher overall end-to-end response times compared to  Kraken , with  SProb  experiencing a dispropor- tionately high queueing delay compared to its cold start delay. This results in the majority of requests getting queued at the containers.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "Although we use specific combinations of applications and traces to highlight the improvements, the results are similar for other workload mixes as well. This results in the majority of requests getting queued at the containers. 6.1.3 Analysis of Key Improvements : This subsection fo- cuses on the key improvements offered by  Kraken  in terms of Container Utilization, Response Latency Distribution and Energy Efficiency.",
      "type": "sliding_window_shuffled",
      "tokens": 79,
      "augmented": true
    },
    {
      "text": "Container Utilization:  Figure 11 plots the average num- ber of requests executed per container (Jobs per container) \n2 MLP misprediction rates are not shown in any Figure \n162 \nKraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC ’21, November 1–4, 2021, Seattle, WA, USA \n0 \n100 \n200 \n300 \nArch Fifer DProb Kraken SProb Xanadu \nResponse Time (ms) \nQueueing Cold Start Execution Time \n(a) Social Network. Although we use specific combinations of applications and traces to highlight the improvements, the results are similar for other workload mixes as well. 0 \n150 \n300 \n450 \n600 \nArch Fifer DProb Kraken SProb Xanadu \nResponse Time (ms) \nQueueing Cold Start Execution Time \n(b) Media Service.",
      "type": "sliding_window_shuffled",
      "tokens": 198,
      "augmented": true
    },
    {
      "text": "0 \n150 \n300 \n450 \n600 \nArch Fifer DProb Kraken SProb Xanadu \nResponse Time (ms) \nQueueing Cold Start Execution Time \n(b) Media Service. 0 \n150 \n300 \n450 \nArch Fifer DProb Kraken SProb Xanadu \nResponse Time (ms) \nQueueing Cold Start Execution Time \n(c) Hotel Reservation. Figure 9: Real System: Breakdown of Average End-to-End Response Times in terms of queueing delay, cold start delay and execution time.",
      "type": "sliding_window_shuffled",
      "tokens": 120,
      "augmented": true
    },
    {
      "text": "Figure 9: Real System: Breakdown of Average End-to-End Response Times in terms of queueing delay, cold start delay and execution time. 99.40% \n99.55% \n99.70% \n99.85% \n100.00% \n0 \n300 \n600 \n900 \n1200 \nArch Fifer Dprob Kraken Sprob Xanadu \nPercentage \n# Containers \n# Containers SLO Guarantees \n(a) Social Network. 99.00% \n99.25% \n99.50% \n99.75% \n100.00% \n0 \n300 \n600 \n900 \n1200 \nArch Fifer DProb Kraken SProb Xanadu \nPercentage \n# Containers \n# Containers SLO Guarantees \n(b) Media Service.",
      "type": "sliding_window_shuffled",
      "tokens": 151,
      "augmented": true
    },
    {
      "text": "98.50% \n99.00% \n99.50% \n100.00% \n0 \n200 \n400 \n600 \nArch Fifer DProb Kraken SProb Xanadu \nPercentage \n# Containers \n# Containers SLO Guarantees \n(c) Hotel Reservation. 99.00% \n99.25% \n99.50% \n99.75% \n100.00% \n0 \n300 \n600 \n900 \n1200 \nArch Fifer DProb Kraken SProb Xanadu \nPercentage \n# Containers \n# Containers SLO Guarantees \n(b) Media Service. Figure 10: Real System: Comparison of Total Number of Containers spawned VS SLOs satisfied by each policy.",
      "type": "sliding_window_shuffled",
      "tokens": 138,
      "augmented": true
    },
    {
      "text": "Figure 10: Real System: Comparison of Total Number of Containers spawned VS SLOs satisfied by each policy. 0 \n300 \n600 \n900 \n1200 \n1500 \nArch Fifer DProb Kraken SProb Xanadu \nJobs per Container \nFigure 11: Real System: Comparison of Container Utilization (a.k.a. The Primary Y-Axis denotes the number of containers spawned, The secondary Y-axis indicates the percentage of SLOs met and the X-axis represents each policy.",
      "type": "sliding_window_shuffled",
      "tokens": 119,
      "augmented": true
    },
    {
      "text": "0 \n300 \n600 \n900 \n1200 \n0.25 0.5 0.75 0.98 0.99 \nResponse Time (ms) \nCDF Archipelago Fifer DProb Kraken SProb SLO Xanadu \nFigure 12: Real System: Response Time Distribution. 0 \n300 \n600 \n900 \n1200 \n1500 \nArch Fifer DProb Kraken SProb Xanadu \nJobs per Container \nFigure 11: Real System: Comparison of Container Utilization (a.k.a. average #jobs executed per Container).",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "0 \n300 \n600 \n900 \n1200 \n0.25 0.5 0.75 0.98 0.99 \nResponse Time (ms) \nCDF Archipelago Fifer DProb Kraken SProb SLO Xanadu \nFigure 12: Real System: Response Time Distribution. across all functions in  Social Network  for the Poisson trace. An ideal scheme would focus on packing more number of requests per container to improve utilization without caus- ing SLO violations.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "Kraken  shows 4x, 2.16x and 2.06x more container utilization compared to  Arch ,  Fifer , and  Xanadu respectively. An ideal scheme would focus on packing more number of requests per container to improve utilization without caus- ing SLO violations. This is because  Kraken  limits the number of containers spawned through function weight assignment and request batching.",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "This is because  Kraken  limits the number of containers spawned through function weight assignment and request batching. Consequently, they exhibit up to 0.24% more SLO Violations compared to  Kraken , for this workload mix. DProb  and  SProb  both exhibit higher utilization compared to  Kraken  (15%) as a result of spawning fewer containers overall, owing to not accounting for  crit- ical  and  common  functions while provisioning containers.",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "Latency Distribution:  The end-to-end latency distribution for all policies for the  Social Network  application with the Twitter trace is plotted in Figure 12. Consequently, they exhibit up to 0.24% more SLO Violations compared to  Kraken , for this workload mix. In particular,  Arch ,  Fifer and  Kraken  show comparable latencies, with P99 values re- maining well within the SLO of 1000ms.",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "However,  Arch  and Fifer  use 3.51x and 2.1x more containers than  Kraken  to \n0 \n0.25 \n0.5 \n0.75 \n1 \nArch Fifer DProb Kraken SProb Xanadu \nEnergy Consumption Rate \n(a) Energy Consumption Rate. In particular,  Arch ,  Fifer and  Kraken  show comparable latencies, with P99 values re- maining well within the SLO of 1000ms. 0 \n300 \n600 \n900 \n1200 \n0.25 0.5 0.75 0.98 0.99 \nLatency (ms) \nCDF Kraken Comm Only \nConn Only SLO \n(b) Response Time Distribution.",
      "type": "sliding_window_shuffled",
      "tokens": 138,
      "augmented": true
    },
    {
      "text": "The tail latency (measured at P99) for  DProb almost exceeds the SLO, whereas it does so for  SProb . Figure 13: Real System: Normalized Energy Consumption of all Schemes and Response Time Distribution of  Kraken ,  Comm Only and  Conn Only achieve this. 0 \n300 \n600 \n900 \n1200 \n0.25 0.5 0.75 0.98 0.99 \nLatency (ms) \nCDF Kraken Comm Only \nConn Only SLO \n(b) Response Time Distribution.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "SProb  does worse than  DProb  at the tail because of its lack of adaptive probability estimation. The tail latency (measured at P99) for  DProb almost exceeds the SLO, whereas it does so for  SProb . Kraken manages to avoid high tail latency by assigning augmented weights to key functions, thus, helping it tolerate incorrect load/probability estimations.",
      "type": "sliding_window_shuffled",
      "tokens": 88,
      "augmented": true
    },
    {
      "text": "SProb  does worse than  DProb  at the tail because of its lack of adaptive probability estimation. Kraken  makes use of 21% more containers to achieve the improved latencies. Xanadu  experiences a sudden rise in tail latency, with it being 100ms more than that of  Kraken , while using 96% more containers.",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "Energy Efficiency:  We measure the energy-consumption as total Energy consumed divided over total time. Xanadu  experiences a sudden rise in tail latency, with it being 100ms more than that of  Kraken , while using 96% more containers. This is due to  Xanadu ’s MLP misprediction and the resultant container over-provisioning.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "Kraken achieves one of the lowest energy consumption rates among all the policies considered, with it bettering existing policies, namely,  Arch ,  Fifer  and  Xanadu  by 26%, 14% and 3% respec- tively (for the workload mix of  Media Service  application with Wiki trace) as depicted in Figure 13a. These savings can go up to 48% compared to  Arch  for applications like  Social Network . Energy Efficiency:  We measure the energy-consumption as total Energy consumed divided over total time.",
      "type": "sliding_window_shuffled",
      "tokens": 118,
      "augmented": true
    },
    {
      "text": "The resultant energy savings of  Kraken  are a direct consequence of the savings in computation and memory usage from the fewer containers spawned. Only  DProb  and SProb  consume lesser energy than  Kraken  (4% lesser), due to their more aggressive container reduction approach. These savings can go up to 48% compared to  Arch  for applications like  Social Network .",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "6.1.4 Ablation Study : This subsection conducts a brick-by- brick evaluation of  Kraken  using  Conn Only  and  Comm Only , \n163 \nSoCC ’21, November 1–4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al. Only  DProb  and SProb  consume lesser energy than  Kraken  (4% lesser), due to their more aggressive container reduction approach. Application Kraken Comm Only Conn Only Social Network (99.94%, 284) (99.91%, 276) (99.89%, 256) Media Service (99.73%, 572) (99.66%, 561) (99.64%, 552) Hotel Reservation (99.87%, 316) (99.77%, 290) (99.74%, 282) Table 5: Real System: Comparing (SLO Guarantees,#Containers Spawned) against  Comm Only  and  Conn Only .",
      "type": "sliding_window_shuffled",
      "tokens": 226,
      "augmented": true
    },
    {
      "text": "From Table 5, it can be seen that  Comm Only  spawns 8% more containers than  Conn Only  for  Social Network . schemes that exclude  Commonality  and  Connectivity  com- ponents from  Kraken , respectively. Application Kraken Comm Only Conn Only Social Network (99.94%, 284) (99.91%, 276) (99.89%, 256) Media Service (99.73%, 572) (99.66%, 561) (99.64%, 552) Hotel Reservation (99.87%, 316) (99.77%, 290) (99.74%, 282) Table 5: Real System: Comparing (SLO Guarantees,#Containers Spawned) against  Comm Only  and  Conn Only .",
      "type": "sliding_window_shuffled",
      "tokens": 175,
      "augmented": true
    },
    {
      "text": "Upon closer examination, we see that this is due to functions having different degrees of  Commonality and  Connectivity . This difference is lesser for the other applications. From Table 5, it can be seen that  Comm Only  spawns 8% more containers than  Conn Only  for  Social Network .",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "Moreover, the majority of functions whose Commonality  and  Connectivity  differ, have a high batch size, thereby reducing the variation in the number of containers spawned. Upon closer examination, we see that this is due to functions having different degrees of  Commonality and  Connectivity . Following this, we observe that the variation in the number of containers in  Social Network  is mainly due to the significant difference in the  Commonality  and  Connectivity of the  Compose Post  function whose batch size is only one.",
      "type": "sliding_window_shuffled",
      "tokens": 110,
      "augmented": true
    },
    {
      "text": "There is lesser difference in containers spawned by  Comm Only ,  Conn Only  and  Kraken  for  Media Service  because we have implemented  Kraken  with a cap on the additional con- tainers spawned due to  Commonality  and  Connectivity  when the sum of their values exceeds a threshold. Following this, we observe that the variation in the number of containers in  Social Network  is mainly due to the significant difference in the  Commonality  and  Connectivity of the  Compose Post  function whose batch size is only one. This threshold is exceeded in  Media Service  for the majority of functions.",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "This threshold is exceeded in  Media Service  for the majority of functions. Due to the difference in container provisioning, the difference in response times between the three schemes is evident at the tail of the response time distribution (Figure 13b). Comm Only  and  Conn Only  are seen to exceed the target SLO at the 99th percentile.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "The tail latency of  Kraken , in comparison, grows slower and remains within the target SLO. Comm Only  and  Conn Only  are seen to exceed the target SLO at the 99th percentile. 6.2 Simulator Results Since the real-system is limited to a 160-core cluster, we use our in-house simulator, which can simulate an 11k-core cluster, to study the scalability of  Kraken .",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "6.2 Simulator Results Since the real-system is limited to a 160-core cluster, we use our in-house simulator, which can simulate an 11k-core cluster, to study the scalability of  Kraken . We mimic a large scale Poisson arrival trace ( 𝜇 = 1000rps), Wiki ( 𝜇 = 284 rps) and Twitter ( 𝜇 = 3332 rps) traces. Figure 14 plots the con- tainers spawned versus the SLO guarantees for each appli- cation for all traces.",
      "type": "sliding_window_shuffled",
      "tokens": 134,
      "augmented": true
    },
    {
      "text": "Kraken  is seen to reduce con- tainer overprovisioning when applications have numerous possible workflows and enough slack per function to exploit. Figure 14 plots the con- tainers spawned versus the SLO guarantees for each appli- cation for all traces. The simulator results closely correlate to those of the real system.",
      "type": "sliding_window_shuffled",
      "tokens": 80,
      "augmented": true
    },
    {
      "text": "Notably,  Kraken  spawns nearly 80% less containers for  Social Network  in comparison to  Arch . Kraken  is seen to reduce con- tainer overprovisioning when applications have numerous possible workflows and enough slack per function to exploit. Container overprovisioning is inflated 15% more than the corresponding real system re- sult, due to the large-scale traces.",
      "type": "sliding_window_shuffled",
      "tokens": 89,
      "augmented": true
    },
    {
      "text": "Table 6 shows the median and tail latencies of each policy averaged across all appli- cations for the three traces. Container overprovisioning is inflated 15% more than the corresponding real system re- sult, due to the large-scale traces. The trend we observe is that traces with higher variability, such as the Twitter trace, af- fect the tail latencies of policies more harshly than the other, more predictable, traces.",
      "type": "sliding_window_shuffled",
      "tokens": 107,
      "augmented": true
    },
    {
      "text": "The trend we observe is that traces with higher variability, such as the Twitter trace, af- fect the tail latencies of policies more harshly than the other, more predictable, traces. However, the tail latencies of  DProb  and  SProb  sometimes exceeds the SLO, since they don’t use  Commonality  and  Connectivity . Nevertheless,  Kraken  is resilient to \nPolicy Poisson Wiki Twitter Med Tail Med Tail Med Tail Arch 336 568 336 568 336 599 Fifer 362 612 360 611 373 833 DProb 371 746 368 753 381 1549 Kraken 366 634 358 633 371 974 SProb 395 1101 382 1073 395 1610 Xanadu 343 723 340 774 340 1244 Table 6: Simulator: Median and tail latencies (in ms) averaged across all applications for the three traces \nunpredictable loads as well, with tail latencies always remain- ing within the SLO (1000 ms).",
      "type": "sliding_window_shuffled",
      "tokens": 240,
      "augmented": true
    },
    {
      "text": "However, the tail latencies of  DProb  and  SProb  sometimes exceeds the SLO, since they don’t use  Commonality  and  Connectivity . 6.2.1 Sensitivity Study : This subsection compares  Kraken against  Oracle , which is an ideal policy that is assumed to be able to predict future load and all path probabilities with 100% accuracy and also has request batching. It is observed that Xanadu  also violates the SLO for the Twitter trace, owing to the reactive scale-outs resulting from MLP mispredictions.",
      "type": "sliding_window_shuffled",
      "tokens": 124,
      "augmented": true
    },
    {
      "text": "Figure 15 shows the breakdown of total number of containers spawned for each application, aver- aged across all realistic large-scale traces using the simulator. Consequently, Oracle  does not suffer from cold starts and minimizes con- tainers spawned. 6.2.1 Sensitivity Study : This subsection compares  Kraken against  Oracle , which is an ideal policy that is assumed to be able to predict future load and all path probabilities with 100% accuracy and also has request batching.",
      "type": "sliding_window_shuffled",
      "tokens": 110,
      "augmented": true
    },
    {
      "text": "Figure 15 shows the breakdown of total number of containers spawned for each application, aver- aged across all realistic large-scale traces using the simulator. This is due to  Kraken ’s load/path probability miscalculations and the usage of  Commonality and  Connectivity  to cope with this. It is observed that  Kraken  spawns more containers ( 7%) than  Oracle , on average.",
      "type": "sliding_window_shuffled",
      "tokens": 89,
      "augmented": true
    },
    {
      "text": "This may be due to Media Service  having higher path unpredictability than  Hotel Reservation  (Table 2) as well as lower slack per function than Social Network  (Figure 7). This is due to  Kraken ’s load/path probability miscalculations and the usage of  Commonality and  Connectivity  to cope with this. It is seen that  Kraken spawns 10% more containers for  Media Service  and 6% more for  Hotel Reservation  and  Social Network .",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "From Figure 16b, it is observed that  Oracle , being clairvoyant, spawns containers in accor- dance with the peaks and valleys of the request arrival trace. Kraken , while spawning more containers, also is seen to lag behind the trend of the trace due to load prediction errors. This may be due to Media Service  having higher path unpredictability than  Hotel Reservation  (Table 2) as well as lower slack per function than Social Network  (Figure 7).",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "Kraken , while spawning more containers, also is seen to lag behind the trend of the trace due to load prediction errors. Moreover, more than 40% of requests show significant variability in inter- arrival times. Performance under Sparse Load:  Analysis of logs col- lected from the Azure cloud platform [ 42 ] shows request volumes that are much lighter (average of 2 requests/hour) than those of the traces we have considered.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "Moreover, more than 40% of requests show significant variability in inter- arrival times. We also spawn con- tainers much more in advance than the predicted arrival time and also keep them alive for at least a minute before evicting them from memory, to account for arrival unpredictability. To deal with such traces, we modified  Kraken ’s load prediction model to predict future request arrival times, owing to the sparse nature of the trace.",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "We also spawn con- tainers much more in advance than the predicted arrival time and also keep them alive for at least a minute before evicting them from memory, to account for arrival unpredictability. Other \n3 These results are not shown in any graph. It is seen that  Kraken  meets the SLOs for all requests from the lightly-loaded trace over 18 hours while averaging 0.85 memory-resident containers at any given second 3 .",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "164 \nKraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC ’21, November 1–4, 2021, Seattle, WA, USA \n98.80% \n99.20% \n99.60% \n100.00% \n0 \n10000 \n20000 \n30000 \nArch Fifer DProb Kraken SProb Xanadu \nPercentage \n# Containers \n# Containers SLO Guarantees \n(a) Social Network. Other \n3 These results are not shown in any graph. 98.50% \n99.00% \n99.50% \n100.00% \n0 \n10000 \n20000 \n30000 \nArch Fifer DProb Kraken SProb Xanadu \nPercentage \n# Containers \n# Containers SLO Guarantees \n(b) Media Service.",
      "type": "sliding_window_shuffled",
      "tokens": 167,
      "augmented": true
    },
    {
      "text": "99.00% \n99.25% \n99.50% \n99.75% \n100.00% \n0 \n3000 \n6000 \n9000 \n12000 \nArch Fifer DProb Kraken SProb Xanadu \nPercentage \n# Containers \n# Containers SLO Guarantees \n(c) Hotel Reservation. Figure 14: Simulator: Comparison of Total Number of Containers spawned VS SLOs satisfied by each policy. 98.50% \n99.00% \n99.50% \n100.00% \n0 \n10000 \n20000 \n30000 \nArch Fifer DProb Kraken SProb Xanadu \nPercentage \n# Containers \n# Containers SLO Guarantees \n(b) Media Service.",
      "type": "sliding_window_shuffled",
      "tokens": 144,
      "augmented": true
    },
    {
      "text": "The Primary Y-Axis denotes the number of containers spawned, The secondary Y-axis indicates the percentage of SLOs met and the X-axis represents each policy. 0 \n2000 \n4000 \n6000 \nOracle Kraken \n# Containers \nNGINX Search Make_Post Text Media User_Tag URL_Shortener Compose_Post Post_Storage Read_Timeline Follow \n(a) Social Network. Figure 14: Simulator: Comparison of Total Number of Containers spawned VS SLOs satisfied by each policy.",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "0 \n2000 \n4000 \n6000 \nOracle Kraken \n# Containers \nNGINX Search Make_Post Text Media User_Tag URL_Shortener Compose_Post Post_Storage Read_Timeline Follow \n(a) Social Network. 0 \n4000 \n8000 \n12000 \nOracle Kraken \n# Containers \nNGINX ID Movie_ID Text User_Service Rating Compose_Review Movie_Review User_Review Review_Storage \n(b) Media Service. 0 \n2000 \n4000 \n6000 \nOracle Kraken \n# Containers \nNGINX Check_Reservation Get_Profiles Search Make_Reservation \n(c) Hotel Reserva- tion.",
      "type": "sliding_window_shuffled",
      "tokens": 147,
      "augmented": true
    },
    {
      "text": "Figure 15: Simulator: Comparison of Function-wise Breakdown of Containers spawned by  Kraken  and  Oracle . 0 \n150 \n300 \n450 \n600 \nOracle Kraken Oracle Kraken Oracle Kraken \nSocial Network Media Service Hotel Reservation \nResponse Time (ms) \nQueueing Cold Start Execution Time \n(a) E2E Response Time Break- down. 0 \n2000 \n4000 \n6000 \nOracle Kraken \n# Containers \nNGINX Check_Reservation Get_Profiles Search Make_Reservation \n(c) Hotel Reserva- tion.",
      "type": "sliding_window_shuffled",
      "tokens": 122,
      "augmented": true
    },
    {
      "text": "275 \n290 \n305 \n320 \n600 \n700 \n800 \n1 10 19 28 37 46 55 \nRequests/second \n# Containers \nSampling interval (minutes) \nOracle Kraken Trace \n(b) Containers spawned over time. Figure 16: Simulator: Comparison of End-to-End (E2E) Response Times and Containers Spawned Over Time (60 minutes) of  Kraken and  Oracle . 0 \n150 \n300 \n450 \n600 \nOracle Kraken Oracle Kraken Oracle Kraken \nSocial Network Media Service Hotel Reservation \nResponse Time (ms) \nQueueing Cold Start Execution Time \n(a) E2E Response Time Break- down.",
      "type": "sliding_window_shuffled",
      "tokens": 144,
      "augmented": true
    },
    {
      "text": "Trace Arch Fifer Kraken Xanadu Comm Only Conn Only Wiki (99.91%, 2737) (99.90%, 2092) (99.86%, 1396) (99.66%, 1737) (99.78%, ) (99.75%, ) Twitter (99.72%, 45,107) (99.63%, 34,210) (99.50%, 22,377) (99.10%, 25,132) (99.22%, ) (99.15%, ) Table 7: Simulator: Comparing (% SLO met,# Containers Spawned) against Existing Policies after Varying the Target SLOs. Figure 16: Simulator: Comparison of End-to-End (E2E) Response Times and Containers Spawned Over Time (60 minutes) of  Kraken and  Oracle . existing policies such as  Arch  and  Fifer  exhibit similar perfor- mance and resource usage when their prediction models and keep-alive times are similarly adjusted.",
      "type": "sliding_window_shuffled",
      "tokens": 239,
      "augmented": true
    },
    {
      "text": "existing policies such as  Arch  and  Fifer  exhibit similar perfor- mance and resource usage when their prediction models and keep-alive times are similarly adjusted. Xanadu , on the other hand, while having 0.74 memory-resident containers per sec- ond, suffers from 55% SLO Violations on average across all applications as a result of MLP mispredictions whose effects are exacerbated in this scenario, due to low request volume. Varying SLO:  Table 7 shows the SLO guarantees and num- ber of containers spawned for existing policies as well as Comm Only  and  Conn Only , when the SLO is reduced from 1000ms to a value 30% higher than the response time of the slowest workflow in each application.",
      "type": "sliding_window_shuffled",
      "tokens": 172,
      "augmented": true
    },
    {
      "text": "The resultant SLOs are 500ms, 910ms and 809ms for  Social Network ,  Media Ser- vice  and  Hotel Reservation  respectively. Varying SLO:  Table 7 shows the SLO guarantees and num- ber of containers spawned for existing policies as well as Comm Only  and  Conn Only , when the SLO is reduced from 1000ms to a value 30% higher than the response time of the slowest workflow in each application. Reducing the SLO, in turn, can potentially reduce the batch sizes of functions as well.",
      "type": "sliding_window_shuffled",
      "tokens": 124,
      "augmented": true
    },
    {
      "text": "However,  Kraken  is able to \nmaintain at least 99.5% SLO guarantee and spawns 50%, 34% and 15% less containers compared to  Arch ,  Fifer  and  Xanadu , respectively. Moreover, the reduced SLO target results in increased SLO violations across all policies. Reducing the SLO, in turn, can potentially reduce the batch sizes of functions as well.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "This difference, in terms of percent of SLO violations, changes from being at most 0.1% to being between 0.1 to 0.35%. However,  Kraken  is able to \nmaintain at least 99.5% SLO guarantee and spawns 50%, 34% and 15% less containers compared to  Arch ,  Fifer  and  Xanadu , respectively. It can be seen that the difference in SLO compli- ance between  Kraken ,  Comm Only , and  Conn Only  increases due to the reduced target SLO.",
      "type": "sliding_window_shuffled",
      "tokens": 116,
      "augmented": true
    },
    {
      "text": "In comparison,  Comm Only  and  Conn Only  fail to spawn enough containers for each important function as they do not consider both these parameters, resulting in increased tail latency and exacerbates the SLO violations. This difference, in terms of percent of SLO violations, changes from being at most 0.1% to being between 0.1 to 0.35%. This is a result of  Kraken  being more resilient at the tail of the response time distribution as it uses both  Commonality  and  Connectivity  while spawning containers.",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "In comparison,  Comm Only  and  Conn Only  fail to spawn enough containers for each important function as they do not consider both these parameters, resulting in increased tail latency and exacerbates the SLO violations. To- wards addressing these challenges, we design and evalu- ate  Kraken , a DAG workflow-aware resource management framework, for efficiently running such applications by uti- lizing minimum resources, while remaining SLO-compliant. 7 Concluding Remarks Adopting serverless functions for executing microservice- based applications introduces critical inefficiencies in terms of scheduling and resource management for the cloud provider, especially when deploying Dynamic DAG Applications.",
      "type": "sliding_window_shuffled",
      "tokens": 156,
      "augmented": true
    },
    {
      "text": "To- wards addressing these challenges, we design and evalu- ate  Kraken , a DAG workflow-aware resource management framework, for efficiently running such applications by uti- lizing minimum resources, while remaining SLO-compliant. Kraken  employs proactive weighted scaling of functions, where the weights are calculated using function invocation probabilities and other parameters pertaining to the appli- cation’s DAG structure. Our experimental evaluation on a 160-core cluster using  Deathstarbench  workload suite and real-world traces demonstrate that  Kraken  spawns up to 76% fewer containers, thereby improving container utilization and cluster-wide energy savings by up to 4 ×  and 48%, respec- tively, compared to state-of-the art schedulers employed in serverless platforms.",
      "type": "sliding_window_shuffled",
      "tokens": 189,
      "augmented": true
    },
    {
      "text": "8 Acknowledgement We are indebted to the anonymous reviewers for their in- sightful comments. Our experimental evaluation on a 160-core cluster using  Deathstarbench  workload suite and real-world traces demonstrate that  Kraken  spawns up to 76% fewer containers, thereby improving container utilization and cluster-wide energy savings by up to 4 ×  and 48%, respec- tively, compared to state-of-the art schedulers employed in serverless platforms. This research was partially supported by NSF grants #1931531, #1955815, #1763681, #2116962, #2122155 and #2028929.",
      "type": "sliding_window_shuffled",
      "tokens": 146,
      "augmented": true
    },
    {
      "text": "We also thank the NSF Chameleon Cloud project CH-819640 for their generous compute grant. All product names used here are for identification purposes only and may be trademarks of their respective companies. This research was partially supported by NSF grants #1931531, #1955815, #1763681, #2116962, #2122155 and #2028929.",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "165 \nSoCC ’21, November 1–4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al. All product names used here are for identification purposes only and may be trademarks of their respective companies. References \n[1]  [n.d.].",
      "type": "sliding_window_shuffled",
      "tokens": 73,
      "augmented": true
    },
    {
      "text": "https://archive.org/details/twitterstream. Twitter Stream traces. References \n[1]  [n.d.].",
      "type": "sliding_window_shuffled",
      "tokens": 34,
      "augmented": true
    },
    {
      "text": "[2]  2019. Accessed: 2020-05-07. https://archive.org/details/twitterstream.",
      "type": "sliding_window_shuffled",
      "tokens": 29,
      "augmented": true
    },
    {
      "text": "Airbnb AWS Case Study. https://aws.amazon.com/solutions/ case-studies/airbnb/. [2]  2019.",
      "type": "sliding_window_shuffled",
      "tokens": 37,
      "augmented": true
    },
    {
      "text": "[3]  2019. Provisioned Concurrency. https://aws.amazon.com/solutions/ case-studies/airbnb/.",
      "type": "sliding_window_shuffled",
      "tokens": 39,
      "augmented": true
    },
    {
      "text": "https://docs.aws.amazon.com/ lambda/latest/dg/configuration-concurrency.html. Provisioned Concurrency. [4]  2020.",
      "type": "sliding_window_shuffled",
      "tokens": 49,
      "augmented": true
    },
    {
      "text": "Amazon States Language. https://docs.aws.amazon.com/step- functions/latest/dg/concepts-amazon-states-language.html. [4]  2020.",
      "type": "sliding_window_shuffled",
      "tokens": 50,
      "augmented": true
    },
    {
      "text": "https://docs.aws.amazon.com/step- functions/latest/dg/concepts-amazon-states-language.html. [5]  2020. AWS Lambda.",
      "type": "sliding_window_shuffled",
      "tokens": 52,
      "augmented": true
    },
    {
      "text": "AWS Lambda. Serverless Functions. https://aws.amazon.com/ lambda/.",
      "type": "sliding_window_shuffled",
      "tokens": 29,
      "augmented": true
    },
    {
      "text": "[6]  2020. https://aws.amazon.com/ lambda/. Azure Durable Functions.",
      "type": "sliding_window_shuffled",
      "tokens": 29,
      "augmented": true
    },
    {
      "text": "[7] 2020. hey HTTP Load Testing Tool. https://docs.microsoft.com/en- us/azure/azure-functions/durable. Azure Durable Functions.",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "[8]  2020. https://github.com/rakyll/hey. [7] 2020. hey HTTP Load Testing Tool.",
      "type": "sliding_window_shuffled",
      "tokens": 33,
      "augmented": true
    },
    {
      "text": "[8]  2020. IBM-Composer. https://cloud.ibm.com/docs/openwhisk?topic= cloud-functions-pkg_composer.",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    },
    {
      "text": "Kubernetes. [9] 2020. https://cloud.ibm.com/docs/openwhisk?topic= cloud-functions-pkg_composer.",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    },
    {
      "text": "Microsoft Azure Serverless Functions. https://azure.microsoft. [10]  2020.",
      "type": "sliding_window_shuffled",
      "tokens": 21,
      "augmented": true
    },
    {
      "text": "[11] 2020. com/en-us/services/functions/. https://azure.microsoft.",
      "type": "sliding_window_shuffled",
      "tokens": 28,
      "augmented": true
    },
    {
      "text": "https://mikhail.io/serverless/ coldstarts/aws/. [13]  2021. AWS Lambda Cold Starts.",
      "type": "sliding_window_shuffled",
      "tokens": 37,
      "augmented": true
    },
    {
      "text": "Azure Functions Cold Starts. https://mikhail.io/serverless/ coldstarts/aws/. [14]  2021.",
      "type": "sliding_window_shuffled",
      "tokens": 35,
      "augmented": true
    },
    {
      "text": "https://mikhail.io/serverless/ coldstarts/azure/. Azure Functions Cold Starts. [15]  2021.",
      "type": "sliding_window_shuffled",
      "tokens": 35,
      "augmented": true
    },
    {
      "text": "https://mikhail.io/ serverless/coldstarts/azure/. [15]  2021. Expedia Case Study - Amazon AWS.",
      "type": "sliding_window_shuffled",
      "tokens": 39,
      "augmented": true
    },
    {
      "text": "https://mikhail.io/ serverless/coldstarts/azure/. Intel Power Gadget. [16]  Feb 24, 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 36,
      "augmented": true
    },
    {
      "text": "Intel Power Gadget. [17]  February 2018. https://github.com/sosy-lab/cpu- energy-meter.",
      "type": "sliding_window_shuffled",
      "tokens": 33,
      "augmented": true
    },
    {
      "text": "https://cloud.google.com/ functions/docs/. Google Cloud Functions. [17]  February 2018.",
      "type": "sliding_window_shuffled",
      "tokens": 27,
      "augmented": true
    },
    {
      "text": "2018. [18]  Istemi Ekin Akkus et al . https://cloud.google.com/ functions/docs/.",
      "type": "sliding_window_shuffled",
      "tokens": 36,
      "augmented": true
    },
    {
      "text": "2018. In  ATC . SAND: Towards High-Performance Serverless Computing.",
      "type": "sliding_window_shuffled",
      "tokens": 22,
      "augmented": true
    },
    {
      "text": "[19]  Mamoun Awad, Latifur Khan, and Bhavani Thuraisingham. 2008. In  ATC .",
      "type": "sliding_window_shuffled",
      "tokens": 33,
      "augmented": true
    },
    {
      "text": "The VLDB Journal  17, 3 (2008), 401–417. Pre- dicting WWW surfing using multiple evidence combination. 2008.",
      "type": "sliding_window_shuffled",
      "tokens": 30,
      "augmented": true
    },
    {
      "text": "2012. [20]  M. A. Awad and I. Khalil. The VLDB Journal  17, 3 (2008), 401–417.",
      "type": "sliding_window_shuffled",
      "tokens": 35,
      "augmented": true
    },
    {
      "text": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)  42, 4 (2012), 1131–1142. 2012. Prediction of User’s Web-Browsing Behavior: Application of Markov Model.",
      "type": "sliding_window_shuffled",
      "tokens": 56,
      "augmented": true
    },
    {
      "text": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)  42, 4 (2012), 1131–1142. 2004. https://doi.org/10.1109/TSMCB.2012.2187441 [21]  Ron Begleiter, Ran El-Yaniv, and Golan Yona.",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "2004. Journal of Artificial Intelligence Research  22 (2004), 385–421. On Prediction Using Variable Order Markov Models.",
      "type": "sliding_window_shuffled",
      "tokens": 31,
      "augmented": true
    },
    {
      "text": "2020. [22]  Marc Brooker, Andreea Florescu, Diana-Maria Popa, Rolf Neugebauer, Alexandru Agache, Alexandra Iordache, Anthony Liguori, and Phil Piwonka. Journal of Artificial Intelligence Research  22 (2004), 385–421.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "Firecracker: Lightweight Virtualization for Serverless Applications. In  NSDI . 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 22,
      "augmented": true
    },
    {
      "text": "2019. [23]  Jyothi Prasad Buddha and Reshma Beesetty. In  NSDI .",
      "type": "sliding_window_shuffled",
      "tokens": 29,
      "augmented": true
    },
    {
      "text": "In The Definitive Guide to AWS Application Integration . Step Functions. 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 19,
      "augmented": true
    },
    {
      "text": "Springer. In The Definitive Guide to AWS Application Integration . [24]  James Cadden, Thomas Unger, Yara Awad, Han Dong, Orran Krieger, and Jonathan Appavoo.",
      "type": "sliding_window_shuffled",
      "tokens": 52,
      "augmented": true
    },
    {
      "text": "[24]  James Cadden, Thomas Unger, Yara Awad, Han Dong, Orran Krieger, and Jonathan Appavoo. SEUSS: skip redundant paths to make serverless fast. 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": "1–15. In  Proceedings of the Fifteenth European Conference on Computer Systems . SEUSS: skip redundant paths to make serverless fast.",
      "type": "sliding_window_shuffled",
      "tokens": 33,
      "augmented": true
    },
    {
      "text": "1–15. [25]  Joao Carreira, Pedro Fonseca, Alexey Tumanov, Andrew Zhang, and Randy Katz. 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 36,
      "augmented": true
    },
    {
      "text": "Cirrus: A Serverless Framework for End-to-End ML Workflows. In  Proceedings of the ACM Symposium on Cloud Computing (Santa Cruz, CA, USA)  (SoCC ’19) . 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": "In  Proceedings of the ACM Symposium on Cloud Computing (Santa Cruz, CA, USA)  (SoCC ’19) . Association for Computing Ma- chinery, New York, NY, USA, 13–24. https://doi.org/10.1145/3357223.",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "https://doi.org/10.1145/3357223. 2019. 3362711 [26]  Benjamin Carver, Jingyuan Zhang, Ao Wang, and Yue Cheng.",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "2019. In search of a fast and efficient serverless dag engine. In  2019 IEEE/ACM Fourth International Parallel Data Systems Workshop (PDSW) .",
      "type": "sliding_window_shuffled",
      "tokens": 34,
      "augmented": true
    },
    {
      "text": "[27]  Nilanjan Daw, Umesh Bellur, and Purushottam Kulkarni. IEEE, \n1–10. In  2019 IEEE/ACM Fourth International Parallel Data Systems Workshop (PDSW) .",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": "Xanadu: Mitigating cascading cold starts in serverless function chain deploy- ments. 2020. [27]  Nilanjan Daw, Umesh Bellur, and Purushottam Kulkarni.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "In  Proceedings of the 21st International Middleware Conference . 356–370. Xanadu: Mitigating cascading cold starts in serverless function chain deploy- ments.",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    },
    {
      "text": "2017. Markov chains: From Theory to Implementation and Experimentation . John Wiley & Sons.",
      "type": "sliding_window_shuffled",
      "tokens": 26,
      "augmented": true
    },
    {
      "text": "John Wiley & Sons. [29]  Yu Gan, Yanqi Zhang, Dailun Cheng, Ankitha Shetty, Priyal Rathi, Nayan Katarki, Ariana Bruno, Justin Hu, Brian Ritchken, Brendon Jackson, et al . 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "In  Proceedings of the Twenty-Fourth International Conference on Archi- tectural Support for Programming Languages and Operating Systems . 2019. An open-source benchmark suite for microservices and their hardware-software implications for cloud & edge systems.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "3–18. [30]  Arpan Gujarati, Sameh Elnikety, Yuxiong He, Kathryn S. McKinley, and Björn B. Brandenburg. In  Proceedings of the Twenty-Fourth International Conference on Archi- tectural Support for Programming Languages and Operating Systems .",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "2017. Swayam: Distributed Autoscaling to Meet SLAs of Machine Learning Inference Services with Resource Efficiency. [30]  Arpan Gujarati, Sameh Elnikety, Yuxiong He, Kathryn S. McKinley, and Björn B. Brandenburg.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "In  USENIX Middleware Conference . [31]  Jashwant Raj Gunasekaran, Prashanth Thinakaran, Mahmut Tay- lan Kandemir, Bhuvan Urgaonkar, George Kesidis, and Chita Das. Swayam: Distributed Autoscaling to Meet SLAs of Machine Learning Inference Services with Resource Efficiency.",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": "Spock: Exploiting Serverless Functions for SLO and Cost Aware Resource Procurement in Public Cloud. 2019. [31]  Jashwant Raj Gunasekaran, Prashanth Thinakaran, Mahmut Tay- lan Kandemir, Bhuvan Urgaonkar, George Kesidis, and Chita Das.",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "199–208. Spock: Exploiting Serverless Functions for SLO and Cost Aware Resource Procurement in Public Cloud. In  2019 IEEE 12th Interna- tional Conference on Cloud Computing (CLOUD) .",
      "type": "sliding_window_shuffled",
      "tokens": 54,
      "augmented": true
    },
    {
      "text": "199–208. 2020. https: //doi.org/10.1109/CLOUD.2019.00043 [32]  Jashwant Raj Gunasekaran, Prashanth Thinakaran, Nachiappan C Nachiappan, Mahmut Taylan Kandemir, and Chita R Das.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "In  Proceedings of the 21st International Middleware Conference . Fifer: Tackling Resource Underutilization in the Serverless Era. 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 33,
      "augmented": true
    },
    {
      "text": "[33]  Eric Jonas, Johann Schleier-Smith, Vikram Sreekanti, Chia-Che Tsai, Anurag Khandelwal, Qifan Pu, Vaishaal Shankar, Joao Carreira, Karl Krauth, Neeraja Yadwadkar, et al . In  Proceedings of the 21st International Middleware Conference . 280–295.",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "[33]  Eric Jonas, Johann Schleier-Smith, Vikram Sreekanti, Chia-Che Tsai, Anurag Khandelwal, Qifan Pu, Vaishaal Shankar, Joao Carreira, Karl Krauth, Neeraja Yadwadkar, et al . 2019. Cloud programming sim- plified: A berkeley view on serverless computing.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "Cloud programming sim- plified: A berkeley view on serverless computing. arXiv preprint arXiv:1902.03383  (2019). [34]  Ram Srivatsa Kannan, Lavanya Subramanian, Ashwin Raju, Jeongseob Ahn, Jason Mars, and Lingjia Tang.",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "2019. GrandSLAm: Guaranteeing SLAs for Jobs in Microservices Execution Frameworks. [34]  Ram Srivatsa Kannan, Lavanya Subramanian, Ashwin Raju, Jeongseob Ahn, Jason Mars, and Lingjia Tang.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "In  EuroSys . [35]  Kate Keahey, Jason Anderson, Zhuo Zhen, Pierre Riteau, Paul Ruth, Dan Stanzione, Mert Cevik, Jacob Colleran, Haryadi S. Gunawi, Cody Hammock, Joe Mambretti, Alexander Barnes, François Halbach, Alex Rocha, and Joe Stubbs. GrandSLAm: Guaranteeing SLAs for Jobs in Microservices Execution Frameworks.",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "2020. [35]  Kate Keahey, Jason Anderson, Zhuo Zhen, Pierre Riteau, Paul Ruth, Dan Stanzione, Mert Cevik, Jacob Colleran, Haryadi S. Gunawi, Cody Hammock, Joe Mambretti, Alexander Barnes, François Halbach, Alex Rocha, and Joe Stubbs. Lessons Learned from the Chameleon Testbed.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "USENIX Association. Lessons Learned from the Chameleon Testbed. In  Proceedings of the 2020 USENIX Annual Technical Conference (USENIX ATC ’20) .",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    },
    {
      "text": "USENIX Association. 2018. [36]  Bernhard Korte and Jens Vygen.",
      "type": "sliding_window_shuffled",
      "tokens": 23,
      "augmented": true
    },
    {
      "text": "2018. Bin-Packing. In  Combinatorial Optimization .",
      "type": "sliding_window_shuffled",
      "tokens": 14,
      "augmented": true
    },
    {
      "text": "Springer, 489–507. [37]  Jörn Kuhlenkamp, Sebastian Werner, and Stefan Tai. In  Combinatorial Optimization .",
      "type": "sliding_window_shuffled",
      "tokens": 35,
      "augmented": true
    },
    {
      "text": "2020. The ifs and buts of less is more: a serverless computing reality check. [37]  Jörn Kuhlenkamp, Sebastian Werner, and Stefan Tai.",
      "type": "sliding_window_shuffled",
      "tokens": 42,
      "augmented": true
    },
    {
      "text": "IEEE, 154–161. In  2020 IEEE International Conference on Cloud Engineering (IC2E) . The ifs and buts of less is more: a serverless computing reality check.",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "[38]  Anup Mohan, Harshad Sane, Kshitij Doshi, Saikrishna Edupuganti, Naren Nayak, and Vadim Sukhomlinov. IEEE, 154–161. 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 60,
      "augmented": true
    },
    {
      "text": "In  11th  { USENIX }  Workshop on Hot Topics in Cloud Computing (HotCloud 19) . Agile cold starts for scalable serverless. 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 38,
      "augmented": true
    },
    {
      "text": "2018. [39]  Edward Oakes, Leon Yang, Dennis Zhou, Kevin Houck, Tyler Harter, Andrea Arpaci-Dusseau, and Remzi Arpaci-Dusseau. In  11th  { USENIX }  Workshop on Hot Topics in Cloud Computing (HotCloud 19) .",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "2018. In USENIX ATC . SOCK: Rapid Task Provisioning with Serverless-Optimized Containers.",
      "type": "sliding_window_shuffled",
      "tokens": 28,
      "augmented": true
    },
    {
      "text": "[40]  Haoran Qiu, Subho S Banerjee, Saurabh Jha, Zbigniew T Kalbarczyk, and Ravishankar K Iyer. 2020. In USENIX ATC .",
      "type": "sliding_window_shuffled",
      "tokens": 58,
      "augmented": true
    },
    {
      "text": "{ FIRM } : An Intelligent Fine-grained Resource Management Framework for SLO-Oriented Microservices. In  14th  { USENIX }  Symposium on Operating Systems Design and Imple- mentation ( { OSDI }  20) . 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 67,
      "augmented": true
    },
    {
      "text": "In  14th  { USENIX }  Symposium on Operating Systems Design and Imple- mentation ( { OSDI }  20) . 805–825. 166 \nKraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC ’21, November 1–4, 2021, Seattle, WA, USA \n[41]  Mohammad Shahrad, Jonathan Balkind, and David Wentzlaff.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "2019. Architectural implications of function-as-a-service computing. 166 \nKraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC ’21, November 1–4, 2021, Seattle, WA, USA \n[41]  Mohammad Shahrad, Jonathan Balkind, and David Wentzlaff.",
      "type": "sliding_window_shuffled",
      "tokens": 79,
      "augmented": true
    },
    {
      "text": "1063–1075. In  Pro- ceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture . Architectural implications of function-as-a-service computing.",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "1063–1075. 2020. [42]  Mohammad Shahrad, Rodrigo Fonseca, Íñigo Goiri, Gohar Chaudhry, Paul Batum, Jason Cooke, Eduardo Laureano, Colby Tresness, Mark Russinovich, and Ricardo Bianchini.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "2020. In  2020  { USENIX }  Annual Technical Conference ( { USENIX }{ ATC }  20) . 205–218.",
      "type": "sliding_window_shuffled",
      "tokens": 39,
      "augmented": true
    },
    {
      "text": "2020. [43]  Paulo Silva, Daniel Fireman, and Thiago Emmanuel Pereira. 205–218.",
      "type": "sliding_window_shuffled",
      "tokens": 29,
      "augmented": true
    },
    {
      "text": "Prebaking Functions to Warm the Serverless Cold Start. 2020. In  Proceedings of the 21st International Middleware Conference .",
      "type": "sliding_window_shuffled",
      "tokens": 29,
      "augmented": true
    },
    {
      "text": "In  Proceedings of the 21st International Middleware Conference . 1–13. [44]  Arjun Singhvi, Kevin Houck, Arjun Balasubramanian, Mo- hammed Danish Shaikh, Shivaram Venkataraman, and Aditya Akella.",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "2019. Archipelago: A scalable low-latency serverless platform. [44]  Arjun Singhvi, Kevin Houck, Arjun Balasubramanian, Mo- hammed Danish Shaikh, Shivaram Venkataraman, and Aditya Akella.",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "Archipelago: A scalable low-latency serverless platform. [45]  Davide Taibi, Nabil El Ioini, Claus Pahl, and Jan Raphael Schmid Niederkofler. arXiv preprint arXiv:1911.09849  (2019).",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "[45]  Davide Taibi, Nabil El Ioini, Claus Pahl, and Jan Raphael Schmid Niederkofler. 2020. Patterns for Serverless Functions (Function-as- a-Service): A Multivocal Literature Review..",
      "type": "sliding_window_shuffled",
      "tokens": 67,
      "augmented": true
    },
    {
      "text": "181–192. Patterns for Serverless Functions (Function-as- a-Service): A Multivocal Literature Review.. In  CLOSER .",
      "type": "sliding_window_shuffled",
      "tokens": 42,
      "augmented": true
    },
    {
      "text": "2020. 181–192. [46]  Ali Tariq, Austin Pahl, Sharat Nimmagadda, Eric Rozner, and Siddharth Lanka.",
      "type": "sliding_window_shuffled",
      "tokens": 42,
      "augmented": true
    },
    {
      "text": "2020. Sequoia: Enabling quality-of-service in serverless com- puting. In  Proceedings of the 11th ACM Symposium on Cloud Computing .",
      "type": "sliding_window_shuffled",
      "tokens": 39,
      "augmented": true
    },
    {
      "text": "311–327. In  Proceedings of the 11th ACM Symposium on Cloud Computing . [47]  Prashanth Thinakaran, Jashwant Raj Gunasekaran, Bikash Sharma, Mahmut Taylan Kandemir, and Chita R. Das.",
      "type": "sliding_window_shuffled",
      "tokens": 67,
      "augmented": true
    },
    {
      "text": "2017. Phoenix: A \nConstraint-Aware Scheduler for Heterogeneous Datacenters. [47]  Prashanth Thinakaran, Jashwant Raj Gunasekaran, Bikash Sharma, Mahmut Taylan Kandemir, and Chita R. Das.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "977–987. In  2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS) . Phoenix: A \nConstraint-Aware Scheduler for Heterogeneous Datacenters.",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": "2019. https://doi.org/10.1109/ICDCS.2017.262 [48]  Prashanth Thinakaran, Jashwant Raj Gunasekaran, Bikash Sharma, Mahmut Taylan Kandemir, and Chita R. Das. 977–987.",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "In  2019 IEEE International Conference on Cluster Computing (CLUSTER) . 2019. Kube-Knots: Re- source Harvesting through Dynamic Container Orchestration in GPU- based Datacenters.",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    },
    {
      "text": "In  2019 IEEE International Conference on Cluster Computing (CLUSTER) . 1–13. https://doi.org/10.1109/CLUSTER.2019.",
      "type": "sliding_window_shuffled",
      "tokens": 35,
      "augmented": true
    },
    {
      "text": "2009. https://doi.org/10.1109/CLUSTER.2019. 8891040 [49]  Guido Urdaneta, Guillaume Pierre, and Maarten Van Steen.",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "Computer Networks  (2009). 2009. Wikipedia workload analysis for decentralized hosting.",
      "type": "sliding_window_shuffled",
      "tokens": 16,
      "augmented": true
    },
    {
      "text": "2018. [50]  Liang Wang, Mengyuan Li, Yinqian Zhang, Thomas Ristenpart, and Michael Swift. Computer Networks  (2009).",
      "type": "sliding_window_shuffled",
      "tokens": 39,
      "augmented": true
    },
    {
      "text": "2018. Peeking Behind the Curtains of Serverless Plat- forms. In  ATC .",
      "type": "sliding_window_shuffled",
      "tokens": 22,
      "augmented": true
    },
    {
      "text": "[51]  Hailong Yang, Quan Chen, Moeiz Riaz, Zhongzhi Luan, Lingjia Tang, and Jason Mars. In  ATC . 2017.",
      "type": "sliding_window_shuffled",
      "tokens": 45,
      "augmented": true
    },
    {
      "text": "In  Computer Architecture News . PowerChief: Intelligent power allocation for multi-stage applications to improve responsiveness on power con- strained CMP. 2017.",
      "type": "sliding_window_shuffled",
      "tokens": 35,
      "augmented": true
    },
    {
      "text": "2018. [52]  Yiming Zhang, Jon Crowcroft, Dongsheng Li, Chengfen Zhang, Huiba Li, Yaozheng Wang, Kai Yu, Yongqiang Xiong, and Guihai Chen. In  Computer Architecture News .",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "In  2018 USENIX Annual Technical Conference . 2018. KylinX: a dynamic library operating system for simplified and efficient cloud virtualization.",
      "type": "sliding_window_shuffled",
      "tokens": 32,
      "augmented": true
    },
    {
      "text": "167 173– 186. In  2018 USENIX Annual Technical Conference .",
      "type": "sliding_window_shuffled",
      "tokens": 20,
      "augmented": true
    }
  ]
}