=== ORIGINAL PDF: 2506.16903v1_RCNet_ΔΣ_IADCs_as_Recurrent_AutoEncoders.pdf ===\n\nRaw text length: 26717 characters\nCleaned text length: 26439 characters\nNumber of segments: 17\n\n=== CLEANED TEXT ===\n\narXiv:2506.16903v1 [cs.AR] 20 Jun 2025 1 RCNet: Σ IADCs as Recurrent AutoEncoders Arnaud Verdant, William Guicquero and J erˆome Chossat Abstract This paper proposes a deep learning model (RCNet) for Delta-Sigma ( Σ) ADCs. Recurrent Neural Networks (RNNs) allow to describe both modulators and filters. This analogy is applied to Incremental ADCs (IADC). High-end optimizers combined with full-custom losses are used to define additional hardware design constraints: quantized weights, signal saturation, temporal noise injection, devices area. Focusing on DC conversion, our early results demonstrate that SNR defined as an Effective Number Of Bits (ENOB) can be optimized under a certain hardware mapping complexity. The proposed RCNet succeeded to provide design tradeoffs in terms of SNR ( 13bit) versus area constraints ( 14pF total capacitor) at a given OSR (80 samples). Interestingly, it appears that the best RCNet architectures do not necessarily rely on high-order modulators, leveraging additional topology exploration degrees of freedom. Index Terms Incremental Analog to Digital Converter, Delta-Sigma, Recurrent Neural Networks, Compter-Aided Design. I. INTRODUCTION The emergence of versatile deep learning frameworks is leading to the rise of data-driven optimization techniques. The human expertise therefore consists in stating the problem, defining the search space and setting input-output mappings. It also focuses on the computational graph, loss functions and regularization terms. This article deals with Σ IADC architectures, relying on this hardware-aware design approach. Arnaud Verdant, William Guicquero are with CEA-LETI, Univ. Grenoble Alpes, France J erˆome Chos- sat is with STMicroelectronics, Grenoble, France. This work is part of the IPCEI Microelectronics and Connectivity and was supported by the French Public Authorities within the frame of France 2030. 2 A. RCNet: the proposed RAE-IADC analogy The presented RCNet methodology relies on the analogy between an IADC based on Σ modulation [1] [4] and a Recurrent AutoEncoder (RAE) (see Fig. 1). RAE are efficient for a wide range of processing tasks, such as outlier detection in time series [5] [6], denoising [7], classification [8] or even compression [9]. This methodology requires defining the hardware exploration space of the RAE model. RAE training for specific conversion task involves to trans- late the hardware performance metrics (e.g., quantization error), the internal signal constraints (e.g., dynamic range excursion), and the design robustness specifications (e.g., against noise and components mismatches) into the deep learning framework as fidelity and regularization losses [10], activation functions [11], weights quantization functions [12] and data-augmentation layers [13]. ΔΣ Modulator ΔΣ Filter ADC DAC 𝑥 𝜉 𝜁 RNN Encoder 𝑥[𝑛] 𝜁[𝑛] L-level 01101 10 11 01 𝑦 Recurrent Auto-Encoder (RAE) Incremental Delta-Sigma ADC (IADC) 𝑦[𝑛] RNN Decoder (𝑥𝑎) (𝑥𝑞) Fig. 1: RCNet: Analogy between Incremental Σ ADC (IADC) and Recurrent Auto-Encoder (RAE). B. Prior work on deep learning for ADC Deep learning constitutes a promising avenue to improve performance and robustness for ad- vanced mixed-signal circuit design. Several approaches have already been proposed for Analog- to-Digital Converters (ADC). Indeed, AI-assisted techniques can ease to mitigate hardware non- idealities in [14] [16], or to optimize system-level parameters in [17] [19]. Neural Networks (NN) inspired topologies were also proposed [20], where an analog network is optimized to approximate any quantization function that provides a binary encoding of the analog input. Additional works such as [21], [22] have also investigated the approximation of any quantization function while taking into account hardware nonidealities into the training stage. Neuromorphic data converters using memristor arrays even enable mismatch self-calibration [23], [24]. 3 Nevertheless, prior work has focused on the hardware mapping of NNs rather than of modelling design variants, thus limiting the degrees of freedom. To further assist the mixed-signal archi- tecture design process, our method extends the search space while considering hardware-related constraints and limitations. Consequently, our computer-aided design approach differs from the state-of-the-art [17] [19], [25]. Here, relying on the IADC and RAE equivalence, the conversion process directly corresponds to a trained NN model. It offers a vast and agnostic topology exploration to identify the architecture that best maps a set of inputs to their corresponding outputs, compliant with predefined hardware-related specifications. C. Main contributions and outline Our two main contributions are the design and optimization of complex high-order IADC architectures offered by RCNet. This Σ IADC - RAE analogy combined with the definition of NN constraints and regularizations eases the optimizer for converter topology exploration en- hancement, taking into account various hardware requirements. Section II presents the proposed analogy between IADC and RAE with submodel component cells, Section III illustrates how to consider hardware-related issues for RAE model design and training, and Section IV reports simulation results for DC conversion. II. Σ ADC AS A RECURRENT AUTOENCODER Given the recursive nature of Σ converters, the underlying idea of this work is to consider such mixed-signal structures as RAE. As depicted in Figure 1, the Σ modulator can be modeled as a RNN Encoder outputting a digital sequence ζ depending on the analog input signal, while the digital filter part is similar to a RNN Decoder that enables to infer the target latent information. The proposed method relies on a supervised training based on a synthetic sample dataset. This way, the Σ converter parameters are optimized to maximize certain performance metrics. It consists in finding optimal configurations given input-output pairs, under implementation specifications (i.e., regularizations and constraints on weights and activations) and conversion performances (i.e., custom regression losses). A. First-order Σ IADC: the preliminary study An analog recursive filter combined to a quantizer constitutes a Σ modulator. It aims at shaping an oversampled continuous band-limited analog signal into a digital sequence. The key 4 idea is the ability to reject the quantization noise out of the signal-of-interest band, allowing the use of a binary quantizer (L 2 levels, as denoted in [2]). The digital filter then provides estimations y of signal x using the binary internal sequence ζ. Figure 2 illustrates a first-order modulator, composed of an analog integrator and a 1-bit quantizer, combined to a first-order digital integrator acting as a low-pass filter. ADC DAC 𝑥 𝜉 𝜁 𝑦[𝑛] 𝜁[𝑛] 2-level - 𝑧 1 𝑊𝜁 𝑥[𝑛] 𝜉[𝑛 1] 𝜁[𝑛 1] 𝑧 1 0.5 sign( ) 𝜉[𝑛] 𝑊𝑥 𝑊𝜉 Ω𝜁 Ω𝑦 𝑦[𝑛 1] 𝑦 0.5 (𝑥𝑎) (𝑥𝑞) Recurrent Auto-Encoder (RAE) Incremental Delta-Sigma ADC (IADC) Fig. 2: First-order IADC and its RAE equivalence. After the initialization (integrator and digital counter resets), a first-order Σ IADC performs the following three operations at each conversion step, repeated N times (i.e., the Over Sampling Ratio, OSR): (x ζ) is integrated to provide ξ, then ξ is quantized to update ζ and finally ζ feeds a digital filter to compute the output y. In practice, this structure relies on internal signals constrained into a dynamic range centered around the quantizer threshold. This is made possible thanks to the negative feedback loop driven by the sign of the output of the integrator ζ. Equations (1) and (2) mathematically express the time domain behavior of the recursive operations, given the following four assumptions: n 1, x[p] 1 2, ζ[p] 1 2 for p 0, and ζ[0] 0. ξ[n] 1 2 n X p 1 x[p] n 1 X p 0 ζ[p] ! (1) ζ[n] 1 2sign(ξ[n]) (2) 5 For a DC conversion of the analog level xa, the quantized signal y[N] xq outputted after N conversion steps is then expressed in (3) as the mean of the bitstream. xq PN n 1 ζ[n] N (3) Figure 2 illustrates the equivalent RNN cell of the modulator, acting as described in equation (4) and (2) for a single time-step. This cell takes as input x[n] (current input), ξ[n 1] and ζ[n 1] (hidden states), and outputs ξ[n] and ζ[n]. ξ[n] W x [n] Wxx[n] Wζζ[n 1] Wξξ[n 1] (4) A first-order modulator is obtained choosing Wx 0.5, Wζ 0.5 and Wξ 1, where W [Wx, Wζ, Wξ] R1 3 and x [n] [x[n]; ζ[n 1]; ξ[n 1]] R3 1. On the other hand, the filter topology of the decoder is based on a SimpleRNN (SRNN) cell presented in Figure 2, performing the bitstream accumulation. For such a simple topology, the recurrent weight and input weight respectively are Ωζ 1 and Ωy 1 N, in order to provide a properly scaled output. B. High-order Σ IADC: the RCNet generalization For a smaller quantization error (i.e., the distance between xa and xq) at a given OSR, con- verters may take advantage of the features offered by high-order IADC. High-order modulators consisting in a cascade of integrators increases the order of the Noise Transfer Function (NTF), at the cost of stability issues. Using the proposed deep learning formalism, the high order structure exposed in Figure 3 may benefit from a more flexible description based on sub-cell components. 𝑦𝑛 encoder cell 1 encoder cell 2 encoder cell 𝐾 𝒙1 𝑛 𝒙𝐾 𝑛 decoder cell 𝐾 decoder cell 𝐾- 1 decoder cell 1 𝒚1 𝑛 𝜻𝑛 (𝑥𝑎) (𝑥𝑞) Fig. 3: Full model top level description using sub-cells. 1) Encoding Modulator: To extend the degrees of freedom of the exploration process, we introduce a RNN cell that performs the inner product between inputs and internal weights to compute delayed and non-delayed outputs (respectively ξ and ξ), as well as their delayed and non-delayed quantized counterparts (respectively ζ and ζ). These 4 outputs typically aim to 6 describe all the possible intra-cycle or inter-cycle types of layer connections embedded inside a modulator stage: Intra-cycle: ξ: non delayed stage-to-stage transfer during a conversion cycle ζ: quantized ξ (typ. L 2), Inter-cycle: ξ : delayed stage-to-stage transfer between successive conversion cycles, ζ : quantized ξ (typ. L 2). Figure 4 details the inner product performed at each k RNN cell, with the common input notation x [n] R4K 1 and its layer-specific set of weights W k R1 (4K 1). Note that the cat operator defines a vertical concatenation. ζ and ξ variables are sequentially updated during intra-cycle operations only (not from a cycle to the next). At the beginning each cycle, ζ and ξ are set to ζ and ξ , therefore giving for k [[1, K]]: ζk[n] ζ k[n] and ξk[n] ξ k[n]. The overall modulator s weights matrix W RK (4K 1), for K 3, is reported as an example in equation (5). W [W 1; W 2; W 3] (5) W01 Wζ11 Wξ11 Wζ 11 Wξ 11 Wζ21 Wξ21 Wζ 21 Wξ 21 Wζ31 Wξ31 Wζ 31 Wξ 31 W02 Wζ12 Wξ12 Wζ 12 Wξ 12 Wζ22 Wξ22 Wζ 22 Wξ 22 Wζ32 Wξ32 Wζ 32 Wξ 32 W03 Wζ13 Wξ13 Wζ 13 Wξ 13 Wζ23 Wξ23 Wζ 23 Wξ 23 Wζ33 Wξ33 Wζ 33 Wξ 33 This generic representation offers the possibility to explore a wide range of modulator topology configurations, in which each cell has access to every cell outputs at its inputs. 2) Decoding Filter: The considered filter topology consists here in a cascade of SRNN cells, as exposed in Figure 4. The first cell performs the accumulation of the incoming sequence ζK, and each of the other cascaded cells performs the accumulation of the previous cell output. The output of each cell can be scaled to ensure a maximum normalized output for a sequence of N cycles (OSR). In addition, in order to enable a scaled signal reconstruction at each cycle n, an additional normalization layer is added at the decoder output. The optimization process can thus be operated taking into account intermediate sequence lengths (i.e., n N). III. RCNET HARDWARE CONSTRAINTS MODELING To properly formulate the optimization problem, a set of hardware-related constraints and requirements has been identified, as detailed in this section and summarized in Table I. 7 cat 𝑥𝑛; cat𝑝 1,𝐾 𝜁𝑝[𝑛] 𝜉𝑝[𝑛] 𝜁𝑝 [𝑛] 𝜉𝑝 [𝑛] 𝒙𝑘 𝑛 𝜁𝑘[𝑛] 𝜉𝑘[𝑛] 𝜁𝑘 [𝑛 1] 𝜉𝑘 [𝑛 1] 0.5 sign( ) 𝑾𝑘. 𝒚𝑘 𝑛 1 𝒚𝑘 1 𝑛 𝒚𝑘 𝑛 𝛀𝑘 ℎ𝑦. 𝒃𝑘 ℎ decoder cell 𝐾 encoder cell 𝐾 0.5 sign( ) Fig. 4: RCNet - Encoder-Decoder cell description. A. Fidelity term optimization and stability regularization A multi-objective loss F is defined to both consider the regression problem aiming at mini- mizing the error between the input signal (xa) and its quantized counterpart (xq), so as managing stability and saturation issues. Our log-domain fidelity loss FLSE whose goal is to match outputs with input signals (respectively xq RS and xa RS, with S a number of training samples, equation (6)) mimics and relaxes the behavior of a max distance (i.e., a Linf norm) operator by computing the log-sum-exp of the conversion error. The additional regularization term FDR (equation (7)) adds a penalization in case of internal signals voltage excursions beyond given bounds ( δk). This regularization caps the internal dynamic range to avoid saturation issues and limit signals distortion. To this end, it considers end-of-conversion inter-stage outputs ξ RS K corresponding to the concatenation of K ξk RS. This additional term advantageously ensures the stability of the recurrent structure at a given OSR N, using the clip δk() function that trims values at input thresholds ( δk), assigning outside values to boundaries. The loss function F(xa, xq, ξ) FLSE(xa, xq, ξ) λDRFDR(ξ), λDR R will be computed for each batch made of S samples, in which the modulator topology involves K stage cells. FLSE(xa, xq, ξ) log log S X s 1 e xa[s] xq[s] !! (6) FDR(ξ) S X s 1 K X k 1 (ξk[s] clip δk (ξk[s]))2 (7) 8 B. Signal saturation data activation In addition to FDR used to ensure the stability of the recurrent network while favoring a proper output scaling of the modulator stages outputs, an activation is introduced to saturate the internal signals ξ and ξ exceeding a maximum allowed amplitude. It consists in a Hardtanh layer that clips tensor values at saturation levels ( δk). C. Encoder structure sparsity The encoder structure sparsity is emphasized by masking latent modulator weights, simplifying the inter-layer connection map of the resulting hardware structure. A pointwise product is applied to the latent weights W l using a learned binary mask M {0, 1}K (4K 1). M is derived from the strict heaviside of a learned latent mask M l whose size is the same as W l. To measure this degree of sparsity, the number of Active encoder Paths AP denotes the number of non-zero W weights. D. Hardware layout strategy Since discrete-time Σ modulators are commonly based on switched capacitors implementa- tions, this paper proposes an adequate sizing that favors a low common denominator for all weight values. To this end, a Quantization Aware Training (QAT) [26] is employed to learn optimized quantized weights W , being derived from latent weights W l and M. The weights are thus uniformly quantized on Q absolute levels, providing W i N. The latent weights quantization is performed during the feedforward phase, combined with a Straigh-Through Estimator for the gradient. The proposed quantization function uses the rounding to the nearest integer round() function. The quantization step q R is a learned latent scaling, as detailed in equation (8). W q clip Q round M W l q q W i (8) E. Thermal noise To increase the IADC robustness to temporal non-idealities, an Additional White Gaussian Noise (AWGN) is considered for in-model data augmentation activated during the training process at each internal node of the modulator. To that end, a learnable unit capacitor Ck is assigned to each stage of the encoder. The resulting capacitors associated to each encoder stage 9 are obtained by multiplying Ck with respect to the W i values. The kTC noise sources due to signals sampling on capacitors can thus be simulated for each weight of the encoder. Note that this data augmentation is also activated for SNR metric evaluation for validation test results, and deactivated for SQNR. F. Hardware silicon surface reduction An additional loss (weighted by λTPT) linearly penalizes the topologies whose the sum of capacitors (CTot) exceeds a Total Capacitor Threshold (TPT). This way, the learning process jointly optimizes the unit capacitors Ck with the quantized encoder weights W , promoting a balance between the silicon surface and the encoder noise robustness. TABLE I: Hardware-aware deep learning modeling. HARDWARE FEATURES DEEP LEARNING MODEL Conversion quantization error Minimize maximum conversion error Input-Output fidelity loss LogSumExp (LSE) loss (FLSE) Modulator Stability Ensure dynamic range, limit swing Activity regularization L2 on skimmed signal (FDR) Operational amplifier output swing Avoid analog signal saturation Custom Activation Clip with STE (i.e., Hardtanh) Layout implementation Define a single unitary capacitor Quantization Aware Training Linear weight quantization Thermal (kTC) Noise robustness Optimize modulator noise rejection Data-Augmentation Signal AWGN wrt. capacitor size Silicon surface and design complexity Limit analytical modulator order Weight regularization Favor a sparse W IV. CASE STUDY: DC CONVERSION USING IADC RCNet has been trained a large number of runs ( 2000), under configurations listed in Table II, with a random initialization and decoder finetuning. To ensure a proper and stable training, the training data are generated using a uniform distribution. Figure 5 illustrates the resulting SQNR and SNR, both defined as an Effective Number Of Bits (ENOB), depending on the sum of capacitors. Each training realization is reported as a colored dot depending on the weights quantization level, for different number of cells K. As expected, the smaller the Q the worst the 10 performance. Nevertheless, SNR results for Q 8 (orange) and Q 32 (green) show that, despite weight quantization, our proposed strategy still provides better configurations compared to a CIFF IADC baseline [1] (black). The best SNRs are encircled. Our results demonstrate a high variability depending on the model initialization. We can still observe the positive influence of the total capacitor on SNR, as well as the ability to find topologies with a small CTot. TABLE II: RCNet model parameters for our simulations. MODEL PARAMETER TYPE SET VALUES Encoder (modulator) sub-cells (K) {2, 3, 4} maximum OSR 80 Input Dynamic Range (xa) [ 0.35, 0.35] In-encoder saturation bounds (δk) 0.4 DAC analog levels { 0.5, 0.5} Weights quantization levels (Q) {4, 8, 32} Loss balancing (λDR, λT P T ) 0.01, 0.0001 Total Capacitor Threshold (TPT) {4, 8, 16, 32} Temporal sampling noise Cell-wise optimized kT C In Figure 6, the Effective Number of Integration Stages (ENIS) characterizes the hardware complexity associated to each learned RCNet. On the other hand, Figure 7 reports the SNR as a function of AP. A small Q limits the ability to explore complex configurations. To compute ENIS, a stage k defines an integration stage if one of the weights Wξkk or Wξ kk is different from zero. Figure 6 demonstrates that the best SNRs are not necessarily linked to the highest total capacitor or number of active paths (even ENIS K), demonstrating the ability of our learning strategy to balance design constraints versus the SNR. The average ENOB per cycle with respect to the SNR at OSR 80 (right column, Figure 6) attempts to characterize each topology depending on its convergence trend, highlighting the conversion speed. While a low ENIS corresponds to a low-order architecture, which requires a larger OSR to reach a given SNR, it appears here that these kind of topologies can even compete with higher-order. It denotes the ability of this strategy to provide unconventional signal shaping such as exponential- like decay integration scheme [27] [29], which enables the reduction of quantization noise for a given OSR. 11 Fig. 5: SQNR and SNR at OSR 80 for K {2, 3, 4} and for Q {4, 8, 32}, as a function of the total capacitor CTot. 12 Fig. 6: Total capacitor CTot versus the number of Active encoder Paths AP, and average ENOB per cycle versus simulated SNR for OSR from 1 to 80. 13 Fig. 7: SNR for different encoder orders K and weight quantization levels Q, as a function of the number of Active encoder Paths (AP). V. CONCLUSION Based on the analogy between an IADC and a RAE, our proposed RCNet deep learning methodology enables an hardware-aware topology exploration, jointly optimizing different design specifications. The proposed method does not rely on any a priori assumptions and can provide architectures offering unique tradeoffs between SNR and hardware constraints. It has also been found that this strategy converges towards topologies that take advantage of unconventional signal shaping. It thus constitutes a powerful tool to investigate innovative topologies difficult to size using standard analytical approaches. Future works will extend to converters aiming at directly estimating latent variables (such as signal frequency), constituting a step towards Analog-to-Information converters. REFERENCES [1] J. Markus et al., Incremental Delta-Sigma Structures for DC Measurement: an Overview, in IEEE Custom Integrated Circuits Conference 2006, Sep. 2006, pp. 41 48. [2] Z. Tan et al., Incremental Delta-Sigma ADCs: A Tutorial Review, IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 67, no. 12, pp. 4161 4173, Dec. 2020. [3] J. Markus et al., Theory and applications of incremental Σ converters, IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 51, no. 4, pp. 678 690, Apr. 2004. [4] J. M. de la Rosa, Sigma-Delta Modulators: Tutorial Overview, Design Guide, and State-of-the-Art Survey, IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 58, no. 1, pp. 1 21, Jan. 2011. 14 [5] T. Kieu et al., Outlier Detection for Time Series with Recurrent Autoencoder Ensembles, in International Joint Conference on Artificial Intelligence, 2019, pp. 2725 2732. [6] T. Xie et al., Anomaly detection for multivariate times series through the multi-scale convolutional recurrent variational autoencoder, Expert Systems with Applications, vol. 231, p. 120725, Nov. 2023. [7] F. Weninger et al., Deep recurrent de-noising auto-encoder and blind de-reverberation for reverberated speech recognition, in IEEE International Conference on Acoustics, Speech and Signal Processing, May 2014, pp. 4623 4627. [8] B. Hou et al., LSTM-Based Auto-Encoder Model for ECG Arrhythmias Classification, IEEE Transactions on Instru- mentation and Measurement, vol. 69, no. 4, pp. 1232 1240, Apr. 2020. [9] R. Yang et al., Learning for Video Compression With Recurrent Auto-Encoder and Recurrent Probability Model, IEEE Journal of Selected Topics in Signal Processing, vol. 15, no. 2, pp. 388 401, Feb. 2021. [10] Q. Wang et al., A Comprehensive Survey of Loss Functions in Machine Learning, Annals of Data Science, vol. 9, no. 2, pp. 187 212, Apr. 2022. [11] S. R. Dubey et al., Activation functions in deep learning: A comprehensive survey and benchmark, Neurocomputing, vol. 503, pp. 92 108, Sep. 2022. [12] G. K. Thiruvathukal et al., A Survey of Quantization Methods for Efficient Neural Network Inference, pp. 291 326, Jan. 2022, book Title: Low-Power Computer Vision Edition: 1 ISBN: 9781003162810 Place: Boca Raton Publisher: Chapman and Hall CRC. [13] A. Mumuni et al., Data augmentation: A comprehensive survey of modern approaches, Array, vol. 16, p. 100258, Dec. 2022. [14] S. Xu et al., Analog-to-digital conversion revolutionized by deep learning, arXiv: Signal Processing, 2018. [15] A. Samiee et al., Deep analog-to-digital converter for wireless communication, in AI and Optical Data Sciences II, K.-i. Kitayama et al., Eds. Online Only, United States: SPIE, Mar. 2021, p. 44. [16] S. Bansal et al., Neural-Network Based Self-Initializing Algorithm for Multi-Parameter Optimization of High-Speed ADCs, IEEE Transactions on Circuits and Systems II: Express Briefs, vol. 68, no. 1, pp. 106 110, Jan. 2021. [17] J. M. de la Rosa, AI-Assisted Sigma-Delta Converters Application to Cognitive Radio, IEEE Transactions on Circuits and Systems II: Express Briefs, vol. 69, no. 6, pp. 2557 2563, Jun. 2022. [18] M. Fayazi et al., Applications of Artificial Intelligence on the Modeling and Optimization for Analog and Mixed-Signal Circuits: A Review, IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 68, no. 6, pp. 2418 2431, Jun. 2021. [19] J.-W. Nam et al., Machine-Learning based Analog and Mixed-signal Circuit Design and Optimization, in International Conference on Information Networking, Jan. 2021, pp. 874 876. [20] D. Tank et al., Simple neural optimization networks: An A D converter, signal decision circuit, and a linear programming circuit, IEEE Transactions on Circuits and Systems, vol. 33, no. 5, pp. 533 541, 1986. [21] W. Cao et al., NeuADC: Neural Network-Inspired Synthesizable Analog-to-Digital Conversion, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 39, no. 9, pp. 1841 1854, Sep. 2020. [22] A. Tankimanova et al., Neural Network-Based Analog-to-Digital Converters, in Memristor and Memristive Neural Networks, A. P. James, Ed. InTech, Apr. 2018. [23] L. Danial et al., A Pipelined Memristive Neural Network Analog-to-Digital Converter, in IEEE International Symposium on Circuits and Systems, Oct. 2020, pp. 1 5. [24] , Delta-Sigma Modulation Neurons for High-Precision Training of Memristive Synapses in Deep Neural Networks, in IEEE International Symposium on Circuits and Systems, May 2019, pp. 1 5. 15 [25] J. Wagner et al., Man or machine design automation of delta-sigma modulators, in 2018 IEEE International Symposium on Circuits and Systems (ISCAS), 2018, pp. 1 5. [26] B. Jacob et al., Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference, in IEEE CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 2704 2713. [27] A. Verdant et al., High-linearity sigma-delta converter, Patent WOEP2016 053687, 2016. [28] W. Guicquero et al., Incremental Delta Sigma Modulation with Dynamic Weighted Integration, in IEEE International Midwest Symposium on Circuits and Systems, 2018, pp. 344 347. [29] B. Wang et al., A 550 µW 20 kHz BW 100.8 DB SNDR linear-exponential multi-bit incremental converter with 256-cycles in 65 nm CMOS, in IEEE Symp. VLSI Circuits, 2018, pp. 207 208.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2506.16903v1 [cs.AR] 20 Jun 2025 1 RCNet: Σ IADCs as Recurrent AutoEncoders Arnaud Verdant, William Guicquero and J erˆome Chossat Abstract This paper proposes a deep learning model (RCNet) for Delta-Sigma ( Σ) ADCs. Recurrent Neural Networks (RNNs) allow to describe both modulators and filters. This analogy is applied to Incremental ADCs (IADC). High-end optimizers combined with full-custom losses are used to define additional hardware design constraints: quantized weights, signal saturation, temporal noise injection, devices area. Focusing on DC conversion, our early results demonstrate that SNR defined as an Effective Number Of Bits (ENOB) can be optimized under a certain hardware mapping complexity. The proposed RCNet succeeded to provide design tradeoffs in terms of SNR ( 13bit) versus area constraints ( 14pF total capacitor) at a given OSR (80 samples). Interestingly, it appears that the best RCNet architectures do not necessarily rely on high-order modulators, leveraging additional topology exploration degrees of freedom. Index Terms Incremental Analog to Digital Converter, Delta-Sigma, Recurrent Neural Networks, Compter-Aided Design. I. INTRODUCTION The emergence of versatile deep learning frameworks is leading to the rise of data-driven optimization techniques. The human expertise therefore consists in stating the problem, defining the search space and setting input-output mappings. It also focuses on the computational graph, loss functions and regularization terms. This article deals with Σ IADC architectures, relying on this hardware-aware design approach. Arnaud Verdant, William Guicquero are with CEA-LETI, Univ. Grenoble Alpes, France J erˆome Chos- sat is with STMicroelectronics, Grenoble, France. This work is part of the IPCEI Microelectronics and Connectivity and was supported by the French Public Authorities within the frame of France 2030. 2 A. RCNet: the proposed RAE-IADC analogy The presented RCNet methodology relies on the analogy between an IADC based on Σ modulation [1] [4] and a Recurrent AutoEncoder (RAE) (see Fig. 1).\n\n--- Segment 2 ---\n2 A. RCNet: the proposed RAE-IADC analogy The presented RCNet methodology relies on the analogy between an IADC based on Σ modulation [1] [4] and a Recurrent AutoEncoder (RAE) (see Fig. 1). RAE are efficient for a wide range of processing tasks, such as outlier detection in time series [5] [6], denoising [7], classification [8] or even compression [9]. This methodology requires defining the hardware exploration space of the RAE model. RAE training for specific conversion task involves to trans- late the hardware performance metrics (e.g., quantization error), the internal signal constraints (e.g., dynamic range excursion), and the design robustness specifications (e.g., against noise and components mismatches) into the deep learning framework as fidelity and regularization losses [10], activation functions [11], weights quantization functions [12] and data-augmentation layers [13]. ΔΣ Modulator ΔΣ Filter ADC DAC 𝑥 𝜉 𝜁 RNN Encoder 𝑥[𝑛] 𝜁[𝑛] L-level 01101 10 11 01 𝑦 Recurrent Auto-Encoder (RAE) Incremental Delta-Sigma ADC (IADC) 𝑦[𝑛] RNN Decoder (𝑥𝑎) (𝑥𝑞) Fig. 1: RCNet: Analogy between Incremental Σ ADC (IADC) and Recurrent Auto-Encoder (RAE). B. Prior work on deep learning for ADC Deep learning constitutes a promising avenue to improve performance and robustness for ad- vanced mixed-signal circuit design. Several approaches have already been proposed for Analog- to-Digital Converters (ADC). Indeed, AI-assisted techniques can ease to mitigate hardware non- idealities in [14] [16], or to optimize system-level parameters in [17] [19]. Neural Networks (NN) inspired topologies were also proposed [20], where an analog network is optimized to approximate any quantization function that provides a binary encoding of the analog input. Additional works such as [21], [22] have also investigated the approximation of any quantization function while taking into account hardware nonidealities into the training stage.\n\n--- Segment 3 ---\nNeural Networks (NN) inspired topologies were also proposed [20], where an analog network is optimized to approximate any quantization function that provides a binary encoding of the analog input. Additional works such as [21], [22] have also investigated the approximation of any quantization function while taking into account hardware nonidealities into the training stage. Neuromorphic data converters using memristor arrays even enable mismatch self-calibration [23], [24]. 3 Nevertheless, prior work has focused on the hardware mapping of NNs rather than of modelling design variants, thus limiting the degrees of freedom. To further assist the mixed-signal archi- tecture design process, our method extends the search space while considering hardware-related constraints and limitations. Consequently, our computer-aided design approach differs from the state-of-the-art [17] [19], [25]. Here, relying on the IADC and RAE equivalence, the conversion process directly corresponds to a trained NN model. It offers a vast and agnostic topology exploration to identify the architecture that best maps a set of inputs to their corresponding outputs, compliant with predefined hardware-related specifications. C. Main contributions and outline Our two main contributions are the design and optimization of complex high-order IADC architectures offered by RCNet. This Σ IADC - RAE analogy combined with the definition of NN constraints and regularizations eases the optimizer for converter topology exploration en- hancement, taking into account various hardware requirements. Section II presents the proposed analogy between IADC and RAE with submodel component cells, Section III illustrates how to consider hardware-related issues for RAE model design and training, and Section IV reports simulation results for DC conversion. II. Σ ADC AS A RECURRENT AUTOENCODER Given the recursive nature of Σ converters, the underlying idea of this work is to consider such mixed-signal structures as RAE. As depicted in Figure 1, the Σ modulator can be modeled as a RNN Encoder outputting a digital sequence ζ depending on the analog input signal, while the digital filter part is similar to a RNN Decoder that enables to infer the target latent information. The proposed method relies on a supervised training based on a synthetic sample dataset. This way, the Σ converter parameters are optimized to maximize certain performance metrics.\n\n--- Segment 4 ---\nThe proposed method relies on a supervised training based on a synthetic sample dataset. This way, the Σ converter parameters are optimized to maximize certain performance metrics. It consists in finding optimal configurations given input-output pairs, under implementation specifications (i.e., regularizations and constraints on weights and activations) and conversion performances (i.e., custom regression losses). A. First-order Σ IADC: the preliminary study An analog recursive filter combined to a quantizer constitutes a Σ modulator. It aims at shaping an oversampled continuous band-limited analog signal into a digital sequence. The key 4 idea is the ability to reject the quantization noise out of the signal-of-interest band, allowing the use of a binary quantizer (L 2 levels, as denoted in [2]). The digital filter then provides estimations y of signal x using the binary internal sequence ζ. Figure 2 illustrates a first-order modulator, composed of an analog integrator and a 1-bit quantizer, combined to a first-order digital integrator acting as a low-pass filter. ADC DAC 𝑥 𝜉 𝜁 𝑦[𝑛] 𝜁[𝑛] 2-level - 𝑧 1 𝑊𝜁 𝑥[𝑛] 𝜉[𝑛 1] 𝜁[𝑛 1] 𝑧 1 0.5 sign( ) 𝜉[𝑛] 𝑊𝑥 𝑊𝜉 Ω𝜁 Ω𝑦 𝑦[𝑛 1] 𝑦 0.5 (𝑥𝑎) (𝑥𝑞) Recurrent Auto-Encoder (RAE) Incremental Delta-Sigma ADC (IADC) Fig. 2: First-order IADC and its RAE equivalence.\n\n--- Segment 5 ---\nADC DAC 𝑥 𝜉 𝜁 𝑦[𝑛] 𝜁[𝑛] 2-level - 𝑧 1 𝑊𝜁 𝑥[𝑛] 𝜉[𝑛 1] 𝜁[𝑛 1] 𝑧 1 0.5 sign( ) 𝜉[𝑛] 𝑊𝑥 𝑊𝜉 Ω𝜁 Ω𝑦 𝑦[𝑛 1] 𝑦 0.5 (𝑥𝑎) (𝑥𝑞) Recurrent Auto-Encoder (RAE) Incremental Delta-Sigma ADC (IADC) Fig. 2: First-order IADC and its RAE equivalence. After the initialization (integrator and digital counter resets), a first-order Σ IADC performs the following three operations at each conversion step, repeated N times (i.e., the Over Sampling Ratio, OSR): (x ζ) is integrated to provide ξ, then ξ is quantized to update ζ and finally ζ feeds a digital filter to compute the output y. In practice, this structure relies on internal signals constrained into a dynamic range centered around the quantizer threshold. This is made possible thanks to the negative feedback loop driven by the sign of the output of the integrator ζ. Equations (1) and (2) mathematically express the time domain behavior of the recursive operations, given the following four assumptions: n 1, x[p] 1 2, ζ[p] 1 2 for p 0, and ζ[0] 0. ξ[n] 1 2 n X p 1 x[p] n 1 X p 0 ζ[p] ! (1) ζ[n] 1 2sign(ξ[n]) (2) 5 For a DC conversion of the analog level xa, the quantized signal y[N] xq outputted after N conversion steps is then expressed in (3) as the mean of the bitstream.\n\n--- Segment 6 ---\nThis is made possible thanks to the negative feedback loop driven by the sign of the output of the integrator ζ. Equations (1) and (2) mathematically express the time domain behavior of the recursive operations, given the following four assumptions: n 1, x[p] 1 2, ζ[p] 1 2 for p 0, and ζ[0] 0. ξ[n] 1 2 n X p 1 x[p] n 1 X p 0 ζ[p] ! (1) ζ[n] 1 2sign(ξ[n]) (2) 5 For a DC conversion of the analog level xa, the quantized signal y[N] xq outputted after N conversion steps is then expressed in (3) as the mean of the bitstream. xq PN n 1 ζ[n] N (3) Figure 2 illustrates the equivalent RNN cell of the modulator, acting as described in equation (4) and (2) for a single time-step. This cell takes as input x[n] (current input), ξ[n 1] and ζ[n 1] (hidden states), and outputs ξ[n] and ζ[n]. ξ[n] W x [n] Wxx[n] Wζζ[n 1] Wξξ[n 1] (4) A first-order modulator is obtained choosing Wx 0.5, Wζ 0.5 and Wξ 1, where W [Wx, Wζ, Wξ] R1 3 and x [n] [x[n]; ζ[n 1]; ξ[n 1]] R3 1. On the other hand, the filter topology of the decoder is based on a SimpleRNN (SRNN) cell presented in Figure 2, performing the bitstream accumulation. For such a simple topology, the recurrent weight and input weight respectively are Ωζ 1 and Ωy 1 N, in order to provide a properly scaled output. B. High-order Σ IADC: the RCNet generalization For a smaller quantization error (i.e., the distance between xa and xq) at a given OSR, con- verters may take advantage of the features offered by high-order IADC.\n\n--- Segment 7 ---\nFor such a simple topology, the recurrent weight and input weight respectively are Ωζ 1 and Ωy 1 N, in order to provide a properly scaled output. B. High-order Σ IADC: the RCNet generalization For a smaller quantization error (i.e., the distance between xa and xq) at a given OSR, con- verters may take advantage of the features offered by high-order IADC. High-order modulators consisting in a cascade of integrators increases the order of the Noise Transfer Function (NTF), at the cost of stability issues. Using the proposed deep learning formalism, the high order structure exposed in Figure 3 may benefit from a more flexible description based on sub-cell components. 𝑦𝑛 encoder cell 1 encoder cell 2 encoder cell 𝐾 𝒙1 𝑛 𝒙𝐾 𝑛 decoder cell 𝐾 decoder cell 𝐾- 1 decoder cell 1 𝒚1 𝑛 𝜻𝑛 (𝑥𝑎) (𝑥𝑞) Fig. 3: Full model top level description using sub-cells. 1) Encoding Modulator: To extend the degrees of freedom of the exploration process, we introduce a RNN cell that performs the inner product between inputs and internal weights to compute delayed and non-delayed outputs (respectively ξ and ξ), as well as their delayed and non-delayed quantized counterparts (respectively ζ and ζ). These 4 outputs typically aim to 6 describe all the possible intra-cycle or inter-cycle types of layer connections embedded inside a modulator stage: Intra-cycle: ξ: non delayed stage-to-stage transfer during a conversion cycle ζ: quantized ξ (typ. L 2), Inter-cycle: ξ : delayed stage-to-stage transfer between successive conversion cycles, ζ : quantized ξ (typ. L 2). Figure 4 details the inner product performed at each k RNN cell, with the common input notation x [n] R4K 1 and its layer-specific set of weights W k R1 (4K 1). Note that the cat operator defines a vertical concatenation.\n\n--- Segment 8 ---\nFigure 4 details the inner product performed at each k RNN cell, with the common input notation x [n] R4K 1 and its layer-specific set of weights W k R1 (4K 1). Note that the cat operator defines a vertical concatenation. ζ and ξ variables are sequentially updated during intra-cycle operations only (not from a cycle to the next). At the beginning each cycle, ζ and ξ are set to ζ and ξ , therefore giving for k [[1, K]]: ζk[n] ζ k[n] and ξk[n] ξ k[n]. The overall modulator s weights matrix W RK (4K 1), for K 3, is reported as an example in equation (5). W [W 1; W 2; W 3] (5) W01 Wζ11 Wξ11 Wζ 11 Wξ 11 Wζ21 Wξ21 Wζ 21 Wξ 21 Wζ31 Wξ31 Wζ 31 Wξ 31 W02 Wζ12 Wξ12 Wζ 12 Wξ 12 Wζ22 Wξ22 Wζ 22 Wξ 22 Wζ32 Wξ32 Wζ 32 Wξ 32 W03 Wζ13 Wξ13 Wζ 13 Wξ 13 Wζ23 Wξ23 Wζ 23 Wξ 23 Wζ33 Wξ33 Wζ 33 Wξ 33 This generic representation offers the possibility to explore a wide range of modulator topology configurations, in which each cell has access to every cell outputs at its inputs. 2) Decoding Filter: The considered filter topology consists here in a cascade of SRNN cells, as exposed in Figure 4. The first cell performs the accumulation of the incoming sequence ζK, and each of the other cascaded cells performs the accumulation of the previous cell output. The output of each cell can be scaled to ensure a maximum normalized output for a sequence of N cycles (OSR). In addition, in order to enable a scaled signal reconstruction at each cycle n, an additional normalization layer is added at the decoder output. The optimization process can thus be operated taking into account intermediate sequence lengths (i.e., n N). III.\n\n--- Segment 9 ---\nThe optimization process can thus be operated taking into account intermediate sequence lengths (i.e., n N). III. RCNET HARDWARE CONSTRAINTS MODELING To properly formulate the optimization problem, a set of hardware-related constraints and requirements has been identified, as detailed in this section and summarized in Table I. 7 cat 𝑥𝑛; cat𝑝 1,𝐾 𝜁𝑝[𝑛] 𝜉𝑝[𝑛] 𝜁𝑝 [𝑛] 𝜉𝑝 [𝑛] 𝒙𝑘 𝑛 𝜁𝑘[𝑛] 𝜉𝑘[𝑛] 𝜁𝑘 [𝑛 1] 𝜉𝑘 [𝑛 1] 0.5 sign( ) 𝑾𝑘. 𝒚𝑘 𝑛 1 𝒚𝑘 1 𝑛 𝒚𝑘 𝑛 𝛀𝑘 ℎ𝑦. 𝒃𝑘 ℎ decoder cell 𝐾 encoder cell 𝐾 0.5 sign( ) Fig. 4: RCNet - Encoder-Decoder cell description. A. Fidelity term optimization and stability regularization A multi-objective loss F is defined to both consider the regression problem aiming at mini- mizing the error between the input signal (xa) and its quantized counterpart (xq), so as managing stability and saturation issues. Our log-domain fidelity loss FLSE whose goal is to match outputs with input signals (respectively xq RS and xa RS, with S a number of training samples, equation (6)) mimics and relaxes the behavior of a max distance (i.e., a Linf norm) operator by computing the log-sum-exp of the conversion error. The additional regularization term FDR (equation (7)) adds a penalization in case of internal signals voltage excursions beyond given bounds ( δk). This regularization caps the internal dynamic range to avoid saturation issues and limit signals distortion.\n\n--- Segment 10 ---\nThe additional regularization term FDR (equation (7)) adds a penalization in case of internal signals voltage excursions beyond given bounds ( δk). This regularization caps the internal dynamic range to avoid saturation issues and limit signals distortion. To this end, it considers end-of-conversion inter-stage outputs ξ RS K corresponding to the concatenation of K ξk RS. This additional term advantageously ensures the stability of the recurrent structure at a given OSR N, using the clip δk() function that trims values at input thresholds ( δk), assigning outside values to boundaries. The loss function F(xa, xq, ξ) FLSE(xa, xq, ξ) λDRFDR(ξ), λDR R will be computed for each batch made of S samples, in which the modulator topology involves K stage cells. FLSE(xa, xq, ξ) log log S X s 1 e xa[s] xq[s] ! ! (6) FDR(ξ) S X s 1 K X k 1 (ξk[s] clip δk (ξk[s]))2 (7) 8 B. Signal saturation data activation In addition to FDR used to ensure the stability of the recurrent network while favoring a proper output scaling of the modulator stages outputs, an activation is introduced to saturate the internal signals ξ and ξ exceeding a maximum allowed amplitude. It consists in a Hardtanh layer that clips tensor values at saturation levels ( δk). C. Encoder structure sparsity The encoder structure sparsity is emphasized by masking latent modulator weights, simplifying the inter-layer connection map of the resulting hardware structure. A pointwise product is applied to the latent weights W l using a learned binary mask M {0, 1}K (4K 1). M is derived from the strict heaviside of a learned latent mask M l whose size is the same as W l. To measure this degree of sparsity, the number of Active encoder Paths AP denotes the number of non-zero W weights. D. Hardware layout strategy Since discrete-time Σ modulators are commonly based on switched capacitors implementa- tions, this paper proposes an adequate sizing that favors a low common denominator for all weight values.\n\n--- Segment 11 ---\nM is derived from the strict heaviside of a learned latent mask M l whose size is the same as W l. To measure this degree of sparsity, the number of Active encoder Paths AP denotes the number of non-zero W weights. D. Hardware layout strategy Since discrete-time Σ modulators are commonly based on switched capacitors implementa- tions, this paper proposes an adequate sizing that favors a low common denominator for all weight values. To this end, a Quantization Aware Training (QAT) [26] is employed to learn optimized quantized weights W , being derived from latent weights W l and M. The weights are thus uniformly quantized on Q absolute levels, providing W i N. The latent weights quantization is performed during the feedforward phase, combined with a Straigh-Through Estimator for the gradient. The proposed quantization function uses the rounding to the nearest integer round() function. The quantization step q R is a learned latent scaling, as detailed in equation (8). W q clip Q round M W l q q W i (8) E. Thermal noise To increase the IADC robustness to temporal non-idealities, an Additional White Gaussian Noise (AWGN) is considered for in-model data augmentation activated during the training process at each internal node of the modulator. To that end, a learnable unit capacitor Ck is assigned to each stage of the encoder. The resulting capacitors associated to each encoder stage 9 are obtained by multiplying Ck with respect to the W i values. The kTC noise sources due to signals sampling on capacitors can thus be simulated for each weight of the encoder. Note that this data augmentation is also activated for SNR metric evaluation for validation test results, and deactivated for SQNR. F. Hardware silicon surface reduction An additional loss (weighted by λTPT) linearly penalizes the topologies whose the sum of capacitors (CTot) exceeds a Total Capacitor Threshold (TPT). This way, the learning process jointly optimizes the unit capacitors Ck with the quantized encoder weights W , promoting a balance between the silicon surface and the encoder noise robustness. TABLE I: Hardware-aware deep learning modeling.\n\n--- Segment 12 ---\nThis way, the learning process jointly optimizes the unit capacitors Ck with the quantized encoder weights W , promoting a balance between the silicon surface and the encoder noise robustness. TABLE I: Hardware-aware deep learning modeling. HARDWARE FEATURES DEEP LEARNING MODEL Conversion quantization error Minimize maximum conversion error Input-Output fidelity loss LogSumExp (LSE) loss (FLSE) Modulator Stability Ensure dynamic range, limit swing Activity regularization L2 on skimmed signal (FDR) Operational amplifier output swing Avoid analog signal saturation Custom Activation Clip with STE (i.e., Hardtanh) Layout implementation Define a single unitary capacitor Quantization Aware Training Linear weight quantization Thermal (kTC) Noise robustness Optimize modulator noise rejection Data-Augmentation Signal AWGN wrt. capacitor size Silicon surface and design complexity Limit analytical modulator order Weight regularization Favor a sparse W IV. CASE STUDY: DC CONVERSION USING IADC RCNet has been trained a large number of runs ( 2000), under configurations listed in Table II, with a random initialization and decoder finetuning. To ensure a proper and stable training, the training data are generated using a uniform distribution. Figure 5 illustrates the resulting SQNR and SNR, both defined as an Effective Number Of Bits (ENOB), depending on the sum of capacitors. Each training realization is reported as a colored dot depending on the weights quantization level, for different number of cells K. As expected, the smaller the Q the worst the 10 performance. Nevertheless, SNR results for Q 8 (orange) and Q 32 (green) show that, despite weight quantization, our proposed strategy still provides better configurations compared to a CIFF IADC baseline [1] (black). The best SNRs are encircled. Our results demonstrate a high variability depending on the model initialization. We can still observe the positive influence of the total capacitor on SNR, as well as the ability to find topologies with a small CTot. TABLE II: RCNet model parameters for our simulations.\n\n--- Segment 13 ---\nWe can still observe the positive influence of the total capacitor on SNR, as well as the ability to find topologies with a small CTot. TABLE II: RCNet model parameters for our simulations. MODEL PARAMETER TYPE SET VALUES Encoder (modulator) sub-cells (K) {2, 3, 4} maximum OSR 80 Input Dynamic Range (xa) [ 0.35, 0.35] In-encoder saturation bounds (δk) 0.4 DAC analog levels { 0.5, 0.5} Weights quantization levels (Q) {4, 8, 32} Loss balancing (λDR, λT P T ) 0.01, 0.0001 Total Capacitor Threshold (TPT) {4, 8, 16, 32} Temporal sampling noise Cell-wise optimized kT C In Figure 6, the Effective Number of Integration Stages (ENIS) characterizes the hardware complexity associated to each learned RCNet. On the other hand, Figure 7 reports the SNR as a function of AP. A small Q limits the ability to explore complex configurations. To compute ENIS, a stage k defines an integration stage if one of the weights Wξkk or Wξ kk is different from zero. Figure 6 demonstrates that the best SNRs are not necessarily linked to the highest total capacitor or number of active paths (even ENIS K), demonstrating the ability of our learning strategy to balance design constraints versus the SNR. The average ENOB per cycle with respect to the SNR at OSR 80 (right column, Figure 6) attempts to characterize each topology depending on its convergence trend, highlighting the conversion speed. While a low ENIS corresponds to a low-order architecture, which requires a larger OSR to reach a given SNR, it appears here that these kind of topologies can even compete with higher-order. It denotes the ability of this strategy to provide unconventional signal shaping such as exponential- like decay integration scheme [27] [29], which enables the reduction of quantization noise for a given OSR. 11 Fig. 5: SQNR and SNR at OSR 80 for K {2, 3, 4} and for Q {4, 8, 32}, as a function of the total capacitor CTot. 12 Fig.\n\n--- Segment 14 ---\n5: SQNR and SNR at OSR 80 for K {2, 3, 4} and for Q {4, 8, 32}, as a function of the total capacitor CTot. 12 Fig. 6: Total capacitor CTot versus the number of Active encoder Paths AP, and average ENOB per cycle versus simulated SNR for OSR from 1 to 80. 13 Fig. 7: SNR for different encoder orders K and weight quantization levels Q, as a function of the number of Active encoder Paths (AP). V. CONCLUSION Based on the analogy between an IADC and a RAE, our proposed RCNet deep learning methodology enables an hardware-aware topology exploration, jointly optimizing different design specifications. The proposed method does not rely on any a priori assumptions and can provide architectures offering unique tradeoffs between SNR and hardware constraints. It has also been found that this strategy converges towards topologies that take advantage of unconventional signal shaping. It thus constitutes a powerful tool to investigate innovative topologies difficult to size using standard analytical approaches. Future works will extend to converters aiming at directly estimating latent variables (such as signal frequency), constituting a step towards Analog-to-Information converters. REFERENCES [1] J. Markus et al., Incremental Delta-Sigma Structures for DC Measurement: an Overview, in IEEE Custom Integrated Circuits Conference 2006, Sep. 2006, pp. 41 48. [2] Z. Tan et al., Incremental Delta-Sigma ADCs: A Tutorial Review, IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 67, no. 12, pp. 4161 4173, Dec. 2020. [3] J. Markus et al., Theory and applications of incremental Σ converters, IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 51, no. 4, pp. 678 690, Apr. 2004. [4] J. M. de la Rosa, Sigma-Delta Modulators: Tutorial Overview, Design Guide, and State-of-the-Art Survey, IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 58, no. 1, pp. 1 21, Jan. 2011.\n\n--- Segment 15 ---\n1, pp. 1 21, Jan. 2011. 14 [5] T. Kieu et al., Outlier Detection for Time Series with Recurrent Autoencoder Ensembles, in International Joint Conference on Artificial Intelligence, 2019, pp. 2725 2732. [6] T. Xie et al., Anomaly detection for multivariate times series through the multi-scale convolutional recurrent variational autoencoder, Expert Systems with Applications, vol. 231, p. 120725, Nov. 2023. [7] F. Weninger et al., Deep recurrent de-noising auto-encoder and blind de-reverberation for reverberated speech recognition, in IEEE International Conference on Acoustics, Speech and Signal Processing, May 2014, pp. 4623 4627. [8] B. Hou et al., LSTM-Based Auto-Encoder Model for ECG Arrhythmias Classification, IEEE Transactions on Instru- mentation and Measurement, vol. 69, no. 4, pp. 1232 1240, Apr. 2020. [9] R. Yang et al., Learning for Video Compression With Recurrent Auto-Encoder and Recurrent Probability Model, IEEE Journal of Selected Topics in Signal Processing, vol. 15, no. 2, pp. 388 401, Feb. 2021. [10] Q. Wang et al., A Comprehensive Survey of Loss Functions in Machine Learning, Annals of Data Science, vol. 9, no. 2, pp. 187 212, Apr. 2022. [11] S. R. Dubey et al., Activation functions in deep learning: A comprehensive survey and benchmark, Neurocomputing, vol. 503, pp. 92 108, Sep. 2022. [12] G. K. Thiruvathukal et al., A Survey of Quantization Methods for Efficient Neural Network Inference, pp. 291 326, Jan. 2022, book Title: Low-Power Computer Vision Edition: 1 ISBN: 9781003162810 Place: Boca Raton Publisher: Chapman and Hall CRC. [13] A. Mumuni et al., Data augmentation: A comprehensive survey of modern approaches, Array, vol. 16, p. 100258, Dec. 2022. [14] S. Xu et al., Analog-to-digital conversion revolutionized by deep learning, arXiv: Signal Processing, 2018.\n\n--- Segment 16 ---\n16, p. 100258, Dec. 2022. [14] S. Xu et al., Analog-to-digital conversion revolutionized by deep learning, arXiv: Signal Processing, 2018. [15] A. Samiee et al., Deep analog-to-digital converter for wireless communication, in AI and Optical Data Sciences II, K.-i. Kitayama et al., Eds. Online Only, United States: SPIE, Mar. 2021, p. 44. [16] S. Bansal et al., Neural-Network Based Self-Initializing Algorithm for Multi-Parameter Optimization of High-Speed ADCs, IEEE Transactions on Circuits and Systems II: Express Briefs, vol. 68, no. 1, pp. 106 110, Jan. 2021. [17] J. M. de la Rosa, AI-Assisted Sigma-Delta Converters Application to Cognitive Radio, IEEE Transactions on Circuits and Systems II: Express Briefs, vol. 69, no. 6, pp. 2557 2563, Jun. 2022. [18] M. Fayazi et al., Applications of Artificial Intelligence on the Modeling and Optimization for Analog and Mixed-Signal Circuits: A Review, IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 68, no. 6, pp. 2418 2431, Jun. 2021. [19] J.-W. Nam et al., Machine-Learning based Analog and Mixed-signal Circuit Design and Optimization, in International Conference on Information Networking, Jan. 2021, pp. 874 876. [20] D. Tank et al., Simple neural optimization networks: An A D converter, signal decision circuit, and a linear programming circuit, IEEE Transactions on Circuits and Systems, vol. 33, no. 5, pp. 533 541, 1986. [21] W. Cao et al., NeuADC: Neural Network-Inspired Synthesizable Analog-to-Digital Conversion, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 39, no. 9, pp. 1841 1854, Sep. 2020. [22] A. Tankimanova et al., Neural Network-Based Analog-to-Digital Converters, in Memristor and Memristive Neural Networks, A. P. James, Ed. InTech, Apr. 2018.\n\n--- Segment 17 ---\nInTech, Apr. 2018. [23] L. Danial et al., A Pipelined Memristive Neural Network Analog-to-Digital Converter, in IEEE International Symposium on Circuits and Systems, Oct. 2020, pp. 1 5. [24] , Delta-Sigma Modulation Neurons for High-Precision Training of Memristive Synapses in Deep Neural Networks, in IEEE International Symposium on Circuits and Systems, May 2019, pp. 1 5. 15 [25] J. Wagner et al., Man or machine design automation of delta-sigma modulators, in 2018 IEEE International Symposium on Circuits and Systems (ISCAS), 2018, pp. 1 5. [26] B. Jacob et al., Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference, in IEEE CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 2704 2713. [27] A. Verdant et al., High-linearity sigma-delta converter, Patent WOEP2016 053687, 2016. [28] W. Guicquero et al., Incremental Delta Sigma Modulation with Dynamic Weighted Integration, in IEEE International Midwest Symposium on Circuits and Systems, 2018, pp. 344 347. [29] B. Wang et al., A 550 µW 20 kHz BW 100.8 DB SNDR linear-exponential multi-bit incremental converter with 256-cycles in 65 nm CMOS, in IEEE Symp. VLSI Circuits, 2018, pp. 207 208.\n\n