[
  {
    "arxiv_id": "2507.07683v1",
    "title": "Accelerating Transposed Convolutions on FPGA-based Edge Devices",
    "authors": [
      "Jude Haris",
      "Jos\u00e9 Cano"
    ],
    "author_affiliations": [
      "",
      ""
    ],
    "abstract": "Transposed Convolutions (TCONV) enable the up-scaling mechanism within\ngenerative Artificial Intelligence (AI) models. However, the predominant\nInput-Oriented Mapping (IOM) method for implementing TCONV has complex output\nmapping, overlapping sums, and ineffectual computations. These inefficiencies\nfurther exacerbate the performance bottleneck of TCONV and generative models on\nresource-constrained edge devices. To address this problem, in this paper we\npropose MM2IM, a hardware-software co-designed accelerator that combines Matrix\nMultiplication (MatMul) with col2IM to process TCONV layers on\nresource-constrained edge devices efficiently. Using the SECDA-TFLite design\ntoolkit, we implement MM2IM and evaluate its performance across 261 TCONV\nproblem configurations, achieving an average speedup of 1.9x against a\ndual-thread ARM Neon optimized CPU baseline. We then evaluate the performance\nof MM2IM on a range of TCONV layers from well-known generative models achieving\nup to 4.2x speedup, and compare it against similar resource-constrained TCONV\naccelerators, outperforming them by at least 2x GOPs/DSP. Finally, we evaluate\nMM2IM on the DCGAN and pix2pix GAN models, achieving up to 3x speedup and 2.4x\nenergy reduction against the CPU baseline.",
    "published": "2025-07-10T12:05:33Z",
    "categories": [
      "cs.AR",
      "cs.DC",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2507.07683v1"
  },
  {
    "arxiv_id": "2507.06549v1",
    "title": "Deep-Learning-Based Pre-Layout Parasitic Capacitance Prediction on SRAM\n  Designs",
    "authors": [
      "Shan Shen",
      "Dingcheng Yang",
      "Yuyang Xie",
      "Chunyan Pei",
      "Wenjian Yu",
      "Bei Yu"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "To achieve higher system energy efficiency, SRAM in SoCs is often customized.\nThe parasitic effects cause notable discrepancies between pre-layout and\npost-layout circuit simulations, leading to difficulty in converging design\nparameters and excessive design iterations. Is it possible to well predict the\nparasitics based on the pre-layout circuit, so as to perform parasitic-aware\npre-layout simulation? In this work, we propose a deep-learning-based 2-stage\nmodel to accurately predict these parasitics in pre-layout stages. The model\ncombines a Graph Neural Network (GNN) classifier and Multi-Layer Perceptron\n(MLP) regressors, effectively managing class imbalance of the net parasitics in\nSRAM circuits. We also employ Focal Loss to mitigate the impact of abundant\ninternal net samples and integrate subcircuit information into the graph to\nabstract the hierarchical structure of schematics. Experiments on 4 real SRAM\ndesigns show that our approach not only surpasses the state-of-the-art model in\nparasitic prediction by a maximum of 19X reduction of error but also\nsignificantly boosts the simulation process by up to 598X speedup.",
    "published": "2025-07-09T05:05:08Z",
    "categories": [
      "cs.LG",
      "cs.AR",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_link": "http://arxiv.org/pdf/2507.06549v1"
  },
  {
    "arxiv_id": "2507.05681v1",
    "title": "GATMesh: Clock Mesh Timing Analysis using Graph Neural Networks",
    "authors": [
      "Muhammad Hadir Khan",
      "Matthew Guthaus"
    ],
    "author_affiliations": [
      "",
      ""
    ],
    "abstract": "Clock meshes are essential in high-performance VLSI systems for minimizing\nskew and handling PVT variations, but analyzing them is difficult due to\nreconvergent paths, multi-source driving, and input mesh buffer skew. SPICE\nsimulations are accurate but slow; yet simplified models miss key effects like\nslew and input skew. We propose GATMesh, a Graph Neural Network (GNN)-based\nframework that models the clock mesh as a graph with augmented structural and\nphysical features. Trained on SPICE data, GATMesh achieves high accuracy with\naverage delay error of 5.27ps on unseen benchmarks, while achieving speed-ups\nof 47146x over multi-threaded SPICE simulation.",
    "published": "2025-07-08T05:18:42Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2507.05681v1"
  },
  {
    "arxiv_id": "2507.05531v1",
    "title": "Bit-Flip Fault Attack: Crushing Graph Neural Networks via Gradual Bit\n  Search",
    "authors": [
      "Sanaz Kazemi Abharian",
      "Sai Manoj Pudukotai Dinakarrao"
    ],
    "author_affiliations": [
      "",
      ""
    ],
    "abstract": "Graph Neural Networks (GNNs) have emerged as a powerful machine learning\nmethod for graph-structured data. A plethora of hardware accelerators has been\nintroduced to meet the performance demands of GNNs in real-world applications.\nHowever, security challenges of hardware-based attacks have been generally\noverlooked. In this paper, we investigate the vulnerability of GNN models to\nhardware-based fault attack, wherein an attacker attempts to misclassify output\nby modifying trained weight parameters through fault injection in a memory\ndevice. Thus, we propose Gradual Bit-Flip Fault Attack (GBFA), a layer-aware\nbit-flip fault attack, selecting a vulnerable bit in each selected weight\ngradually to compromise the GNN's performance by flipping a minimal number of\nbits. To achieve this, GBFA operates in two steps. First, a Markov model is\ncreated to predict the execution sequence of layers based on features extracted\nfrom memory access patterns, enabling the launch of the attack within a\nspecific layer. Subsequently, GBFA identifies vulnerable bits within the\nselected weights using gradient ranking through an in-layer search. We evaluate\nthe effectiveness of the proposed GBFA attack on various GNN models for node\nclassification tasks using the Cora and PubMed datasets. Our findings show that\nGBFA significantly degrades prediction accuracy, and the variation in its\nimpact across different layers highlights the importance of adopting a\nlayer-aware attack strategy in GNNs. For example, GBFA degrades GraphSAGE's\nprediction accuracy by 17% on the Cora dataset with only a single bit flip in\nthe last layer.",
    "published": "2025-07-07T23:06:29Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2507.05531v1"
  },
  {
    "arxiv_id": "2507.04535v1",
    "title": "da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs",
    "authors": [
      "Chang Sun",
      "Zhiqiang Que",
      "Vladimir Loncar",
      "Wayne Luk",
      "Maria Spiropulu"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Neural networks with a latency requirement on the order of microseconds, like\nthe ones used at the CERN Large Hadron Collider, are typically deployed on\nFPGAs fully unrolled and pipelined. A bottleneck for the deployment of such\nneural networks is area utilization, which is directly related to the required\nconstant matrix-vector multiplication (CMVM) operations. In this work, we\npropose an efficient algorithm for implementing CMVM operations with\ndistributed arithmetic (DA) on FPGAs that simultaneously optimizes for area\nconsumption and latency. The algorithm achieves resource reduction similar to\nstate-of-the-art algorithms while being significantly faster to compute. The\nproposed algorithm is open-sourced and integrated into the \\texttt{hls4ml}\nlibrary, a free and open-source library for running real-time neural network\ninference on FPGAs. We show that the proposed algorithm can reduce on-chip\nresources by up to a third for realistic, highly quantized neural networks\nwhile simultaneously reducing latency, enabling the implementation of\npreviously infeasible networks.",
    "published": "2025-07-06T21:01:32Z",
    "categories": [
      "cs.AR",
      "cs.LG",
      "hep-ex",
      "B.2.4; B.6"
    ],
    "pdf_link": "http://arxiv.org/pdf/2507.04535v1"
  },
  {
    "arxiv_id": "2507.03522v1",
    "title": "A Flexible Instruction Set Architecture for Efficient GEMMs",
    "authors": [
      "Alexandre de Limas Santana",
      "Adri\u00e0 Armejach",
      "Francesc Martinez",
      "Erich Focht",
      "Marc Casas"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "GEneral Matrix Multiplications (GEMMs) are recurrent in high-performance\ncomputing and deep learning workloads. Typically, high-end CPUs accelerate GEMM\nworkloads with Single-Instruction Multiple Data (SIMD) or vector Instruction\nSet Architectures (ISAs). Since these ISAs face significant issues when running\nGEMM workloads, particularly when dealing with small, tall, or skinny matrices,\nmatrix ISAs have been proposed and implemented by major hardware vendors in the\nlast years. Although these matrix ISAs deliver larger throughput when running\nGEMMs than their SIMD/vector counterparts, they are rigid solutions unable to\ndynamically adapt themselves to application-specific aspects like the data\nformat. This paper demonstrates that the state-of-the-art matrix ISAs deliver\nsuboptimal performance when running the most commonly used convolution and\ntransformer models.\n  This paper proposes the Matrix Tile Extension (MTE), the first matrix ISA\nthat completely decouples the instruction set architecture from the\nmicroarchitecture and seamlessly interacts with existing vector ISAs. MTE\nincurs minimal implementation overhead since it only requires a few additional\ninstructions and a 64-bit Control Status Register (CSR) to keep its state.\nSpecifically, MTE can i) vectorize GEMMs across the three dimensions M, N, and\nK; ii) leverage the capacity of the existing vector register file; and iii)\ndecouple the tile shape from the underlying microarchitecture. MTE achieves\nspeed-ups of 1.35x over the best state-of-the-art matrix ISA.",
    "published": "2025-07-04T12:17:00Z",
    "categories": [
      "cs.AR",
      "cs.LG",
      "C.1.0"
    ],
    "pdf_link": "http://arxiv.org/pdf/2507.03522v1"
  },
  {
    "arxiv_id": "2507.02226v1",
    "title": "DecoRTL: A Run-time Decoding Framework for RTL Code Generation with LLMs",
    "authors": [
      "Mohammad Akyash",
      "Kimia Azar",
      "Hadi Kamali"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "As one of their many applications, large language models (LLMs) have recently\nshown promise in automating register transfer level (RTL) code generation.\nHowever, conventional LLM decoding strategies, originally designed for natural\nlanguage, often fail to meet the structural and semantic demands of RTL,\nleading to hallucinated, repetitive, or invalid code outputs. In this paper, we\nfirst investigate the root causes of these decoding failures through an\nempirical analysis of token-level entropy during RTL generation. Our findings\nreveal that LLMs exhibit low confidence in regions of structural ambiguity or\nsemantic complexity, showing that standard decoding strategies fail to\ndifferentiate between regions requiring determinism (syntax-critical regions)\nand those that benefit from creative exploratory variability (design-critical\nregions). Then, to overcome this, we introduce DecoRTL, a novel run-time\ndecoding strategy, that is both syntax-aware and contrastive for RTL code\ngeneration. DecoRTL integrates two complementary components: (i)\nself-consistency sampling, which generates multiple candidates and re-ranks\nthem based on token-level agreement to promote correctness while maintaining\ndiversity; and (ii) syntax-aware temperature adaptation, which classifies\ntokens by their syntactical and functional roles and adjusts the sampling\ntemperature accordingly, enforcing low temperature for syntax-critical tokens\nand higher temperature for exploratory ones. Our approach operates entirely at\ninference time without requiring any additional model fine-tuning. Through\nevaluations on multiple open-source LLMs using the VerilogEval benchmark, we\ndemonstrate significant improvements in syntactic validity, functional\ncorrectness, and output diversity, while the execution overhead (performance\noverhead) is imperceptible.",
    "published": "2025-07-03T01:17:44Z",
    "categories": [
      "cs.PL",
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2507.02226v1"
  },
  {
    "arxiv_id": "2507.00937v1",
    "title": "RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point\n  Clouds on Unmanned Ground Vehicles",
    "authors": [
      "David Hunt",
      "Shaocheng Luo",
      "Spencer Hallyburton",
      "Shafii Nillongo",
      "Yi Li",
      "Tingjun Chen",
      "Miroslav Pajic"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Low-cost indoor mobile robots have gained popularity with the increasing\nadoption of automation in homes and commercial spaces. However, existing lidar\nand camera-based solutions have limitations such as poor performance in\nvisually obscured environments, high computational overhead for data\nprocessing, and high costs for lidars. In contrast, mmWave radar sensors offer\na cost-effective and lightweight alternative, providing accurate ranging\nregardless of visibility. However, existing radar-based localization suffers\nfrom sparse point cloud generation, noise, and false detections. Thus, in this\nwork, we introduce RaGNNarok, a real-time, lightweight, and generalizable graph\nneural network (GNN)-based framework to enhance radar point clouds, even in\ncomplex and dynamic environments. With an inference time of just 7.3 ms on the\nlow-cost Raspberry Pi 5, RaGNNarok runs efficiently even on such\nresource-constrained devices, requiring no additional computational resources.\nWe evaluate its performance across key tasks, including localization, SLAM, and\nautonomous navigation, in three different environments. Our results demonstrate\nstrong reliability and generalizability, making RaGNNarok a robust solution for\nlow-cost indoor mobile robots.",
    "published": "2025-07-01T16:42:43Z",
    "categories": [
      "cs.RO",
      "cs.AR",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2507.00937v1"
  },
  {
    "arxiv_id": "2506.21371v1",
    "title": "MAx-DNN: Multi-Level Arithmetic Approximation for Energy-Efficient DNN\n  Hardware Accelerators",
    "authors": [
      "Vasileios Leon",
      "Georgios Makris",
      "Sotirios Xydis",
      "Kiamal Pekmestzi",
      "Dimitrios Soudris"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Nowadays, the rapid growth of Deep Neural Network (DNN) architectures has\nestablished them as the defacto approach for providing advanced Machine\nLearning tasks with excellent accuracy. Targeting low-power DNN computing, this\npaper examines the interplay of fine-grained error resilience of DNN workloads\nin collaboration with hardware approximation techniques, to achieve higher\nlevels of energy efficiency. Utilizing the state-of-the-art ROUP approximate\nmultipliers, we systematically explore their fine-grained distribution across\nthe network according to our layer-, filter-, and kernel-level approaches, and\nexamine their impact on accuracy and energy. We use the ResNet-8 model on the\nCIFAR-10 dataset to evaluate our approximations. The proposed solution delivers\nup to 54% energy gains in exchange for up to 4% accuracy loss, compared to the\nbaseline quantized model, while it provides 2x energy gains with better\naccuracy versus the state-of-the-art DNN approximations.",
    "published": "2025-06-26T15:21:12Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.21371v1"
  },
  {
    "arxiv_id": "2506.20810v1",
    "title": "FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated\n  LSTMs",
    "authors": [
      "Shashwat Khandelwal",
      "Jakoba Petri-Koenig",
      "Thomas B. Preu\u00dfer",
      "Michaela Blott",
      "Shreejith Shanker"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Recurrent neural networks (RNNs), particularly LSTMs, are effective for\ntime-series tasks like sentiment analysis and short-term stock prediction.\nHowever, their computational complexity poses challenges for real-time\ndeployment in resource constrained environments. While FPGAs offer a promising\nplatform for energy-efficient AI acceleration, existing tools mainly target\nfeed-forward networks, and LSTM acceleration typically requires full custom\nimplementation. In this paper, we address this gap by leveraging the\nopen-source and extensible FINN framework to enable the generalized deployment\nof LSTMs on FPGAs. Specifically, we leverage the Scan operator from the Open\nNeural Network Exchange (ONNX) specification to model the recurrent nature of\nLSTM computations, enabling support for mixed quantisation within them and\nfunctional verification of LSTM-based models. Furthermore, we introduce custom\ntransformations within the FINN compiler to map the quantised ONNX computation\ngraph to hardware blocks from the HLS kernel library of the FINN compiler and\nVitis HLS. We validate the proposed tool-flow by training a quantised ConvLSTM\nmodel for a mid-price stock prediction task using the widely used dataset and\ngenerating a corresponding hardware IP of the model using our flow, targeting\nthe XCZU7EV device. We show that the generated quantised ConvLSTM accelerator\nthrough our flow achieves a balance between performance (latency) and resource\nconsumption, while matching (or bettering) inference accuracy of\nstate-of-the-art models with reduced precision. We believe that the\ngeneralisable nature of the proposed flow will pave the way for\nresource-efficient RNN accelerator designs on FPGAs.",
    "published": "2025-06-25T20:07:46Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR",
      "eess.SP"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.20810v1"
  },
  {
    "arxiv_id": "2506.20752v1",
    "title": "Characterization and Mitigation of Training Instabilities in\n  Microscaling Formats",
    "authors": [
      "Huangyuan Su",
      "Mujin Kwun",
      "Stephanie Gil",
      "Sham Kakade",
      "Nikhil Anand"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Training large language models is an expensive, compute-bound process that\nmust be repeated as models scale, algorithms improve, and new data is\ncollected. To address this, next-generation hardware accelerators increasingly\nsupport lower-precision arithmetic formats, such as the Microscaling (MX)\nformats introduced in NVIDIA's Blackwell architecture. These formats use a\nshared scale within blocks of parameters to extend representable range and\nperform forward/backward GEMM operations in reduced precision for efficiency\ngains. In this work, we investigate the challenges and viability of\nblock-scaled precision formats during model training. Across nearly one\nthousand language models trained from scratch -- spanning compute budgets from\n$2 \\times 10^{17}$ to $4.8 \\times 10^{19}$ FLOPs and sweeping over a broad\nrange of weight-activation precision combinations -- we consistently observe\nthat training in MX formats exhibits sharp, stochastic instabilities in the\nloss, particularly at larger compute scales. To explain this phenomenon, we\nconduct controlled experiments and ablations on a smaller proxy model that\nexhibits similar behavior as the language model, sweeping across architectural\nsettings, hyperparameters, and precision formats. These experiments motivate a\nsimple model in which multiplicative gradient bias introduced by the\nquantization of layer-norm affine parameters and a small fraction of\nactivations can trigger runaway divergence. Through \\emph{in situ} intervention\nexperiments on our proxy model, we demonstrate that instabilities can be\naverted or delayed by modifying precision schemes mid-training. Guided by these\nfindings, we evaluate stabilization strategies in the LLM setting and show that\ncertain hybrid configurations recover performance competitive with\nfull-precision training. We release our code at\nhttps://github.com/Hither1/systems-scaling.",
    "published": "2025-06-25T18:25:08Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.20752v1"
  },
  {
    "arxiv_id": "2506.18495v1",
    "title": "AnalogNAS-Bench: A NAS Benchmark for Analog In-Memory Computing",
    "authors": [
      "Aniss Bessalah",
      "Hatem Mohamed Abdelmoumen",
      "Karima Benatchba",
      "Hadjer Benmeziane"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      ""
    ],
    "abstract": "Analog In-memory Computing (AIMC) has emerged as a highly efficient paradigm\nfor accelerating Deep Neural Networks (DNNs), offering significant energy and\nlatency benefits over conventional digital hardware. However, state-of-the-art\nneural networks are not inherently designed for AIMC, as they fail to account\nfor its unique non-idealities. Neural Architecture Search (NAS) is thus needed\nto systematically discover neural architectures optimized explicitly for AIMC\nconstraints. However, comparing NAS methodologies and extracting insights about\nrobust architectures for AIMC requires a dedicated NAS benchmark that\nexplicitly accounts for AIMC-specific hardware non-idealities. To address this,\nwe introduce AnalogNAS-Bench, the first NAS benchmark tailored specifically for\nAIMC. Our study reveals three key insights: (1) standard quantization\ntechniques fail to capture AIMC-specific noises, (2) robust architectures tend\nto feature wider and branched blocks, (3) skip connections improve resilience\nto temporal drift noise. These insights highlight the limitations of current\nNAS benchmarks for AIMC and pave the way for future analog-aware NAS. All the\nimplementations used in this paper can be found at\nhttps://github.com/IBM/analog-nas/tree/main/analognasbench.",
    "published": "2025-06-23T10:44:32Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.18495v1"
  },
  {
    "arxiv_id": "2506.16903v1",
    "title": "RCNet: $\u0394\u03a3$ IADCs as Recurrent AutoEncoders",
    "authors": [
      "Arnaud Verdant",
      "William Guicquero",
      "J\u00e9r\u00f4me Chossat"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "This paper proposes a deep learning model (RCNet) for Delta-Sigma\n($\\Delta\\Sigma$) ADCs. Recurrent Neural Networks (RNNs) allow to describe both\nmodulators and filters. This analogy is applied to Incremental ADCs (IADC).\nHigh-end optimizers combined with full-custom losses are used to define\nadditional hardware design constraints: quantized weights, signal saturation,\ntemporal noise injection, devices area. Focusing on DC conversion, our early\nresults demonstrate that $SNR$ defined as an Effective Number Of Bits (ENOB)\ncan be optimized under a certain hardware mapping complexity. The proposed\nRCNet succeeded to provide design tradeoffs in terms of $SNR$ ($>$13bit) versus\narea constraints ($<$14pF total capacitor) at a given $OSR$ (80 samples).\nInterestingly, it appears that the best RCNet architectures do not necessarily\nrely on high-order modulators, leveraging additional topology exploration\ndegrees of freedom.",
    "published": "2025-06-20T10:55:01Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.16903v1"
  },
  {
    "arxiv_id": "2506.14606v1",
    "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees",
    "authors": [
      "Ahmed Heakl",
      "Sarim Hashmi",
      "Chaimaa Abi",
      "Celine Lee",
      "Abdulrahman Mahmoud"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "The hardware ecosystem is rapidly evolving, with increasing interest in\ntranslating low-level programs across different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\nlongevity of existing code. A particularly challenging class of this\ntranspilation problem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity, memory models, and execution paradigms. In this work, we introduce\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\ntranslation power of pre-trained large language models (LLMs) with the rigor of\nestablished software testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\nand benchmarks to establish a common foundation for ISA-level code translation\nresearch.",
    "published": "2025-06-17T15:06:54Z",
    "categories": [
      "cs.CL",
      "cs.AR",
      "cs.LG",
      "cs.PL",
      "cs.SE"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.14606v1"
  },
  {
    "arxiv_id": "2506.14074v1",
    "title": "Comprehensive Verilog Design Problems: A Next-Generation Benchmark\n  Dataset for Evaluating Large Language Models and Agents on RTL Design and\n  Verification",
    "authors": [
      "Nathaniel Pinckney",
      "Chenhui Deng",
      "Chia-Tung Ho",
      "Yun-Da Tsai",
      "Mingjie Liu",
      "Wenfei Zhou",
      "Brucek Khailany",
      "Haoxing Ren"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "We present the Comprehensive Verilog Design Problems (CVDP) benchmark, a new\ndataset and infrastructure to advance LLM and agent research in hardware design\nand verification. CVDP includes 783 problems across 13 task categories,\ncovering RTL generation, verification, debugging, specification alignment, and\ntechnical Q&A authored by experienced hardware engineers. Problems are offered\nin both non-agentic and agentic formats. The benchmark introduces more\nrealistic and challenging contexts than prior work, with state-of-the-art\nmodels achieving no more than 34% pass@1 on code generation. Agentic\ntasks$\\unicode{x2013}$especially those involving RTL reuse and\nverification$\\unicode{x2013}$are particularly difficult. Evaluation uses\nopen-source tools and model scoring infrastructure, with comprehension tasks\nassessed via BLEU and LLM-based judging. CVDP reveals substantial gaps in\ncurrent model capabilities, underscoring the need for continued research toward\nrobust, real-world hardware design automation.",
    "published": "2025-06-17T00:11:13Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.14074v1"
  },
  {
    "arxiv_id": "2506.12708v3",
    "title": "Serving Large Language Models on Huawei CloudMatrix384",
    "authors": [
      "Pengfei Zuo",
      "Huimin Lin",
      "Junbo Deng",
      "Nan Zou",
      "Xingkun Yang",
      "Yingyu Diao",
      "Weifeng Gao",
      "Ke Xu",
      "Zhangyu Chen",
      "Shirui Lu",
      "Zhao Qiu",
      "Peiyang Li",
      "Xianyu Chang",
      "Zhengzhong Yu",
      "Fangzheng Miao",
      "Jia Zheng",
      "Ying Li",
      "Yuan Feng",
      "Bei Wang",
      "Zaijian Zong",
      "Mosong Zhou",
      "Wenli Zhou",
      "Houjiang Chen",
      "Xingyu Liao",
      "Yipeng Li",
      "Wenxiao Zhang",
      "Ping Zhu",
      "Yinggang Wang",
      "Chuanjie Xiao",
      "Depeng Liang",
      "Dong Cao",
      "Juncheng Liu",
      "Yongqiang Yang",
      "Xiaolong Bai",
      "Yi Li",
      "Huaguo Xie",
      "Huatao Wu",
      "Zhibin Yu",
      "Lv Chen",
      "Hu Liu",
      "Yujun Ding",
      "Haipei Zhu",
      "Jing Xia",
      "Yi Xiong",
      "Zhou Yu",
      "Heng Liao"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks.",
    "published": "2025-06-15T03:41:34Z",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.12708v3"
  },
  {
    "arxiv_id": "2506.14830v1",
    "title": "Optimization of bi-directional gated loop cell based on multi-head\n  attention mechanism for SSD health state classification model",
    "authors": [
      "Zhizhao Wen",
      "Ruoxin Zhang",
      "Chao Wang"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "Aiming at the critical role of SSD health state prediction in data\nreliability assurance, this study proposes a hybrid BiGRU-MHA model that\nincorporates a multi-head attention mechanism to enhance the accuracy and\nstability of storage device health classification. The model innovatively\nintegrates temporal feature extraction and key information focusing\ncapabilities. Specifically, it leverages the bidirectional timing modeling\nadvantages of the BiGRU network to capture both forward and backward\ndependencies of SSD degradation features. Simultaneously, the multi-head\nattention mechanism dynamically assigns feature weights, improving the model's\nsensitivity to critical health indicators. Experimental results show that the\nproposed model achieves classification accuracies of 92.70% on the training set\nand 92.44% on the test set, with a minimal performance gap of only 0.26%,\ndemonstrating excellent generalization ability. Further analysis using the\nreceiver operating characteristic (ROC) curve shows an area under the curve\n(AUC) of 0.94 on the test set, confirming the model's robust binary\nclassification performance. This work not only presents a new technical\napproach for SSD health prediction but also addresses the generalization\nbottleneck of traditional models, offering a verifiable method with practical\nvalue for preventive maintenance of industrial-grade storage systems. The\nresults show the model can significantly reduce data loss risks by providing\nearly failure warnings and help optimize maintenance costs, supporting\nintelligent decision-making in building reliable storage systems for cloud\ncomputing data centers and edge storage environments.",
    "published": "2025-06-13T22:01:57Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.14830v1"
  },
  {
    "arxiv_id": "2506.11925v1",
    "title": "Real-World Deployment of a Lane Change Prediction Architecture Based on\n  Knowledge Graph Embeddings and Bayesian Inference",
    "authors": [
      "M. Manzour",
      "Catherine M. Elias",
      "Omar M. Shehata",
      "R. Izquierdo",
      "M. A. Sotelo"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Research on lane change prediction has gained a lot of momentum in the last\ncouple of years. However, most research is confined to simulation or results\nobtained from datasets, leaving a gap between algorithmic advances and on-road\ndeployment. This work closes that gap by demonstrating, on real hardware, a\nlane-change prediction system based on Knowledge Graph Embeddings (KGEs) and\nBayesian inference. Moreover, the ego-vehicle employs a longitudinal braking\naction to ensure the safety of both itself and the surrounding vehicles. Our\narchitecture consists of two modules: (i) a perception module that senses the\nenvironment, derives input numerical features, and converts them into\nlinguistic categories; and communicates them to the prediction module; (ii) a\npretrained prediction module that executes a KGE and Bayesian inference model\nto anticipate the target vehicle's maneuver and transforms the prediction into\nlongitudinal braking action. Real-world hardware experimental validation\ndemonstrates that our prediction system anticipates the target vehicle's lane\nchange three to four seconds in advance, providing the ego vehicle sufficient\ntime to react and allowing the target vehicle to make the lane change safely.",
    "published": "2025-06-13T16:24:28Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.11925v1"
  },
  {
    "arxiv_id": "2506.10235v1",
    "title": "LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog\n  Topology Generation",
    "authors": [
      "Chen-Chia Chang",
      "Wan-Hsuan Lin",
      "Yikang Shen",
      "Yiran Chen",
      "Xin Zhang"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Automation of analog topology design is crucial due to customized\nrequirements of modern applications with heavily manual engineering efforts.\nThe state-of-the-art work applies a sequence-to-sequence approach and\nsupervised finetuning on language models to generate topologies given user\nspecifications. However, its circuit formulation is inefficient due to O(|V |2)\ntoken length and suffers from low precision sensitivity to numeric inputs. In\nthis work, we introduce LaMAGIC2, a succinct float-input canonical formulation\nwith identifier (SFCI) for language model-based analog topology generation.\nSFCI addresses these challenges by improving component-type recognition through\nidentifier-based representations, reducing token length complexity to O(|V |),\nand enhancing numeric precision sensitivity for better performance under tight\ntolerances. Our experiments demonstrate that LaMAGIC2 achieves 34% higher\nsuccess rates under a tight tolerance of 0.01 and 10X lower MSEs compared to a\nprior method. LaMAGIC2 also exhibits better transferability for circuits with\nmore vertices with up to 58.5% improvement. These advancements establish\nLaMAGIC2 as a robust framework for analog topology generation.",
    "published": "2025-06-11T23:28:00Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.10235v1"
  },
  {
    "arxiv_id": "2506.08911v1",
    "title": "Implementing Keyword Spotting on the MCUX947 Microcontroller with\n  Integrated NPU",
    "authors": [
      "Petar Jaku\u0161",
      "Hrvoje D\u017eapo"
    ],
    "author_affiliations": [
      "",
      ""
    ],
    "abstract": "This paper presents a keyword spotting (KWS) system implemented on the NXP\nMCXN947 microcontroller with an integrated Neural Processing Unit (NPU),\nenabling real-time voice interaction on resource-constrained devices. The\nsystem combines MFCC feature extraction with a CNN classifier, optimized using\nQuantization Aware Training to reduce model size with minimal accuracy drop.\nExperimental results demonstrate a 59x speedup in inference time when\nleveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy\nwith a model size of 30.58 KB, demonstrating the feasibility of efficient,\nlow-power voice interfaces on embedded platforms.",
    "published": "2025-06-10T15:38:21Z",
    "categories": [
      "cs.HC",
      "cs.AR",
      "cs.LG",
      "cs.PF",
      "cs.SD"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.08911v1"
  },
  {
    "arxiv_id": "2506.07366v1",
    "title": "MoE-GPS: Guidlines for Prediction Strategy for Dynamic Expert\n  Duplication in MoE Load Balancing",
    "authors": [
      "Haiyue Ma",
      "Zhixu Du",
      "Yiran Chen"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "In multi-GPU Mixture-of-Experts (MoE) network, experts are distributed across\ndifferent GPUs, which creates load imbalance as each expert processes different\nnumber of tokens. Recent works improve MoE inference load balance by\ndynamically duplicating popular experts to more GPUs to process excessive\ntokens, which requires predicting the distribution before routing. In this\npaper, we discuss the tradeoff of prediction strategies, accuracies, overhead,\nand end-to-end system performance. We propose MoE-GPS, a framework that guides\nthe selection of the optimal predictor design under various system\nconfigurations, by quantifying the performance impact to system-level model\nruntime. Specifically, we advocate for Distribution-Only Prediction, a\nprediction strategy that only predicts overall token distribution which\nsignificantly reduces overhead compared to the traditional Token-to-Expert\nPrediction. On Mixtral 8x7B MMLU dataset, MoE-GPS suggests Distribution-Only\nPrediction which improves end-to-end inference performance by more than 23%\ncompared with Token-to-Expert Prediction.",
    "published": "2025-06-09T02:32:09Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.07366v1"
  },
  {
    "arxiv_id": "2506.07069v1",
    "title": "Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented\n  Rasterization",
    "authors": [
      "Zhican Wang",
      "Guanghui He",
      "Dantong Liu",
      "Lingjun Gao",
      "Shell Xu Hu",
      "Chen Zhang",
      "Zhuoran Song",
      "Nicholas Lane",
      "Wayne Luk",
      "Hongxiang Fan"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has recently gained significant attention for\nhigh-quality and efficient view synthesis, making it widely adopted in fields\nsuch as AR/VR, robotics, and autonomous driving. Despite its impressive\nalgorithmic performance, real-time rendering on resource-constrained devices\nremains a major challenge due to tight power and area budgets. This paper\npresents an architecture-algorithm co-design to address these inefficiencies.\nFirst, we reveal substantial redundancy caused by repeated computation of\ncommon terms/expressions during the conventional rasterization. To resolve\nthis, we propose axis-oriented rasterization, which pre-computes and reuses\nshared terms along both the X and Y axes through a dedicated hardware design,\neffectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by\nidentifying the resource and performance inefficiency of the sorting process,\nwe introduce a novel neural sorting approach that predicts order-independent\nblending weights using an efficient neural network, eliminating the need for\ncostly hardware sorters. A dedicated training framework is also proposed to\nimprove its algorithmic stability. Third, to uniformly support rasterization\nand neural network inference, we design an efficient reconfigurable processing\narray that maximizes hardware utilization and throughput. Furthermore, we\nintroduce a $\\pi$-trajectory tile schedule, inspired by Morton encoding and\nHilbert curve, to optimize Gaussian reuse and reduce memory access overhead.\nComprehensive experiments demonstrate that the proposed design preserves\nrendering quality while achieving a speedup of $23.4\\sim27.8\\times$ and energy\nsavings of $28.8\\sim51.4\\times$ compared to edge GPUs for real-world scenes. We\nplan to open-source our design to foster further development in this field.",
    "published": "2025-06-08T10:14:54Z",
    "categories": [
      "cs.GR",
      "cs.AR",
      "cs.CV",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.07069v1"
  },
  {
    "arxiv_id": "2507.02871v1",
    "title": "ZettaLith: An Architectural Exploration of Extreme-Scale AI Inference\n  Acceleration",
    "authors": [
      "Kia Silverbrook"
    ],
    "author_affiliations": [
      ""
    ],
    "abstract": "The high computational cost and power consumption of current and anticipated\nAI systems present a major challenge for widespread deployment and further\nscaling. Current hardware approaches face fundamental efficiency limits. This\npaper introduces ZettaLith, a scalable computing architecture designed to\nreduce the cost and power of AI inference by over 1,000x compared to current\nGPU-based systems. Based on architectural analysis and technology projections,\na single ZettaLith rack could potentially achieve 1.507 zettaFLOPS in 2027 -\nrepresenting a theoretical 1,047x improvement in inference performance, 1,490x\nbetter power efficiency, and could be 2,325x more cost-effective than current\nleading GPU racks for FP4 transformer inference. The ZettaLith architecture\nachieves these gains by abandoning general purpose GPU applications, and via\nthe multiplicative effect of numerous co-designed architectural innovations\nusing established digital electronic technologies, as detailed in this paper.\nZettaLith's core architectural principles scale down efficiently to exaFLOPS\ndesktop systems and petaFLOPS mobile chips, maintaining their roughly 1,000x\nadvantage. ZettaLith presents a simpler system architecture compared to the\ncomplex hierarchy of current GPU clusters. ZettaLith is optimized exclusively\nfor AI inference and is not applicable for AI training.",
    "published": "2025-06-08T07:15:47Z",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2507.02871v1"
  },
  {
    "arxiv_id": "2506.06817v1",
    "title": "ASPO: Constraint-Aware Bayesian Optimization for FPGA-based Soft\n  Processors",
    "authors": [
      "Haoran Wu",
      "Ce Guo",
      "Wayne Luk",
      "Robert Mullins"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      ""
    ],
    "abstract": "Bayesian Optimization (BO) has shown promise in tuning processor design\nparameters. However, standard BO does not support constraints involving\ncategorical parameters such as types of branch predictors and division\ncircuits. In addition, optimization time of BO grows with processor complexity,\nwhich becomes increasingly significant especially for FPGA-based soft\nprocessors. This paper introduces ASPO, an approach that leverages disjunctive\nform to enable BO to handle constraints involving categorical parameters.\nUnlike existing methods that directly apply standard BO, the proposed ASPO\nmethod, for the first time, customizes the mathematical mechanism of BO to\naddress challenges faced by soft-processor designs on FPGAs. Specifically, ASPO\nsupports categorical parameters using a novel customized BO covariance kernel.\nIt also accelerates the design evaluation procedure by penalizing the BO\nacquisition function with potential evaluation time and by reusing FPGA\nsynthesis checkpoints from previously evaluated configurations. ASPO targets\nthree soft processors: RocketChip, BOOM, and EL2 VeeR. The approach is\nevaluated based on seven RISC-V benchmarks. Results show that ASPO can reduce\nexecution time for the ``multiply'' benchmark on the BOOM processor by up to\n35\\% compared to the default configuration. Furthermore, it reduces design time\nfor the BOOM processor by up to 74\\% compared to Boomerang, a state-of-the-art\nhardware-oriented BO approach.",
    "published": "2025-06-07T14:46:40Z",
    "categories": [
      "cs.AR",
      "cs.LG",
      "cs.NE",
      "cs.PF"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.06817v1"
  },
  {
    "arxiv_id": "2506.06787v1",
    "title": "FuncGNN: Learning Functional Semantics of Logic Circuits with Graph\n  Neural Networks",
    "authors": [
      "Qiyun Zhao"
    ],
    "author_affiliations": [
      ""
    ],
    "abstract": "As integrated circuit scale grows and design complexity rises, effective\ncircuit representation helps support logic synthesis, formal verification, and\nother automated processes in electronic design automation. And-Inverter Graphs\n(AIGs), as a compact and canonical structure, are widely adopted for\nrepresenting Boolean logic in these workflows. However, the increasing\ncomplexity and integration density of modern circuits introduce structural\nheterogeneity and global logic information loss in AIGs, posing significant\nchallenges to accurate circuit modeling. To address these issues, we propose\nFuncGNN, which integrates hybrid feature aggregation to extract\nmulti-granularity topological patterns, thereby mitigating structural\nheterogeneity and enhancing logic circuit representations. FuncGNN further\nintroduces gate-aware normalization that adapts to circuit-specific gate\ndistributions, improving robustness to structural heterogeneity. Finally,\nFuncGNN employs multi-layer integration to merge intermediate features across\nlayers, effectively synthesizing local and global semantic information for\ncomprehensive logic representations. Experimental results on two logic-level\nanalysis tasks (i.e., signal probability prediction and truth-table distance\nprediction) demonstrate that FuncGNN outperforms existing state-of-the-art\nmethods, achieving improvements of 2.06% and 18.71%, respectively, while\nreducing training time by approximately 50.6% and GPU memory usage by about\n32.8%.",
    "published": "2025-06-07T13:04:07Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.06787v1"
  },
  {
    "arxiv_id": "2506.06773v1",
    "title": "Taming Wild Branches: Overcoming Hard-to-Predict Branches using the\n  Bullseye Predictor",
    "authors": [
      "Emet Behrendt",
      "Shing Wai Pun",
      "Prashant J. Nair"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "Branch prediction is key to the performance of out-of-order processors. While\nthe CBP-2016 winner TAGE-SC-L combines geometric-history tables, a statistical\ncorrector, and a loop predictor, over half of its remaining mispredictions stem\nfrom a small set of hard-to-predict (H2P) branches. These branches occur under\ndiverse global histories, causing repeated thrashing in TAGE and eviction\nbefore usefulness counters can mature. Prior work shows that simply enlarging\nthe tables offers only marginal improvement.\n  We augment a 159 KB TAGE-SC-L predictor with a 28 KB H2P-targeted subsystem\ncalled the Bullseye predictor. It identifies problematic PCs using a\nset-associative H2P Identification Table (HIT) and steers them to one of two\nbranch-specific perceptrons, one indexed by hashed local history and the other\nby folded global history. A short trial phase tracks head-to-head accuracy in\nan H2P cache. A branch becomes perceptron-resident only if the perceptron's\nsustained accuracy and output magnitude exceed dynamic thresholds, after which\nTAGE updates for that PC are suppressed to reduce pollution. The HIT, cache,\nand perceptron operate fully in parallel with TAGE-SC-L, providing higher\nfidelity on the H2P tail. This achieves an average MPKI of 3.4045 and CycWpPKI\nof 145.09.",
    "published": "2025-06-07T11:50:11Z",
    "categories": [
      "cs.AR",
      "cs.LG",
      "cs.PF",
      "C.1.2; B.2.1; C.4; C.0"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.06773v1"
  },
  {
    "arxiv_id": "2506.06505v1",
    "title": "InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models",
    "authors": [
      "Keisuke Sugiura",
      "Hiroki Matsutani"
    ],
    "author_affiliations": [
      "",
      ""
    ],
    "abstract": "Training deep neural networks (DNNs) requires significantly more computation\nand memory than inference, making runtime adaptation of DNNs challenging on\nresource-limited IoT platforms. We propose InstantFT, an FPGA-based method for\nultra-fast CNN fine-tuning on IoT devices, by optimizing the forward and\nbackward computations in parameter-efficient fine-tuning (PEFT). Experiments on\ndatasets with concept drift demonstrate that InstantFT fine-tunes a pre-trained\nCNN 17.4x faster than existing Low-Rank Adaptation (LoRA)-based approaches,\nwhile achieving comparable accuracy. Our FPGA-based InstantFT reduces the\nfine-tuning time to just 0.36s and improves energy-efficiency by 16.3x,\nenabling on-the-fly adaptation of CNNs to non-stationary data distributions.",
    "published": "2025-06-06T20:01:09Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.06505v1"
  },
  {
    "arxiv_id": "2506.05994v1",
    "title": "RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration\n  with Content-Addressable Memory",
    "authors": [
      "Yi-Chun Liao",
      "Chieh-Lin Tsai",
      "Yuan-Hao Chang",
      "Cam\u00e9lia Slimani",
      "Jalil Boukhobza",
      "Tei-Wei Kuo"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Although deep learning has demonstrated remarkable capabilities in learning\nfrom unstructured data, modern tree-based ensemble models remain superior in\nextracting relevant information and learning from structured datasets. While\nseveral efforts have been made to accelerate tree-based models, the inherent\ncharacteristics of the models pose significant challenges for conventional\naccelerators. Recent research leveraging content-addressable memory (CAM)\noffers a promising solution for accelerating tree-based models, yet existing\ndesigns suffer from excessive memory consumption and low utilization. This work\naddresses these challenges by introducing RETENTION, an end-to-end framework\nthat significantly reduces CAM capacity requirement for tree-based model\ninference. We propose an iterative pruning algorithm with a novel pruning\ncriterion tailored for bagging-based models (e.g., Random Forest), which\nminimizes model complexity while ensuring controlled accuracy degradation.\nAdditionally, we present a tree mapping scheme that incorporates two innovative\ndata placement strategies to alleviate the memory redundancy caused by the\nwidespread use of don't care states in CAM. Experimental results show that\nimplementing the tree mapping scheme alone achieves $1.46\\times$ to $21.30\n\\times$ better space efficiency, while the full RETENTION framework yields\n$4.35\\times$ to $207.12\\times$ improvement with less than 3% accuracy loss.\nThese results demonstrate that RETENTION is highly effective in reducing CAM\ncapacity requirement, providing a resource-efficient direction for tree-based\nmodel acceleration.",
    "published": "2025-06-06T11:25:51Z",
    "categories": [
      "cs.LG",
      "cs.AR",
      "cs.ET"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.05994v1"
  },
  {
    "arxiv_id": "2506.05007v1",
    "title": "QiMeng: Fully Automated Hardware and Software Design for Processor Chip",
    "authors": [
      "Rui Zhang",
      "Yuanbo Wen",
      "Shuyao Cheng",
      "Di Huang",
      "Shaohui Peng",
      "Jiaming Guo",
      "Pengwei Jin",
      "Jiacheng Zhao",
      "Tianrui Ma",
      "Yaoyu Zhu",
      "Yifan Hao",
      "Yongwei Zhao",
      "Shengwen Liang",
      "Ying Wang",
      "Xing Hu",
      "Zidong Du",
      "Huimin Cui",
      "Ling Li",
      "Qi Guo",
      "Yunji Chen"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Processor chip design technology serves as a key frontier driving\nbreakthroughs in computer science and related fields. With the rapid\nadvancement of information technology, conventional design paradigms face three\nmajor challenges: the physical constraints of fabrication technologies, the\nescalating demands for design resources, and the increasing diversity of\necosystems. Automated processor chip design has emerged as a transformative\nsolution to address these challenges. While recent breakthroughs in Artificial\nIntelligence (AI), particularly Large Language Models (LLMs) techniques, have\nopened new possibilities for fully automated processor chip design, substantial\nchallenges remain in establishing domain-specific LLMs for processor chip\ndesign.\n  In this paper, we propose QiMeng, a novel system for fully automated hardware\nand software design of processor chips. QiMeng comprises three hierarchical\nlayers. In the bottom-layer, we construct a domain-specific Large Processor\nChip Model (LPCM) that introduces novel designs in architecture, training, and\ninference, to address key challenges such as knowledge representation gap, data\nscarcity, correctness assurance, and enormous solution space. In the\nmiddle-layer, leveraging the LPCM's knowledge representation and inference\ncapabilities, we develop the Hardware Design Agent and the Software Design\nAgent to automate the design of hardware and software for processor chips.\nCurrently, several components of QiMeng have been completed and successfully\napplied in various top-layer applications, demonstrating significant advantages\nand providing a feasible solution for efficient, fully automated\nhardware/software design of processor chips. Future research will focus on\nintegrating all components and performing iterative top-down and bottom-up\ndesign processes to establish a comprehensive QiMeng system.",
    "published": "2025-06-05T13:17:50Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.05007v1"
  },
  {
    "arxiv_id": "2506.04667v2",
    "title": "FlashDMoE: Fast Distributed MoE in a Single Kernel",
    "authors": [
      "Osayamen Jonathan Aimuyo",
      "Byungsoo Oh",
      "Rachee Singh"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "The computational sparsity of Mixture-of-Experts (MoE) models enables\nsub-linear growth in compute cost as model size increases, thus offering a\nscalable path to training massive neural networks. However, existing\nimplementations suffer from \\emph{low GPU utilization}, \\emph{significant\nlatency overhead}, and a fundamental \\emph{inability to leverage task\nlocality}, primarily due to CPU-managed scheduling, host-initiated\ncommunication, and frequent kernel launches. To overcome these limitations, we\ndevelop FlashDMoE, a fully GPU-resident MoE operator that fuses expert\ncomputation and inter-GPU communication into a \\emph{single persistent GPU\nkernel}. FlashDMoE enables fine-grained pipelining of dispatch, compute, and\ncombine phases, eliminating launch overheads and reducing idle gaps. Unlike\nexisting work, FlashDMoE obviates bulk-synchronous collectives for one-sided,\ndevice-initiated, inter-GPU (R)DMA transfers, thus unlocking \\emph{payload\nefficiency}, where we eliminate bloated or redundant network payloads in\nsparsely activated layers. When evaluated on a single 8-H100 GPU node with MoE\nmodels having up to 128 experts and 16K token sequences, FlashDMoE achieves up\nto \\textbf{9}$\\times$ higher GPU utilization, \\textbf{6}$\\times$ lower latency,\n\\textbf{5.7}$\\times$ higher throughput, and \\textbf{4}$\\times$ better overlap\nefficiency compared to state-of-the-art baselines, despite using FP32 while\nbaselines use FP16. FlashDMoE demonstrates that principled GPU kernel-hardware\nco-design is key to unlocking the performance ceiling of large-scale\ndistributed ML workloads.",
    "published": "2025-06-05T06:29:14Z",
    "categories": [
      "cs.DC",
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.04667v2"
  },
  {
    "arxiv_id": "2506.04544v2",
    "title": "hdl2v: A Code Translation Dataset for Enhanced LLM Verilog Generation",
    "authors": [
      "Charles Hong",
      "Brendan Roberts",
      "Huijae An",
      "Alex Um",
      "Advay Ratan",
      "Yakun Sophia Shao"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Large language models (LLMs) are playing an increasingly large role in\ndomains such as code generation, including hardware code generation, where\nVerilog is the key language. However, the amount of publicly available Verilog\ncode pales in comparison to the amount of code available for software languages\nlike Python. In this work, we present hdl2v (\"HDL-to-Verilog\"), a dataset which\nseeks to increase the amount of available human-written Verilog data by\ntranslating or compiling three other hardware description languages - VHDL,\nChisel, and PyMTL3 - to Verilog. Furthermore, we demonstrate the value of hdl2v\nin enhancing LLM Verilog generation by improving performance of a 32\nbillion-parameter open-weight model by up to 23% (pass@10) in VerilogEvalV2,\nwithout utilizing any data augmentation or knowledge distillation from larger\nmodels. We also show hdl2v's ability to boost the performance of a data\naugmentation-based fine-tuning approach by 63%. Finally, we characterize and\nanalyze our dataset to better understand which characteristics of\nHDL-to-Verilog datasets can be expanded upon in future work for even better\nperformance.",
    "published": "2025-06-05T01:29:18Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG",
      "cs.PL"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.04544v2"
  },
  {
    "arxiv_id": "2506.04301v1",
    "title": "The Cost of Dynamic Reasoning: Demystifying AI Agents and Test-Time\n  Scaling from an AI Infrastructure Perspective",
    "authors": [
      "Jiin Kim",
      "Byeongjun Shin",
      "Jinha Chung",
      "Minsoo Rhu"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      ""
    ],
    "abstract": "Large-language-model (LLM)-based AI agents have recently showcased impressive\nversatility by employing dynamic reasoning, an adaptive, multi-step process\nthat coordinates with external tools. This shift from static, single-turn\ninference to agentic, multi-turn workflows broadens task generalization and\nbehavioral flexibility, but it also introduces serious concerns about\nsystem-level cost, efficiency, and sustainability. This paper presents the\nfirst comprehensive system-level analysis of AI agents, quantifying their\nresource usage, latency behavior, energy consumption, and datacenter-wide power\nconsumption demands across diverse agent designs and test-time scaling\nstrategies. We further characterize how AI agent design choices, such as\nfew-shot prompting, reflection depth, and parallel reasoning, impact\naccuracy-cost tradeoffs. Our findings reveal that while agents improve accuracy\nwith increased compute, they suffer from rapidly diminishing returns, widening\nlatency variance, and unsustainable infrastructure costs. Through detailed\nevaluation of representative agents, we highlight the profound computational\ndemands introduced by AI agent workflows, uncovering a looming sustainability\ncrisis. These results call for a paradigm shift in agent design toward\ncompute-efficient reasoning, balancing performance with deployability under\nreal-world constraints.",
    "published": "2025-06-04T14:37:54Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.04301v1"
  },
  {
    "arxiv_id": "2506.03938v1",
    "title": "FPGA-Enabled Machine Learning Applications in Earth Observation: A\n  Systematic Review",
    "authors": [
      "C\u00e9dric L\u00e9onard",
      "Dirk Stober",
      "Martin Schulz"
    ],
    "author_affiliations": [
      "Technical University of Munich, Munich, Germany",
      "Technical University of Munich, Munich, Germany",
      "Technical University of Munich, Munich, Germany"
    ],
    "abstract": "New UAV technologies and the NewSpace era are transforming Earth Observation\nmissions and data acquisition. Numerous small platforms generate large data\nvolume, straining bandwidth and requiring onboard decision-making to transmit\nhigh-quality information in time. While Machine Learning allows real-time\nautonomous processing, FPGAs balance performance with adaptability to\nmission-specific requirements, enabling onboard deployment. This review\nsystematically analyzes 66 experiments deploying ML models on FPGAs for Remote\nSensing applications. We introduce two distinct taxonomies to capture both\nefficient model architectures and FPGA implementation strategies. For\ntransparency and reproducibility, we follow PRISMA 2020 guidelines and share\nall data and code at https://github.com/CedricLeon/Survey_RS-ML-FPGA.",
    "published": "2025-06-04T13:30:47Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.03938v1"
  },
  {
    "arxiv_id": "2506.03474v1",
    "title": "CORE: Constraint-Aware One-Step Reinforcement Learning for\n  Simulation-Guided Neural Network Accelerator Design",
    "authors": [
      "Yifeng Xiao",
      "Yurong Xu",
      "Ning Yan",
      "Masood Mortazavi",
      "Pierluigi Nuzzo"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Simulation-based design space exploration (DSE) aims to efficiently optimize\nhigh-dimensional structured designs under complex constraints and expensive\nevaluation costs. Existing approaches, including heuristic and multi-step\nreinforcement learning (RL) methods, struggle to balance sampling efficiency\nand constraint satisfaction due to sparse, delayed feedback, and large hybrid\naction spaces. In this paper, we introduce CORE, a constraint-aware, one-step\nRL method for simulationguided DSE. In CORE, the policy agent learns to sample\ndesign configurations by defining a structured distribution over them,\nincorporating dependencies via a scaling-graph-based decoder, and by reward\nshaping to penalize invalid designs based on the feedback obtained from\nsimulation. CORE updates the policy using a surrogate objective that compares\nthe rewards of designs within a sampled batch, without learning a value\nfunction. This critic-free formulation enables efficient learning by\nencouraging the selection of higher-reward designs. We instantiate CORE for\nhardware-mapping co-design of neural network accelerators, demonstrating that\nit significantly improves sample efficiency and achieves better accelerator\nconfigurations compared to state-of-the-art baselines. Our approach is general\nand applicable to a broad class of discrete-continuous constrained design\nproblems.",
    "published": "2025-06-04T01:08:34Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR",
      "I.2.6; C.3"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.03474v1"
  },
  {
    "arxiv_id": "2506.01827v1",
    "title": "Memory Access Characterization of Large Language Models in CPU\n  Environment and its Potential Impacts",
    "authors": [
      "Spencer Banasik"
    ],
    "author_affiliations": [
      ""
    ],
    "abstract": "As machine learning algorithms are shown to be an increasingly valuable tool,\nthe demand for their access has grown accordingly. Oftentimes, it is infeasible\nto run inference with larger models without an accelerator, which may be\nunavailable in environments that have constraints such as energy consumption,\nsecurity, or cost. To increase the availability of these models, we aim to\nimprove the LLM inference speed on a CPU-only environment by modifying the\ncache architecture. To determine what improvements could be made, we conducted\ntwo experiments using Llama.cpp and the QWEN model: running various cache\nconfigurations and evaluating their performance, and outputting a trace of the\nmemory footprint. Using these experiments, we investigate the memory access\npatterns and performance characteristics to identify potential optimizations.",
    "published": "2025-06-02T16:12:22Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.01827v1"
  },
  {
    "arxiv_id": "2506.01566v1",
    "title": "FlexiSAGA: A Flexible Systolic Array GEMM Accelerator for Sparse and\n  Dense Processing",
    "authors": [
      "Mika Markus M\u00fcller",
      "Konstantin L\u00fcbeck",
      "Alexander Louis-Ferdinand Jung",
      "Jannik Steinmetz",
      "Oliver Bringmann"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Artificial Intelligence (AI) algorithms, such as Deep Neural Networks (DNNs),\nhave become an important tool for a wide range of applications, from computer\nvision to natural language processing. However, the computational complexity of\nDNN inference poses a significant challenge, particularly for processing on\nresource-constrained edge devices. One promising approach to address this\nchallenge is the exploitation of sparsity in DNN operator weights.\n  In this work, we present FlexiSAGA, an architecturally configurable and\ndataflow-flexible AI hardware accelerator for the sparse and dense processing\nof general matrix multiplications (GEMMs). FlexiSAGA supports seven different\nsparse and dense dataflows, enabling efficient processing of resource intensive\nDNN operators. Additionally, we propose a DNN pruning method specifically\ntailored towards the FlexiSAGA architecture, allowing for near-optimal\nprocessing of dense and sparse convolution and fully-connected operators,\nfacilitating a DNN/HW co-design flow. Our results show a whole DNN\nsparse-over-dense inference speedup ranging from 1.41 up to 4.28, outperforming\ncommercial and literature-reported accelerator platforms.",
    "published": "2025-06-02T11:45:37Z",
    "categories": [
      "cs.PF",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.01566v1"
  },
  {
    "arxiv_id": "2506.01166v1",
    "title": "VUSA: Virtually Upscaled Systolic Array Architecture to Exploit\n  Unstructured Sparsity in AI Acceleration",
    "authors": [
      "Shereef Helal",
      "Alberto Garcia-Ortiz",
      "Lennart Bamberg"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "Leveraging high degrees of unstructured sparsity is a promising approach to\nenhance the efficiency of deep neural network DNN accelerators - particularly\nimportant for emerging Edge-AI applications. We introduce VUSA, a\nsystolic-array architecture that virtually grows based on the present sparsity\nto perform larger matrix multiplications with the same number of physical\nmultiply-accumulate MAC units. The proposed architecture achieves saving by 37%\nand 68% in area and power efficiency, respectively, at the same\npeak-performance, compared to a baseline systolic array architecture in a\ncommercial 16-nm technology. Still, the proposed architecture supports\nacceleration for any DNN with any sparsity - even no sparsity at all. Thus, the\nproposed architecture is application-independent, making it viable for\ngeneral-purpose AI acceleration.",
    "published": "2025-06-01T20:59:20Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.01166v1"
  },
  {
    "arxiv_id": "2506.00438v1",
    "title": "PointODE: Lightweight Point Cloud Learning with Neural Ordinary\n  Differential Equations on Edge",
    "authors": [
      "Keisuke Sugiura",
      "Mizuki Yasuda",
      "Hiroki Matsutani"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "Embedded edge devices are often used as a computing platform to run\nreal-world point cloud applications, but recent deep learning-based methods may\nnot fit on such devices due to limited resources. In this paper, we aim to fill\nthis gap by introducing PointODE, a parameter-efficient ResNet-like\narchitecture for point cloud feature extraction based on a stack of MLP blocks\nwith residual connections. We leverage Neural ODE (Ordinary Differential\nEquation), a continuous-depth version of ResNet originally developed for\nmodeling the dynamics of continuous-time systems, to compress PointODE by\nreusing the same parameters across MLP blocks. The point-wise normalization is\nproposed for PointODE to handle the non-uniform distribution of feature points.\nWe introduce PointODE-Elite as a lightweight version with 0.58M trainable\nparameters and design its dedicated accelerator for embedded FPGAs. The\naccelerator consists of a four-stage pipeline to parallelize the feature\nextraction for multiple points and stores the entire parameters on-chip to\neliminate most of the off-chip data transfers. Compared to the ARM Cortex-A53\nCPU, the accelerator implemented on a Xilinx ZCU104 board speeds up the feature\nextraction by 4.9x, leading to 3.7x faster inference and 3.5x better\nenergy-efficiency. Despite the simple architecture, PointODE-Elite shows\ncompetitive accuracy to the state-of-the-art models on both synthetic and\nreal-world classification datasets, greatly improving the trade-off between\naccuracy and inference cost.",
    "published": "2025-05-31T07:34:54Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.00438v1"
  },
  {
    "arxiv_id": "2506.00424v2",
    "title": "COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware\n  using Transfer Learning",
    "authors": [
      "Chamika Sudusinghe",
      "Gerasimos Gerogiannis",
      "Damitha Lenadora",
      "Charles Block",
      "Josep Torrellas",
      "Charith Mendis"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Sparse tensor programs are essential in deep learning and graph analytics,\ndriving the need for optimized processing. To meet this demand, specialized\nhardware accelerators are being developed. Optimizing these programs for\naccelerators is challenging for two reasons: program performance is highly\nsensitive to variations in sparse inputs, and early-stage accelerators rely on\nexpensive simulators. Therefore, ML-based cost models used for optimizing such\nprograms on general-purpose hardware are often ineffective for early-stage\naccelerators, as they require large datasets for proper training. To this end,\nwe introduce COGNATE, a novel framework that leverages inexpensive data samples\nfrom general-purpose hardware (e.g., CPUs) to train cost models, followed by\nfew-shot fine-tuning on emerging hardware. COGNATE exploits the homogeneity of\ninput features across hardware platforms while effectively mitigating\nheterogeneity, enabling cost model training with just 5% of the data samples\nneeded by accelerator-specific models to achieve comparable performance. We\nconduct extensive experiments to demonstrate that COGNATE outperforms existing\ntechniques, achieving average speedups of 1.47x (up to 5.46x) for SpMM and\n1.39x (up to 4.22x) for SDDMM.",
    "published": "2025-05-31T06:59:55Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR",
      "cs.ET"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.00424v2"
  },
  {
    "arxiv_id": "2505.24852v2",
    "title": "Chameleon: A MatMul-Free Temporal Convolutional Network Accelerator for\n  End-to-End Few-Shot and Continual Learning from Sequential Data",
    "authors": [
      "Douwe den Blanken",
      "Charlotte Frenkel"
    ],
    "author_affiliations": [
      "",
      ""
    ],
    "abstract": "On-device learning at the edge enables low-latency, private personalization\nwith improved long-term robustness and reduced maintenance costs. Yet,\nachieving scalable, low-power end-to-end on-chip learning, especially from\nreal-world sequential data with a limited number of examples, is an open\nchallenge. Indeed, accelerators supporting error backpropagation optimize for\nlearning performance at the expense of inference efficiency, while simplified\nlearning algorithms often fail to reach acceptable accuracy targets. In this\nwork, we present Chameleon, leveraging three key contributions to solve these\nchallenges. (i) A unified learning and inference architecture supports few-shot\nlearning (FSL), continual learning (CL) and inference at only 0.5% area\noverhead to the inference logic. (ii) Long temporal dependencies are\nefficiently captured with temporal convolutional networks (TCNs), enabling the\nfirst demonstration of end-to-end on-chip FSL and CL on sequential data and\ninference on 16-kHz raw audio. (iii) A dual-mode, matrix-multiplication-free\ncompute array allows either matching the power consumption of state-of-the-art\ninference-only keyword spotting (KWS) accelerators or enabling $4.3\\times$\nhigher peak GOPS. Fabricated in 40-nm CMOS, Chameleon sets new accuracy records\non Omniglot for end-to-end on-chip FSL (96.8%, 5-way 1-shot, 98.8%, 5-way\n5-shot) and CL (82.2% final accuracy for learning 250 classes with 10 shots),\nwhile maintaining an inference accuracy of 93.3% on the 12-class Google Speech\nCommands dataset at an extreme-edge power budget of 3.1 $\\mu$W.",
    "published": "2025-05-30T17:49:30Z",
    "categories": [
      "cs.AR",
      "cs.LG",
      "C.3; B.6.0; B.7.0; I.2.6; B.5.0"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.24852v2"
  },
  {
    "arxiv_id": "2505.24721v1",
    "title": "Running Conventional Automatic Speech Recognition on Memristor Hardware:\n  A Simulated Approach",
    "authors": [
      "Nick Rossenbach",
      "Benedikt Hilmes",
      "Leon Brackmann",
      "Moritz Gunz",
      "Ralf Schl\u00fcter"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Memristor-based hardware offers new possibilities for energy-efficient\nmachine learning (ML) by providing analog in-memory matrix multiplication.\nCurrent hardware prototypes cannot fit large neural networks, and related\nliterature covers only small ML models for tasks like MNIST or single word\nrecognition. Simulation can be used to explore how hardware properties affect\nlarger models, but existing software assumes simplified hardware. We propose a\nPyTorch-based library based on \"Synaptogen\" to simulate neural network\nexecution with accurately captured memristor hardware properties. For the first\ntime, we show how an ML system with millions of parameters would behave on\nmemristor hardware, using a Conformer trained on the speech recognition task\nTED-LIUMv2 as example. With adjusted quantization-aware training, we limit the\nrelative degradation in word error rate to 25% when using a 3-bit weight\nprecision to execute linear operations via simulated analog computation.",
    "published": "2025-05-30T15:42:41Z",
    "categories": [
      "cs.LG",
      "cs.AR",
      "cs.ET"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.24721v1"
  },
  {
    "arxiv_id": "2505.24183v2",
    "title": "CodeV-R1: Reasoning-Enhanced Verilog Generation",
    "authors": [
      "Yaoyu Zhu",
      "Di Huang",
      "Hanqi Lyu",
      "Xiaoyun Zhang",
      "Chongxiao Li",
      "Wenxuan Shi",
      "Yutong Wu",
      "Jianan Mu",
      "Jinghua Wang",
      "Yang Zhao",
      "Pengwei Jin",
      "Shuyao Cheng",
      "Shengwen Liang",
      "Xishan Zhang",
      "Rui Zhang",
      "Zidong Du",
      "Qi Guo",
      "Xing Hu",
      "Yunji Chen"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Large language models (LLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,\nautomatable verification, such as software programming and mathematical\nproblems. Extending RLVR to electronic design automation (EDA), especially\nautomatically generating hardware description languages (HDLs) like Verilog\nfrom natural-language (NL) specifications, however, poses three key challenges:\nthe lack of automated and accurate verification environments, the scarcity of\nhigh-quality NL-code pairs, and the prohibitive computation cost of RLVR. To\nthis end, we introduce CodeV-R1, an RLVR framework for training Verilog\ngeneration LLMs. First, we develop a rule-based testbench generator that\nperforms robust equivalence checking against golden references. Second, we\npropose a round-trip data synthesis method that pairs open-source Verilog\nsnippets with LLM-generated NL descriptions, verifies code-NL-code consistency\nvia the generated testbench, and filters out inequivalent examples to yield a\nhigh-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training\npipeline: distillation for the cold start of reasoning abilities, followed by\nadaptive DAPO, our novel RLVR algorithm that can reduce training cost by\nadaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves\n68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,\nsurpassing prior state-of-the-art by 12~20%, while matching or even exceeding\nthe performance of 671B DeepSeek-R1. We will release our model, training\npipeline, and dataset to facilitate research in EDA and LLM communities.",
    "published": "2025-05-30T03:51:06Z",
    "categories": [
      "cs.LG",
      "cs.AR",
      "cs.PL"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.24183v2"
  },
  {
    "arxiv_id": "2506.03183v1",
    "title": "Edge Computing for Physics-Driven AI in Computational MRI: A Feasibility\n  Study",
    "authors": [
      "Ya\u015far Utku Al\u00e7alar",
      "Yu Cao",
      "Mehmet Ak\u00e7akaya"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "Physics-driven artificial intelligence (PD-AI) reconstruction methods have\nemerged as the state-of-the-art for accelerating MRI scans, enabling higher\nspatial and temporal resolutions. However, the high resolution of these scans\ngenerates massive data volumes, leading to challenges in transmission, storage,\nand real-time processing. This is particularly pronounced in functional MRI,\nwhere hundreds of volumetric acquisitions further exacerbate these demands.\nEdge computing with FPGAs presents a promising solution for enabling PD-AI\nreconstruction near the MRI sensors, reducing data transfer and storage\nbottlenecks. However, this requires optimization of PD-AI models for hardware\nefficiency through quantization and bypassing traditional FFT-based approaches,\nwhich can be a limitation due to their computational demands. In this work, we\npropose a novel PD-AI computational MRI approach optimized for FPGA-based edge\ncomputing devices, leveraging 8-bit complex data quantization and eliminating\nredundant FFT/IFFT operations. Our results show that this strategy improves\ncomputational efficiency while maintaining reconstruction quality comparable to\nconventional PD-AI methods, and outperforms standard clinical methods. Our\napproach presents an opportunity for high-resolution MRI reconstruction on\nresource-constrained devices, highlighting its potential for real-world\ndeployment.",
    "published": "2025-05-30T02:35:43Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.AR",
      "cs.CV",
      "cs.LG",
      "physics.med-ph"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.03183v1"
  },
  {
    "arxiv_id": "2505.22868v1",
    "title": "CrossNAS: A Cross-Layer Neural Architecture Search Framework for PIM\n  Systems",
    "authors": [
      "Md Hasibul Amin",
      "Mohammadreza Mohammadi",
      "Jason D. Bakos",
      "Ramtin Zand"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      ""
    ],
    "abstract": "In this paper, we propose the CrossNAS framework, an automated approach for\nexploring a vast, multidimensional search space that spans various design\nabstraction layers-circuits, architecture, and systems-to optimize the\ndeployment of machine learning workloads on analog processing-in-memory (PIM)\nsystems. CrossNAS leverages the single-path one-shot weight-sharing strategy\ncombined with the evolutionary search for the first time in the context of PIM\nsystem mapping and optimization. CrossNAS sets a new benchmark for PIM neural\narchitecture search (NAS), outperforming previous methods in both accuracy and\nenergy efficiency while maintaining comparable or shorter search times.",
    "published": "2025-05-28T21:00:49Z",
    "categories": [
      "cs.ET",
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.22868v1"
  },
  {
    "arxiv_id": "2506.15697v1",
    "title": "DeepRTL2: A Versatile Model for RTL-Related Tasks",
    "authors": [
      "Yi Liu",
      "Hongji Zhang",
      "Yunhao Zhou",
      "Zhengyuan Shi",
      "Changran Xu",
      "Qiang Xu"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "The integration of large language models (LLMs) into electronic design\nautomation (EDA) has significantly advanced the field, offering transformative\nbenefits, particularly in register transfer level (RTL) code generation and\nunderstanding. While previous studies have demonstrated the efficacy of\nfine-tuning LLMs for these generation-based tasks, embedding-based tasks, which\nare equally critical to EDA workflows, have been largely overlooked. These\ntasks, including natural language code search, RTL code functionality\nequivalence checking, and performance prediction, are essential for\naccelerating and optimizing the hardware design process. To address this gap,\nwe present DeepRTL2, a family of versatile LLMs that unifies both generation-\nand embedding-based tasks related to RTL. By simultaneously tackling a broad\nrange of tasks, DeepRTL2 represents the first model to provide a comprehensive\nsolution to the diverse challenges in EDA. Through extensive experiments, we\nshow that DeepRTL2 achieves state-of-the-art performance across all evaluated\ntasks.",
    "published": "2025-05-28T09:28:39Z",
    "categories": [
      "cs.AR",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.15697v1"
  },
  {
    "arxiv_id": "2505.21923v1",
    "title": "FALCON: An ML Framework for Fully Automated Layout-Constrained Analog\n  Circuit Design",
    "authors": [
      "Asal Mehradfar",
      "Xuzhe Zhao",
      "Yilun Huang",
      "Emir Ceyani",
      "Yankai Yang",
      "Shihao Han",
      "Hamidreza Aghasi",
      "Salman Avestimehr"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Designing analog circuits from performance specifications is a complex,\nmulti-stage process encompassing topology selection, parameter inference, and\nlayout feasibility. We introduce FALCON, a unified machine learning framework\nthat enables fully automated, specification-driven analog circuit synthesis\nthrough topology selection and layout-constrained optimization. Given a target\nperformance, FALCON first selects an appropriate circuit topology using a\nperformance-driven classifier guided by human design heuristics. Next, it\nemploys a custom, edge-centric graph neural network trained to map circuit\ntopology and parameters to performance, enabling gradient-based parameter\ninference through the learned forward model. This inference is guided by a\ndifferentiable layout cost, derived from analytical equations capturing\nparasitic and frequency-dependent effects, and constrained by design rules. We\ntrain and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave\ncircuits, generated and simulated using Cadence Spectre across 20\nexpert-designed topologies. Through this evaluation, FALCON demonstrates >99\\%\naccuracy in topology inference, <10\\% relative error in performance prediction,\nand efficient layout-aware design that completes in under 1 second per\ninstance. Together, these results position FALCON as a practical and extensible\nfoundation model for end-to-end analog circuit design automation.",
    "published": "2025-05-28T03:16:08Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR",
      "cs.CE"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.21923v1"
  },
  {
    "arxiv_id": "2505.20250v1",
    "title": "Efficient Optimization Accelerator Framework for Multistate Ising\n  Problems",
    "authors": [
      "Chirag Garg",
      "Sayeef Salahuddin"
    ],
    "author_affiliations": [
      "",
      ""
    ],
    "abstract": "Ising Machines are a prominent class of hardware architectures that aim to\nsolve NP-hard combinatorial optimization problems. These machines consist of a\nnetwork of interacting binary spins/neurons that evolve to represent the\noptimum ground state energy solution. Generally, combinatorial problems are\ntransformed into quadratic unconstrained binary optimization (QUBO) form to\nharness the computational efficiency of these Ising machines. However, this\ntransformation, especially for multi-state problems, often leads to a more\ncomplex exploration landscape than the original problem, thus severely\nimpacting the solution quality. To address this challenge, we model the spin\ninteractions as a generalized boolean logic function to significantly reduce\nthe exploration space. We benchmark the graph coloring problem from the class\nof multi-state NP-hard optimization using probabilistic Ising solvers to\nillustrate the effectiveness of our framework. The proposed methodology\nachieves similar accuracy compared to state-of-the-art heuristics and machine\nlearning algorithms, and demonstrates significant improvement over the existing\nIsing methods. Additionally, we demonstrate that combining parallel tempering\nwith our existing framework further reduces the coloring error by up to 50%\ncompared to the conventionally used Gibbs sampling algorithm. We also design a\n1024-neuron all-to-all connected probabilistic Ising accelerator that shows up\nto 10000x performance acceleration compared to heuristics while reducing the\nnumber of required physical neurons by 1.5-4x compared to conventional Ising\nmachines. Indeed, this accelerator solution demonstrates improvement across all\nmetrics over the current methods, i.e., energy, performance, area, and solution\nquality. Thus, this work expands the potential of existing Ising hardware to\nsolve a broad class of these multistate optimization problems.",
    "published": "2025-05-26T17:23:47Z",
    "categories": [
      "cs.AR",
      "cs.DC",
      "cs.ET",
      "cs.LG",
      "stat.CO"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.20250v1"
  },
  {
    "arxiv_id": "2505.18574v2",
    "title": "Autocomp: LLM-Driven Code Optimization for Tensor Accelerators",
    "authors": [
      "Charles Hong",
      "Sahil Bhatia",
      "Alvin Cheung",
      "Yakun Sophia Shao"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      ""
    ],
    "abstract": "Hardware accelerators, especially those designed for tensor processing, have\nbecome ubiquitous in today's computing landscape. However, even with\nsignificant efforts in building compilers, programming these tensor\naccelerators remains challenging, leaving much of their potential\nunderutilized. Recently, large language models (LLMs), trained on large amounts\nof code, have shown significant promise in code generation and optimization\ntasks, but generating low-resource languages like specialized tensor\naccelerator code still poses a significant challenge. We tackle this challenge\nwith Autocomp, an approach that empowers accelerator programmers to leverage\ndomain knowledge and hardware feedback to optimize code via an automated\nLLM-driven search. We accomplish this by: 1) formulating each optimization pass\nas a structured two-phase prompt, divided into planning and code generation\nphases, 2) inserting domain knowledge during planning via a concise and\nadaptable optimization menu, and 3) integrating correctness and performance\nmetrics from hardware as feedback at each search iteration. Across three\ncategories of representative workloads and two different accelerators, we\ndemonstrate that Autocomp-optimized code runs 5.6x (GEMM) and 2.7x\n(convolution) faster than the vendor-provided library, and outperforms\nexpert-level hand-tuned code by 1.4x (GEMM), 1.1x (convolution), and 1.3x\n(fine-grained linear algebra). Additionally, we demonstrate that optimization\nschedules generated from Autocomp can be reused across similar tensor\noperations, improving speedups by up to 24% under a fixed sample budget.",
    "published": "2025-05-24T07:35:34Z",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.18574v2"
  },
  {
    "arxiv_id": "2505.17626v1",
    "title": "Leveraging Stochastic Depth Training for Adaptive Inference",
    "authors": [
      "Guilherme Korol",
      "Antonio Carlos Schneider Beck",
      "Jeronimo Castrillon"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "Dynamic DNN optimization techniques such as layer-skipping offer increased\nadaptability and efficiency gains but can lead to i) a larger memory footprint\nas in decision gates, ii) increased training complexity (e.g., with\nnon-differentiable operations), and iii) less control over performance-quality\ntrade-offs due to its inherent input-dependent execution. To approach these\nissues, we propose a simpler yet effective alternative for adaptive inference\nwith a zero-overhead, single-model, and time-predictable inference. Central to\nour approach is the observation that models trained with Stochastic Depth -- a\nmethod for faster training of residual networks -- become more resilient to\narbitrary layer-skipping at inference time. We propose a method to first select\nnear Pareto-optimal skipping configurations from a stochastically-trained model\nto adapt the inference at runtime later. Compared to original ResNets, our\nmethod shows improvements of up to 2X in power efficiency at accuracy drops as\nlow as 0.71%.",
    "published": "2025-05-23T08:36:56Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.17626v1"
  },
  {
    "arxiv_id": "2505.16968v3",
    "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark",
    "authors": [
      "Ahmed Heakl",
      "Sarim Hashmi",
      "Gustavo Bertolo Stahl",
      "Seung Hun Eddie Han",
      "Salman Khan",
      "Abdulrahman Mahmoud"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "We introduce CASS, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level (CUDA\n<--> HIP) and assembly-level (Nvidia SASS <--> AMD RDNA3) translation. The\ndataset comprises 70k verified code pairs across host and device, addressing a\ncritical gap in low-level GPU code portability. Leveraging this resource, we\ntrain the CASS family of domain-specific language models, achieving 95% source\ntranslation accuracy and 37.5% assembly translation accuracy, substantially\noutperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our\ngenerated code matches native performance in over 85% of test cases, preserving\nruntime and memory behavior. To support rigorous evaluation, we introduce\nCASS-Bench, a curated benchmark spanning 16 GPU domains with ground-truth\nexecution. All data, models, and evaluation tools are released as open source\nto foster progress in GPU compiler tooling, binary compatibility, and\nLLM-guided hardware translation.",
    "published": "2025-05-22T17:48:53Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.PL"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.16968v3"
  },
  {
    "arxiv_id": "2505.15701v1",
    "title": "HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL\n  Graph Databases",
    "authors": [
      "Pingqing Zheng",
      "Jiayin Qin",
      "Fuqi Zhang",
      "Shang Wu",
      "Yu Cao",
      "Caiwen Ding",
      " Yang",
      " Zhao"
    ],
    "author_affiliations": [
      "Katie",
      "Katie",
      "Katie",
      "Katie",
      "Katie",
      "Katie",
      "Katie",
      ""
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated their potential in hardware\ndesign tasks, such as Hardware Description Language (HDL) generation and\ndebugging. Yet, their performance in real-world, repository-level HDL projects\nwith thousands or even tens of thousands of code lines is hindered. To this\nend, we propose HDLxGraph, a novel framework that integrates Graph Retrieval\nAugmented Generation (Graph RAG) with LLMs, introducing HDL-specific graph\nrepresentations by incorporating Abstract Syntax Trees (ASTs) and Data Flow\nGraphs (DFGs) to capture both code graph view and hardware graph view.\nHDLxGraph utilizes a dual-retrieval mechanism that not only mitigates the\nlimited recall issues inherent in similarity-based semantic retrieval by\nincorporating structural information, but also enhances its extensibility to\nvarious real-world tasks by a task-specific retrieval finetuning. Additionally,\nto address the lack of comprehensive HDL search benchmarks, we introduce\nHDLSearch, a multi-granularity evaluation dataset derived from real-world\nrepository-level projects. Experimental results demonstrate that HDLxGraph\nsignificantly improves average search accuracy, debugging efficiency and\ncompletion quality by 12.04%, 12.22% and 5.04% compared to similarity-based\nRAG, respectively. The code of HDLxGraph and collected HDLSearch benchmark are\navailable at https://github.com/Nick-Zheng-Q/HDLxGraph.",
    "published": "2025-05-21T16:14:10Z",
    "categories": [
      "cs.AR",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.15701v1"
  },
  {
    "arxiv_id": "2505.14924v1",
    "title": "SecCAN: An Extended CAN Controller with Embedded Intrusion Detection",
    "authors": [
      "Shashwat Khandelwal",
      "Shreejith Shanker"
    ],
    "author_affiliations": [
      "",
      ""
    ],
    "abstract": "Recent research has highlighted the vulnerability of in-vehicle network\nprotocols such as controller area networks (CAN) and proposed machine\nlearning-based intrusion detection systems (IDSs) as an effective mitigation\ntechnique. However, their efficient integration into vehicular architecture is\nnon-trivial, with existing methods relying on electronic control units\n(ECUs)-coupled IDS accelerators or dedicated ECUs as IDS accelerators. Here,\ninitiating IDS requires complete reception of a CAN message from the\ncontroller, incurring data movement and software overheads. In this paper, we\npresent SecCAN, a novel CAN controller architecture that embeds IDS capability\nwithin the datapath of the controller. This integration allows IDS to tap\nmessages directly from within the CAN controller as they are received from the\nbus, removing overheads incurred by existing ML-based IDSs. A custom-quantised\nmachine-learning accelerator is developed as the IDS engine and embedded into\nSecCAN's receive data path, with optimisations to overlap the IDS inference\nwith the protocol's reception window. We implement SecCAN on AMD XCZU7EV FPGA\nto quantify its performance and benefits in hardware, using multiple attack\ndatasets. We show that SecCAN can completely hide the IDS latency within the\nCAN reception window for all CAN packet sizes and detect multiple attacks with\nstate-of-the-art accuracy with zero software overheads on the ECU and low\nenergy overhead (73.7 uJ per message) for IDS inference. Also, SecCAN incurs\nlimited resource overhead compared to a standard CAN controller (< 30% LUT, <\n1% FF), making it ideally suited for automotive deployment.",
    "published": "2025-05-20T21:16:03Z",
    "categories": [
      "eess.SY",
      "cs.AR",
      "cs.LG",
      "cs.SY"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.14924v1"
  },
  {
    "arxiv_id": "2505.14314v2",
    "title": "Low-Cost FlashAttention with Fused Exponential and Multiplication\n  Hardware Operators",
    "authors": [
      "Kosmas Alexandridis",
      "Vasileios Titopoulos",
      "Giorgos Dimitrakopoulos"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "Attention mechanisms, particularly within Transformer architectures and large\nlanguage models (LLMs), have revolutionized sequence modeling in machine\nlearning and artificial intelligence applications. To compute attention for\nincreasingly long sequences, specialized accelerators have been proposed to\nexecute key attention steps directly in hardware. Among the various recently\nproposed architectures, those based on variants of the FlashAttention\nalgorithm, originally designed for GPUs, stand out due to their optimized\ncomputation, tiling capabilities, and reduced memory traffic. In this work, we\nfocus on optimizing the kernel of floating-point-based FlashAttention using new\nhardware operators that fuse the computation of exponentials and vector\nmultiplications, e.g., e^x, V. The proposed ExpMul hardware operators\nsignificantly reduce the area and power costs of FlashAttention-based hardware\naccelerators. When implemented in a 28nm ASIC technology, they achieve\nimprovements of 28.8% in area and 17.6% in power, on average, compared to\nstate-of-the-art hardware architectures with separate exponentials and vector\nmultiplications hardware operators.",
    "published": "2025-05-20T13:00:59Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.14314v2"
  },
  {
    "arxiv_id": "2505.14201v1",
    "title": "FLASH-D: FlashAttention with Hidden Softmax Division",
    "authors": [
      "Kosmas Alexandridis",
      "Vasileios Titopoulos",
      "Giorgos Dimitrakopoulos"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "The transformer's attention mechanism has revolutionized AI and machine\nlearning, with its efficient computation being crucial to its performance.\nHowever, calculating attention involves matrix operations interspersed with\nsoftmax rescaling, which inherently slows down computation and requires\nprocessing the entire input sequence. Building on online softmax computation,\nFlashAttention integrates softmax calculation with matrix arithmetic, enabling\ntiled computation independent of sequence length. While optimized for GPUs,\nFlashAttention's simplicity makes it amenable to direct hardware acceleration.\nThis work re-evaluates the core FlashAttention kernel, presenting FLASH-D a\nmathematically equivalent, yet simplified, formulation that achieves: (a)\nhiding softmax division within other non-linear function evaluations; (b)\ninherently numerically stable computation of exponentials, eliminating the need\nfor maximum value subtraction; and (c) a reduction in computational cost\nwithout introducing numerical approximations to the FlashAttention kernel.\nImportantly, the essential FlashAttention properties that facilitate efficient\ntiled implementation are fully preserved. Hardware implementation results at\n28nm demonstrate that this proposed formulation achieves a 22.8% reduction in\narea and a 20.3% reduction in power, on average, compared to state-of-the-art\nparallel hardware architectures without any performance penalty.",
    "published": "2025-05-20T11:01:33Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.14201v1"
  },
  {
    "arxiv_id": "2505.13357v1",
    "title": "Introducing Instruction-Accurate Simulators for Performance Estimation\n  of Autotuning Workloads",
    "authors": [
      "Rebecca Pelke",
      "Nils Bosbach",
      "Lennart M. Reimann",
      "Rainer Leupers"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      ""
    ],
    "abstract": "Accelerating Machine Learning (ML) workloads requires efficient methods due\nto their large optimization space. Autotuning has emerged as an effective\napproach for systematically evaluating variations of implementations.\nTraditionally, autotuning requires the workloads to be executed on the target\nhardware (HW). We present an interface that allows executing autotuning\nworkloads on simulators. This approach offers high scalability when the\navailability of the target HW is limited, as many simulations can be run in\nparallel on any accessible HW. Additionally, we evaluate the feasibility of\nusing fast instruction-accurate simulators for autotuning. We train various\npredictors to forecast the performance of ML workload implementations on the\ntarget HW based on simulation statistics. Our results demonstrate that the\ntuned predictors are highly effective. The best workload implementation in\nterms of actual run time on the target HW is always within the top 3 % of\npredictions for the tested x86, ARM, and RISC-V-based architectures. In the\nbest case, this approach outperforms native execution on the target HW for\nembedded architectures when running as few as three samples on three simulators\nin parallel.",
    "published": "2025-05-19T16:59:07Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.13357v1"
  },
  {
    "arxiv_id": "2505.12523v1",
    "title": "Energy-Aware Deep Learning on Resource-Constrained Hardware",
    "authors": [
      "Josh Millar",
      "Hamed Haddadi",
      "Anil Madhavapeddy"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "The use of deep learning (DL) on Internet of Things (IoT) and mobile devices\noffers numerous advantages over cloud-based processing. However, such devices\nface substantial energy constraints to prolong battery-life, or may even\noperate intermittently via energy-harvesting. Consequently,\n\\textit{energy-aware} approaches for optimizing DL inference and training on\nsuch resource-constrained devices have garnered recent interest. We present an\noverview of such approaches, outlining their methodologies, implications for\nenergy consumption and system-level efficiency, and their limitations in terms\nof supported network types, hardware platforms, and application scenarios. We\nhope our review offers a clear synthesis of the evolving energy-aware DL\nlandscape and serves as a foundation for future research in energy-constrained\ncomputing.",
    "published": "2025-05-18T19:17:03Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.12523v1"
  },
  {
    "arxiv_id": "2505.11849v1",
    "title": "VeriReason: Reinforcement Learning with Testbench Feedback for\n  Reasoning-Enhanced Verilog Generation",
    "authors": [
      "Yiting Wang",
      "Guoheng Sun",
      "Wanghao Ye",
      "Gang Qu",
      "Ang Li"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Automating Register Transfer Level (RTL) code generation using Large Language\nModels (LLMs) offers substantial promise for streamlining digital circuit\ndesign and reducing human effort. However, current LLM-based approaches face\nsignificant challenges with training data scarcity, poor specification-code\nalignment, lack of verification mechanisms, and balancing generalization with\nspecialization. Inspired by DeepSeek-R1, we introduce VeriReason, a framework\nintegrating supervised fine-tuning with Guided Reward Proximal Optimization\n(GRPO) reinforcement learning for RTL generation. Using curated training\nexamples and a feedback-driven reward model, VeriReason combines testbench\nevaluations with structural heuristics while embedding self-checking\ncapabilities for autonomous error correction. On the VerilogEval Benchmark,\nVeriReason delivers significant improvements: achieving 83.1% functional\ncorrectness on the VerilogEval Machine benchmark, substantially outperforming\nboth comparable-sized models and much larger commercial systems like GPT-4\nTurbo. Additionally, our approach demonstrates up to a 2.8X increase in\nfirst-attempt functional correctness compared to baseline methods and exhibits\nrobust generalization to unseen designs. To our knowledge, VeriReason\nrepresents the first system to successfully integrate explicit reasoning\ncapabilities with reinforcement learning for Verilog generation, establishing a\nnew state-of-the-art for automated RTL synthesis. The models and datasets are\navailable at: https://huggingface.co/collections/AI4EDA-CASE Code is Available\nat: https://github.com/NellyW8/VeriReason",
    "published": "2025-05-17T05:25:01Z",
    "categories": [
      "cs.AI",
      "cs.AR",
      "cs.LG",
      "cs.PL"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.11849v1"
  },
  {
    "arxiv_id": "2505.11594v1",
    "title": "SageAttention3: Microscaling FP4 Attention for Inference and An\n  Exploration of 8-Bit Training",
    "authors": [
      "Jintao Zhang",
      "Jia Wei",
      "Pengle Zhang",
      "Xiaoming Xu",
      "Haofeng Huang",
      "Haoxu Wang",
      "Kai Jiang",
      "Jun Zhu",
      "Jianfei Chen"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "The efficiency of attention is important due to its quadratic time\ncomplexity. We enhance the efficiency of attention through two key\ncontributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to\naccelerate attention computation. Our implementation achieves 1038 TOPS on\nRTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090.\nExperiments show that our FP4 attention can accelerate inference of various\nmodels in a plug-and-play way. Second, we pioneer low-bit attention to training\ntasks. Existing low-bit attention works like FlashAttention3 and SageAttention\nfocus only on inference. However, the efficiency of training large models is\nalso important. To explore whether low-bit attention can be effectively applied\nto training tasks, we design an accurate and efficient 8-bit attention for both\nforward and backward propagation. Experiments indicate that 8-bit attention\nachieves lossless performance in fine-tuning tasks but exhibits slower\nconvergence in pretraining tasks. The code will be available at\nhttps://github.com/thu-ml/SageAttention.",
    "published": "2025-05-16T18:01:54Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR",
      "cs.CV",
      "cs.PF"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.11594v1"
  },
  {
    "arxiv_id": "2505.11067v1",
    "title": "Assessing the Performance of Analog Training for Transfer Learning",
    "authors": [
      "Omobayode Fagbohungbe",
      "Corey Lammie",
      "Malte J. Rasch",
      "Takashi Ando",
      "Tayfun Gokmen",
      "Vijay Narayanan"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Analog in-memory computing is a next-generation computing paradigm that\npromises fast, parallel, and energy-efficient deep learning training and\ntransfer learning (TL). However, achieving this promise has remained elusive\ndue to a lack of suitable training algorithms. Analog memory devices exhibit\nasymmetric and non-linear switching behavior in addition to device-to-device\nvariation, meaning that most, if not all, of the current off-the-shelf training\nalgorithms cannot achieve good training outcomes. Also, recently introduced\nalgorithms have enjoyed limited attention, as they require bi-directionally\nswitching devices of unrealistically high symmetry and precision and are highly\nsensitive. A new algorithm chopped TTv2 (c-TTv2), has been introduced, which\nleverages the chopped technique to address many of the challenges mentioned\nabove. In this paper, we assess the performance of the c-TTv2 algorithm for\nanalog TL using a Swin-ViT model on a subset of the CIFAR100 dataset. We also\ninvestigate the robustness of our algorithm to changes in some device\nspecifications, including weight transfer noise, symmetry point skew, and\nsymmetry point variability",
    "published": "2025-05-16T10:02:32Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR",
      "cs.CV",
      "cs.DC",
      "cs.NE"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.11067v1"
  },
  {
    "arxiv_id": "2506.00008v1",
    "title": "AI Accelerators for Large Language Model Inference: Architecture\n  Analysis and Scaling Strategies",
    "authors": [
      "Amit Sharma"
    ],
    "author_affiliations": [
      ""
    ],
    "abstract": "The rapid growth of large-language models (LLMs) is driving a new wave of\nspecialized hardware for inference. This paper presents the first\nworkload-centric, cross-architectural performance study of commercial AI\naccelerators, spanning GPU-based chips, hybrid packages, and wafer-scale\nengines. We compare memory hierarchies, compute fabrics, and on-chip\ninterconnects, and observe up to 3.7x performance variation across\narchitectures as batch size and sequence length change. Four scaling techniques\nfor trillion-parameter models are examined; expert parallelism offers an 8.4x\nparameter-to-compute advantage but incurs 2.1x higher latency variance than\ntensor parallelism. These findings provide quantitative guidance for matching\nworkloads to accelerators and reveal architectural gaps that next-generation\ndesigns must address.",
    "published": "2025-05-13T20:21:20Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.00008v1"
  },
  {
    "arxiv_id": "2505.08599v1",
    "title": "MINIMALIST: switched-capacitor circuits for efficient in-memory\n  computation of gated recurrent units",
    "authors": [
      "Sebastian Billaudelle",
      "Laura Kriener",
      "Filippo Moro",
      "Tristan Torchet",
      "Melika Payvand"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Recurrent neural networks (RNNs) have been a long-standing candidate for\nprocessing of temporal sequence data, especially in memory-constrained systems\nthat one may find in embedded edge computing environments. Recent advances in\ntraining paradigms have now inspired new generations of efficient RNNs. We\nintroduce a streamlined and hardware-compatible architecture based on minimal\ngated recurrent units (GRUs), and an accompanying efficient mixed-signal\nhardware implementation of the model. The proposed design leverages\nswitched-capacitor circuits not only for in-memory computation (IMC), but also\nfor the gated state updates. The mixed-signal cores rely solely on commodity\ncircuits consisting of metal capacitors, transmission gates, and a clocked\ncomparator, thus greatly facilitating scaling and transfer to other technology\nnodes.\n  We benchmark the performance of our architecture on time series data,\nintroducing all constraints required for a direct mapping to the hardware\nsystem. The direct compatibility is verified in mixed-signal simulations,\nreproducing data recorded from the software-only network model.",
    "published": "2025-05-13T14:13:41Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.08599v1"
  },
  {
    "arxiv_id": "2506.00007v1",
    "title": "Emerging ML-AI Techniques for Analog and RF EDA",
    "authors": [
      "Zhengfeng Wu",
      "Ziyi Chen",
      "Nnaemeka Achebe",
      "Vaibhav V. Rao",
      "Pratik Shrestha",
      "Ioannis Savidis"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "This survey explores the integration of machine learning (ML) into EDA\nworkflows for analog and RF circuits, addressing challenges unique to analog\ndesign, which include complex constraints, nonlinear design spaces, and high\ncomputational costs. State-of-the-art learning and optimization techniques are\nreviewed for circuit tasks such as constraint formulation, topology generation,\ndevice modeling, sizing, placement, and routing. The survey highlights the\ncapability of ML to enhance automation, improve design quality, and reduce\ntime-to-market while meeting the target specifications of an analog or RF\ncircuit. Emerging trends and cross-cutting challenges, including robustness to\nvariations and considerations of interconnect parasitics, are also discussed.",
    "published": "2025-05-12T22:06:33Z",
    "categories": [
      "cs.AR",
      "cs.CE",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.00007v1"
  },
  {
    "arxiv_id": "2505.13479v1",
    "title": "RTL++: Graph-enhanced LLM for RTL Code Generation",
    "authors": [
      "Mohammad Akyash",
      "Kimia Azar",
      "Hadi Kamali"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "As hardware design complexity escalates, there is an urgent need for advanced\nautomation in electronic design automation (EDA). Traditional register transfer\nlevel (RTL) design methods are manual, time-consuming, and prone to errors.\nWhile commercial (instruction-tuned) large language models (LLMs) shows\npromising performance for automation, they pose security and privacy concerns.\nOpen-source models offer alternatives; however, they frequently fall short in\nquality/correctness, largely due to limited, high-quality RTL code data\nessential for effective training and generalization. This paper proposes RTL++,\na first-of-its-kind LLM-assisted method for RTL code generation that utilizes\ngraph representations of code structures to enhance the quality of generated\ncode. By encoding RTL code into a textualized control flowgraphs (CFG) and data\nflow graphs (DFG), RTL++ captures the inherent hierarchy, dependencies, and\nrelationships within the code. This structured graph-based approach enhances\nthe context available to LLMs, enabling them to better understand and generate\ninstructions. By focusing on data generation through graph representations,\nRTL++ addresses the limitations of previous approaches that rely solely on code\nand suffer from lack of diversity. Experimental results demonstrate that RTL++\noutperforms state-of-the-art models fine-tuned for RTL generation, as evaluated\nusing the VerilogEval benchmark's Pass@1/5/10 metric, as well as the RTLLM1.1\nmodel, which highlight the effectiveness of graph-enhanced context in advancing\nthe capabilities of LLM-assisted RTL code generation.",
    "published": "2025-05-11T00:17:26Z",
    "categories": [
      "cs.PL",
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.13479v1"
  },
  {
    "arxiv_id": "2505.05893v1",
    "title": "LightNobel: Improving Sequence Length Limitation in Protein Structure\n  Prediction Model via Adaptive Activation Quantization",
    "authors": [
      "Seunghee Han",
      "Soongyu Choi",
      "Joo-Young Kim"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "Recent advances in Protein Structure Prediction Models (PPMs), such as\nAlphaFold2 and ESMFold, have revolutionized computational biology by achieving\nunprecedented accuracy in predicting three-dimensional protein folding\nstructures. However, these models face significant scalability challenges,\nparticularly when processing proteins with long amino acid sequences (e.g.,\nsequence length > 1,000). The primary bottleneck that arises from the\nexponential growth in activation sizes is driven by the unique data structure\nin PPM, which introduces an additional dimension that leads to substantial\nmemory and computational demands. These limitations have hindered the effective\nscaling of PPM for real-world applications, such as analyzing large proteins or\ncomplex multimers with critical biological and pharmaceutical relevance.\n  In this paper, we present LightNobel, the first hardware-software co-designed\naccelerator developed to overcome scalability limitations on the sequence\nlength in PPM. At the software level, we propose Token-wise Adaptive Activation\nQuantization (AAQ), which leverages unique token-wise characteristics, such as\ndistogram patterns in PPM activations, to enable fine-grained quantization\ntechniques without compromising accuracy. At the hardware level, LightNobel\nintegrates the multi-precision reconfigurable matrix processing unit (RMPU) and\nversatile vector processing unit (VVPU) to enable the efficient execution of\nAAQ. Through these innovations, LightNobel achieves up to 8.44x, 8.41x speedup\nand 37.29x, 43.35x higher power efficiency over the latest NVIDIA A100 and H100\nGPUs, respectively, while maintaining negligible accuracy loss. It also reduces\nthe peak memory requirement up to 120.05x in PPM, enabling scalable processing\nfor proteins with long sequences.",
    "published": "2025-05-09T09:01:10Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "q-bio.BM",
      "B.7; I.2; J.3"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.05893v1"
  },
  {
    "arxiv_id": "2505.04524v1",
    "title": "Edge-GPU Based Face Tracking for Face Detection and Recognition\n  Acceleration",
    "authors": [
      "Asma Baobaid",
      "Mahmoud Meribout"
    ],
    "author_affiliations": [
      "",
      ""
    ],
    "abstract": "Cost-effective machine vision systems dedicated to real-time and accurate\nface detection and recognition in public places are crucial for many modern\napplications. However, despite their high performance, which could be reached\nusing specialized edge or cloud AI hardware accelerators, there is still room\nfor improvement in throughput and power consumption. This paper aims to suggest\na combined hardware-software approach that optimizes face detection and\nrecognition systems on one of the latest edge GPUs, namely NVIDIA Jetson AGX\nOrin. First, it leverages the simultaneous usage of all its hardware engines to\nimprove processing time. This offers an improvement over previous works where\nthese tasks were mainly allocated automatically and exclusively to the CPU or,\nto a higher extent, to the GPU core. Additionally, the paper suggests\nintegrating a face tracker module to avoid redundantly running the face\nrecognition algorithm for every frame but only when a new face appears in the\nscene. The results of extended experiments suggest that simultaneous usage of\nall the hardware engines that are available in the Orin GPU and tracker\nintegration into the pipeline yield an impressive throughput of 290 FPS (frames\nper second) on 1920 x 1080 input size frames containing in average of 6\nfaces/frame. Additionally, a substantial saving of power consumption of around\n800 mW was achieved when compared to running the task on the CPU/GPU engines\nonly and without integrating a tracker into the Orin GPU\\'92s pipeline. This\nhardware-codesign approach can pave the way to design high-performance machine\nvision systems at the edge, critically needed in video monitoring in public\nplaces where several nearby cameras are usually deployed for a same scene.",
    "published": "2025-05-07T15:57:53Z",
    "categories": [
      "cs.CV",
      "cs.AR",
      "cs.LG",
      "eess.IV"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.04524v1"
  },
  {
    "arxiv_id": "2505.08793v1",
    "title": "Onboard Optimization and Learning: A Survey",
    "authors": [
      "Monirul Islam Pavel",
      "Siyi Hu",
      "Mahardhika Pratama",
      "Ryszard Kowalczyk"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      ""
    ],
    "abstract": "Onboard learning is a transformative approach in edge AI, enabling real-time\ndata processing, decision-making, and adaptive model training directly on\nresource-constrained devices without relying on centralized servers. This\nparadigm is crucial for applications demanding low latency, enhanced privacy,\nand energy efficiency. However, onboard learning faces challenges such as\nlimited computational resources, high inference costs, and security\nvulnerabilities. This survey explores a comprehensive range of methodologies\nthat address these challenges, focusing on techniques that optimize model\nefficiency, accelerate inference, and support collaborative learning across\ndistributed devices. Approaches for reducing model complexity, improving\ninference speed, and ensuring privacy-preserving computation are examined\nalongside emerging strategies that enhance scalability and adaptability in\ndynamic environments. By bridging advancements in hardware-software co-design,\nmodel compression, and decentralized learning, this survey provides insights\ninto the current state of onboard learning to enable robust, efficient, and\nsecure AI deployment at the edge.",
    "published": "2025-05-07T07:47:14Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.08793v1"
  },
  {
    "arxiv_id": "2505.13462v1",
    "title": "End-to-end fully-binarized network design: from Generic Learned\n  Thermometer to Block Pruning",
    "authors": [
      "Thien Nguyen",
      "William Guicquero"
    ],
    "author_affiliations": [
      "",
      ""
    ],
    "abstract": "Existing works on Binary Neural Network (BNN) mainly focus on model's weights\nand activations while discarding considerations on the input raw data. This\narticle introduces Generic Learned Thermometer (GLT), an encoding technique to\nimprove input data representation for BNN, relying on learning non linear\nquantization thresholds. This technique consists in multiple data binarizations\nwhich can advantageously replace a conventional Analog to Digital Conversion\n(ADC) that uses natural binary coding. Additionally, we jointly propose a\ncompact topology with light-weight grouped convolutions being trained thanks to\nblock pruning and Knowledge Distillation (KD), aiming at reducing furthermore\nthe model size so as its computational complexity. We show that GLT brings\nversatility to the BNN by intrinsically performing global tone mapping,\nenabling significant accuracy gains in practice (demonstrated by simulations on\nthe STL-10 and VWW datasets). Moreover, when combining GLT with our proposed\nblock-pruning technique, we successfully achieve lightweight (under 1Mb),\nfully-binarized models with limited accuracy degradation while being suitable\nfor in-sensor always-on inference use cases.",
    "published": "2025-05-05T13:50:29Z",
    "categories": [
      "cs.LG",
      "cs.AR",
      "cs.CV",
      "eess.IV",
      "stat.ML"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.13462v1"
  },
  {
    "arxiv_id": "2505.02516v1",
    "title": "Machine-Learning-Powered Neural Interfaces for Smart Prosthetics and\n  Diagnostics",
    "authors": [
      "MohammadAli Shaeri",
      "Jinhan Liu",
      "Mahsa Shoaran"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "Advanced neural interfaces are transforming applications ranging from\nneuroscience research to diagnostic tools (for mental state recognition, tremor\nand seizure detection) as well as prosthetic devices (for motor and\ncommunication recovery). By integrating complex functions into miniaturized\nneural devices, these systems unlock significant opportunities for personalized\nassistive technologies and adaptive therapeutic interventions. Leveraging\nhigh-density neural recordings, on-site signal processing, and machine learning\n(ML), these interfaces extract critical features, identify disease\nneuro-markers, and enable accurate, low-latency neural decoding. This\nintegration facilitates real-time interpretation of neural signals, adaptive\nmodulation of brain activity, and efficient control of assistive devices.\nMoreover, the synergy between neural interfaces and ML has paved the way for\nself-sufficient, ubiquitous platforms capable of operating in diverse\nenvironments with minimal hardware costs and external dependencies. In this\nwork, we review recent advancements in AI-driven decoding algorithms and\nenergy-efficient System-on-Chip (SoC) platforms for next-generation\nminiaturized neural devices. These innovations highlight the potential for\ndeveloping intelligent neural interfaces, addressing critical challenges in\nscalability, reliability, interpretability, and user adaptability.",
    "published": "2025-05-05T09:49:13Z",
    "categories": [
      "cs.AI",
      "cs.AR",
      "cs.LG",
      "eess.SP",
      "q-bio.NC",
      "I.2.0; B.7.0; I.5.1; C.3"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.02516v1"
  },
  {
    "arxiv_id": "2505.02314v1",
    "title": "NeuroSim V1.5: Improved Software Backbone for Benchmarking\n  Compute-in-Memory Accelerators with Device and Circuit-level Non-idealities",
    "authors": [
      "James Read",
      "Ming-Yen Lee",
      "Wei-Hsing Huang",
      "Yuan-Chun Luo",
      "Anni Lu",
      "Shimeng Yu"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "The exponential growth of artificial intelligence (AI) applications has\nexposed the inefficiency of conventional von Neumann architectures, where\nfrequent data transfers between compute units and memory create significant\nenergy and latency bottlenecks. Analog Computing-in-Memory (ACIM) addresses\nthis challenge by performing multiply-accumulate (MAC) operations directly in\nthe memory arrays, substantially reducing data movement. However, designing\nrobust ACIM accelerators requires accurate modeling of device- and\ncircuit-level non-idealities. In this work, we present NeuroSim V1.5,\nintroducing several key advances: (1) seamless integration with TensorRT's\npost-training quantization flow enabling support for more neural networks\nincluding transformers, (2) a flexible noise injection methodology built on\npre-characterized statistical models, making it straightforward to incorporate\ndata from SPICE simulations or silicon measurements, (3) expanded device\nsupport including emerging non-volatile capacitive memories, and (4) up to 6.5x\nfaster runtime than NeuroSim V1.4 through optimized behavioral simulation. The\ncombination of these capabilities uniquely enables systematic design space\nexploration across both accuracy and hardware efficiency metrics. Through\nmultiple case studies, we demonstrate optimization of critical design\nparameters while maintaining network accuracy. By bridging high-fidelity noise\nmodeling with efficient simulation, NeuroSim V1.5 advances the design and\nvalidation of next-generation ACIM accelerators. All NeuroSim versions are\navailable open-source at https://github.com/neurosim/NeuroSim.",
    "published": "2025-05-05T02:07:04Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.02314v1"
  },
  {
    "arxiv_id": "2505.02181v1",
    "title": "Efficient FPGA Implementation of Time-Domain Popcount for Low-Complexity\n  Machine Learning",
    "authors": [
      "Shengyu Duan",
      "Marcos L. L. Sartori",
      "Rishad Shafik",
      "Alex Yakovlev",
      "Emre Ozer"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Population count (popcount) is a crucial operation for many low-complexity\nmachine learning (ML) algorithms, including Tsetlin Machine (TM)-a promising\nnew ML method, particularly well-suited for solving classification tasks. The\ninference mechanism in TM consists of propositional logic-based structures\nwithin each class, followed by a majority voting scheme, which makes the\nclassification decision. In TM, the voters are the outputs of Boolean clauses.\nThe voting mechanism comprises two operations: popcount for each class and\ndetermining the class with the maximum vote by means of an argmax operation.\n  While TMs offer a lightweight ML alternative, their performance is often\nlimited by the high computational cost of popcount and comparison required to\nproduce the argmax result. In this paper, we propose an innovative approach to\naccelerate and optimize these operations by performing them in the time domain.\nOur time-domain implementation uses programmable delay lines (PDLs) and\narbiters to efficiently manage these tasks through delay-based mechanisms. We\nalso present an FPGA design flow for practical implementation of the\ntime-domain popcount, addressing delay skew and ensuring that the behavior\nmatches that of the model's intended functionality. By leveraging the natural\ncompatibility of the proposed popcount with asynchronous architectures, we\ndemonstrate significant improvements in an asynchronous TM, including up to 38%\nreduction in latency, 43.1% reduction in dynamic power, and 15% savings in\nresource utilization, compared to synchronous TMs using adder-based popcount.",
    "published": "2025-05-04T16:44:15Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.02181v1"
  },
  {
    "arxiv_id": "2505.13461v1",
    "title": "FPGA-based Acceleration for Convolutional Neural Networks: A\n  Comprehensive Review",
    "authors": [
      "Junye Jiang",
      "Yaan Zhou",
      "Yuanhao Gong",
      "Haoxuan Yuan",
      "Shuanglong Liu"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Convolutional Neural Networks (CNNs) are fundamental to deep learning,\ndriving applications across various domains. However, their growing complexity\nhas significantly increased computational demands, necessitating efficient\nhardware accelerators. Field-Programmable Gate Arrays (FPGAs) have emerged as a\nleading solution, offering reconfigurability, parallelism, and energy\nefficiency. This paper provides a comprehensive review of FPGA-based hardware\naccelerators specifically designed for CNNs. It presents and summarizes the\nperformance evaluation framework grounded in existing studies and explores key\noptimization strategies, such as parallel computing, dataflow optimization, and\nhardware-software co-design. It also compares various FPGA architectures in\nterms of latency, throughput, compute efficiency, power consumption, and\nresource utilization. Finally, the paper highlights future challenges and\nopportunities, emphasizing the potential for continued innovation in this\nfield.",
    "published": "2025-05-04T04:03:37Z",
    "categories": [
      "cs.LG",
      "cs.AR",
      "C.3"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.13461v1"
  },
  {
    "arxiv_id": "2505.01386v2",
    "title": "Carbon Aware Transformers Through Joint Model-Hardware Optimization",
    "authors": [
      "Irene Wang",
      "Newsha Ardalani",
      "Mostafa Elhoushi",
      "Daniel Jiang",
      "Samuel Hsia",
      "Ekin Sumbul",
      "Divya Mahajan",
      "Carole-Jean Wu",
      "Bilge Acun"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "The rapid growth of machine learning (ML) systems necessitates a more\ncomprehensive evaluation of their environmental impact, particularly their\ncarbon footprint, which comprises operational carbon from training and\ninference execution and embodied carbon from hardware manufacturing and its\nentire life-cycle. Despite the increasing importance of embodied emissions,\nthere is a lack of tools and frameworks to holistically quantify and optimize\nthe total carbon footprint of ML systems. To address this, we propose\nCATransformers, a carbon-aware architecture search framework that enables\nsustainability-driven co-optimization of ML models and hardware architectures.\nBy incorporating both operational and embodied carbon metrics into early design\nspace exploration of domain-specific hardware accelerators, CATransformers\ndemonstrates that optimizing for carbon yields design choices distinct from\nthose optimized solely for latency or energy efficiency. We apply our framework\nto multi-modal CLIP-based models, producing CarbonCLIP, a family of CLIP models\nachieving up to 17% reduction in total carbon emissions while maintaining\naccuracy and latency compared to state-of-the-art edge small CLIP baselines.\nThis work underscores the need for holistic optimization methods to design\nhigh-performance, environmentally sustainable AI systems.",
    "published": "2025-05-02T16:49:10Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.01386v2"
  },
  {
    "arxiv_id": "2505.01107v1",
    "title": "CIMFlow: An Integrated Framework for Systematic Design and Evaluation of\n  Digital CIM Architectures",
    "authors": [
      "Yingjie Qi",
      "Jianlei Yang",
      "Yiou Wang",
      "Yikun Wang",
      "Dayu Wang",
      "Ling Tang",
      "Cenlin Duan",
      "Xiaolin He",
      "Weisheng Zhao"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Digital Compute-in-Memory (CIM) architectures have shown great promise in\nDeep Neural Network (DNN) acceleration by effectively addressing the \"memory\nwall\" bottleneck. However, the development and optimization of digital CIM\naccelerators are hindered by the lack of comprehensive tools that encompass\nboth software and hardware design spaces. Moreover, existing design and\nevaluation frameworks often lack support for the capacity constraints inherent\nin digital CIM architectures. In this paper, we present CIMFlow, an integrated\nframework that provides an out-of-the-box workflow for implementing and\nevaluating DNN workloads on digital CIM architectures. CIMFlow bridges the\ncompilation and simulation infrastructures with a flexible instruction set\narchitecture (ISA) design, and addresses the constraints of digital CIM through\nadvanced partitioning and parallelism strategies in the compilation flow. Our\nevaluation demonstrates that CIMFlow enables systematic prototyping and\noptimization of digital CIM architectures across diverse configurations,\nproviding researchers and designers with an accessible platform for extensive\ndesign space exploration.",
    "published": "2025-05-02T08:38:30Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.01107v1"
  },
  {
    "arxiv_id": "2504.20401v1",
    "title": "Nonlinear Computation with Linear Optics via Source-Position Encoding",
    "authors": [
      "N. Richardson",
      "C. Bosch",
      "R. P. Adams"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "Optical computing systems provide an alternate hardware model which appears\nto be aligned with the demands of neural network workloads. However, the\nchallenge of implementing energy efficient nonlinearities in optics -- a key\nrequirement for realizing neural networks -- is a conspicuous missing link. In\nthis work we introduce a novel method to achieve nonlinear computation in fully\nlinear media. Our method can operate at low power and requires only the ability\nto drive the optical system at a data-dependent spatial position. Leveraging\nthis positional encoding, we formulate a fully automated,\ntopology-optimization-based hardware design framework for extremely specialized\noptical neural networks, drawing on modern advancements in optimization and\nmachine learning. We evaluate our optical designs on machine learning\nclassification tasks: demonstrating significant improvements over linear\nmethods, and competitive performance when compared to standard artificial\nneural networks.",
    "published": "2025-04-29T03:55:05Z",
    "categories": [
      "physics.optics",
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.20401v1"
  },
  {
    "arxiv_id": "2504.19797v1",
    "title": "Dynamic Tsetlin Machine Accelerators for On-Chip Training at the Edge\n  using FPGAs",
    "authors": [
      "Gang Mao",
      "Tousif Rahman",
      "Sidharth Maheshwari",
      "Bob Pattison",
      "Zhuang Shao",
      "Rishad Shafik",
      "Alex Yakovlev"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "The increased demand for data privacy and security in machine learning (ML)\napplications has put impetus on effective edge training on Internet-of-Things\n(IoT) nodes. Edge training aims to leverage speed, energy efficiency and\nadaptability within the resource constraints of the nodes. Deploying and\ntraining Deep Neural Networks (DNNs)-based models at the edge, although\naccurate, posit significant challenges from the back-propagation algorithm's\ncomplexity, bit precision trade-offs, and heterogeneity of DNN layers. This\npaper presents a Dynamic Tsetlin Machine (DTM) training accelerator as an\nalternative to DNN implementations. DTM utilizes logic-based on-chip inference\nwith finite-state automata-driven learning within the same Field Programmable\nGate Array (FPGA) package. Underpinned on the Vanilla and Coalesced Tsetlin\nMachine algorithms, the dynamic aspect of the accelerator design allows for a\nrun-time reconfiguration targeting different datasets, model architectures, and\nmodel sizes without resynthesis. This makes the DTM suitable for targeting\nmultivariate sensor-based edge tasks. Compared to DNNs, DTM trains with fewer\nmultiply-accumulates, devoid of derivative computation. It is a data-centric ML\nalgorithm that learns by aligning Tsetlin automata with input data to form\nlogical propositions enabling efficient Look-up-Table (LUT) mapping and frugal\nBlock RAM usage in FPGA training implementations. The proposed accelerator\noffers 2.54x more Giga operations per second per Watt (GOP/s per W) and uses 6x\nless power than the next-best comparable design.",
    "published": "2025-04-28T13:38:53Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.19797v1"
  },
  {
    "arxiv_id": "2504.19746v1",
    "title": "FineQ: Software-Hardware Co-Design for Low-Bit Fine-Grained\n  Mixed-Precision Quantization of LLMs",
    "authors": [
      "Xilong Xie",
      "Liang Wang",
      "Limin Xiao",
      "Meng Han",
      "Lin Sun",
      "Shuai Zheng",
      "Xiangrong Xu"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Large language models (LLMs) have significantly advanced the natural language\nprocessing paradigm but impose substantial demands on memory and computational\nresources. Quantization is one of the most effective ways to reduce memory\nconsumption of LLMs. However, advanced single-precision quantization methods\nexperience significant accuracy degradation when quantizing to ultra-low bits.\nExisting mixed-precision quantization methods are quantized by groups with\ncoarse granularity. Employing high precision for group data leads to\nsubstantial memory overhead, whereas low precision severely impacts model\naccuracy. To address this issue, we propose FineQ, software-hardware co-design\nfor low-bit fine-grained mixed-precision quantization of LLMs. First, FineQ\npartitions the weights into finer-grained clusters and considers the\ndistribution of outliers within these clusters, thus achieving a balance\nbetween model accuracy and memory overhead. Then, we propose an outlier\nprotection mechanism within clusters that uses 3 bits to represent outliers and\nintroduce an encoding scheme for index and data concatenation to enable aligned\nmemory access. Finally, we introduce an accelerator utilizing temporal coding\nthat effectively supports the quantization algorithm while simplifying the\nmultipliers in the systolic array. FineQ achieves higher model accuracy\ncompared to the SOTA mixed-precision quantization algorithm at a close average\nbit-width. Meanwhile, the accelerator achieves up to 1.79x energy efficiency\nand reduces the area of the systolic array by 61.2%.",
    "published": "2025-04-28T12:47:23Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.19746v1"
  },
  {
    "arxiv_id": "2504.19659v1",
    "title": "Hardware/Software Co-Design of RISC-V Extensions for Accelerating Sparse\n  DNNs on FPGAs",
    "authors": [
      "Muhammad Sabih",
      "Abrarul Karim",
      "Jakob Wittmann",
      "Frank Hannig",
      "J\u00fcrgen Teich"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "The customizability of RISC-V makes it an attractive choice for accelerating\ndeep neural networks (DNNs). It can be achieved through instruction set\nextensions and corresponding custom functional units. Yet, efficiently\nexploiting these opportunities requires a hardware/software co-design approach\nin which the DNN model, software, and hardware are designed together. In this\npaper, we propose novel RISC-V extensions for accelerating DNN models\ncontaining semi-structured and unstructured sparsity. While the idea of\naccelerating structured and unstructured pruning is not new, our novel design\noffers various advantages over other designs. To exploit semi-structured\nsparsity, we take advantage of the fine-grained (bit-level) configurability of\nFPGAs and suggest reserving a few bits in a block of DNN weights to encode the\ninformation about sparsity in the succeeding blocks. The proposed custom\nfunctional unit utilizes this information to skip computations. To exploit\nunstructured sparsity, we propose a variable cycle sequential\nmultiply-and-accumulate unit that performs only as many multiplications as the\nnon-zero weights. Our implementation of unstructured and semi-structured\npruning accelerators can provide speedups of up to a factor of 3 and 4,\nrespectively. We then propose a combined design that can accelerate both types\nof sparsities, providing speedups of up to a factor of 5. Our designs consume a\nsmall amount of additional FPGA resources such that the resulting co-designs\nenable the acceleration of DNNs even on small FPGAs. We benchmark our designs\non standard TinyML applications such as keyword spotting, image classification,\nand person detection.",
    "published": "2025-04-28T10:19:39Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.19659v1"
  },
  {
    "arxiv_id": "2504.19649v2",
    "title": "Intelligent4DSE: Optimizing High-Level Synthesis Design Space\n  Exploration with Graph Neural Networks and Large Language Models",
    "authors": [
      "Lei Xu",
      "Shanshan Wang",
      "Emmanuel Casseau",
      "Chenglong Xiao"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      ""
    ],
    "abstract": "High-level synthesis (HLS) design space exploration (DSE) is an optimization\nprocess in electronic design automation (EDA) that systematically explores\nhigh-level design configurations to achieve Pareto-optimal hardware\nimplementations balancing performance, area, and power (PPA). To optimize this\nprocess, HLS prediction tasks often employ message-passing neural networks\n(MPNNs), leveraging complex architectures to achieve high accuracy. These\npredictors serve as evaluators in the DSE process, effectively bypassing the\ntime-consuming estimations traditionally required by HLS tools. However,\nexisting models often prioritize structural complexity and minimization of\ntraining loss, overlooking task-specific characteristics. Additionally, while\nevolutionary algorithms are widely used in DSE, they typically require\nextensive domain-specific knowledge to design effective crossover and mutation\noperators. To address these limitations, we propose CoGNNs-LLMEA, a framework\nthat integrates a graph neural network with task-adaptive message passing and a\nlarge language model-enhanced evolutionary algorithm. As a predictive model,\nCoGNNs directly leverages intermediate representations generated from source\ncode after compiler front-end processing, enabling prediction of quality of\nresults (QoR) without invoking HLS tools. Due to its strong adaptability to\ntasks, CoGNNs can be tuned to predict post-HLS and post-implementation\noutcomes, effectively bridging the gap between high-level abstractions and\nphysical implementation characteristics. CoGNNs achieves state-of-the-art\nprediction accuracy in post-HLS QoR prediction, reducing mean prediction errors\nby 2.8$\\times$ for latency and 3.4$\\times$ for resource utilization compared to\nbaseline models.",
    "published": "2025-04-28T10:08:56Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.19649v2"
  },
  {
    "arxiv_id": "2504.19323v2",
    "title": "NSFlow: An End-to-End FPGA Framework with Scalable Dataflow Architecture\n  for Neuro-Symbolic AI",
    "authors": [
      "Hanchen Yang",
      "Zishen Wan",
      "Ritik Raj",
      "Joongun Park",
      "Ziwei Li",
      "Ananda Samajdar",
      "Arijit Raychowdhury",
      "Tushar Krishna"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Neuro-Symbolic AI (NSAI) is an emerging paradigm that integrates neural\nnetworks with symbolic reasoning to enhance the transparency, reasoning\ncapabilities, and data efficiency of AI systems. Recent NSAI systems have\ngained traction due to their exceptional performance in reasoning tasks and\nhuman-AI collaborative scenarios. Despite these algorithmic advancements,\nexecuting NSAI tasks on existing hardware (e.g., CPUs, GPUs, TPUs) remains\nchallenging, due to their heterogeneous computing kernels, high memory\nintensity, and unique memory access patterns. Moreover, current NSAI algorithms\nexhibit significant variation in operation types and scales, making them\nincompatible with existing ML accelerators. These challenges highlight the need\nfor a versatile and flexible acceleration framework tailored to NSAI workloads.\nIn this paper, we propose NSFlow, an FPGA-based acceleration framework designed\nto achieve high efficiency, scalability, and versatility across NSAI systems.\nNSFlow features a design architecture generator that identifies workload data\ndependencies and creates optimized dataflow architectures, as well as a\nreconfigurable array with flexible compute units, re-organizable memory, and\nmixed-precision capabilities. Evaluating across NSAI workloads, NSFlow achieves\n31x speedup over Jetson TX2, more than 2x over GPU, 8x speedup over TPU-like\nsystolic array, and more than 3x over Xilinx DPU. NSFlow also demonstrates\nenhanced scalability, with only 4x runtime increase when symbolic workloads\nscale by 150x. To the best of our knowledge, NSFlow is the first framework to\nenable real-time generalizable NSAI algorithms acceleration, demonstrating a\npromising solution for next-generation cognitive systems.",
    "published": "2025-04-27T18:28:43Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG",
      "cs.PF"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.19323v2"
  },
  {
    "arxiv_id": "2504.18628v1",
    "title": "Periodic Online Testing for Sparse Systolic Tensor Arrays",
    "authors": [
      "Christodoulos Peltekis",
      "Chrysostomos Nicopoulos",
      "Giorgos Dimitrakopoulos"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "Modern Machine Learning (ML) applications often benefit from structured\nsparsity, a technique that efficiently reduces model complexity and simplifies\nhandling of sparse data in hardware. Sparse systolic tensor arrays -\nspecifically designed to accelerate these structured-sparse ML models - play a\npivotal role in enabling efficient computations. As ML is increasingly\nintegrated into safety-critical systems, it is of paramount importance to\nensure the reliability of these systems. This paper introduces an online\nerror-checking technique capable of detecting and locating permanent faults\nwithin sparse systolic tensor arrays before computation begins. The new\ntechnique relies on merely four test vectors and exploits the weight values\nalready loaded within the systolic array to comprehensively test the system.\nFault-injection campaigns within the gate-level netlist, while executing three\nwell-established Convolutional Neural Networks (CNN), validate the efficiency\nof the proposed approach, which is shown to achieve very high fault coverage,\nwhile incurring minimal performance and area overheads.",
    "published": "2025-04-25T18:10:45Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.18628v1"
  },
  {
    "arxiv_id": "2504.17584v1",
    "title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference",
    "authors": [
      "Qingyuan Liu",
      "Liyan Chen",
      "Yanning Yang",
      "Haocheng Wang",
      "Dong Du",
      "Zhigang Mao",
      "Naifeng Jing",
      "Yubin Xia",
      "Haibo Chen"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes.",
    "published": "2025-04-24T14:14:07Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.17584v1"
  },
  {
    "arxiv_id": "2504.17376v1",
    "title": "On-Device Qwen2.5: Efficient LLM Inference with Model Compression and\n  Hardware Acceleration",
    "authors": [
      "Maoyang Xiang",
      "Ramesh Fernando",
      "Bo Wang"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "Transformer-based Large Language Models (LLMs) have significantly advanced AI\ncapabilities but pose considerable challenges for deployment on edge devices\ndue to high computational demands, memory bandwidth constraints, and energy\nconsumption. This paper addresses these challenges by presenting an efficient\nframework for deploying the Qwen2.5-0.5B model on the Xilinx Kria KV260 edge\nplatform, a heterogeneous system integrating an ARM Cortex-A53 CPU with\nreconfigurable FPGA logic. Leveraging Activation-aware Weight Quantization\n(AWQ) with FPGA-accelerated execution pipelines, the proposed approach enhances\nboth model compression rate and system throughput. Additionally, we propose a\nhybrid execution strategy that intelligently offloads compute-intensive\noperations to the FPGA while utilizing the CPU for lighter tasks, effectively\nbalancing the computational workload and maximizing overall performance. Our\nframework achieves a model compression rate of 55.08% compared to the original\nmodel and produces output at a rate of 5.1 tokens per second, outperforming the\nbaseline performance of 2.8 tokens per second.",
    "published": "2025-04-24T08:50:01Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.17376v1"
  },
  {
    "arxiv_id": "2504.16269v2",
    "title": "COBRA: Algorithm-Architecture Co-optimized Binary Transformer\n  Accelerator for Edge Inference",
    "authors": [
      "Ye Qiao",
      "Zhiheng Chen",
      "Yian Wang",
      "Yifan Zhang",
      "Yunzhe Deng",
      "Sitao Huang"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Transformer-based models have demonstrated superior performance in various\nfields, including natural language processing and computer vision. However,\ntheir enormous model size and high demands in computation, memory, and\ncommunication limit their deployment to edge platforms for local, secure\ninference. Binary transformers offer a compact, low-complexity solution for\nedge deployment with reduced bandwidth needs and acceptable accuracy. However,\nexisting binary transformers perform inefficiently on current hardware due to\nthe lack of binary specific optimizations. To address this, we introduce COBRA,\nan algorithm-architecture co-optimized binary Transformer accelerator for edge\ncomputing. COBRA features a real 1-bit binary multiplication unit, enabling\nmatrix operations with -1, 0, and +1 values, surpassing ternary methods. With\nfurther hardware-friendly optimizations in the attention block, COBRA achieves\nup to 3,894.7 GOPS throughput and 448.7 GOPS/Watt energy efficiency on edge\nFPGAs, delivering a 311x energy efficiency improvement over GPUs and a 3.5x\nthroughput improvement over the state-of-the-art binary accelerator, with only\nnegligible inference accuracy degradation.",
    "published": "2025-04-22T21:03:43Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.16269v2"
  },
  {
    "arxiv_id": "2504.16266v2",
    "title": "TeLLMe: An Energy-Efficient Ternary LLM Accelerator for Prefilling and\n  Decoding on Edge FPGAs",
    "authors": [
      "Ye Qiao",
      "Zhiheng Chen",
      "Yifan Zhang",
      "Yian Wang",
      "Sitao Huang"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Deploying large language models (LLMs) on edge platforms is challenged by\ntheir high computational and memory demands. Although recent low-bit\nquantization methods (e.g., BitNet, DeepSeek) compress weights to as little as\n1.58 bits with minimal accuracy loss, edge deployment is still constrained by\nlimited on-chip resources, power budgets, and the often-neglected latency of\nthe prefill phase. We present TeLLMe, the first ternary LLM accelerator for\nlow-power FPGAs (e.g., AMD KV260) that fully supports both prefill and\nautoregressive decoding using 1.58-bit weights and 8-bit activations. Our\ncontributions include: (1) a table-lookup matrix engine for ternary matmul that\nmerges grouped activations with online precomputation to minimize resource use;\n(2) a fused, bandwidth-efficient attention module featuring a reversed\nreordering scheme to accelerate prefill; and (3) a tightly integrated\nnormalization and quantization--dequantization unit optimized for ultra-low-bit\ninference. Under a 7W power budget, TeLLMe delivers up to 9 tokens/s throughput\nover 1,024-token contexts and prefill latencies of 0.55--1.15 s for 64--128\ntoken prompts, marking a significant energy-efficiency advance and establishing\na new edge FPGA benchmark for generative AI.",
    "published": "2025-04-22T21:00:58Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.16266v2"
  },
  {
    "arxiv_id": "2506.00002v1",
    "title": "Advancing AI-assisted Hardware Design with Hierarchical Decentralized\n  Training and Personalized Inference-Time Optimization",
    "authors": [
      "Hao Mark Chen",
      "Zehuan Zhang",
      "Wanru Zhao",
      "Nicholas Lane",
      "Hongxiang Fan"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Recent years have witnessed a significant increase in the adoption of AI\ntechniques to enhance electronic design automation. In particular, the\nemergence of Large Language Models (LLMs) has sparked significant interest in\nLLM-assisted hardware design generation, spanning applications from classical\ndigital circuits to quantum computing. Despite substantial progress in this\ndirection, the quality of LLM-generated hardware design still cannot meet the\nrequirements for practical deployment. In this work, we identify three critical\nchallenges hindering the development of LLM-assisted hardware design\ngeneration: 1) limited data availability, 2) varied data quality, 3) inadequate\ninference-time efficiency. To address these fundamental challenges, this paper\nintroduces a two-stage framework for AI-assisted hardware design by exploring\ndecentralized training and personalized inference. In the first stage, we\npropose to harness private domain design sources through a hierarchical\ndecentralized training mechanism that addresses data-sharing constraints. To\nmitigate the impact of low-quality data, we identify optimization opportunities\nin hardware generation tasks, using user-defined metrics for model aggregation.\nThe second stage focuses on client personalization to enhance both speed and\nquality. We introduce a new metric, Trueput, to analyze LLM-assisted hardware\ngeneration efficiency. To optimize Trueput, we implement personalized\ninference-time acceleration and customized sampling strategies. Evaluating both\nclassical and quantum benchmarks, our experimental results demonstrate that the\nproposed two-stage framework can significantly improve the model capability for\nhardware design generation. As orthogonal enhancements to existing methods, our\nframework can achieve $33\\% \\sim 50\\%$ semantic accuracy improvement and $2.3$\ntimes speedup, depending on the difficulty of the generation tasks.",
    "published": "2025-04-21T15:41:28Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2506.00002v1"
  },
  {
    "arxiv_id": "2505.03763v1",
    "title": "Splitwiser: Efficient LM inference with constrained resources",
    "authors": [
      "Asad Aali",
      "Adney Cardoza",
      "Melissa Capo"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "Efficient inference of LLMs remains a crucial challenge, with two main\nphases: a compute-intensive prompt computation and a memory-intensive token\ngeneration. Despite existing batching and scheduling techniques, token\ngeneration phases fail to fully utilize compute resources, especially when\ncompared to prompt computation phases. To address these challenges, we propose\nSplitwiser, a methodology that splits the two phases of an LLM inference\nrequest onto the same GPU, thereby reducing overhead and improving memory\naccess and cache utilization. By eliminating the need to transfer data across\ndevices, Splitwiser aims to minimize network-related overheads. In this report,\nwe describe the basic structure of our proposed pipeline while sharing\npreliminary results and analysis. We implement our proposed multiprocessing\ndesign on two widely-used and independent LLM architectures: Huggingface and\nvLLM. We open-source our code for the respective implementations: 1)\nHuggingface (https://github.com/asad-aali/splitwiser), and 2) vLLM\n(https://github.com/adney11/vllm-sysml).",
    "published": "2025-04-21T00:21:08Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.03763v1"
  },
  {
    "arxiv_id": "2504.14365v1",
    "title": "Accelerating LLM Inference with Flexible N:M Sparsity via A Fully\n  Digital Compute-in-Memory Accelerator",
    "authors": [
      "Akshat Ramachandran",
      "Souvik Kundu",
      "Arnab Raha",
      "Shamik Kundu",
      "Deepak K. Mathaikutty",
      "Tushar Krishna"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Large language model (LLM) pruning with fixed N:M structured sparsity\nsignificantly limits the expressivity of the sparse model, yielding sub-optimal\nperformance. In contrast, supporting multiple N:M patterns to provide sparse\nrepresentational freedom introduces costly overhead in hardware. To address\nthese challenges for LLMs, we first present a flexible layer-wise\noutlier-density-aware N:M sparsity (FLOW) selection method. FLOW enables the\nidentification of optimal layer-wise N and M values (from a given range) by\nsimultaneously accounting for the presence and distribution of outliers,\nallowing a higher degree of representational freedom. To deploy sparse models\nwith such N:M flexibility, we then introduce a flexible, low-overhead digital\ncompute-in-memory architecture (FlexCiM). FlexCiM supports diverse sparsity\npatterns by partitioning a digital CiM (DCiM) macro into smaller sub-macros,\nwhich are adaptively aggregated and disaggregated through distribution and\nmerging mechanisms for different N and M values. Extensive experiments on both\ntransformer-based and recurrence-based state space foundation models (SSMs)\ndemonstrate that FLOW outperforms existing alternatives with an accuracy\nimprovement of up to 36%, while FlexCiM achieves up to 1.75x lower inference\nlatency and 1.5x lower energy consumption compared to existing sparse\naccelerators. Code is available at: https://github.com/FLOW-open-project/FLOW",
    "published": "2025-04-19T17:47:01Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.14365v1"
  },
  {
    "arxiv_id": "2505.03756v1",
    "title": "Improving the Serving Performance of Multi-LoRA Large Language Models\n  via Efficient LoRA and KV Cache Management",
    "authors": [
      "Hang Zhang",
      "Jiuchen Shi",
      "Yixiao Wang",
      "Quan Chen",
      "Yizhou Shan",
      "Minyi Guo"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Multiple Low-Rank Adapters (Multi-LoRAs) are gaining popularity for\ntask-specific Large Language Model (LLM) applications. For multi-LoRA serving,\ncaching hot KV caches and LoRA adapters in high bandwidth memory of\naccelerations can improve inference performance. However, existing Multi-LoRA\ninference systems fail to optimize serving performance like Time-To-First-Toke\n(TTFT), neglecting usage dependencies when caching LoRAs and KVs. We therefore\npropose FASTLIBRA, a Multi-LoRA caching system to optimize the serving\nperformance. FASTLIBRA comprises a dependency-aware cache manager and a\nperformance-driven cache swapper. The cache manager maintains the usage\ndependencies between LoRAs and KV caches during the inference with a unified\ncaching pool. The cache swapper determines the swap-in or out of LoRAs and KV\ncaches based on a unified cost model, when the HBM is idle or busy,\nrespectively. Experimental results show that ELORA reduces the TTFT by 63.4% on\naverage, compared to state-of-the-art works.",
    "published": "2025-04-19T13:17:34Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG",
      "cs.PF"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.03756v1"
  },
  {
    "arxiv_id": "2504.14152v1",
    "title": "FGMP: Fine-Grained Mixed-Precision Weight and Activation Quantization\n  for Hardware-Accelerated LLM Inference",
    "authors": [
      "Coleman Hooper",
      "Charbel Sakr",
      "Ben Keller",
      "Rangharajan Venkatesan",
      "Kurt Keutzer",
      "Sophia Shao",
      "Brucek Khailany"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Quantization is a powerful tool to improve large language model (LLM)\ninference efficiency by utilizing more energy-efficient low-precision datapaths\nand reducing memory footprint. However, accurately quantizing LLM weights and\nactivations to low precision is challenging without degrading model accuracy.\nWe propose fine-grained mixed precision (FGMP) quantization, a post-training\nmixed-precision quantization hardware-software co-design methodology that\nmaintains accuracy while quantizing the majority of weights and activations to\nreduced precision. Our work makes the following contributions: 1) We develop a\npolicy that uses the perturbation in each value, weighted by the Fisher\ninformation, to select which weight and activation blocks to keep in higher\nprecision. This approach preserves accuracy by identifying which weight and\nactivation blocks need to be retained in higher precision to minimize the\nperturbation in the model loss. 2) We also propose a sensitivity-weighted\nclipping approach for fine-grained quantization which helps retain accuracy for\nblocks that are quantized to low precision. 3) We then propose hardware\naugmentations to leverage the efficiency benefits of FGMP quantization. Our\nhardware implementation encompasses i) datapath support for FGMP at block\ngranularity, and ii) a mixed-precision activation quantization unit to assign\nactivation blocks to high or low precision on the fly with minimal runtime and\nenergy overhead. Our design, prototyped using NVFP4 (an FP4 format with\nmicroscaling) as the low-precision datatype and FP8 as the high-precision\ndatatype, facilitates efficient FGMP quantization, attaining <1% perplexity\ndegradation on Wikitext-103 for the Llama-2-7B model relative to an all-FP8\nbaseline design while consuming 14% less energy during inference and requiring\n30% less weight memory.",
    "published": "2025-04-19T02:51:45Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.14152v1"
  },
  {
    "arxiv_id": "2504.11981v1",
    "title": "Hardware-Friendly Delayed-Feedback Reservoir for Multivariate\n  Time-Series Classification",
    "authors": [
      "Sosei Ikeda",
      "Hiromitsu Awano",
      "Takashi Sato"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "Reservoir computing (RC) is attracting attention as a machine-learning\ntechnique for edge computing. In time-series classification tasks, the number\nof features obtained using a reservoir depends on the length of the input\nseries. Therefore, the features must be converted to a constant-length\nintermediate representation (IR), such that they can be processed by an output\nlayer. Existing conversion methods involve computationally expensive matrix\ninversion that significantly increases the circuit size and requires processing\npower when implemented in hardware. In this article, we propose a simple but\neffective IR, namely, dot-product-based reservoir representation (DPRR), for RC\nbased on the dot product of data features. Additionally, we propose a\nhardware-friendly delayed-feedback reservoir (DFR) consisting of a nonlinear\nelement and delayed feedback loop with DPRR. The proposed DFR successfully\nclassified multivariate time series data that has been considered particularly\ndifficult to implement efficiently in hardware. In contrast to conventional DFR\nmodels that require analog circuits, the proposed model can be implemented in a\nfully digital manner suitable for high-level syntheses. A comparison with\nexisting machine-learning methods via field-programmable gate array\nimplementation using 12 multivariate time-series classification tasks confirmed\nthe superior accuracy and small circuit size of the proposed method.",
    "published": "2025-04-16T11:22:38Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.11981v1"
  },
  {
    "arxiv_id": "2504.11227v1",
    "title": "VEXP: A Low-Cost RISC-V ISA Extension for Accelerated Softmax\n  Computation in Transformers",
    "authors": [
      "Run Wang",
      "Gamze Islamoglu",
      "Andrea Belano",
      "Viviane Potocnik",
      "Francesco Conti",
      "Angelo Garofalo",
      "Luca Benini"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "While Transformers are dominated by Floating-Point (FP)\nMatrix-Multiplications, their aggressive acceleration through dedicated\nhardware or many-core programmable systems has shifted the performance\nbottleneck to non-linear functions like Softmax. Accelerating Softmax is\nchallenging due to its non-pointwise, non-linear nature, with exponentiation as\nthe most demanding step. To address this, we design a custom arithmetic block\nfor Bfloat16 exponentiation leveraging a novel approximation algorithm based on\nSchraudolph's method, and we integrate it into the Floating-Point Unit (FPU) of\nthe RISC-V cores of a compute cluster, through custom Instruction Set\nArchitecture (ISA) extensions, with a negligible area overhead of 1\\%. By\noptimizing the software kernels to leverage the extension, we execute Softmax\nwith 162.7$\\times$ less latency and 74.3$\\times$ less energy compared to the\nbaseline cluster, achieving an 8.2$\\times$ performance improvement and\n4.1$\\times$ higher energy efficiency for the FlashAttention-2 kernel in GPT-2\nconfiguration. Moreover, the proposed approach enables a multi-cluster system\nto efficiently execute end-to-end inference of pre-trained Transformer models,\nsuch as GPT-2, GPT-3 and ViT, achieving up to 5.8$\\times$ and 3.6$\\times$\nreduction in latency and energy consumption, respectively, without requiring\nre-training and with negligible accuracy loss.",
    "published": "2025-04-15T14:28:48Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.11227v1"
  },
  {
    "arxiv_id": "2504.11497v1",
    "title": "LLM-based AI Agent for Sizing of Analog and Mixed Signal Circuit",
    "authors": [
      "Chang Liu",
      "Emmanuel A. Olowe",
      "Danial Chitnis"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "The design of Analog and Mixed-Signal (AMS) integrated circuits (ICs) often\ninvolves significant manual effort, especially during the transistor sizing\nprocess. While Machine Learning techniques in Electronic Design Automation\n(EDA) have shown promise in reducing complexity and minimizing human\nintervention, they still face challenges such as numerous iterations and a lack\nof knowledge about AMS circuit design. Recently, Large Language Models (LLMs)\nhave demonstrated significant potential across various fields, showing a\ncertain level of knowledge in circuit design and indicating their potential to\nautomate the transistor sizing process. In this work, we propose an LLM-based\nAI agent for AMS circuit design to assist in the sizing process. By integrating\nLLMs with external circuit simulation tools and data analysis functions and\nemploying prompt engineering strategies, the agent successfully optimized\nmultiple circuits to achieve target performance metrics. We evaluated the\nperformance of different LLMs to assess their applicability and optimization\neffectiveness across seven basic circuits, and selected the best-performing\nmodel Claude 3.5 Sonnet for further exploration on an operational amplifier,\nwith complementary input stage and class AB output stage. This circuit was\nevaluated against nine performance metrics, and we conducted experiments under\nthree distinct performance requirement groups. A success rate of up to 60% was\nachieved for reaching the target requirements. Overall, this work demonstrates\nthe potential of LLMs to improve AMS circuit design.",
    "published": "2025-04-14T22:18:16Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.11497v1"
  },
  {
    "arxiv_id": "2504.10369v1",
    "title": "SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired\n  Symbolic Reasoning",
    "authors": [
      "Yiting Wang",
      "Wanghao Ye",
      "Ping Guo",
      "Yexiao He",
      "Ziyao Wang",
      "Yexiao He",
      "Bowei Tian",
      "Shwai He",
      "Guoheng Sun",
      "Zheyu Shen",
      "Sihan Chen",
      "Ankur Srivastava",
      "Qingfu Zhang",
      "Gang Qu",
      "Ang Li"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Optimizing Register Transfer Level (RTL) code is crucial for improving the\npower, performance, and area (PPA) of digital circuits in the early stages of\nsynthesis. Manual rewriting, guided by synthesis feedback, can yield\nhigh-quality results but is time-consuming and error-prone. Most existing\ncompiler-based approaches have difficulty handling complex design constraints.\nLarge Language Model (LLM)-based methods have emerged as a promising\nalternative to address these challenges. However, LLM-based approaches often\nface difficulties in ensuring alignment between the generated code and the\nprovided prompts. This paper presents SymRTLO, a novel neuron-symbolic RTL\noptimization framework that seamlessly integrates LLM-based code rewriting with\nsymbolic reasoning techniques. Our method incorporates a retrieval-augmented\ngeneration (RAG) system of optimization rules and Abstract Syntax Tree\n(AST)-based templates, enabling LLM-based rewriting that maintains syntactic\ncorrectness while minimizing undesired circuit behaviors. A symbolic module is\nproposed for analyzing and optimizing finite state machine (FSM) logic,\nallowing fine-grained state merging and partial specification handling beyond\nthe scope of pattern-based compilers. Furthermore, a fast verification\npipeline, combining formal equivalence checks with test-driven validation,\nfurther reduces the complexity of verification. Experiments on the RTL-Rewriter\nbenchmark with Synopsys Design Compiler and Yosys show that SymRTLO improves\npower, performance, and area (PPA) by up to 43.9%, 62.5%, and 51.1%,\nrespectively, compared to the state-of-the-art methods.",
    "published": "2025-04-14T16:15:55Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG",
      "cs.PL"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.10369v1"
  },
  {
    "arxiv_id": "2504.10240v2",
    "title": "GNN-ACLP: Graph Neural Networks based Analog Circuit Link Prediction",
    "authors": [
      "Guanyuan Pan",
      "Tiansheng Zhou",
      "Bingtao Ma",
      "Yaqi Wang",
      "Jianxiang Zhao",
      "Zhi Li",
      "Yugui Lin",
      "Pietro Lio",
      "Shuai Wang"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Circuit link prediction identifying missing component connections from\nincomplete netlists is crucial in automating analog circuit design. However,\nexisting methods face three main challenges: 1) Insufficient use of topological\npatterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to\nthe complexity of annotations hinders model generalization; 3) Limited\nadaptability to various netlist formats. We propose GNN-ACLP, a Graph Neural\nNetworks (GNNs) based framework featuring three innovations to tackle these\nchallenges. First, we introduce the SEAL (Subgraphs, Embeddings, and Attributes\nfor Link Prediction) framework and achieve port-level accuracy in circuit link\nprediction. Second, we propose Netlist Babel Fish, a netlist format conversion\ntool leveraging retrieval-augmented generation (RAG) with a large language\nmodel (LLM) to enhance the compatibility of netlist formats. Finally, we\nconstruct SpiceNetlist, a comprehensive dataset that contains 775 annotated\ncircuits across 10 different component classes. Experimental results achieve\naccuracy improvements of 16.08% on SpiceNetlist, 11.38% on Image2Net, and\n16.01% on Masala-CHAI in intra-dataset evaluation, while maintaining accuracy\nfrom 92.05% to 99.07% in cross-dataset evaluation, exhibiting robust feature\ntransfer capabilities.",
    "published": "2025-04-14T14:02:09Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.10240v2"
  },
  {
    "arxiv_id": "2504.09870v1",
    "title": "Ember: A Compiler for Efficient Embedding Operations on Decoupled\n  Access-Execute Architectures",
    "authors": [
      "Marco Siracusa",
      "Olivia Hsu",
      "Victor Soria-Pardos",
      "Joshua Randall",
      "Arnaud Grasset",
      "Eric Biscondi",
      "Doug Joseph",
      "Randy Allen",
      "Fredrik Kjolstad",
      "Miquel Moret\u00f3 Planas",
      "Adri\u00e0 Armejach"
    ],
    "author_affiliations": [
      "Barcelona Supercomputing Center",
      "Stanford University",
      "Barcelona Supercomputing Center",
      "Arm",
      "Arm",
      "Arm",
      "Arm",
      "Barcelona Supercomputing Center",
      "Stanford University",
      "Barcelona Supercomputing Center",
      "Barcelona Supercomputing Center"
    ],
    "abstract": "Irregular embedding lookups are a critical bottleneck in recommender models,\nsparse large language models, and graph learning models. In this paper, we\nfirst demonstrate that, by offloading these lookups to specialized access\nunits, Decoupled Access-Execute (DAE) processors achieve 2.6$\\times$ higher\nperformance and 6.4$\\times$ higher performance/watt than GPUs on end-to-end\nmodels. Then, we propose the Ember compiler for automatically generating\noptimized DAE code from PyTorch and TensorFlow. Conversely from other DAE\ncompilers, Ember features multiple intermediate representations specifically\ndesigned for different optimization levels. In this way, Ember can implement\nall optimizations to match the performance of hand-written code, unlocking the\nfull potential of DAE architectures at scale.",
    "published": "2025-04-14T04:29:46Z",
    "categories": [
      "cs.AR",
      "cs.LG",
      "cs.PL",
      "C.1.2; C.1.3; D.3.4"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.09870v1"
  },
  {
    "arxiv_id": "2504.09775v3",
    "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
    "authors": [
      "Abhimanyu Rajeshkumar Bambhaniya",
      "Hanjiang Wu",
      "Suvinay Subramanian",
      "Sudarshan Srinivasan",
      "Souvik Kundu",
      "Amir Yazdanbakhsh",
      "Midhilesh Elavazhagan",
      "Madhu Kumar",
      "Tushar Krishna"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
    "published": "2025-04-14T00:29:49Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.09775v3"
  },
  {
    "arxiv_id": "2504.09485v1",
    "title": "GenEDA: Unleashing Generative Reasoning on Netlist via Multimodal\n  Encoder-Decoder Aligned Foundation Model",
    "authors": [
      "Wenji Fang",
      "Jing Wang",
      "Yao Lu",
      "Shang Liu",
      "Zhiyao Xie"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "The success of foundation AI has motivated the research of circuit foundation\nmodels, which are customized to assist the integrated circuit (IC) design\nprocess. However, existing pre-trained circuit models are typically limited to\nstandalone encoders for predictive tasks or decoders for generative tasks.\nThese two model types are developed independently, operate on different circuit\nmodalities, and reside in separate latent spaces, which restricts their ability\nto complement each other for more advanced applications. In this work, we\npresent GenEDA, the first framework that aligns circuit encoders with decoders\nwithin a shared latent space. GenEDA bridges the gap between graph-based\ncircuit representations and text-based large language models (LLMs), enabling\ncommunication between their respective latent spaces. To achieve the alignment,\nwe propose two paradigms that support both open-source trainable LLMs and\ncommercial frozen LLMs. Built on this aligned architecture, GenEDA enables\nthree unprecedented generative reasoning tasks over netlists, where the model\nreversely generates the high-level functionality from low-level netlists in\ndifferent granularities. These tasks extend traditional gate-type prediction to\ndirect generation of full-circuit functionality. Experiments demonstrate that\nGenEDA significantly boosts advanced LLMs' (e.g., GPT-4o and DeepSeek-V3)\nperformance in all tasks.",
    "published": "2025-04-13T08:56:22Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.09485v1"
  },
  {
    "arxiv_id": "2504.09260v1",
    "title": "NetTAG: A Multimodal RTL-and-Layout-Aligned Netlist Foundation Model via\n  Text-Attributed Graph",
    "authors": [
      "Wenji Fang",
      "Wenkai Li",
      "Shang Liu",
      "Yao Lu",
      "Hongce Zhang",
      "Zhiyao Xie"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Circuit representation learning has shown promise in advancing Electronic\nDesign Automation (EDA) by capturing structural and functional circuit\nproperties for various tasks. Existing pre-trained solutions rely on graph\nlearning with complex functional supervision, such as truth table simulation.\nHowever, they only handle simple and-inverter graphs (AIGs), struggling to\nfully encode other complex gate functionalities. While large language models\n(LLMs) excel at functional understanding, they lack the structural awareness\nfor flattened netlists. To advance netlist representation learning, we present\nNetTAG, a netlist foundation model that fuses gate semantics with graph\nstructure, handling diverse gate types and supporting a variety of functional\nand physical tasks. Moving beyond existing graph-only methods, NetTAG\nformulates netlists as text-attributed graphs, with gates annotated by symbolic\nlogic expressions and physical characteristics as text attributes. Its\nmultimodal architecture combines an LLM-based text encoder for gate semantics\nand a graph transformer for global structure. Pre-trained with gate and graph\nself-supervised objectives and aligned with RTL and layout stages, NetTAG\ncaptures comprehensive circuit intrinsics. Experimental results show that\nNetTAG consistently outperforms each task-specific method on four largely\ndifferent functional and physical tasks and surpasses state-of-the-art AIG\nencoders, demonstrating its versatility.",
    "published": "2025-04-12T15:39:25Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.09260v1"
  },
  {
    "arxiv_id": "2504.09072v1",
    "title": "MGS: Markov Greedy Sums for Accurate Low-Bitwidth Floating-Point\n  Accumulation",
    "authors": [
      "Vikas Natesh",
      "H. T. Kung",
      "David Kong"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "We offer a novel approach, MGS (Markov Greedy Sums), to improve the accuracy\nof low-bitwidth floating-point dot products in neural network computations. In\nconventional 32-bit floating-point summation, adding values with different\nexponents may lead to loss of precision in the mantissa of the smaller term,\nwhich is right-shifted to align with the larger term's exponent. Such shifting\n(a.k.a. 'swamping') is a significant source of numerical errors in accumulation\nwhen implementing low-bitwidth dot products (e.g., 8-bit floating point) as the\nmantissa has a small number of bits. We avoid most swamping errors by arranging\nthe terms in dot product summation based on their exponents and summing the\nmantissas without overflowing the low-bitwidth accumulator. We design, analyze,\nand implement the algorithm to minimize 8-bit floating point error at inference\ntime for several neural networks. In contrast to traditional sequential\nsummation, our method has significantly lowered numerical errors, achieving\nclassification accuracy on par with high-precision floating-point baselines for\nmultiple image classification tasks. Our dMAC hardware units can reduce power\nconsumption by up to 34.1\\% relative to conventional MAC units.",
    "published": "2025-04-12T04:19:03Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.09072v1"
  },
  {
    "arxiv_id": "2504.08398v1",
    "title": "MixDiT: Accelerating Image Diffusion Transformer Inference with\n  Mixed-Precision MX Quantization",
    "authors": [
      "Daeun Kim",
      "Jinwoo Hwang",
      "Changhun Oh",
      "Jongse Park"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      ""
    ],
    "abstract": "Diffusion Transformer (DiT) has driven significant progress in image\ngeneration tasks. However, DiT inferencing is notoriously compute-intensive and\nincurs long latency even on datacenter-scale GPUs, primarily due to its\niterative nature and heavy reliance on GEMM operations inherent to its\nencoder-based structure. To address the challenge, prior work has explored\nquantization, but achieving low-precision quantization for DiT inferencing with\nboth high accuracy and substantial speedup remains an open problem. To this\nend, this paper proposes MixDiT, an algorithm-hardware co-designed acceleration\nsolution that exploits mixed Microscaling (MX) formats to quantize DiT\nactivation values. MixDiT quantizes the DiT activation tensors by selectively\napplying higher precision to magnitude-based outliers, which produce\nmixed-precision GEMM operations. To achieve tangible speedup from the\nmixed-precision arithmetic, we design a MixDiT accelerator that enables\nprecision-flexible multiplications and efficient MX precision conversions. Our\nexperimental results show that MixDiT delivers a speedup of 2.10-5.32 times\nover RTX 3090, with no loss in FID.",
    "published": "2025-04-11T10:03:06Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.08398v1"
  },
  {
    "arxiv_id": "2504.08852v1",
    "title": "ML For Hardware Design Interpretability: Challenges and Opportunities",
    "authors": [
      "Raymond Baartmans",
      "Andrew Ensinger",
      "Victor Agostinelli",
      "Lizhong Chen"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      ""
    ],
    "abstract": "The increasing size and complexity of machine learning (ML) models have\ndriven the growing need for custom hardware accelerators capable of efficiently\nsupporting ML workloads. However, the design of such accelerators remains a\ntime-consuming process, heavily relying on engineers to manually ensure design\ninterpretability through clear documentation and effective communication.\nRecent advances in large language models (LLMs) offer a promising opportunity\nto automate these design interpretability tasks, particularly the generation of\nnatural language descriptions for register-transfer level (RTL) code, what we\nrefer to as \"RTL-to-NL tasks.\" In this paper, we examine how design\ninterpretability, particularly in RTL-to-NL tasks, influences the efficiency of\nthe hardware design process. We review existing work adapting LLMs for these\ntasks, highlight key challenges that remain unaddressed, including those\nrelated to data, computation, and model development, and identify opportunities\nto address them. By doing so, we aim to guide future research in leveraging ML\nto automate RTL-to-NL tasks and improve hardware design interpretability,\nthereby accelerating the hardware design process and meeting the increasing\ndemand for custom hardware accelerators in machine learning and beyond.",
    "published": "2025-04-11T03:47:51Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.08852v1"
  },
  {
    "arxiv_id": "2504.06996v1",
    "title": "Neural Signal Compression using RAMAN tinyML Accelerator for BCI\n  Applications",
    "authors": [
      "Adithya Krishna",
      "Sohan Debnath",
      "Andr\u00e9 van Schaik",
      "Mahesh Mehendale",
      "Chetan Singh Thakur"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "High-quality, multi-channel neural recording is indispensable for\nneuroscience research and clinical applications. Large-scale brain recordings\noften produce vast amounts of data that must be wirelessly transmitted for\nsubsequent offline analysis and decoding, especially in brain-computer\ninterfaces (BCIs) utilizing high-density intracortical recordings with hundreds\nor thousands of electrodes. However, transmitting raw neural data presents\nsignificant challenges due to limited communication bandwidth and resultant\nexcessive heating. To address this challenge, we propose a neural signal\ncompression scheme utilizing Convolutional Autoencoders (CAEs), which achieves\na compression ratio of up to 150 for compressing local field potentials (LFPs).\nThe CAE encoder section is implemented on RAMAN, an energy-efficient tinyML\naccelerator designed for edge computing, and subsequently deployed on an Efinix\nTi60 FPGA with 37.3k LUTs and 8.6k register utilization. RAMAN leverages\nsparsity in activation and weights through zero skipping, gating, and weight\ncompression techniques. Additionally, we employ hardware-software\nco-optimization by pruning CAE encoder model parameters using a hardware-aware\nbalanced stochastic pruning strategy, resolving workload imbalance issues and\neliminating indexing overhead to reduce parameter storage requirements by up to\n32.4%. Using the proposed compact depthwise separable convolutional autoencoder\n(DS-CAE) model, the compressed neural data from RAMAN is reconstructed offline\nwith superior signal-to-noise and distortion ratios (SNDR) of 22.6 dB and 27.4\ndB, along with R2 scores of 0.81 and 0.94, respectively, evaluated on two\nmonkey neural recordings.",
    "published": "2025-04-09T16:09:00Z",
    "categories": [
      "cs.AR",
      "cs.HC",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.06996v1"
  },
  {
    "arxiv_id": "2505.03745v1",
    "title": "AccLLM: Accelerating Long-Context LLM Inference Via Algorithm-Hardware\n  Co-Design",
    "authors": [
      "Yanbiao Liang",
      "Huihong Shi",
      "Haikuo Shao",
      "Zhongfeng Wang"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      ""
    ],
    "abstract": "Recently, large language models (LLMs) have achieved huge success in the\nnatural language processing (NLP) field, driving a growing demand to extend\ntheir deployment from the cloud to edge devices. However, deploying LLMs on\nresource-constrained edge devices poses significant challenges, including (1)\nintensive computations and huge model sizes, (2) great memory and bandwidth\ndemands introduced by the autoregressive generation process, and (3) limited\nscalability for handling long sequences. To address these challenges, we\npropose AccLLM, a comprehensive acceleration framework that enables efficient\nand fast long-context LLM inference through algorithm and hardware co-design.\nAt the algorithmic level, we integrate (1) pruning, (2) {\\Lambda}-shaped\nattention, and (3) an innovative W2A8KV4 (2-bit weights, 8-bit activations, and\n4-bit KV cache) quantization scheme, thus effectively reducing memory and\nbandwidth requirements while facilitating LLMs' long-sequence generation. At\nthe hardware level, we design a dedicated FPGA-based accelerator with a\nreconfigurable computing engine to effectively and flexibly accommodate diverse\noperations arising from our compression algorithm, thereby fully translating\nthe algorithmic innovations into tangible hardware efficiency. We validate\nAccLLM on the Xilinx Alveo U280 FPGA, demonstrating a 4.07x energy efficiency\nand a 2.98x throughput compared to the state-of-the-art work FlightLLM.",
    "published": "2025-04-07T02:52:30Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2505.03745v1"
  },
  {
    "arxiv_id": "2504.00957v2",
    "title": "Enabling Efficient Processing of Spiking Neural Networks with On-Chip\n  Learning on Commodity Neuromorphic Processors for Edge AI Systems",
    "authors": [
      "Rachmad Vidya Wicaksana Putra",
      "Pasindu Wickramasinghe",
      "Muhammad Shafique"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "The rising demand for energy-efficient edge AI systems (e.g., mobile\nagents/robots) has increased the interest in neuromorphic computing, since it\noffers ultra-low power/energy AI computation through spiking neural network\n(SNN) algorithms on neuromorphic processors. However, their efficient\nimplementation strategy has not been comprehensively studied, hence limiting\nSNN deployments for edge AI systems. Toward this, we propose a design\nmethodology to enable efficient SNN processing on commodity neuromorphic\nprocessors. To do this, we first study the key characteristics of targeted\nneuromorphic hardware (e.g., memory and compute budgets), and leverage this\ninformation to perform compatibility analysis for network selection. Afterward,\nwe employ a mapping strategy for efficient SNN implementation on the targeted\nprocessor. Furthermore, we incorporate an efficient on-chip learning mechanism\nto update the systems' knowledge for adapting to new input classes and dynamic\nenvironments. The experimental results show that the proposed methodology leads\nthe system to achieve low latency of inference (i.e., less than 50ms for image\nclassification, less than 200ms for real-time object detection in video\nstreaming, and less than 1ms in keyword recognition) and low latency of on-chip\nlearning (i.e., less than 2ms for keyword recognition), while incurring less\nthan 250mW of processing power and less than 15mJ of energy consumption across\nthe respective different applications and scenarios. These results show the\npotential of the proposed methodology in enabling efficient edge AI systems for\ndiverse application use-cases.",
    "published": "2025-04-01T16:52:03Z",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.00957v2"
  },
  {
    "arxiv_id": "2504.00520v1",
    "title": "SCRec: A Scalable Computational Storage System with Statistical Sharding\n  and Tensor-train Decomposition for Recommendation Models",
    "authors": [
      "Jinho Yang",
      "Ji-Hoon Kim",
      "Joo-Young Kim"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "Deep Learning Recommendation Models (DLRMs) play a crucial role in delivering\npersonalized content across web applications such as social networking and\nvideo streaming. However, with improvements in performance, the parameter size\nof DLRMs has grown to terabyte (TB) scales, accompanied by memory bandwidth\ndemands exceeding TB/s levels. Furthermore, the workload intensity within the\nmodel varies based on the target mechanism, making it difficult to build an\noptimized recommendation system. In this paper, we propose SCRec, a scalable\ncomputational storage recommendation system that can handle TB-scale industrial\nDLRMs while guaranteeing high bandwidth requirements. SCRec utilizes a software\nframework that features a mixed-integer programming (MIP)-based cost model,\nefficiently fetching data based on data access patterns and adaptively\nconfiguring memory-centric and compute-centric cores. Additionally, SCRec\nintegrates hardware acceleration cores to enhance DLRM computations,\nparticularly allowing for the high-performance reconstruction of approximated\nembedding vectors from extremely compressed tensor-train (TT) format. By\ncombining its software framework and hardware accelerators, while eliminating\ndata communication overhead by being implemented on a single server, SCRec\nachieves substantial improvements in DLRM inference performance. It delivers up\nto 55.77$\\times$ speedup compared to a CPU-DRAM system with no loss in accuracy\nand up to 13.35$\\times$ energy efficiency gains over a multi-GPU system.",
    "published": "2025-04-01T08:12:45Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.00520v1"
  },
  {
    "arxiv_id": "2503.23943v1",
    "title": "DOMAC: Differentiable Optimization for High-Speed Multipliers and\n  Multiply-Accumulators",
    "authors": [
      "Chenhao Xue",
      "Yi Ren",
      "Jinwei Zhou",
      "Kezhi Li",
      "Chen Zhang",
      "Yibo Lin",
      "Lining Zhang",
      "Qiang Xu",
      "Guangyu Sun"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Multipliers and multiply-accumulators (MACs) are fundamental building blocks\nfor compute-intensive applications such as artificial intelligence. With the\ndiminishing returns of Moore's Law, optimizing multiplier performance now\nnecessitates process-aware architectural innovations rather than relying solely\non technology scaling. In this paper, we introduce DOMAC, a novel approach that\nemploys differentiable optimization for designing multipliers and MACs at\nspecific technology nodes. DOMAC establishes an analogy between optimizing\nmulti-staged parallel compressor trees and training deep neural networks.\nBuilding on this insight, DOMAC reformulates the discrete optimization\nchallenge into a continuous problem by incorporating differentiable timing and\narea objectives. This formulation enables us to utilize existing deep learning\ntoolkit for highly efficient implementation of the differentiable solver.\nExperimental results demonstrate that DOMAC achieves significant enhancements\nin both performance and area efficiency compared to state-of-the-art baselines\nand commercial IPs in multiplier and MAC designs.",
    "published": "2025-03-31T10:49:05Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.23943v1"
  },
  {
    "arxiv_id": "2503.23076v1",
    "title": "Concorde: Fast and Accurate CPU Performance Modeling with Compositional\n  Analytical-ML Fusion",
    "authors": [
      "Arash Nasr-Esfahany",
      "Mohammad Alizadeh",
      "Victor Lee",
      "Hanna Alam",
      "Brett W. Coon",
      "David Culler",
      "Vidushi Dadu",
      "Martin Dixon",
      "Henry M. Levy",
      "Santosh Pandey",
      "Parthasarathy Ranganathan",
      "Amir Yazdanbakhsh"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Cycle-level simulators such as gem5 are widely used in microarchitecture\ndesign, but they are prohibitively slow for large-scale design space\nexplorations. We present Concorde, a new methodology for learning fast and\naccurate performance models of microarchitectures. Unlike existing simulators\nand learning approaches that emulate each instruction, Concorde predicts the\nbehavior of a program based on compact performance distributions that capture\nthe impact of different microarchitectural components. It derives these\nperformance distributions using simple analytical models that estimate bounds\non performance induced by each microarchitectural component, providing a simple\nyet rich representation of a program's performance characteristics across a\nlarge space of microarchitectural parameters. Experiments show that Concorde is\nmore than five orders of magnitude faster than a reference cycle-level\nsimulator, with about 2% average Cycles-Per-Instruction (CPI) prediction error\nacross a range of SPEC, open-source, and proprietary benchmarks. This enables\nrapid design-space exploration and performance sensitivity analyses that are\ncurrently infeasible, e.g., in about an hour, we conducted a first-of-its-kind\nfine-grained performance attribution to different microarchitectural components\nacross a diverse set of programs, requiring nearly 150 million CPI evaluations.",
    "published": "2025-03-29T13:25:20Z",
    "categories": [
      "cs.AR",
      "cs.LG",
      "cs.PF"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.23076v1"
  },
  {
    "arxiv_id": "2503.22900v1",
    "title": "Learning Library Cell Representations in Vector Space",
    "authors": [
      "Rongjian Liang",
      "Yi-Chen Lu",
      "Wen-Hao Liu",
      "Haoxing Ren"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      ""
    ],
    "abstract": "We propose Lib2Vec, a novel self-supervised framework to efficiently learn\nmeaningful vector representations of library cells, enabling ML models to\ncapture essential cell semantics. The framework comprises three key components:\n(1) an automated method for generating regularity tests to quantitatively\nevaluate how well cell representations reflect inter-cell relationships; (2) a\nself-supervised learning scheme that systematically extracts training data from\nLiberty files, removing the need for costly labeling; and (3) an\nattention-based model architecture that accommodates various pin counts and\nenables the creation of property-specific cell and arc embeddings. Experimental\nresults demonstrate that Lib2Vec effectively captures functional and electrical\nsimilarities. Moreover, linear algebraic operations on cell vectors reveal\nmeaningful relationships, such as vector(BUF) - vector(INV) + vector(NAND) ~\nvector(AND), showcasing the framework's nuanced representation capabilities.\nLib2Vec also enhances downstream circuit learning applications, especially when\nlabeled data is scarce.",
    "published": "2025-03-28T22:04:57Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.22900v1"
  },
  {
    "arxiv_id": "2503.22567v2",
    "title": "Benchmarking Ultra-Low-Power $\u03bc$NPUs",
    "authors": [
      "Josh Millar",
      "Yushan Huang",
      "Sarab Sethi",
      "Hamed Haddadi",
      "Anil Madhavapeddy"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Efficient on-device neural network (NN) inference has various advantages over\ncloud-based processing, including predictable latency, enhanced privacy,\ngreater reliability, and reduced operating costs for vendors. This has sparked\nthe recent rapid development of microcontroller-scale NN accelerators, often\nreferred to as neural processing units ($\\mu$NPUs), designed specifically for\nultra-low-power applications.\n  In this paper we present the first comparative evaluation of a number of\ncommercially-available $\\mu$NPUs, as well as the first independent benchmarks\nfor several of these platforms. We develop and open-source a model compilation\nframework to enable consistent benchmarking of quantized models across diverse\n$\\mu$NPU hardware. Our benchmark targets end-to-end performance and includes\nmodel inference latency, power consumption, and memory overhead, alongside\nother factors. The resulting analysis uncovers both expected performance trends\nas well as surprising disparities between hardware specifications and actual\nperformance, including $\\mu$NPUs exhibiting unexpected scaling behaviors with\nincreasing model complexity. Our framework provides a foundation for further\nevaluation of $\\mu$NPU platforms alongside valuable insights for both hardware\ndesigners and software developers in this rapidly evolving space.",
    "published": "2025-03-28T16:14:06Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.22567v2"
  },
  {
    "arxiv_id": "2504.03711v1",
    "title": "A Survey of Circuit Foundation Model: Foundation AI Models for VLSI\n  Circuit Design and EDA",
    "authors": [
      "Wenji Fang",
      "Jing Wang",
      "Yao Lu",
      "Shang Liu",
      "Yuchao Wu",
      "Yuzhe Ma",
      "Zhiyao Xie"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Artificial intelligence (AI)-driven electronic design automation (EDA)\ntechniques have been extensively explored for VLSI circuit design applications.\nMost recently, foundation AI models for circuits have emerged as a new\ntechnology trend. Unlike traditional task-specific AI solutions, these new AI\nmodels are developed through two stages: 1) self-supervised pre-training on a\nlarge amount of unlabeled data to learn intrinsic circuit properties; and 2)\nefficient fine-tuning for specific downstream applications, such as early-stage\ndesign quality evaluation, circuit-related context generation, and functional\nverification. This new paradigm brings many advantages: model generalization,\nless reliance on labeled circuit data, efficient adaptation to new tasks, and\nunprecedented generative capability. In this paper, we propose referring to AI\nmodels developed with this new paradigm as circuit foundation models (CFMs).\nThis paper provides a comprehensive survey of the latest progress in circuit\nfoundation models, unprecedentedly covering over 130 relevant works. Over 90%\nof our introduced works were published in or after 2022, indicating that this\nemerging research trend has attracted wide attention in a short period. In this\nsurvey, we propose to categorize all existing circuit foundation models into\ntwo primary types: 1) encoder-based methods performing general circuit\nrepresentation learning for predictive tasks; and 2) decoder-based methods\nleveraging large language models (LLMs) for generative tasks. For our\nintroduced works, we cover their input modalities, model architecture,\npre-training strategies, domain adaptation techniques, and downstream design\napplications. In addition, this paper discussed the unique properties of\ncircuits from the data perspective. These circuit properties have motivated\nmany works in this domain and differentiated them from general AI techniques.",
    "published": "2025-03-28T07:27:27Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2504.03711v1"
  },
  {
    "arxiv_id": "2503.20507v2",
    "title": "Harmonia: A Multi-Agent Reinforcement Learning Approach to Data\n  Placement and Migration in Hybrid Storage Systems",
    "authors": [
      "Rakesh Nadig",
      "Vamanan Arulchelvan",
      "Rahul Bera",
      "Taha Shahroodi",
      "Gagandeep Singh",
      "Andreas Kakolyris",
      "Mohammad Sadrosadati",
      "Jisung Park",
      "Onur Mutlu"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Hybrid storage systems (HSS) combine multiple storage devices with diverse\ncharacteristics to achieve high performance and capacity at low cost. The\nperformance of an HSS highly depends on the effectiveness of two key policies:\n(1) the data-placement policy, which determines the best-fit storage device for\nincoming data, and (2) the data-migration policy, which rearranges stored data\nacross the devices to sustain high HSS performance. Prior works focus on\nimproving only data placement or only data migration in HSS, which leads to\nrelatively low HSS performance. Unfortunately, no prior work tries to optimize\nboth policies together. Our goal is to design a holistic data-management\ntechnique that optimizes both data-placement and data-migration policies to\nfully exploit the potential of an HSS, and thus significantly improve system\nperformance. We demonstrate the need for multiple reinforcement learning (RL)\nagents to accomplish our goal. We propose Harmonia, a multi-agent RL-based\ndata-management technique that employs two lightweight autonomous RL agents, a\ndata-placement agent and a data-migration agent, which adapt their policies for\nthe current workload and HSS configuration, and coordinate with each other to\nimprove overall HSS performance. We evaluate Harmonia on a real HSS with up to\nfour heterogeneous and diverse storage devices. Our evaluation using 17\ndata-intensive workloads on performance-optimized (cost-optimized) HSS with two\nstorage devices shows that, on average, Harmonia outperforms the\nbest-performing prior approach by 49.5% (31.7%). On an HSS with three (four)\ndevices, Harmonia outperforms the best-performing prior work by 37.0% (42.0%).\nHarmonia's performance benefits come with low latency (240ns for inference) and\nstorage overheads (206 KiB in DRAM for both RL agents together). We will\nopen-source Harmonia's implementation to aid future research on HSS.",
    "published": "2025-03-26T12:47:52Z",
    "categories": [
      "cs.AR",
      "cs.DC",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.20507v2"
  },
  {
    "arxiv_id": "2503.19640v1",
    "title": "An Efficient Data Reuse with Tile-Based Adaptive Stationary for\n  Transformer Accelerators",
    "authors": [
      "Tseng-Jen Li",
      "Tian-Sheuan Chang"
    ],
    "author_affiliations": [
      "",
      ""
    ],
    "abstract": "Transformer-based models have become the \\textit{de facto} backbone across\nmany fields, such as computer vision and natural language processing. However,\nas these models scale in size, external memory access (EMA) for weight and\nactivations becomes a critical bottleneck due to its significantly higher\nenergy consumption compared to internal computations. While most prior work has\nfocused on optimizing the self-attention mechanism, little attention has been\ngiven to optimizing data transfer during linear projections, where EMA costs\nare equally important. In this paper, we propose the Tile-based Adaptive\nStationary (TAS) scheme that selects the input or weight stationary in a tile\ngranularity, based on the input sequence length. Our experimental results\ndemonstrate that TAS can significantly reduce EMA by more than 97\\% compared to\ntraditional stationary schemes, while being compatible with various attention\noptimization techniques and hardware accelerators.",
    "published": "2025-03-25T13:29:58Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.19640v1"
  },
  {
    "arxiv_id": "2503.18599v2",
    "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
    "authors": [
      "Minsu Kim",
      "Seongmin Hong",
      "RyeoWook Ko",
      "Soongyu Choi",
      "Hunjong Lee",
      "Junsoo Kim",
      "Joo-Young Kim",
      "Jongse Park"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
    "published": "2025-03-24T11:56:50Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.18599v2"
  },
  {
    "arxiv_id": "2503.17513v1",
    "title": "Improving Quantization with Post-Training Model Expansion",
    "authors": [
      "Giuseppe Franco",
      "Pablo Monteagudo-Lago",
      "Ian Colbert",
      "Nicholas Fraser",
      "Michaela Blott"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "The size of a model has been a strong predictor of its quality, as well as\nits cost. As such, the trade-off between model cost and quality has been\nwell-studied. Post-training optimizations like quantization and pruning have\ntypically focused on reducing the overall volume of pre-trained models to\nreduce inference costs while maintaining model quality. However, recent\nadvancements have introduced optimization techniques that, interestingly,\nexpand models post-training, increasing model size to improve quality when\nreducing volume. For instance, to enable 4-bit weight and activation\nquantization, incoherence processing often necessitates inserting online\nHadamard rotations in the compute graph, and preserving highly sensitive\nweights often calls for additional higher precision computations. However, if\napplication requirements cannot be met, the prevailing solution is to relax\nquantization constraints. In contrast, we demonstrate post-training model\nexpansion is a viable strategy to improve model quality within a quantization\nco-design space, and provide theoretical justification. We show it is possible\nto progressively and selectively expand the size of a pre-trained large\nlanguage model (LLM) to improve model quality without end-to-end retraining. In\nparticular, when quantizing the weights and activations to 4 bits for Llama3\n1B, we reduce the zero-shot accuracy gap to full precision by an average of 3%\nrelative to both QuaRot and SpinQuant with only 5% more parameters, which is\nstill a 3.8% reduction in volume relative to a BF16 reference model.",
    "published": "2025-03-21T19:56:59Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.17513v1"
  },
  {
    "arxiv_id": "2503.16939v1",
    "title": "On-Sensor Convolutional Neural Networks with Early-Exits",
    "authors": [
      "Hazem Hesham Yousef Shalby",
      "Arianna De Vecchi",
      "Alice Scandelli",
      "Pietro Bartoli",
      "Diana Trojaniello",
      "Manuel Roveri",
      "Federica Villa"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Tiny Machine Learning (TinyML) is a novel research field aiming at\nintegrating Machine Learning (ML) within embedded devices with limited memory,\ncomputation, and energy. Recently, a new branch of TinyML has emerged, focusing\non integrating ML directly into the sensors to further reduce the power\nconsumption of embedded devices. Interestingly, despite their state-of-the-art\nperformance in many tasks, none of the current solutions in the literature aims\nto optimize the implementation of Convolutional Neural Networks (CNNs)\noperating directly into sensors. In this paper, we introduce for the first time\nin the literature the optimized design and implementation of Depth-First CNNs\noperating on the Intelligent Sensor Processing Unit (ISPU) within an Inertial\nMeasurement Unit (IMU) by STMicroelectronics. Our approach partitions the CNN\nbetween the ISPU and the microcontroller (MCU) and employs an Early-Exit\nmechanism to stop the computations on the IMU when enough confidence about the\nresults is achieved, hence significantly reducing power consumption. When using\na NUCLEO-F411RE board, this solution achieved an average current consumption of\n4.8 mA, marking an 11% reduction compared to the regular inference pipeline on\nthe MCU, while having equal accuracy.",
    "published": "2025-03-21T08:31:07Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.16939v1"
  },
  {
    "arxiv_id": "2503.16731v3",
    "title": "Design and Implementation of an FPGA-Based Hardware Accelerator for\n  Transformer",
    "authors": [
      "Richie Li",
      "Sicheng Chen"
    ],
    "author_affiliations": [
      "",
      ""
    ],
    "abstract": "Transformer-based large language models (LLMs) rely heavily on intensive\nmatrix multiplications for attention and feed-forward layers, with the Q, K,\nand V linear projections in the Multi-Head Self-Attention (MHA) module\nconstituting a decisive performance bottleneck. In this work, we introduce a\nhighly optimized tiled matrix multiplication accelerator on a\nresource-constrained Xilinx KV260 FPGA that not only addresses this challenge\nbut sets a new standard for efficiency and performance. Our design exploits\npersistent on-chip storage, a robust two-level tiling strategy for maximal data\nreuse, and a systolic-like unrolled compute engine that together deliver\nunparalleled speed and energy efficiency. Integrated with DistilBERT for Q, K,\nand V projections, our accelerator achieves an unequivocal 7x speedup over ARM\nCPU implementations (PyTorch) and an extraordinary 200x improvement over naive\nNumPy, reaching a throughput of up to 3.1~GFLOPs for matrix multiplications on\n(64,768) x (768,3072) matrices while operating at a conservative 100 MHz. These\nresults decisively demonstrate the transformative potential of FPGA-based\nacceleration for critical Transformer operations, paving the way for scalable\nand energy-efficient deep learning inference on edge devices.",
    "published": "2025-03-20T22:15:42Z",
    "categories": [
      "cs.AR",
      "cs.CL",
      "cs.LG",
      "B.7.1; C.1.4"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.16731v3"
  },
  {
    "arxiv_id": "2503.16583v1",
    "title": "Explainable AI-Guided Efficient Approximate DNN Generation for Multi-Pod\n  Systolic Arrays",
    "authors": [
      "Ayesha Siddique",
      "Khurram Khalil",
      "Khaza Anuarul Hoque"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "Approximate deep neural networks (AxDNNs) are promising for enhancing energy\nefficiency in real-world devices. One of the key contributors behind this\nenhanced energy efficiency in AxDNNs is the use of approximate multipliers.\nUnfortunately, the simulation of approximate multipliers does not usually scale\nwell on CPUs and GPUs. As a consequence, this slows down the overall simulation\nof AxDNNs aimed at identifying the appropriate approximate multipliers to\nachieve high energy efficiency with a minimum accuracy loss. To address this\nproblem, we present a novel XAI-Gen methodology, which leverages the analytical\nmodel of the emerging hardware accelerator (e.g., Google TPU v4) and\nexplainable artificial intelligence (XAI) to precisely identify the\nnon-critical layers for approximation and quickly discover the appropriate\napproximate multipliers for AxDNN layers. Our results show that XAI-Gen\nachieves up to 7x lower energy consumption with only 1-2% accuracy loss. We\nalso showcase the effectiveness of the XAI-Gen approach through a neural\narchitecture search (XAI-NAS) case study. Interestingly, XAI-NAS achieves 40\\%\nhigher energy efficiency with up to 5x less execution time when compared to the\nstate-of-the-art NAS methods for generating AxDNNs.",
    "published": "2025-03-20T14:26:47Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.16583v1"
  },
  {
    "arxiv_id": "2503.14153v1",
    "title": "Speculative Decoding for Verilog: Speed and Quality, All in One",
    "authors": [
      "Changran Xu",
      "Yi Liu",
      "Yunhao Zhou",
      "Shan Huang",
      "Ningyi Xu",
      "Qiang Xu"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "The rapid advancement of large language models (LLMs) has revolutionized code\ngeneration tasks across various programming languages. However, the unique\ncharacteristics of programming languages, particularly those like Verilog with\nspecific syntax and lower representation in training datasets, pose significant\nchallenges for conventional tokenization and decoding approaches. In this\npaper, we introduce a novel application of speculative decoding for Verilog\ncode generation, showing that it can improve both inference speed and output\nquality, effectively achieving speed and quality all in one. Unlike standard\nLLM tokenization schemes, which often fragment meaningful code structures, our\napproach aligns decoding stops with syntactically significant tokens, making it\neasier for models to learn the token distribution. This refinement addresses\ninherent tokenization issues and enhances the model's ability to capture\nVerilog's logical constructs more effectively. Our experimental results show\nthat our method achieves up to a 5.05x speedup in Verilog code generation and\nincreases pass@10 functional accuracy on RTLLM by up to 17.19% compared to\nconventional training strategies. These findings highlight speculative decoding\nas a promising approach to bridge the quality gap in code generation for\nspecialized programming languages.",
    "published": "2025-03-18T11:21:53Z",
    "categories": [
      "cs.LG",
      "cs.AR",
      "cs.CL"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.14153v1"
  },
  {
    "arxiv_id": "2503.13116v4",
    "title": "VeriLeaky: Navigating IP Protection vs Utility in Fine-Tuning for\n  LLM-Driven Verilog Coding",
    "authors": [
      "Zeng Wang",
      "Minghao Shao",
      "Mohammed Nabeel",
      "Prithwish Basu Roy",
      "Likhitha Mankali",
      "Jitendra Bhandari",
      "Ramesh Karri",
      "Ozgur Sinanoglu",
      "Muhammad Shafique",
      "Johann Knechtel"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Large language models (LLMs) offer significant potential for coding, yet\nfine-tuning (FT) with curated data is essential for niche languages like\nVerilog. Using proprietary intellectual property (IP) for FT presents a serious\nrisk, as FT data can be leaked through LLM inference. This leads to a critical\ndilemma for design houses: seeking to build externally accessible LLMs offering\ncompetitive Verilog coding, how can they leverage in-house IP to enhance FT\nutility while ensuring IP protection?\n  For the first time in the literature, we study this dilemma. Using LLaMA\n3.1-8B, we conduct in-house FT on a baseline Verilog dataset (RTLCoder)\nsupplemented with our own in-house IP, which is validated through multiple\ntape-outs. To rigorously assess IP leakage, we quantify structural similarity\n(AST/Dolos) and functional equivalence (Synopsys Formality) between generated\ncodes and our in-house IP. We show that our IP can indeed be leaked, confirming\nthe threat. As defense, we evaluate logic locking of Verilog codes (ASSURE).\nThis offers some level of protection, yet reduces the IP's utility for FT and\ndegrades the LLM's performance. Our study shows the need for novel strategies\nthat are both effective and minimally disruptive to FT, an essential effort for\nenabling design houses to fully utilize their proprietary IP toward LLM-driven\nVerilog coding.",
    "published": "2025-03-17T12:38:03Z",
    "categories": [
      "cs.CR",
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.13116v4"
  },
  {
    "arxiv_id": "2503.13572v3",
    "title": "VeriContaminated: Assessing LLM-Driven Verilog Coding for Data\n  Contamination",
    "authors": [
      "Zeng Wang",
      "Minghao Shao",
      "Jitendra Bhandari",
      "Likhitha Mankali",
      "Ramesh Karri",
      "Ozgur Sinanoglu",
      "Muhammad Shafique",
      "Johann Knechtel"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Large Language Models (LLMs) have revolutionized code generation, achieving\nexceptional results on various established benchmarking frameworks. However,\nconcerns about data contamination - where benchmark data inadvertently leaks\ninto pre-training or fine-tuning datasets - raise questions about the validity\nof these evaluations. While this issue is known, limiting the industrial\nadoption of LLM-driven software engineering, hardware coding has received\nlittle to no attention regarding these risks. For the first time, we analyze\nstate-of-the-art (SOTA) evaluation frameworks for Verilog code generation\n(VerilogEval and RTLLM), using established methods for contamination detection\n(CCD and Min-K% Prob). We cover SOTA commercial and open-source LLMs\n(CodeGen2.5, Minitron 4b, Mistral 7b, phi-4 mini, LLaMA-{1,2,3.1},\nGPT-{2,3.5,4o}, Deepseek-Coder, and CodeQwen 1.5), in baseline and fine-tuned\nmodels (RTLCoder and Verigen). Our study confirms that data contamination is a\ncritical concern. We explore mitigations and the resulting trade-offs for code\nquality vs fairness (i.e., reducing contamination toward unbiased\nbenchmarking).",
    "published": "2025-03-17T12:26:49Z",
    "categories": [
      "cs.AR",
      "cs.CR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.13572v3"
  },
  {
    "arxiv_id": "2503.16514v3",
    "title": "VeriMind: Agentic LLM for Automated Verilog Generation with a Novel\n  Evaluation Metric",
    "authors": [
      "Bardia Nadimi",
      "Ghali Omar Boutaib",
      "Hao Zheng"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "Designing Verilog modules requires meticulous attention to correctness,\nefficiency, and adherence to design specifications. However, manually writing\nVerilog code remains a complex and time-consuming task that demands both expert\nknowledge and iterative refinement. Leveraging recent advancements in large\nlanguage models (LLMs) and their structured text generation capabilities, we\npropose VeriMind, an agentic LLM framework for Verilog code generation that\nsignificantly automates and optimizes the synthesis process. Unlike traditional\nLLM-based code generators, VeriMind employs a structured reasoning approach:\ngiven a user-provided prompt describing design requirements, the system first\nformulates a detailed train of thought before the final Verilog code is\ngenerated. This multi-step methodology enhances interpretability, accuracy, and\nadaptability in hardware design. In addition, we introduce a novel evaluation\nmetric-pass@ARC-which combines the conventional pass@k measure with Average\nRefinement Cycles (ARC) to capture both success rate and the efficiency of\niterative refinement. Experimental results on diverse hardware design tasks\ndemonstrated that our approach achieved up to $8.3\\%$ improvement on pass@k\nmetric and $8.1\\%$ on pass@ARC metric. These findings underscore the\ntransformative potential of agentic LLMs in automated hardware design, RTL\ndevelopment, and digital system synthesis.",
    "published": "2025-03-15T23:43:06Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG",
      "cs.PL"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.16514v3"
  },
  {
    "arxiv_id": "2503.11246v1",
    "title": "Cost-effective Deep Learning Infrastructure with NVIDIA GPU",
    "authors": [
      "Aatiz Ghimire",
      "Shahnawaz Alam",
      "Siman Giri",
      "Madhav Prasad Ghimire"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      ""
    ],
    "abstract": "The growing demand for computational power is driven by advancements in deep\nlearning, the increasing need for big data processing, and the requirements of\nscientific simulations for academic and research purposes. Developing countries\nlike Nepal often struggle with the resources needed to invest in new and better\nhardware for these purposes. However, optimizing and building on existing\ntechnology can still meet these computing demands effectively. To address these\nneeds, we built a cluster using four NVIDIA GeForce GTX 1650 GPUs. The cluster\nconsists of four nodes: one master node that controls and manages the entire\ncluster, and three compute nodes dedicated to processing tasks. The master node\nis equipped with all necessary software for package management, resource\nscheduling, and deployment, such as Anaconda and Slurm. In addition, a Network\nFile Storage (NFS) system was integrated to provide the additional storage\nrequired by the cluster. Given that the cluster is accessible via ssh by a\npublic domain address, which poses significant cybersecurity risks, we\nimplemented fail2ban to mitigate brute force attacks and enhance security.\nDespite the continuous challenges encountered during the design and\nimplementation process, this project demonstrates how powerful computational\nclusters can be built to handle resource-intensive tasks in various demanding\nfields.",
    "published": "2025-03-14T09:54:36Z",
    "categories": [
      "cs.DC",
      "cs.AR",
      "cs.LG",
      "cs.SE",
      "cs.SY",
      "eess.SY"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.11246v1"
  },
  {
    "arxiv_id": "2503.08823v2",
    "title": "ResBench: Benchmarking LLM-Generated FPGA Designs with Resource\n  Awareness",
    "authors": [
      "Ce Guo",
      "Tong Zhao"
    ],
    "author_affiliations": [
      "",
      ""
    ],
    "abstract": "Field-Programmable Gate Arrays (FPGAs) are widely used in modern hardware\ndesign, yet writing Hardware Description Language (HDL) code for FPGA\nimplementation remains a complex and time-consuming task. Large Language Models\n(LLMs) have emerged as a promising tool for HDL generation, but existing\nbenchmarks for LLM-based code generation primarily focus on functional\ncorrectness while overlooking hardware resource usage. Furthermore, current\nbenchmarks offer limited diversity and do not fully represent the wide range of\nreal-world FPGA applications. To address these shortcomings, we introduce\nResBench, the first resource-focused benchmark explicitly designed to\ndistinguish between resource-optimized and inefficient LLM-generated HDL code.\nResBench consists of 56 problems across 12 categories, covering applications\nfrom finite state machines to financial computing. Our open-source evaluation\nframework automatically tests LLMs by generating Verilog code, verifying\ncorrectness, and measuring resource usage. The experiments, which primarily\nanalyze Lookup Table (LUT) usage, reveal significant differences among LLMs,\ndemonstrating ResBench's capability to identify models that generate more\nresource-optimized FPGA designs.",
    "published": "2025-03-11T18:54:17Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.CL",
      "cs.ET",
      "cs.LG",
      "I.2.2"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.08823v2"
  },
  {
    "arxiv_id": "2503.06342v1",
    "title": "Exploring the Performance Improvement of Tensor Processing Engines\n  through Transformation in the Bit-weight Dimension of MACs",
    "authors": [
      "Qizhe Wu",
      "Huawen Liang",
      "Yuchen Gui",
      "Zhichen Zeng",
      "Zerong He",
      "Linfeng Tao",
      "Xiaotian Wang",
      "Letian Zhao",
      "Zhaoxi Zeng",
      "Wei Yuan",
      "Wei Wu",
      "Xi Jin"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "General matrix-matrix multiplication (GEMM) is a cornerstone of AI\ncomputations, making tensor processing engines (TPEs) increasingly critical in\nGPUs and domain-specific architectures. Existing architectures primarily\noptimize dataflow or operand reuse strategies. However, considering the\ninteraction between matrix multiplication and multiply-accumulators (MACs)\noffers greater optimization potential. This work introduces a novel hardware\nperspective on matrix multiplication, focusing on the bit-weight dimension of\nMACs. We propose a finer-grained TPE notation using matrix triple loops as an\nexample, introducing new methods for designing and optimizing PE\nmicroarchitectures. Based on this notation and its transformations, we propose\nfour optimization techniques that improve timing, area, and power consumption.\nImplementing our design in RTL using the SMIC-28nm process, we evaluate its\neffectiveness across four classic TPE architectures: systolic array, 3D-Cube,\nmultiplier-adder tree, and 2D-Matrix. Our techniques achieve area efficiency\nimprovements of 1.27x, 1.28x, 1.56x, and 1.44x, and energy efficiency gains of\n1.04x, 1.56x, 1.49x, and 1.20x, respectively. Applied to a bit-slice\narchitecture, our approach achieves a 12.10x improvement in energy efficiency\nand 2.85x in area efficiency compared to Laconic. Our Verilog HDL code, along\nwith timing, area, and power reports, is available at\nhttps://github.com/wqzustc/High-Performance-Tensor-Processing-Engines",
    "published": "2025-03-08T21:21:23Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.06342v1"
  },
  {
    "arxiv_id": "2503.08700v1",
    "title": "Real-Time Semantic Segmentation of Aerial Images Using an Embedded\n  U-Net: A Comparison of CPU, GPU, and FPGA Workflows",
    "authors": [
      "Julien Posso",
      "Hugo Kieffer",
      "Nicolas Menga",
      "Omar Hlimi",
      "S\u00e9bastien Tarris",
      "Hubert Guerard",
      "Guy Bois",
      "Matthieu Couderc",
      "Eric Jenn"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "This study introduces a lightweight U-Net model optimized for real-time\nsemantic segmentation of aerial images, targeting the efficient utilization of\nCommercial Off-The-Shelf (COTS) embedded computing platforms. We maintain the\naccuracy of the U-Net on a real-world dataset while significantly reducing the\nmodel's parameters and Multiply-Accumulate (MAC) operations by a factor of 16.\nOur comprehensive analysis covers three hardware platforms (CPU, GPU, and FPGA)\nand five different toolchains (TVM, FINN, Vitis AI, TensorFlow GPU, and cuDNN),\nassessing each on metrics such as latency, power consumption, memory footprint,\nenergy efficiency, and FPGA resource usage. The results highlight the\ntrade-offs between these platforms and toolchains, with a particular focus on\nthe practical deployment challenges in real-world applications. Our findings\ndemonstrate that while the FPGA with Vitis AI emerges as the superior choice\ndue to its performance, energy efficiency, and maturity, it requires\nspecialized hardware knowledge, emphasizing the need for a balanced approach in\nselecting embedded computing solutions for semantic segmentation tasks",
    "published": "2025-03-07T08:33:28Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.08700v1"
  },
  {
    "arxiv_id": "2503.04426v1",
    "title": "FORTALESA: Fault-Tolerant Reconfigurable Systolic Array for DNN\n  Inference",
    "authors": [
      "Natalia Cherezova",
      "Artur Jutman",
      "Maksim Jenihhin"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "The emergence of Deep Neural Networks (DNNs) in mission- and safety-critical\napplications brings their reliability to the front. High performance demands of\nDNNs require the use of specialized hardware accelerators. Systolic array\narchitecture is widely used in DNN accelerators due to its parallelism and\nregular structure. This work presents a run-time reconfigurable systolic array\narchitecture with three execution modes and four implementation options. All\nfour implementations are evaluated in terms of resource utilization,\nthroughput, and fault tolerance improvement. The proposed architecture is used\nfor reliability enhancement of DNN inference on systolic array through\nheterogeneous mapping of different network layers to different execution modes.\nThe approach is supported by a novel reliability assessment method based on\nfault propagation analysis. It is used for the exploration of the appropriate\nexecution mode-layer mapping for DNN inference. The proposed architecture\nefficiently protects registers and MAC units of systolic array PEs from\ntransient and permanent faults. The reconfigurability feature enables a speedup\nof up to $3\\times$, depending on layer vulnerability. Furthermore, it requires\n$6\\times$ less resources compared to static redundancy and $2.5\\times$ less\nresources compared to the previously proposed solution for transient faults.",
    "published": "2025-03-06T13:35:59Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.04426v1"
  },
  {
    "arxiv_id": "2503.11687v1",
    "title": "Review of Machine Learning for Micro-Electronic Design Verification",
    "authors": [
      "Christopher Bennett",
      "Kerstin Eder"
    ],
    "author_affiliations": [
      "",
      ""
    ],
    "abstract": "Microelectronic design verification remains a critical bottleneck in device\ndevelopment, traditionally mitigated by expanding verification teams and\ncomputational resources. Since the late 1990s, machine learning (ML) has been\nproposed to enhance verification efficiency, yet many techniques have not\nachieved mainstream adoption. This review, from the perspective of verification\nand ML practitioners, examines the application of ML in dynamic-based\ntechniques for functional verification of microelectronic designs, and provides\na starting point for those new to this interdisciplinary field. Historical\ntrends, techniques, ML types, and evaluation baselines are analysed to\nunderstand why previous research has not been widely adopted in industry. The\nreview highlights the application of ML, the techniques used and critically\ndiscusses their limitations and successes. Although there is a wealth of\npromising research, real-world adoption is hindered by challenges in comparing\ntechniques, identifying suitable applications, and the expertise required for\nimplementation. This review proposes that the field can progress through the\ncreation and use of open datasets, common benchmarks, and verification targets.\nBy establishing open evaluation criteria, industry can guide future research.\nParallels with ML in software verification suggest potential for collaboration.\nAdditionally, greater use of open-source designs and verification environments\ncan allow more researchers from outside the hardware verification discipline to\ncontribute to the challenge of verifying microelectronic designs.",
    "published": "2025-03-05T15:41:09Z",
    "categories": [
      "cs.AR",
      "cs.LG",
      "A.1"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.11687v1"
  },
  {
    "arxiv_id": "2503.03088v2",
    "title": "AHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for\n  Segment Anything Model",
    "authors": [
      "Wenlun Zhang",
      "Yunshan Zhong",
      "Shimpei Ando",
      "Kentaro Yoshioka"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      ""
    ],
    "abstract": "The Segment Anything Model (SAM) has demonstrated strong versatility across\nvarious visual tasks. However, its large storage requirements and high\ncomputational cost pose challenges for practical deployment. Post-training\nquantization (PTQ) has emerged as an effective strategy for efficient\ndeployment, but we identify two key challenges in SAM that hinder the\neffectiveness of existing PTQ methods: the heavy-tailed and skewed distribution\nof post-GELU activations, and significant inter-channel variation in linear\nprojection activations. To address these challenges, we propose AHCPTQ, an\naccurate and hardware-efficient PTQ method for SAM. AHCPTQ introduces\nhardware-compatible Hybrid Log-Uniform Quantization (HLUQ) to manage post-GELU\nactivations, employing log2 quantization for dense small values and uniform\nquantization for sparse large values to enhance quantization resolution.\nAdditionally, AHCPTQ incorporates Channel-Aware Grouping (CAG) to mitigate\ninter-channel variation by progressively clustering activation channels with\nsimilar distributions, enabling them to share quantization parameters and\nimproving hardware efficiency. The combination of HLUQ and CAG not only\nenhances quantization effectiveness but also ensures compatibility with\nefficient hardware execution. For instance, under the W4A4 configuration on the\nSAM-L model, AHCPTQ achieves 36.6% mAP on instance segmentation with the DINO\ndetector, while achieving a 7.89x speedup and 8.64x energy efficiency over its\nfloating-point counterpart in FPGA implementation.",
    "published": "2025-03-05T01:04:45Z",
    "categories": [
      "cs.CV",
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.03088v2"
  },
  {
    "arxiv_id": "2503.00205v1",
    "title": "AnalogGenie: A Generative Engine for Automatic Discovery of Analog\n  Circuit Topologies",
    "authors": [
      "Jian Gao",
      "Weidong Cao",
      "Junyi Yang",
      "Xuan Zhang"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      ""
    ],
    "abstract": "The massive and large-scale design of foundational semiconductor integrated\ncircuits (ICs) is crucial to sustaining the advancement of many emerging and\nfuture technologies, such as generative AI, 5G/6G, and quantum computing.\nExcitingly, recent studies have shown the great capabilities of foundational\nmodels in expediting the design of digital ICs. Yet, applying generative AI\ntechniques to accelerate the design of analog ICs remains a significant\nchallenge due to critical domain-specific issues, such as the lack of a\ncomprehensive dataset and effective representation methods for analog circuits.\nThis paper proposes, $\\textbf{AnalogGenie}$, a\n$\\underline{\\textbf{Gen}}$erat$\\underline{\\textbf{i}}$ve\n$\\underline{\\textbf{e}}$ngine for automatic design/discovery of\n$\\underline{\\textbf{Analog}}$ circuit topologies--the most challenging and\ncreative task in the conventional manual design flow of analog ICs. AnalogGenie\naddresses two key gaps in the field: building a foundational comprehensive\ndataset of analog circuit topology and developing a scalable sequence-based\ngraph representation universal to analog circuits. Experimental results show\nthe remarkable generation performance of AnalogGenie in broadening the variety\nof analog ICs, increasing the number of devices within a single design, and\ndiscovering unseen circuit topologies far beyond any prior arts. Our work paves\nthe way to transform the longstanding time-consuming manual design flow of\nanalog ICs to an automatic and massive manner powered by generative AI. Our\nsource code is available at https://github.com/xz-group/AnalogGenie.",
    "published": "2025-02-28T21:41:20Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.00205v1"
  },
  {
    "arxiv_id": "2502.21196v1",
    "title": "AMPLE: Event-Driven Accelerator for Mixed-Precision Inference of Graph\n  Neural Networks",
    "authors": [
      "Pedro Gimenes",
      "Yiren Zhao",
      "George Constantinides"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "Graph Neural Networks (GNNs) have recently gained attention due to their\nperformance on non-Euclidean data. The use of custom hardware architectures\nproves particularly beneficial for GNNs due to their irregular memory access\npatterns, resulting from the sparse structure of graphs. However, existing FPGA\naccelerators are limited by their double buffering mechanism, which doesn't\naccount for the irregular node distribution in typical graph datasets. To\naddress this, we introduce \\textbf{AMPLE} (Accelerated Message Passing Logic\nEngine), an FPGA accelerator leveraging a new event-driven programming flow. We\ndevelop a mixed-arithmetic architecture, enabling GNN inference to be quantized\nat a node-level granularity. Finally, prefetcher for data and instructions is\nimplemented to optimize off-chip memory access and maximize node parallelism.\nEvaluation on citation and social media graph datasets ranging from $2$K to\n$700$K nodes showed a mean speedup of $243\\times$ and $7.2\\times$ against CPU\nand GPU counterparts, respectively.",
    "published": "2025-02-28T16:14:16Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2502.21196v1"
  },
  {
    "arxiv_id": "2502.17936v3",
    "title": "The Art of Beating the Odds with Predictor-Guided Random Design Space\n  Exploration",
    "authors": [
      "Felix Arnold",
      "Maxence Bouvier",
      "Ryan Amaudruz",
      "Renzo Andri",
      "Lukas Cavigelli"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "This work introduces an innovative method for improving combinational digital\ncircuits through random exploration in MIG-based synthesis. High-quality\ncircuits are crucial for performance, power, and cost, making this a critical\narea of active research. Our approach incorporates next-state prediction and\niterative selection, significantly accelerating the synthesis process. This\nnovel method achieves up to 14x synthesis speedup and up to 20.94% better MIG\nminimization on the EPFL Combinational Benchmark Suite compared to\nstate-of-the-art techniques. We further explore various predictor models and\nshow that increased prediction accuracy does not guarantee an equivalent\nincrease in synthesis quality of results or speedup, observing that randomness\nremains a desirable factor.",
    "published": "2025-02-25T07:56:25Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2502.17936v3"
  },
  {
    "arxiv_id": "2502.16540v2",
    "title": "D2S-FLOW: Automated Parameter Extraction from Datasheets for SPICE Model\n  Generation Using Large Language Models",
    "authors": [
      "Hong Cai Chen",
      "Yi Pin Xu",
      "Yang Zhang"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "In electronic design, engineers often manually search through extensive\ndocuments to retrieve component parameters required for constructing SPICE\nmodels, a process that is both labor-intensive and time-consuming. To address\nthis challenge, we present an automated framework called D2S-FLOW that\nleverages large language models (LLMs) to extract electrical parameters from\ndatasheets and generate SPICE models with high precision and efficiency,\nsignificantly reducing the need for manual intervention. Unlike traditional RAG\nsystems, D2S-FLOW employs a workflow to enhance precision in handling\nunstructured documents and inconsistent naming conventions through three\ninnovative mechanisms: Attention-Guided Document Focusing (AGDF), Hierarchical\nDocument-Enhanced Retrieval (HDER), and Heterogeneous Named Entity\nNormalization (HNEN). AGDF narrows retrieval to user-selected documents, HDER\nutilizes document structure for precise parameter localization, and HNEN\nstandardizes terminology via semantic inference. Experimental results\ndemonstrate that the framework achieves an Exact Match (EM) of 0.86, an F1\nscore of 0.92, and an Exact Correctness (EC) of 0.96, outperforming the\nstrongest baseline by 19.4%, 5.7%, and 13.1%, respectively. Additionally, it\nreduces API token consumption by 38% and minimizes the irrelevant information\nratio to 4%, showcasing substantial improvements in resource efficiency. This\nresearch provides an effective automated solution for circuit design.",
    "published": "2025-02-23T11:19:44Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.AR",
      "cs.IR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2502.16540v2"
  },
  {
    "arxiv_id": "2502.15470v2",
    "title": "PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding\n  with a Processing-In-Memory-Enabled Computing System",
    "authors": [
      "Yintao He",
      "Haiyu Mao",
      "Christina Giannoula",
      "Mohammad Sadrosadati",
      "Juan G\u00f3mez-Luna",
      "Huawei Li",
      "Xiaowei Li",
      "Ying Wang",
      "Onur Mutlu"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Large language models (LLMs) are widely used for natural language\nunderstanding and text generation. An LLM model relies on a time-consuming step\ncalled LLM decoding to generate output tokens. Several prior works focus on\nimproving the performance of LLM decoding using parallelism techniques, such as\nbatching and speculative decoding. State-of-the-art LLM decoding has both\ncompute-bound and memory-bound kernels. Some prior works statically identify\nand map these different kernels to a heterogeneous architecture consisting of\nboth processing-in-memory (PIM) units and computation-centric accelerators. We\nobserve that characteristics of LLM decoding kernels (e.g., whether or not a\nkernel is memory-bound) can change dynamically due to parameter changes to meet\nuser and/or system demands, making (1) static kernel mapping to PIM units and\ncomputation-centric accelerators suboptimal, and (2) one-size-fits-all approach\nof designing PIM units inefficient due to a large degree of heterogeneity even\nin memory-bound kernels.\n  In this paper, we aim to accelerate LLM decoding while considering the\ndynamically changing characteristics of the kernels involved. We propose PAPI\n(PArallel Decoding with PIM), a PIM-enabled heterogeneous architecture that\nexploits dynamic scheduling of compute-bound or memory-bound kernels to\nsuitable hardware units. PAPI has two key mechanisms: (1) online kernel\ncharacterization to dynamically schedule kernels to the most suitable hardware\nunits at runtime and (2) a PIM-enabled heterogeneous computing system that\nharmoniously orchestrates both computation-centric processing units and hybrid\nPIM units with different computing capabilities. Our experimental results on\nthree broadly-used LLMs show that PAPI achieves 1.8$\\times$ and 11.1$\\times$\nspeedups over a state-of-the-art heterogeneous LLM accelerator and a\nstate-of-the-art PIM-only LLM accelerator, respectively.",
    "published": "2025-02-21T13:52:31Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2502.15470v2"
  },
  {
    "arxiv_id": "2502.15832v1",
    "title": "DeepRTL: Bridging Verilog Understanding and Generation with a Unified\n  Representation Model",
    "authors": [
      "Yi Liu",
      "Changran Xu",
      "Yunhao Zhou",
      "Zeju Li",
      "Qiang Xu"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Recent advancements in large language models (LLMs) have shown significant\npotential for automating hardware description language (HDL) code generation\nfrom high-level natural language instructions. While fine-tuning has improved\nLLMs' performance in hardware design tasks, prior efforts have largely focused\non Verilog generation, overlooking the equally critical task of Verilog\nunderstanding. Furthermore, existing models suffer from weak alignment between\nnatural language descriptions and Verilog code, hindering the generation of\nhigh-quality, synthesizable designs. To address these issues, we present\nDeepRTL, a unified representation model that excels in both Verilog\nunderstanding and generation. Based on CodeT5+, DeepRTL is fine-tuned on a\ncomprehensive dataset that aligns Verilog code with rich, multi-level natural\nlanguage descriptions. We also introduce the first benchmark for Verilog\nunderstanding and take the initiative to apply embedding similarity and GPT\nScore to evaluate the models' understanding capabilities. These metrics capture\nsemantic similarity more accurately than traditional methods like BLEU and\nROUGE, which are limited to surface-level n-gram overlaps. By adapting\ncurriculum learning to train DeepRTL, we enable it to significantly outperform\nGPT-4 in Verilog understanding tasks, while achieving performance on par with\nOpenAI's o1-preview model in Verilog generation tasks.",
    "published": "2025-02-20T11:07:55Z",
    "categories": [
      "cs.AR",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2502.15832v1"
  },
  {
    "arxiv_id": "2502.14307v1",
    "title": "\u03bcRL: Discovering Transient Execution Vulnerabilities Using\n  Reinforcement Learning",
    "authors": [
      "M. Caner Tol",
      "Kemal Derya",
      "Berk Sunar"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "We propose using reinforcement learning to address the challenges of\ndiscovering microarchitectural vulnerabilities, such as Spectre and Meltdown,\nwhich exploit subtle interactions in modern processors. Traditional methods\nlike random fuzzing fail to efficiently explore the vast instruction space and\noften miss vulnerabilities that manifest under specific conditions. To overcome\nthis, we introduce an intelligent, feedback-driven approach using RL. Our RL\nagents interact with the processor, learning from real-time feedback to\nprioritize instruction sequences more likely to reveal vulnerabilities,\nsignificantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various\nmicroarchitectures, providing a scalable solution across processor generations.\nBy automating the exploration process, we reduce the need for human\nintervention, enabling continuous learning that uncovers hidden\nvulnerabilities. Additionally, our approach detects subtle signals, such as\ntiming anomalies or unusual cache behavior, that may indicate\nmicroarchitectural weaknesses. This proposal advances hardware security testing\nby introducing a more efficient, adaptive, and systematic framework for\nprotecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL\nagent was indeed able to generate instruction sequences that cause significant\nobservable byte leakages through transient execution without generating any\n$\\mu$code assists, faults or interrupts. The newly identified leaky sequences\nstem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW,\nCLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give\ncredence to the proposed approach.",
    "published": "2025-02-20T06:42:03Z",
    "categories": [
      "cs.CR",
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2502.14307v1"
  },
  {
    "arxiv_id": "2502.13921v2",
    "title": "Exploring Code Language Models for Automated HLS-based Hardware\n  Generation: Benchmark, Infrastructure and Analysis",
    "authors": [
      "Jiahao Gai",
      "Hao Mark Chen",
      "Zhican Wang",
      "Hongyu Zhou",
      "Wanru Zhao",
      "Nicholas Lane",
      "Hongxiang Fan"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Recent advances in code generation have illuminated the potential of\nemploying large language models (LLMs) for general-purpose programming\nlanguages such as Python and C++, opening new opportunities for automating\nsoftware development and enhancing programmer productivity. The potential of\nLLMs in software programming has sparked significant interest in exploring\nautomated hardware generation and automation. Although preliminary endeavors\nhave been made to adopt LLMs in generating hardware description languages\n(HDLs), several challenges persist in this direction. First, the volume of\navailable HDL training data is substantially smaller compared to that for\nsoftware programming languages. Second, the pre-trained LLMs, mainly tailored\nfor software code, tend to produce HDL designs that are more error-prone.\nThird, the generation of HDL requires a significantly higher number of tokens\ncompared to software programming, leading to inefficiencies in cost and energy\nconsumption. To tackle these challenges, this paper explores leveraging LLMs to\ngenerate High-Level Synthesis (HLS)-based hardware design. Although code\ngeneration for domain-specific programming languages is not new in the\nliterature, we aim to provide experimental results, insights, benchmarks, and\nevaluation infrastructure to investigate the suitability of HLS over low-level\nHDLs for LLM-assisted hardware design generation. To achieve this, we first\nfinetune pre-trained models for HLS-based hardware generation, using a\ncollected dataset with text prompts and corresponding reference HLS designs. An\nLLM-assisted framework is then proposed to automate end-to-end hardware code\ngeneration, which also investigates the impact of chain-of-thought and feedback\nloops promoting techniques on HLS-design generation. Limited by the timeframe\nof this research, we plan to evaluate more advanced reasoning models in the\nfuture.",
    "published": "2025-02-19T17:53:59Z",
    "categories": [
      "cs.LG",
      "cs.AR",
      "cs.SE"
    ],
    "pdf_link": "http://arxiv.org/pdf/2502.13921v2"
  },
  {
    "arxiv_id": "2502.12444v1",
    "title": "SparAMX: Accelerating Compressed LLMs Token Generation on AMX-powered\n  CPUs",
    "authors": [
      "Ahmed F. AbouElhamayed",
      "Jordan Dotzel",
      "Yash Akhauri",
      "Chi-Chih Chang",
      "Sameh Gobriel",
      "J. Pablo Mu\u00f1oz",
      "Vui Seng Chua",
      "Nilesh Jain",
      "Mohamed S. Abdelfattah"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Large language models have high compute, latency, and memory requirements.\nWhile specialized accelerators such as GPUs and TPUs typically run these\nworkloads, CPUs are more widely available and consume less energy. Accelerating\nLLMs with CPUs enables broader AI access at a lower cost and power consumption.\nThis acceleration potential for CPUs is especially relevant during the\nmemory-bound decoding stage of LLM inference, which processes one token at a\ntime and is becoming increasingly utilized with reasoning models. We utilize\nAdvanced Matrix Extensions (AMX) support on the latest Intel CPUs together with\nunstructured sparsity to achieve a $1.42 \\times$ reduction in end-to-end\nlatency compared to the current PyTorch implementation by applying our\ntechnique in linear layers. We provide a set of open-source customized sparse\nkernels that can speed up any PyTorch model by automatically replacing all\nlinear layers with our custom sparse implementation. Furthermore, we\ndemonstrate for the first time the use of unstructured sparsity in the\nattention computation achieving a $1.14 \\times$ speedup over the current\nsystems without compromising accuracy. Code:\nhttps://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning/tree/main/SparAMX",
    "published": "2025-02-18T02:26:34Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR",
      "cs.PF"
    ],
    "pdf_link": "http://arxiv.org/pdf/2502.12444v1"
  },
  {
    "arxiv_id": "2502.11256v2",
    "title": "Unveiling Environmental Impacts of Large Language Model Serving: A\n  Functional Unit View",
    "authors": [
      "Yanran Wu",
      "Inez Hua",
      "Yi Ding"
    ],
    "author_affiliations": [
      "",
      "",
      ""
    ],
    "abstract": "Large language models (LLMs) offer powerful capabilities but come with\nsignificant environmental impact, particularly in carbon emissions. Existing\nstudies benchmark carbon emissions but lack a standardized basis for comparison\nacross different model configurations. To address this, we introduce the\nconcept of functional unit (FU) as a standardized basis and develop FUEL, the\nfirst FU-based framework for evaluating LLM serving's environmental impact.\nThrough three case studies, we uncover key insights and trade-offs in reducing\ncarbon emissions by optimizing model size, quantization strategy, and hardware\nchoice, paving the way for more sustainable LLM serving. The code is available\nat https://github.com/jojacola/FUEL.",
    "published": "2025-02-16T20:20:18Z",
    "categories": [
      "cs.LG",
      "cs.AR",
      "cs.CL"
    ],
    "pdf_link": "http://arxiv.org/pdf/2502.11256v2"
  },
  {
    "arxiv_id": "2503.11663v1",
    "title": "MEADOW: Memory-efficient Dataflow and Data Packing for Low Power Edge\n  LLMs",
    "authors": [
      "Abhishek Moitra",
      "Arkapravo Ghosh",
      "Shrey Agarwal",
      "Aporva Amarnath",
      "Karthik Swaminathan",
      "Priyadarshini Panda"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "The computational and memory challenges of large language models (LLMs) have\nsparked several optimization approaches towards their efficient implementation.\nWhile prior LLM-targeted quantization, and prior works on sparse acceleration\nhave significantly mitigated the memory and computation bottleneck, they do so\nassuming high power platforms such as GPUs and server-class FPGAs with large\noff-chip memory bandwidths and employ a generalized matrix multiplication\n(GEMM) execution of all the layers in the decoder. In such a GEMM-based\nexecution, data is fetched from an off-chip memory, computed and stored back.\nHowever, at reduced off-chip memory capacities, as is the case with low-power\nedge devices, this implementation strategy significantly increases the\nattention computation latency owing to the repeated storage and fetch of large\nintermediate tokens to and from the off-chip memory. Moreover, fetching the\nweight matrices from a bandwidth constrained memory further aggravates the\nmemory bottleneck problem. To this end, we introduce MEADOW, a framework that\nsignificantly reduces the off-chip memory access for LLMs with a novel\ntoken-parallel head-sequential (TPHS) dataflow. Additionally, MEADOW applies\nweight packing that performs loss-less decomposition of large weight matrices\nto their unique elements thereby, reducing the enormous weight fetch latency.\nMEADOW demonstrates 1.5x and 2.5x lower decode and prefill latency,\nrespectively, compared to a GEMM-based LLM implementation on the low power\nXilinx ZCU102 FPGA platform that consumes less than 10W. Additionally, MEADOW\nachieves an end-to-end latency improvement of over 40%, compared to prior LLM\noptimization works.",
    "published": "2025-02-14T23:50:37Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.11663v1"
  },
  {
    "arxiv_id": "2503.11662v2",
    "title": "Lorecast: Layout-Aware Performance and Power Forecasting from Natural\n  Language",
    "authors": [
      "Runzhi Wang",
      "Prianka Sengupta",
      "Cristhian Roman-Vicharra",
      "Yiran Chen",
      "Jiang Hu"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "In chip design planning, obtaining reliable performance and power forecasts\nfor various design options is of critical importance. Traditionally, this\ninvolves using system-level models, which often lack accuracy, or trial\nsynthesis, which is both labor-intensive and time-consuming. We introduce a new\nmethodology, called Lorecast, which accepts English prompts as input to rapidly\ngenerate layout-aware performance and power estimates. This approach bypasses\nthe need for HDL code development and synthesis, making it both fast and\nuser-friendly. Experimental results demonstrate that Lorecast achieves accuracy\nwithin a few percent of error compared to post-layout analysis, while\nsignificantly reducing turnaround time.",
    "published": "2025-02-14T23:08:39Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.11662v2"
  },
  {
    "arxiv_id": "2502.15763v1",
    "title": "Hybrid Offline-online Scheduling Method for Large Language Model\n  Inference Optimization",
    "authors": [
      "Bowen Pang",
      "Kai Li",
      "Ruifeng She",
      "Feifan Wang"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      ""
    ],
    "abstract": "With the development of large language models (LLMs), it has become\nincreasingly important to optimize hardware usage and improve throughput. In\nthis paper, we study the inference optimization of the serving system that\ndeploys LLMs. To optimize system throughput and maximize hardware utilization,\nwe formulate the inference optimization problem as a mixed-integer programming\n(MIP) model and propose a hybrid offline-online method as solution. The offline\nmethod improves large-scale inference systems by introducing a Minimizing\nMakespan Bin Packing Problem. We further provide a theoretical lower bound\ncomputation method. Then, we propose an online sorting and preemptive\nscheduling method to better utilize hardware. In the online iteration\nscheduling process, a Lagrangian method is applied to evaluate the cost\nefficiency of inserting prefill stages versus decode stages at each iteration\nand dynamically determine when to preempt decoding tasks and insert prefill\ntasks. Experiments using real-world data from the LLaMA-65B model and the GSM8K\ndataset demonstrate that system utilization improves from 80.2% to 89.1%, and\nthe total inference time decreases from 201.00 to 190.58 seconds. A 100-cases\nstudy shows that our method consistently outperforms the baseline method and\nimproves the utilization rate by 8.0% on average. Finally, we discuss potential\nfuture extensions, including stochastic modeling, reinforcement learning-based\nschedulers, and dynamic decision-making strategies for system throughput and\nhardware utilization.",
    "published": "2025-02-14T16:00:00Z",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2502.15763v1"
  },
  {
    "arxiv_id": "2502.10089v1",
    "title": "A Hybrid Edge Classifier: Combining TinyML-Optimised CNN with RRAM-CMOS\n  ACAM for Energy-Efficient Inference",
    "authors": [
      "Kieran Woodward",
      "Eiman Kanjo",
      "Georgios Papandroulidakis",
      "Shady Agwa",
      "Themis Prodromakis"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "In recent years, the development of smart edge computing systems to process\ninformation locally is on the rise. Many near-sensor machine learning (ML)\napproaches have been implemented to introduce accurate and energy efficient\ntemplate matching operations in resource-constrained edge sensing systems, such\nas wearables. To introduce novel solutions that can be viable for extreme edge\ncases, hybrid solutions combining conventional and emerging technologies have\nstarted to be proposed. Deep Neural Networks (DNN) optimised for edge\napplication alongside new approaches of computing (both device and architecture\n-wise) could be a strong candidate in implementing edge ML solutions that aim\nat competitive accuracy classification while using a fraction of the power of\nconventional ML solutions. In this work, we are proposing a hybrid\nsoftware-hardware edge classifier aimed at the extreme edge near-sensor\nsystems. The classifier consists of two parts: (i) an optimised digital tinyML\nnetwork, working as a front-end feature extractor, and (ii) a back-end\nRRAM-CMOS analogue content addressable memory (ACAM), working as a final stage\ntemplate matching system. The combined hybrid system exhibits a competitive\ntrade-off in accuracy versus energy metric with $E_{front-end}$ = $96.23 nJ$\nand $E_{back-end}$ = $1.45 nJ$ for each classification operation compared with\n78.06$\\mu$J for the original teacher model, representing a 792-fold reduction,\nmaking it a viable solution for extreme edge applications.",
    "published": "2025-02-14T11:21:36Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2502.10089v1"
  },
  {
    "arxiv_id": "2502.10001v1",
    "title": "EmbBERT-Q: Breaking Memory Barriers in Embedded NLP",
    "authors": [
      "Riccardo Bravin",
      "Massimo Pavan",
      "Hazem Hesham Yousef Shalby",
      "Fabrizio Pittorino",
      "Manuel Roveri"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Large Language Models (LLMs) have revolutionized natural language processing,\nsetting new standards across a wide range of applications. However, their\nrelevant memory and computational demands make them impractical for deployment\non technologically-constrained tiny devices such as wearable devices and\nInternet-of-Things units. To address this limitation, we introduce EmbBERT-Q, a\nnovel tiny language model specifically designed for tiny devices with stringent\nmemory constraints. EmbBERT-Q achieves state-of-the-art (SotA) accuracy in\nNatural Language Processing tasks in this scenario, with a total memory\nfootprint (weights and activations) of just 781 kB, representing a 25x\nreduction in size with respect to SotA models. By combining architectural\ninnovations with hardware-compatible 8-bit quantization, EmbBERT-Q consistently\noutperforms several baseline models scaled down to a 2 MB memory budget (i.e.,\nthe maximum memory typically available in tiny devices), including heavily\ncompressed versions of BERT and MAMBA. Extensive experimental evaluations on\nboth a selected benchmark dataset, TinyNLP, specifically curated to evaluate\nTiny Language Models in NLP tasks and real-world scenarios, and the GLUE\nbenchmark, demonstrate EmbBERT-Q ability to deliver competitive accuracy with\nrespect to existing approaches, achieving an unmatched balance between memory\nand performance. To ensure the complete and immediate reproducibility of all\nour results, we release all code, scripts, and model checkpoints at\nhttps://github.com/RiccardoBravin/tiny-LLM.",
    "published": "2025-02-14T08:33:31Z",
    "categories": [
      "cs.CL",
      "cs.AR",
      "cs.DC",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2502.10001v1"
  },
  {
    "arxiv_id": "2502.08807v2",
    "title": "InTAR: Inter-Task Auto-Reconfigurable Accelerator Design for High Data\n  Volume Variation in DNNs",
    "authors": [
      "Zifan He",
      "Anderson Truong",
      "Yingqi Cao",
      "Jason Cong"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      ""
    ],
    "abstract": "The rise of deep neural networks (DNNs) has driven an increased demand for\ncomputing power and memory. Modern DNNs exhibit high data volume variation\n(HDV) across tasks, which poses challenges for FPGA acceleration: conventional\naccelerators rely on fixed execution patterns (dataflow or sequential) that can\nlead to pipeline stalls or necessitate frequent off-chip memory accesses. To\naddress these challenges, we introduce the Inter-Task Auto-Reconfigurable\nAccelerator (InTAR), a novel accelerator design methodology for HDV\napplications on FPGAs. InTAR combines the high computational efficiency of\nsequential execution with the reduced off-chip memory overhead of dataflow\nexecution. It switches execution patterns automatically with a static schedule\ndetermined before circuit design based on resource constraints and problem\nsizes. Unlike previous reconfigurable accelerators, InTAR encodes\nreconfiguration schedules during circuit design, allowing model-specific\noptimizations that allocate only the necessary logic and interconnects. Thus,\nInTAR achieves a high clock frequency with fewer resources and low\nreconfiguration time. Furthermore, InTAR supports high-level tools such as HLS\nfor fast design generation. We implement a set of multi-task HDV DNN kernels\nusing InTAR. Compared with dataflow and sequential accelerators, InTAR exhibits\n$\\mathbf{1.8\\times}$ and $\\mathbf{7.1 \\times}$ speedups correspondingly.\nMoreover, we extend InTAR to GPT-2 medium as a more complex example, which is\n$\\mathbf{3.65 \\sim 39.14\\times}$ faster and a $\\mathbf{1.72 \\sim 10.44\\times}$\nmore DSP efficient than SoTA accelerators (Allo and DFX) on FPGAs.\nAdditionally, this design demonstrates $\\mathbf{1.66 \\sim 7.17\\times}$ better\npower efficiency than GPUs. Code: https://github.com/OswaldHe/InTAR",
    "published": "2025-02-12T21:43:51Z",
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2502.08807v2"
  },
  {
    "arxiv_id": "2502.08141v1",
    "title": "LowRA: Accurate and Efficient LoRA Fine-Tuning of LLMs under 2 Bits",
    "authors": [
      "Zikai Zhou",
      "Qizheng Zhang",
      "Hermann Kumbong",
      "Kunle Olukotun"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      ""
    ],
    "abstract": "Fine-tuning large language models (LLMs) is increasingly costly as models\nscale to hundreds of billions of parameters, and even parameter-efficient\nfine-tuning (PEFT) methods like LoRA remain resource-intensive. We introduce\nLowRA, the first framework to enable LoRA fine-tuning below 2 bits per\nparameter with minimal performance loss. LowRA optimizes fine-grained\nquantization - mapping, threshold selection, and precision assignment - while\nleveraging efficient CUDA kernels for scalable deployment. Extensive\nevaluations across 4 LLMs and 4 datasets show that LowRA achieves a superior\nperformance-precision trade-off above 2 bits and remains accurate down to 1.15\nbits, reducing memory usage by up to 50%. Our results highlight the potential\nof ultra-low-bit LoRA fine-tuning for resource-constrained environments.",
    "published": "2025-02-12T05:48:26Z",
    "categories": [
      "cs.LG",
      "cs.AR",
      "cs.CL",
      "cs.PF"
    ],
    "pdf_link": "http://arxiv.org/pdf/2502.08141v1"
  },
  {
    "arxiv_id": "2503.18002v2",
    "title": "Neuromorphic Principles for Efficient Large Language Models on Intel\n  Loihi 2",
    "authors": [
      "Steven Abreu",
      "Sumit Bam Shrestha",
      "Rui-Jie Zhu",
      "Jason Eshraghian"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      ""
    ],
    "abstract": "Large language models (LLMs) deliver impressive performance but require large\namounts of energy. In this work, we present a MatMul-free LLM architecture\nadapted for Intel's neuromorphic processor, Loihi 2. Our approach leverages\nLoihi 2's support for low-precision, event-driven computation and stateful\nprocessing. Our hardware-aware quantized model on GPU demonstrates that a 370M\nparameter MatMul-free model can be quantized with no accuracy loss. Based on\npreliminary results, we report up to 3x higher throughput with 2x less energy,\ncompared to transformer-based LLMs on an edge GPU, with significantly better\nscaling. Further hardware optimizations will increase throughput and decrease\nenergy consumption. These results show the potential of neuromorphic hardware\nfor efficient inference and pave the way for efficient reasoning models capable\nof generating complex, long-form text rapidly and cost-effectively.",
    "published": "2025-02-12T02:40:44Z",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2503.18002v2"
  },
  {
    "arxiv_id": "2502.07842v2",
    "title": "Column-wise Quantization of Weights and Partial Sums for Accurate and\n  Efficient Compute-In-Memory Accelerators",
    "authors": [
      "Jiyoon Kim",
      "Kang Eun Jeon",
      "Yulhwa Kim",
      "Jong Hwan Ko"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      ""
    ],
    "abstract": "Compute-in-memory (CIM) is an efficient method for implementing deep neural\nnetworks (DNNs) but suffers from substantial overhead from analog-to-digital\nconverters (ADCs), especially as ADC precision increases. Low-precision ADCs\ncan reduce this overhead but introduce partial-sum quantization errors\ndegrading accuracy. Additionally, low-bit weight constraints, imposed by cell\nlimitations and the need for multiple cells for higher-bit weights, present\nfurther challenges. While fine-grained partial-sum quantization has been\nstudied to lower ADC resolution effectively, weight granularity, which limits\noverall partial-sum quantized accuracy, remains underexplored. This work\naddresses these challenges by aligning weight and partial-sum quantization\ngranularities at the column-wise level. Our method improves accuracy while\nmaintaining dequantization overhead, simplifies training by removing two-stage\nprocesses, and ensures robustness to memory cell variations via independent\ncolumn-wise scale factors. We also propose an open-source CIM-oriented\nconvolution framework to handle fine-grained weights and partial-sums\nefficiently, incorporating a novel tiling method and group convolution.\nExperimental results on ResNet-20 (CIFAR-10, CIFAR-100) and ResNet-18\n(ImageNet) show accuracy improvements of 0.99%, 2.69%, and 1.01%, respectively,\ncompared to the best-performing related works. Additionally, variation analysis\nreveals the robustness of our method against memory cell variations. These\nfindings highlight the effectiveness of our quantization scheme in enhancing\naccuracy and robustness while maintaining hardware efficiency in CIM-based DNN\nimplementations. Our code is available at\nhttps://github.com/jiyoonkm/ColumnQuant.",
    "published": "2025-02-11T05:32:14Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2502.07842v2"
  },
  {
    "arxiv_id": "2502.07834v1",
    "title": "MEMHD: Memory-Efficient Multi-Centroid Hyperdimensional Computing for\n  Fully-Utilized In-Memory Computing Architectures",
    "authors": [
      "Do Yeong Kang",
      "Yeong Hwan Oh",
      "Chanwook Hwang",
      "Jinhee Kim",
      "Kang Eun Jeon",
      "Jong Hwan Ko"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "The implementation of Hyperdimensional Computing (HDC) on In-Memory Computing\n(IMC) architectures faces significant challenges due to the mismatch between\nhighdimensional vectors and IMC array sizes, leading to inefficient memory\nutilization and increased computation cycles. This paper presents MEMHD, a\nMemory-Efficient Multi-centroid HDC framework designed to address these\nchallenges. MEMHD introduces a clustering-based initialization method and\nquantization aware iterative learning for multi-centroid associative memory.\nThrough these approaches and its overall architecture, MEMHD achieves a\nsignificant reduction in memory requirements while maintaining or improving\nclassification accuracy. Our approach achieves full utilization of IMC arrays\nand enables one-shot (or few-shot) associative search. Experimental results\ndemonstrate that MEMHD outperforms state-of-the-art binary HDC models,\nachieving up to 13.69% higher accuracy with the same memory usage, or 13.25x\nmore memory efficiency at the same accuracy level. Moreover, MEMHD reduces\ncomputation cycles by up to 80x and array usage by up to 71x compared to\nbaseline IMC mapping methods when mapped to 128x128 IMC arrays, while\nsignificantly improving energy and computation cycle efficiency.",
    "published": "2025-02-11T00:53:15Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2502.07834v1"
  },
  {
    "arxiv_id": "2502.06921v2",
    "title": "GraNNite: Enabling High-Performance Execution of Graph Neural Networks\n  on Resource-Constrained Neural Processing Units",
    "authors": [
      "Arghadip Das",
      "Shamik Kundu",
      "Arnab Raha",
      "Soumendu Ghosh",
      "Deepak Mathaikutty",
      "Vijay Raghunathan"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Graph Neural Networks (GNNs) are vital for learning from graph-structured\ndata, enabling applications in network analysis, recommendation systems, and\nspeech analytics. Deploying them on edge devices like client PCs and laptops\nenhances real-time processing, privacy, and cloud independence. GNNs aid\nRetrieval-Augmented Generation (RAG) for Large Language Models (LLMs) and\nenable event-based vision tasks. However, irregular memory access, sparsity,\nand dynamic structures cause high latency and energy overhead on\nresource-constrained devices. While modern edge processors integrate CPUs,\nGPUs, and NPUs, NPUs designed for data-parallel tasks struggle with irregular\nGNN computations. We introduce GraNNite, the first hardware-aware framework\noptimizing GNN execution on commercial-off-the-shelf (COTS) SOTA DNN\naccelerators via a structured three-step methodology: (1) enabling NPU\nexecution, (2) optimizing performance, and (3) trading accuracy for efficiency\ngains. Step 1 employs GraphSplit for workload distribution and StaGr for static\naggregation, while GrAd and NodePad handle dynamic graphs. Step 2 boosts\nperformance using EffOp for control-heavy tasks and GraSp for sparsity\nexploitation. Graph Convolution optimizations PreG, SymG, and CacheG reduce\nredundancy and memory transfers. Step 3 balances quality versus efficiency,\nwhere QuantGr applies INT8 quantization, and GrAx1, GrAx2, and GrAx3 accelerate\nattention, broadcast-add, and SAGE-max aggregation. On Intel Core Ultra AI PCs,\nGraNNite achieves 2.6X to 7.6X speedups over default NPU mappings and up to\n8.6X energy gains over CPUs and GPUs, delivering 10.8X and 6.7X higher\nperformance than CPUs and GPUs, respectively, across GNN models.",
    "published": "2025-02-10T17:03:02Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "pdf_link": "http://arxiv.org/pdf/2502.06921v2"
  },
  {
    "arxiv_id": "2502.07823v1",
    "title": "Runtime Tunable Tsetlin Machines for Edge Inference on eFPGAs",
    "authors": [
      "Tousif Rahman",
      "Gang Mao",
      "Bob Pattison",
      "Sidharth Maheshwari",
      "Marcos Sartori",
      "Adrian Wheeldon",
      "Rishad Shafik",
      "Alex Yakovlev"
    ],
    "author_affiliations": [
      "",
      "",
      "",
      "",
      "",
      "",
      "",
      ""
    ],
    "abstract": "Embedded Field-Programmable Gate Arrays (eFPGAs) allow for the design of\nhardware accelerators of edge Machine Learning (ML) applications at a lower\npower budget compared with traditional FPGA platforms. However, the limited\neFPGA logic and memory significantly constrain compute capabilities and model\nsize. As such, ML application deployment on eFPGAs is in direct contrast with\nthe most recent FPGA approaches developing architecture-specific\nimplementations and maximizing throughput over resource frugality. This paper\nfocuses on the opposite side of this trade-off: the proposed eFPGA accelerator\nfocuses on minimizing resource usage and allowing flexibility for on-field\nrecalibration over throughput. This allows for runtime changes in model size,\narchitecture, and input data dimensionality without offline resynthesis. This\nis made possible through the use of a bitwise compressed inference architecture\nof the Tsetlin Machine (TM) algorithm. TM compute does not require any\nmultiplication operations, being limited to only bitwise AND, OR, NOT,\nsummations and additions. Additionally, TM model compression allows the entire\nmodel to fit within the on-chip block RAM of the eFPGA. The paper uses this\naccelerator to propose a strategy for runtime model tuning in the field. The\nproposed approach uses 2.5x fewer Look-up-Tables (LUTs) and 3.38x fewer\nregisters than the current most resource-fugal design and achieves up to 129x\nenergy reduction compared with low-power microcontrollers running the same ML\napplication.",
    "published": "2025-02-10T12:49:22Z",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_link": "http://arxiv.org/pdf/2502.07823v1"
  }
]