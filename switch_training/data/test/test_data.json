[
  {
    "text": "A Fast and Accurate Online Sequen- tial Learning Algorithm for Feedforward Networks. IEEE Transactions on Neural Networks, 17(6):1411 1423, November 2006. [7] Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O. Hero III, and Pramod K. Varshney. A Primer on Zeroth-Order Optimization in Signal Processing and Machine Learning: Principals, Recent Advances, and Applications. IEEE Signal Processing Magazine, 37(5):43 54, September 2020. [8] Mineto Tsukada, Masaaki Kondo, and Hiroki Matsutani. A Neural Network-Based On-device Learning Anomaly Detector for Edge Devices. IEEE Transactions on Computers, 69(7):1027 1044, July 2020. [9] Yequan Zhao, Hai Li, Ian Young, and Zheng Zhang. Poor Man s Training on MCUs: A Memory-Efficient Quantized Back-Propagation-Free Approach. arXiv preprint arXiv:2411.05873, November 2024. [10] Haoyu Ren, Darko Anicic, and Thomas A. Runkler. TinyOL: TinyML with Online-Learning on Microcontrollers. In Proceedings of the International Joint Conference on Neural Networks (IJCNN), pages 1 8, July 2021. [11] Yann Lecun, L eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-Based Learning Applied to Document Recognition. Proceedings of the IEEE, 86(11):2278 2324, November 1998. [12] Javier Duarte, Song Han, Philip Harris, Sergo Jindariani, Edward Kreinar, Benjamin Kreis, Jennifer Ngadiuba, Maurizio Pierini, Ryan Rivera, Nhan Tran, and Zhenbin Wu. Fast Inference of Deep Neural Networks in FPGAs for Particle Physics. Journal of Instrumentation, 13(7):P07027, July 2018. 8",
    "source": "2506.06505v1_InstantFT_An_FPGA-Based_Runtime_Subsecond_Fine-tun.pdf",
    "length": 1526,
    "tokens": 435
  },
  {
    "text": "The results indicate that focusing on optimising the quality of a single template per class might be more beneficial than increasing the number of templates, particularly given the minimal returns observed with additional templates. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL., NO., FEBRUARY 2025 11 D. Energy efficiency estimates For the back-end classifier, each template matching opera- tion consumes 185fJ per cell. With our architecture requiring 10 templates of 784 features each, the energy consumption is: Eback-end Ntemplates Nfeatures Ecell 10 784 185fJ 1.45nJ (14) The total energy consumption of our front end network can be calculated considering both sparsity and quantisation effects. Given our total MAC operations of 23,785,120 from Table I, and accounting for 80 sparsity, the effective number of MAC operations is 4,757,024. However, we must also con- sider that lack of the final softmax fully-connected layer using this ACAM optimised approach, therefore we can remove an addition 7,850 operations that were used by this final layer, meaning the total operations for the front end classifier is 4,749,174. Using the energy figures from Horowitz [31] for 8-bit operations (0.2pJ for multiply and 0.03pJ for add) and memory access costs (20pJ for 32KB cache), we can calculate the energy consumption of the front-end feature extractor. For each MAC operation, the computation energy is 0.23pJ and the memory access energy is 20pJ, giving a total front-end energy consumption of 96.07nJ per inference. This combines with our back-end ACAM energy of 1.45nJ for template matching, resulting in a total system energy consumption of 97.52nJ per classification operation. In comparison the teacher model has a total energy consumption of 78.06ÂµJ demonstrating the proposed methodology achieved a 792 times energy reduction. This demonstrates that our hybrid approach achieves signif- icant energy efficiency, with the ACAM back-end consuming less energy than traditional digital implementations. This paves the way for new opportunities in multi-modal sensing enabling smaller, more efficient deep learning models that can be implemented in devices such as wearables while preserving battery life. VI.",
    "source": "2502.10089v1_A_Hybrid_Edge_Classifier_Combining_TinyML-Optimise.pdf",
    "length": 2228,
    "tokens": 485
  },
  {
    "text": "Initially, the model is fine-tuned on line- level and block-level data, subsequently progressing to module-level data. At each level, we start by aligning the detailed specifications with the code before moving to the high-level functional descrip- tions. And fine-tuning typically starts with GPT-annotated data, followed by human-annotated data for each annotation granularity. Figure 3 provides an illustration of this process. We adopt such strat- egy because a particular focus is placed on aligning Verilog modules with their high-level functional descriptions, which poses the greatest challenge and offers substantial practical applications. This curriculum learning strategy enables the model to incrementally build knowledge from simpler to more complex scenarios. As a result, the models demonstrate impressive performance across both Verilog understanding and generation benchmarks. Note that we exclude the cases in the bench- marks from our training dataset. We primarily follow the instruction tuning script of CodeT5 3 in the fine-tuning process, with a modification to expand the input context length to the maximum of 2048 tokens. We utilize the distributed framework, DeepSpeed, to efficiently fine-tune the model across a cluster equipped with eight NVIDIA A800 GPUs, each with 80GB of memory. During inference, we adjust the temperature to 0.8 for understanding tasks and to 0.5 for generation tasks, while other hyperparameters remain at their default settings to ensure optimal performance. Further details on the adopted curriculum learning strategy are provided in Appendix H. 4.4 UNDERSTANDING EVALUATION For evaluating LLMs capabilities in Verilog understanding, we utilize the benchmark introduced in Section 3.4. The evaluation measures the similarity between the generated descriptions and the ground truth summaries. Previous works usually use BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) scores for this purpose (Wang et al., 2023a). BLEU assesses how many n-grams, i.e., se- quences of n words, in the machine-generated text appear in the reference text (focusing on preci- sion). In contrast, ROUGE counts how many n-grams from the reference appear in the generated text (focusing on recall). However, both metrics primarily capture lexical rather than semantic similarity, which may not fully reflect the accuracy of the generated descriptions.",
    "source": "2502.15832v1_DeepRTL_Bridging_Verilog_Understanding_and_Generat.pdf",
    "length": 2385,
    "tokens": 495
  },
  {
    "text": "Note however that, this meta- information is not on the data-dependence chain, and we do not consider it for memoization. To summarize, among the above discussed features, we identify head orientation as the only suitable memoization candidate for boosting the compute reuse scope. Thus, we memoize both head orientation and its corresponding projec- tion matrix (i.e., projection computation results) in a memory buffer, namely, Pbuff, and use the head orientation to index the address pointer of that Pbuff stored in DRAM. How Much to Memoize? The occupied DRAM size is mainly determined by Pbuff. In fact, with a VR screen size of 1, 000 1, 000, one Pbuff occupies 8MB in DRAM. Since this puts a high demand on memory, one edge VR headset cannot afford to memoize for all possible head orientations. Thus, we want to limit the number of Pbuff that we need to store. To address this, we need to carefully decide how much his- tory is to be memoized for leveraging computation reuse. We performed a study on the VR video dataset [3] to investigate the head orientation traces of 20 users watching 5 widely- variant 360 VR videos (summarized in Tab. II). Typically, the resolution of the IMU traces can be as high as 20 bits per ï¬eld [3], [28]. From the dataset, we report the average reuse distance, i.e., the average number of preceding frames with same head orientation to be memoized, and show it in Fig. 5a. It can be concluded from these results that, memoizing the last two frames is sufï¬cient for most of the cases. Memoizing more frames may not bring much additional beneï¬ts because of the high sensitivity of the IMU sensors. Storing only two head orientations (in registers) and their associated Pbuff in the DRAM occupies only 16MB memory space. Further, we also observe that, the duration for which the head orientation does not change for three consecutive frames sums up to only 28 of the video runtime on average (refer to Fig. 5b), limiting the memoization opportunities to those instances. Such low reuse ratio is expected because of the high sensitivity of the IMU sensors. However, a higher reuse ratio can be achieved by relaxing the precision of the IMU output.",
    "source": "DejaView.pdf",
    "length": 2183,
    "tokens": 494
  },
  {
    "text": "arXiv:2503.13116v4 [cs.CR] 17 Jun 2025 VeriLeaky: Navigating IP Protection vs Utility in Fine-Tuning for LLM-Driven Verilog Coding Zeng Wang , Minghao Shao , Mohammed Nabeel , Prithwish Basu Roy , Likhitha Mankali , Jitendra Bhandari , Ramesh Karri , Ozgur Sinanoglu , Muhammad Shafique , Johann Knechtel NYU Tandon School of Engineering, USA NYU Abu Dhabi, UAE Email:{zw3464, shao.minghao, mtn2, pb2718, likhitha.mankali, jb7410, rkarri, ozgursin, muhammad.shafique, Abstract Large language models (LLMs) offer significant potential for coding, yet fine-tuning (FT) with curated data is essential for niche languages like Verilog. Using proprietary intellectual property (IP) for FT presents a serious risk, as FT data can be leaked through LLM inference. This leads to a critical dilemma for design houses: seeking to build externally accessible LLMs offering competitive Verilog coding, how can they leverage in-house IP to enhance FT utility while ensuring IP protection? For the first time in the literature, we study this dilemma. Using LLaMA 3.1-8B, we conduct in-house FT on a baseline Verilog dataset (RTLCoder) supplemented with our own in- house IP, which is validated through multiple tape-outs. To rigorously assess IP leakage, we quantify structural similarity (AST Dolos) and functional equivalence (Synopsys Formality) between generated codes and our in-house IP. We show that our IP can indeed be leaked, confirming the threat. As defense, we evaluate logic locking of Verilog codes (ASSURE). This offers some level of protection, yet reduces the IP s utility for FT and degrades the LLM s performance. Our study shows the need for novel strategies that are both effective and minimally disruptive to FT, an essential effort for enabling design houses to fully utilize their proprietary IP toward LLM-driven Verilog coding. Codes are available at Index Terms Large Language Models, Verilog Code Genera- tion, Data Extraction, IP Protection, Logic Locking I.",
    "source": "2503.13116v4_VeriLeaky_Navigating_IP_Protection_vs_Utility_in_F.pdf",
    "length": 1974,
    "tokens": 495
  },
  {
    "text": "We set up the Message Passing Interface (MPI) using MPICH on the cluster to enable efficient parallel computing. Open MPI was installed on all nodes, ensuring the availability of runtime libraries, compiler wrappers, and development tools. Environment variables such as PATH and LD_LIBRARY_PATH were configured globally or managed dynamically using Lmod. We verified the installation by compiling and running test MPI programs using mpicc and mpirun . Static IPs and passwordless SSH were configured already before to facilitate seamless communication between nodes. Additionally, MPI was integrated with SLURM, utilizing srun for job execution to leverage SLURM s resource management capabilities. This setup provides a robust framework for scalable parallel computing in the cluster. The NVIDIA GTX 1650 GPUs in our cluster support CUDA for GPU-accelerated parallel computations but lack the features required for clustering GPUs across nodes, such as GPUDirect RDMA. This limitation prevents the GPUs from being used together in a unified multi-GPU setup. [33] Instead, each GPU operates independently, and tasks must be run on individual GPUs within their respective nodes. While this restricts the scalability of GPU workloads, it remains suitable for single-node GPU computations and smaller parallel tasks. Figure 5: Failed Login attempts in cluster 6 A PREPRINT - MARCH 17, 2025 Monitoring is a critical aspect of HPC cluster management, ensuring system health, performance optimization, and fault detection. We installed Ganglia due to its ease of setup and widespread use in HPC environments, allowing efficient tracking of system metrics across all compute nodes [13, 10]. Ganglia provides real-time resource utilization insights, helping to maintain workload balance and optimize job scheduling. Additionally, we tested Prometheus (on and Grafana (on as alternative monitoring solutions. Prometheus, with its time-series monitoring and alerting capabilities, allows detailed system metric collection via exporters [17]. Grafana, when integrated with Prometheus, provides an intuitive visualization interface for tracking CPU, memory, and GPU usage in real time. While both Prometheus and Grafana are viable options under the given IP configurations, we preferred Ganglia for its simpler deployment and efficient monitoring of our SLURM-managed cluster. Our cluster faced a significant cybersecurity threat daily due to internet connectivity with nearly 2000-6000 failed login attempts to the root account within 24 hours, which we suspect were primarily bot-driven attacks.",
    "source": "2503.11246v1_Cost-effective_Deep_Learning_Infrastructure_with_N.pdf",
    "length": 2585,
    "tokens": 493
  },
  {
    "text": "14: Comparison of FORTALESA implementation options considering reliability for VGG-11 0 0.5 1 1.5 2 2.5 Area-power product 200 300 400 500 600 700 800 900 1000 Max throughput [GOP s] PM-DMR0-TMR3 PM-DMR0-TMR4 PM-DMRA-TMR3 PM-DMRA-TMR4 TMR Regs (24x32) TMR Regs (48x48) TMR Regs MAC (24x32) TMR Regs MAC (48x48) TMR SA (24x32) TMR SA (48x48) ECC OREG (48x48) [23] ECC IREG WREG (48x48) [23] TMR (24x32) FORTALESA ECC TMR (48x48) Fig. 15: Comparison of FORTALESA implementation options with static TMR and selective ECC [23] registers, it requires, on average, 2.5 more resources than FORTALESA, which protects all registers and MAC units. E. Comparison with the state-of-the-art The comparison of the proposed architecture with other methods to enhance fault tolerance of systolic arrays is pre- sented in Table V. The proposed architecture protects both registers and MAC units in the PEs and since it utilizes a form of spatial redundancy it covers both transient and permanent faults. Faults are corrected in both fault-tolerant modes without interrupting inference execution for recovery. VII. CONCLUSION In this work, we proposed run-time reconfigurable fault- tolerant systolic array architecture FORTALESA with three execution modes and four implementation options. All four implementation options were evaluated in terms of resource utilization, throughput, and fault tolerance improvement. The proposed architecture is used for reliability enhancement of DNN inference on systolic array through heterogeneous map- ping of different network layers to different execution modes. TABLE V: Comparison with the state-of-the-art Work Covered faults Protected parts Fault Permanent Transient Registers MAC correction [27] [28] 1 [29] 2 [23] 3 [30] [31] 4 Our 1 Requires retraining of the DNN for each faulty chip. 2 Faults in configurational memory of FPGA. 3 Only single-bit fault correction (ECC).",
    "source": "2503.04426v1_FORTALESA_Fault-Tolerant_Reconfigurable_Systolic_A.pdf",
    "length": 1900,
    "tokens": 498
  },
  {
    "text": "Available: [21] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K uttler, M. Lewis, W. tau Yih, T. Rockt aschel, S. Riedel, and D. Kiela, Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021. [Online]. Available: [22] V. Karpukhin, B. O guz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W. tau Yih, Dense passage retrieval for open-domain question answering, 2020. [Online]. Available: [23] J. Mazanec and O. Hamzaoui, Choose the k-nn algorithm for your billion-scale use case with opensearch aws big data blog, 9 2022, [Online; accessed 2025-03- 08]. [Online]. Available: choose-the-k-nn-algorithm-for-your-billion-scale-use-case-with-opensearch [24] A. Chirkin, Accelerating vector search: Nvidia cuvs ivf-pq part 1, deep dive nvidia technical blog, 7 2024, [Online; accessed 2025-03-08]. [Online]. Available: accelerating-vector-search-nvidia-cuvs-ivf-pq-deep-dive-part-1 [25] Choose the k-nn algorithm for your billion-scale use case with opensearch aws big data blog, 9 2022, [Online; accessed 2025-04-11]. [Online]. Available: choose-the-k-nn-algorithm-for-your-billion-scale-use-case-with-opensearch [26] K. Jain, A. Parayil, A. Mallick, E. Choukse, X. Qin, J. Zhang, I nigo Goiri, R. Wang, C. Bansal, V. R uhle, A. Kulkarni, S. Kofsky, and S. Rajmohan, Intelligent router for llm workloads: Improving performance through workload-aware load balancing, 2025. [Online].",
    "source": "2504.09775v3_Understanding_and_Optimizing_Multi-Stage_AI_Infere.pdf",
    "length": 1419,
    "tokens": 492
  },
  {
    "text": "[39] A. Vedaldi and A. Zisserman, Vgg convolutional neural networks practical, Department of Engineering Science, University of Oxford, vol. 66, 2016. [40] R. Gao, X. Hou, J. Qin, J. Chen, L. Liu, F. Zhu, Z. Zhang, and L. Shao, Zero-vae-gan: Generating unseen features for generalized and trans- ductive zero-shot learning, IEEE Transactions on Image Processing, vol. 29, pp. 3665 3680, 2020. [41] Y. Zhou, U. Gupta, S. Dai, R. Zhao, N. Srivastava, H. Jin, J. Featherston, Y.-H. Lai, G. Liu, G. A. Velasquez, W. Wang, and Z. Zhang, Rosetta: A Realistic High-Level Synthesis Benchmark Suite for Software- Programmable FPGAs, Int l Symp. on Field-Programmable Gate Ar- rays (FPGA), Feb 2018. [42] L.-N. Pouchet et al., Polybench: The polyhedral benchmark suite, URL: cs. ucla. edu pouchet software polybench, vol. 437, pp. 1 1, 2012. [43] L. Guo, Y. Chi, J. Lau, L. Song, X. Tian, M. Khatti, W. Qiao, J. Wang, E. Ustun, Z. Fang et al., Tapa: a scalable task-parallel dataflow programming framework for modern fpgas with co-optimization of hls and physical design, ACM Transactions on Reconfigurable Technology and Systems, vol. 16, no. 4, pp. 1 31, 2023. [44] J. Duarte et al., Fast inference of deep neural networks in FPGAs for particle physics, JINST, vol. 13, no. 07, p. P07027, 2018. [45] Amd versal premium series product selection guide. [Online].",
    "source": "2502.08807v2_InTAR_Inter-Task_Auto-Reconfigurable_Accelerator_D.pdf",
    "length": 1352,
    "tokens": 440
  },
  {
    "text": "Scalability and Fault Tolerance in Decentralized Systems. By prioritizing localized learning and reducing reliance on central servers, decentralized approaches enhance scalability, improve latency efficiency, and strengthen fault tolerance. These properties make decentralized learning particularly suitable for autonomous edge systems, IoT networks, and federated AI applications. Through dynamic resource allocation and distributed model updates, adaptive learning ensures robust and efficient AI decision- making in continuously evolving real-world settings. 14 5 Security and Privacy The deployment of deep learning models on edge devices introduces significant secu- rity and privacy challenges [125]. Unlike centralized cloud-based AI, onboard learning operates in resource-constrained environments, making it susceptible to adversar- ial attacks, data breaches, and inefficiencies in privacy-preserving mechanisms [126]. These vulnerabilities enhance risks to model integrity, confidentiality, and robustness, necessitating dedicated security measures. This section systematically examines key security threats and mitigation strategies, focusing on three fundamental aspects: pri- vacy protection in decentralized learning, secure model execution, and trustworthy AI for onboard learning (summarized in Table 3. 5.1 Privacy Protection in Decentralized Learning Federated learning (FL) enables collaborative training across edge devices without requiring raw data transmission, mitigating privacy risks [127, 128]. However, despite its decentralized nature, FL remains vulnerable to inference attacks, where adver- saries exploit model updates to extract sensitive information. Differential privacy (DP) addresses these risks by adding controlled noise to model updates, preventing malicious parties from reconstructing training data [129]. While DP improves privacy, its implementation in onboard scenarios presents challenges. Excessive noise injection degrades model accuracy, and the heterogeneous nature of edge devices complicates uniform privacy protection. Furthermore, DP introduces computational overhead, which can hinder real-time learning in resource- constrained environments. To optimize trade-offs between privacy and utility, adaptive DP techniques dynamically adjust noise levels based on model sensitivity and device constraints. Recent approaches, such as Hierarchical Split Federated Learning, imple- ment local differential privacy (LDP) at both client and server levels, ensuring stronger privacy guarantees while maintaining computational efficiency. Beyond DP, federated learning remains susceptible to membership inference attacks (MIA) [130, 131] and adversarial model inversion [132], where attackers infer whether specific data points were used for training.",
    "source": "2505.08793v1_Onboard_Optimization_and_Learning_A_Survey.pdf",
    "length": 2794,
    "tokens": 481
  },
  {
    "text": "Revisiting DNN Training for Intermittently-Powered Energy-Harvesting Micro-Computers Cyan Subhra Mishra, Deeksha Chaudhary, Jack Sampson, Mahmut Taylan Knademir, Chita R Das The Pennsylvania State University {cyan, dmc6955, jms1257, mtk2, Abstract The deployment of Deep Neural Networks (DNNs) in energy-constrained envi- ronments, such as Energy Harvesting Wireless Sensor Networks (EH-WSNs), introduces significant challenges due to the intermittent nature of power availability. This study introduces NExUME, a novel training methodology designed specifically for DNNs operating under such constraints. We propose a dynamic adjustment of training parameters dropout rates and quantization levels that adapt in real-time to the available energy, which varies in energy harvesting scenarios. This approach utilizes a model that integrates the characteristics of the network architecture and the specific energy harvesting profile. It dynamically adjusts training strategies, such as the intensity and timing of dropout and quantization, based on predictions of energy availability. This method not only conserves energy but also enhances the network s adaptability, ensuring robust learning and inference capabilities even under stringent power constraints. Our results show a 6 to 22 improvement in accuracy over current methods, with an increase of less than 5 in computational overhead. This paper details the development of the adaptive training framework, describes the integration of energy profiles with dropout and quantization adjustments, and presents a comprehensive evaluation using real- world data. Additionally, we introduce a novel dataset aimed at furthering the application of energy harvesting in computational settings. 1 Introduction The increasing demand for ubiquitous, sustainable, and energy-efficient computing, combined with advancements in energy harvesting systems, has spurred significant research into battery-less devices (Gobieski et al., 2019; Resch et al., 2020; Mishra et al., 2021; Saffari et al., 2021; Afzal et al., 2022). Such platforms represent the future of the Internet of Things (IoT) and energy harvesting wireless sensor networks (EH-WSNs). Equipped with modern machine learning (ML) techniques, these devices can revolutionize computing, monitoring, and analytics in remote, risky, and critical environments such as oil wells, mines, deep forests, oceans, remote industries, and smart cities.",
    "source": "NexUME.pdf",
    "length": 2441,
    "tokens": 486
  },
  {
    "text": "Contrary to Fig. 4(a), the 3T1R design follows a precharging and evaluating operation cycle instead of a discharging initialisation and evaluation. The larger charging cell could be preferable in sparse activation applications, while the more conventional 3T1R precharging cell design could be preferred in normally distributed activation applications due to its smaller size. More specifically, if the input voltage IN is below the lower bound VLOW , the resistance of the nMOS transistor MA is rel- atively high which drives V D (voltage at the intermediate node of the 1T1R voltage divider circuit) to an appropriate voltage to enable discharge through the nMOS transistor MMLO of matchline MLLOW . At the same time, the pMOS transis- tor MMHI is non-conductive and the matchline MLHIGH is not discharging through the pMOS device. If the input voltage IN exceeds the higher threshold, the resistance of the nMOS transistor MA is relatively low which drives V D to an appropriate voltage to make the pMOS transistor MMHI ON to discharge the matchline MLHIGH while the nMOS transistor MMLO is OFF and the matchline MLLOW is not discharging fast enough. If the input voltage IN is lower than the high threshold and higher than the low threshold, then a match case occurs. The safe V D value makes both pMOS transistor MMHI and nMOS transistor MMLO partially or completely OFF so that the matchlines MLHIGH and MLLOW are not discharging. By evaluating both matchlines a final decision about the match mismatch case can be asserted. Furthermore, being able to evaluate each threshold separately provides better assessment in case of mismatching and makes the cell differentiable. Thus, we can understand how to train the RRAM weights by assessing which threshold is surpasses or not. IV. EXPERIMENTAL SETUP A. Datasets preparation The CIFAR-10 dataset [28] has been used to evaluate model performance as it is a widely recognised benchmark in the field of image classification. This dataset consists of 60,000 32x32 colour images across 10 classes, with 6,000 images per class. The dataset is divided into 50,000 training images and 10,000 test images.",
    "source": "2502.10089v1_A_Hybrid_Edge_Classifier_Combining_TinyML-Optimise.pdf",
    "length": 2150,
    "tokens": 472
  },
  {
    "text": "In Proceedings of the 49th Annual International Symposium on Computer Architecture (New York, New York) (ISCA 22). Association for Computing Machinery, New York, NY, USA, 993 1011. doi:10.1145 3470496.3533727 [42] Nevine Nassif, Ashley O. Munch, Carleton L. Molnar, Gerald Pasdast, Sitara- man V. Lyer, Zibing Yang, Oscar Mendoza, Mark Huddart, Srikrishnan Venkatara- man, Sireesha Kandula, Rafi Marom, Alexandra M. Kern, Bill Bowhill, David R. Mulvihill, Srikanth Nimmagadda, Varma Kalidindi, Jonathan Krause, Moham- mad M. Haq, Roopali Sharma, and Kevin Duda. 2022. Sapphire Rapids: The Next- Generation Intel Xeon Scalable Processor. In 2022 IEEE International Solid-State Circuits Conference (ISSCC), Vol. 65. 44 46. doi:10.1109 ISSCC42614.2022.9731107 [43] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Cherni- avskii, Yinghai Lu, Raghuraman Krishnamoorthi, Ansha Yu, Volodymyr Kon- dratenko, Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong, and Misha Smelyanskiy. 2019. Deep Learning Recommendation Model for Personalization and Recommendation Systems. CoRR abs 1906.00091 (2019). [44] Tan Nguyen, Colin MacLean, Marco Siracusa, Douglas Doerfler, Nicholas J. Wright, and Samuel Williams. 2022. FPGA-based HPC accelerators: An evaluation on performance and energy efficiency. Concurrency and Com- putation: Practice and Experience 34, 20 (2022), e6570.",
    "source": "2504.09870v1_Ember_A_Compiler_for_Efficient_Embedding_Operation.pdf",
    "length": 1553,
    "tokens": 486
  },
  {
    "text": "3) Execution strategies: Figure 6 shows ï¬ve different execution strategies for a two-layer convolution execution experiencing two power cycles with different levels. In Fig- ure 6(a), a naive scheduling strategy is employed on a Simple architecture. In this scheduling strategy, the RCA is only active when the harvested power is adequate to support the maximal power requirement among all the convolution layers and, in the simple architecture, one convolution layer can only be mapped to one ReRAM (no ReRAM duplication). In the remainder of this paper, this execution strategy is referred to as Naive1. We assume that Naive1 is also designed with the proposed lightweight circuits. In Figure 6(b), a naive scheduling scheme is applied, but this time on the proposed ResiRCA architecture, which supports ReRAM duplication. This execution strategy is referred as Naive2. None of Naive1 and Naive2 executions can go through power cycle PC-i and the power utilization is very low, as there is a signiï¬cant mismatch between the power producer and consumer. Figure 6(c) presents a ï¬‚exible scheduling strategy applied to ResiRCA. In this strategy, the loop tiling technique integrated with the ReRAM duplication is enabled to obtain resilient MAC computation blocks. The layers are scheduled in a sequential fashion. This execution style is called Sequential. In the example, we allow activating L1 ReRAMs with aG 4 and one partial L2 ReRAM in a sequential fashion.",
    "source": "ResiRCA.pdf",
    "length": 1461,
    "tokens": 314
  },
  {
    "text": "2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (Long Beach, California, USA) (NIPS 17). Curran Associates Inc., Red Hook, NY, USA, 6000 6010. [82] Ying Wei, Yi Chieh Huang, Haiming Tang, Nithya Sankaran, Ish Chadha, Dai Dai, Olakanmi Oluwole, Vishnu Balan, and Edward Lee. 2023. 9.3 NVLink-C2C: A Coherent Off Package Chip-to-Chip Interconnect with 40Gbps pin Single- ended Signaling. In 2023 IEEE International Solid-State Circuits Conference (ISSCC). 15 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen 160 162. [83] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. 2022. ZeroQuant: efficient and affordable post-training quantization for large-scale transformers. In Proceedings of the 36th International Conference on Neural Information Processing Systems (New Orleans, LA, USA) (NIPS 22). Curran Associates Inc., Red Hook, NY, USA, Article 1970, 16 pages. [84] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung- Gon Chun. 2022. Orca: A Distributed Serving System for Transformer-Based Generative Models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). USENIX Association, Carlsbad, CA, 521 538. https: www.usenix.org conference osdi22 presentation yu [85] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. 2025.",
    "source": "2504.17584v1_L3_DIMM-PIM_Integrated_Architecture_and_Coordinati.pdf",
    "length": 1649,
    "tokens": 488
  },
  {
    "text": "We show that ensemble models are inherently fault- tolerant over single models, since in the former, failure of a model would incur some accuracy loss without complete failure of the requests. It is observed from our failure- resilience results that Cocktail can adapt to instance fail- ures by limiting the accuracy loss within 0.6 . 2 Background and Motivation We start by providing a brief overview of model-serving in public cloud and ensembling, followed by a detailed analysis of their performance to motivate the need for Cocktail. 2.1 Model Serving in Public Cloud Figure 2 shows the overall architecture of a model-serving framework. There are diverse applications that are typically developed, trained and hosted as web services. These services allow end-users to submit queries via web server interface. Since these inference requests are often user-facing, it is imperative to administer them under a strict service level ob- jective (SLO). We deï¬ne SLO as the end-to-end response latency required by an application. Services like Ads and News Feed [39,44] would require SLOs within 100ms, while facial tag recommendation [83] can tolerate up to 1000ms. A myriad of model architectures are available to train these applications which by themselves can be deployed on appli- cation frameworks like TensorFlow [1], PyTorch [62] etc. Table 1 shows the different models available for image predic- tion, that are pretrained on Keras using ImageNet [29] dataset. Each model has unique accuracy and latencies depending on the model architecture. Typically denser models are designed with more parameters (ex.",
    "source": "cocktail.pdf",
    "length": 1614,
    "tokens": 333
  },
  {
    "text": "Overall, our estimates indicate that even modest user traffic (on the order of tens of millions of queries per day) becomes gigawatt-scale once per-query energy exceeds 100 Wh, a threshold representative of current agentic workloads. If we were to scale the same per-query figures to Google s 13.7 billion daily searches, the power numbers would raise single-turn ShareGPT (70B) to 1.5 GW and Reflexion (70B) to nearly 200 GW, far beyond any announced datacenter project and exceeding the power budgets of many national grids. To put this number into perspective, a 200 GW is almost half of the entire U.S. grid s average load (which amounts to 4,178 103 GWh (365 24 hours) 476.9 GW [52]), a scale usually discussed only for nation-wide decarbonization plans, not for a single industry or technology, one that fundamentally reshapes generation, transmission, and sustainability planning. Sustainability challenges of agentic test-time scaling. Collectively, our findings show that AI agent performance does not scale proportionally with the associated compute, energy, and power costs. Once accuracy saturates, additional test-time scaling yields diminishing returns while imposing substantial system-level burdens. This cost inefficiency is not merely theoretical; it poses concrete constraints on real-world deployments. For instance, OpenAI s recently introduced Deep Research system [34], designed for complex multi-step rea- soning, can take up to 30 minutes per request [33]. To keep infrastructure costs manageable, OpenAI limits usage to 25 runs every 30 days for ChatGPT Plus users and 250 runs for Pro users [33]. These limits highlight the financial and computational challenges of sustaining AI systems that rely heavily on intensive test-time computation. Based on these findings, we argue that building scalable and sustainable AI agents requires moving away from un- constrained test-time scaling. Instead, AI agents should be designed with compute-aware agentic workflows that deliver strong performance through efficient inference, rather than single-handedly relying on extended reasoning depth. VII. RELATED WORK AI agent workflows. Recent advances in LLM-based AI agents have introduced diverse workflows that combine language-based reasoning with external tool use. ReAct [64] interleaves reasoning and tool invocation through step-by- step decision making, while Reflexion [47] enhances agent behavior through self-evaluation and feedback.",
    "source": "2506.04301v1_The_Cost_of_Dynamic_Reasoning_Demystifying_AI_Agen.pdf",
    "length": 2461,
    "tokens": 502
  },
  {
    "text": "Dataset U1 R1 Layout Cross-Stage Alignment (awaring func. phys.) Gate embedding Masked Gate [MASK] Netlist R1 R3 U4 U1 U5 U3 R4 R2 U2 TAG [MASK] [MASK] TAG U5 U1 U2 ğ‘ğ‘– mask Netlist Embeddings Fig. 8. Multimodal pre-training techniques used in NetTAG [113], including representative self-supervised learning methods such as contrastive learning, mask-reconstruction, and cross-design-stage alignment. constraints guide AIG-based representation learning, reinforcing the structural-functionality align- ment. These cross-stage alignment techniques enhance the capability of netlist encoders, improving their generalization across multiple circuit design stages and boosting performance in downstream EDA applications. Self-supervised post-synthesis netlist encoder with multimodal fusion. While many existing netlist encoders focus on simpler AND-Inverter gates, they struggle with more complex post-synthesis netlists that involve various gates from standard liberty cells. To address this challenge, two recent works have advanced netlist encoding by incorporating multimodal fusion (i.e., NetTAG [113]) or AIG-netlist alignment (i.e., DeepCell [114]) to unprecedentedly handle the complexities of post-synthesis netlists. As shown in Figure 8, in NetTAG [113], post-synthesis netlists are represented as text-attributed graphs, where each node corresponds to a gate and is associated with attributes that include both functional symbolic logic expressions and physical characteristics (such as area, power, and delay). The model employs a two-stage multimodal hybrid architecture: first, an LLM-based text encoder processes the textual attributes of the gates to generate semantic-rich embeddings. Then, a graph transformer refines these embeddings by capturing the global circuit structure through graph-based attention mechanisms. During pre-training, the model utilizes four key self-supervised objectives. Expression contrastive learning enhances the LLM s understanding of Boolean logic by contrasting symbolic expressions. Masked gate reconstruction is a graph-based task where certain gates are masked, and the model predicts the gate type, encouraging it to capture structural roles.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 2192,
    "tokens": 486
  },
  {
    "text": "10323 10337 (2023). PMLR [21] Kohama, H., Minoura, H., Hirakawa, T., Yamashita, T., Fujiyoshi, H.: Single- shot pruning for pre-trained models: Rethinking the importance of magnitude pruning. In: Proceedings of the IEEE CVF International Conference on Com- puter Vision, pp. 1433 1442 (2023) [22] Chang, J., Lu, Y., Xue, P., Xu, Y., Wei, Z.: Iterative clustering pruning for convolutional neural networks. Knowledge-Based Systems 265, 110386 (2023) [23] Jiang, Z., Xu, Y., Xu, H., Wang, Z., Liu, J., Chen, Q., Qiao, C.: Computation and communication efficient federated learning with adaptive model pruning. IEEE Transactions on Mobile Computing 23(3), 2003 2021 (2023) [24] Guo, X., Wang, W.-S., Zhang, J., Gong, L.-S.: An online growing-and-pruning algorithm of a feedforward neural network for nonlinear systems modeling. IEEE Transactions on Automation Science and Engineering (2024) 22 [25] JelË‡cicov a, Z., Verhelst, M.: Delta keyword transformer: Bringing transformers to the edge through dynamically pruned multi-head self-attention. arXiv preprint arXiv:2204.03479 (2022) [26] Jeon, Y., Lee, C., Cho, E., Ro, Y.: Mr. biq: Post-training non-uniform quan- tization based on minimizing the reconstruction error. In: Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition, pp. 12329 12338 (2022) [27] Kim, J., Park, K., Lee, C., Kim, H.-y., Kim, J., Jeon, Y.: Towards next- level post-training quantization of hyper-scale transformers.",
    "source": "2505.08793v1_Onboard_Optimization_and_Learning_A_Survey.pdf",
    "length": 1467,
    "tokens": 433
  },
  {
    "text": "It focuses on their performance in CNN applications, emphasizing that FPGA-based accelerators deliver superior performance for deep learning tasks. A. Hardware Platforms Central Processing Units (CPUs): CPUs are fundamental components in modern computing systems, adhering to the von Neumann architecture. In the context of CNN models, CPUs are well-suited for handling complex logical operations, such as model initialization, data preprocessing, and managing communication between hardware components. They excel in processing diverse data types and executing intricate com- putations independently. However, their limited capacity for parallel processing makes them less efficient for the high- volume matrix operations and convolutions characteristic of CNNs. Additionally, CPUs often face challenges like high power consumption and restricted memory bandwidth, which can become bottlenecks when processing the large-scale data and computations required by CNN models [10]. Graphics Processing Units (GPUs): GPUs have been rapidly developed to handle large-scale computations, particu- larly in applications like image processing and CNN models. Unlike CPUs, GPUs are optimized for parallel computing, capable of processing massive amounts of similar data si- multaneously, making them well-suited for tasks like CNN training, where multiple convolutions and matrix operations can be executed in parallel. The architecture of GPUs, with larger arithmetic logic units, enables faster processing speeds and enhances their parallel computing capabilities. For example, in CNN-based applications, GPUs signifi- cantly speed up tasks such as training deep learning models by accelerating the computation of convolutional layers and weight updates [11]. However, GPUs have limitations in single-threaded performance, which makes them less ideal for tasks requiring complex logic or single-core execution. Additionally, GPUs face challenges such as high power con- sumption, substantial cooling needs, and limited memory capacity when dealing with extremely large datasets, which can hinder performance in ultra-large-scale CNN models. Application-Specific Integrated Circuits (ASICs): ASICs are specialized integrated circuits designed to meet specific product requirements. The main advantage of ASICs in deep learning applications lies in their customization, which allows for the design of high-performance circuits tailored to the spe- cific operations of CNNs, such as convolution, activation, and pooling layers. This customization allows for the elimination of redundant components, significantly reducing chip area and power consumption compared to CPUs and GPUs [12]. However, the highly specialized nature of ASICs comes with trade-offs.",
    "source": "2505.13461v1_FPGA-based_Acceleration_for_Convolutional_Neural_N.pdf",
    "length": 2745,
    "tokens": 500
  },
  {
    "text": "l) Class Sum and Argmax: When computing the class sums for CoTM, the binary multiplication is optimized to a logic AND operation between the clause and the corresponding weight bit by bit (see Algorithm 2). The Argmax is a comparison tree with n class sums as input. The maximum class sum and its class index will be cached and then used for comparison with the maximum class sum in the next group of class sums until all class sums are compared. B. DTM Training Modules: This subsection presents the implementation details of the training modules for the DTM. It is recommended to read this section and its algorithms with reference to Fig. 24. Much like in the background section, this subsection presents how the feedback to TAs is passed from class level to clause level to TA level. a) Class-Level Feedback: Training requires both the target class and a randomly chosen class (referred to as the negated class) for each training datapoint. The target class is padded into the input data stream sent to the accelerator (presented 4For example if a paragraph has a bold label Class-level feedback - view this part of Fig 2 against the same algorithm name in this section.",
    "source": "2504.19797v1_Dynamic_Tsetlin_Machine_Accelerators_for_On-Chip_T.pdf",
    "length": 1174,
    "tokens": 242
  },
  {
    "text": "Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, X. Zhang, X. Yu, Y. Wu, Z. F. Wu, Z. Gou, Z. Shao, Z. Li, Z. Gao, A. Liu, B. Xue, B. Wang, B. Wu, B. Feng, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Chen, D. Ji, E. Li, F. Lin, F. Dai, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Bao, H. Xu, H. Wang, H. Ding, H. Xin, H. Gao, H. Qu, H. Li, J. Guo, J. Li, J. Wang, J. Chen, J. Yuan, J. Qiu, J. Li, J. L. Cai, J. Ni, J. Liang, J. Chen, K. Dong, K. Hu, K. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Zhao, L. Wang, L. Zhang, L. Xu, L. Xia, M. Zhang, M. Zhang, M. Tang, M. Li, M. Wang, M. Li, N. Tian, P. Huang, P. Zhang, Q. Wang, Q. Chen, Q. Du, R. Ge, R. Zhang, R. Pan, R. Wang, R. J. Chen, R. L. Jin, R. Chen, S. Lu, S. Zhou, S. Chen, S. Ye, S. Wang, S. Yu, S. Zhou, S. Pan, S. S. Li, S. Zhou, S. Wu, S. Ye, T. Yun, T. Pei, T. Sun, T. Wang, W. Zeng, W. Zhao, W. Liu, W. Liang, W. Gao, W. Yu, W. Zhang, W. L. Xiao, W. An, X. Liu, X. Wang, 10 X. Chen, X. Nie, X. Cheng, X. Liu, X. Xie, X. Liu, X. Yang, X. Li, X. Su, X. Lin, X. Q. Li, X. Jin, X. Shen, X. Chen, X. Sun, X. Wang, X.",
    "source": "2505.18574v2_Autocomp_LLM-Driven_Code_Optimization_for_Tensor_A.pdf",
    "length": 1103,
    "tokens": 549
  },
  {
    "text": "Hence, as shown in Figure 6a and 6b, the TMU achieves 5.7 and 5.6 higher requests s and requests s watt than traditional cores, 5.2 and 6.3 better than doubling core s outstanding requests. In this way, as shown in Figure 6c, the TMU utilizes 4 8 more memory bandwidth than a traditional core, requiring smaller and fewer cores to saturate the processor s HBM bandwidth. From the core s perspective, offloading embedding lookup to the TMU allows to fully utilize the core s resources for compute opera- tions like embedding reductions. In the end, as shown in Figure 7, decoupling embedding lookup from computation on multicore pro- cessors achieves an average 5.8 speedup on the embedding opera- tions in Section 2.2. These speedups are mostly proportional to the temporal locality and compute-per-lookup ratio of the model, all the way to 17 improvements for SpAttn which has no compute and can be fully offloaded to the TMU. 3.3 Impact of DAE in Datacenters In the remainder of this section, we demonstrate that DAE multi- core processors outperform GPUs in end-to-end GNN models. We compare performance against an Nvidia T4 GPU [47], as it offers similar peak memory bandwidth and computational performance com-Youtube roadNet-CA web-Google wiki-Talk arxiv mag products proteins bioKg wikiKg2 RM1 L0 RM1 L1 RM1 L2 RM2 L0 RM2 L1 RM2 L2 RM3 L0 RM3 L1 RM3 L2 1 emb blk 2 emb blk 4 emb blk 8 emb blk 0x 2x 4x 6x 8x 10x 12x Speedup of DAE optimizations 17.0 MP SpMM(GNN) KG SLS(DLRM) SpAttn(LLM) Figure 7: Performance benefit of offloading embedding lookup to a near-core access unit like the TMU (details in Section 3.1). All embedding operations are high-performance multicore implementations from the literature (Section 2). For graph-learning models, the inputs and feature sizes are reported in Table 2.",
    "source": "2504.09870v1_Ember_A_Compiler_for_Efficient_Embedding_Operation.pdf",
    "length": 1807,
    "tokens": 469
  },
  {
    "text": "Algorithm 1 Pseudocode for the runtime adaptation. Require: ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘œwith sorted configurations ğ‘ğ‘“ğ‘”ğ‘–for 0 ğ‘– ğµğ‘  Require: Î”ğ‘Ÿğ‘’ğ‘for maximum idle time before decreasing skipping Require: ğ‘šğ‘–ğ‘›ğ‘ğ‘ğ‘for the minimum acceptable accuracy 1: ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘œ ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘œ[ğ‘–] ğ‘–(ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦(ğ‘ğ‘“ğ‘”ğ‘–) ğ‘šğ‘–ğ‘›ğ‘ğ‘ğ‘) {Filter out configurations with accuracy below the user s threshold} 2: ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ_ğ‘ğ‘“ğ‘” ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘œ[0] {Start with no skipping} 3: loop 4: ğ‘Ÿğ‘’ğ‘ ğ‘›ğ‘’ğ‘¤_ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡() {Wait for new inference request} 5: if ğ‘ğ‘¢ğ‘ ğ‘¦() then 6: ğ‘‘ğ‘Ÿğ‘œğ‘_ğ‘–ğ‘›ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’(ğ‘Ÿğ‘’ğ‘) 7: ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ_ğ‘ğ‘“ğ‘” ğ‘–ğ‘›ğ‘ğ‘Ÿğ‘’ğ‘ğ‘ ğ‘’_ğ‘ ğ‘˜ğ‘–ğ‘ğ‘ğ‘–ğ‘›ğ‘”() {Increase number of skipped blocks by one if possible, otherwise returns ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘œ.ğ‘™ğ‘ğ‘ ğ‘¡()} 8: else 9: if ğ‘¡ğ‘›ğ‘œğ‘¤ ğ‘¡ğ‘™ğ‘ğ‘ ğ‘¡ Î”ğ‘Ÿğ‘’ğ‘then 10: ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ_ğ‘ğ‘“ğ‘” ğ‘‘ğ‘’ğ‘ğ‘Ÿğ‘’ğ‘ğ‘ ğ‘’_ğ‘ ğ‘˜ğ‘–ğ‘ğ‘ğ‘–ğ‘›ğ‘”() {Decrease the num- ber of skipped blocks by one if possible, otherwise returns ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘œ.ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡()} 11: end if 12: ğ‘ğ‘Ÿğ‘œğ‘ğ‘’ğ‘ ğ‘ _ğ‘–ğ‘›ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’(ğ‘Ÿğ‘’ğ‘, ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ_ğ‘ğ‘“ğ‘”) 13: ğ‘¡ğ‘™ğ‘ğ‘ ğ‘¡ ğ‘¡ğ‘›ğ‘œğ‘¤ 14: end if 15: end loop the device is busy, the inference requested cannot be processed and it is dropped (line 6). The rationale behind it follows the MLPerf inference server standard [21] that defines a request as dropped if it arrives at a busy device.",
    "source": "2505.17626v1_Leveraging_Stochastic_Depth_Training_for_Adaptive_.pdf",
    "length": 1085,
    "tokens": 887
  },
  {
    "text": "The Cube contains 1000 (10 10 10) PEs, and others are 32 32 PEs. We also evaluate the performance of OPT3, OPT4C, and OPT4E (32 32PEgs) in comparison with other bit-slice architectures. Others TPU Ascend Trapezoid FlexFlow Laconic Bitlet Sibia Bitwave Frequency(MHz) 1000 1000 1000 1000 1000 1000 250 250 Area(um2) 370631 320783 283704 332848 213248 415800 1069000 861681 Power(W) 0.25 0.24 0.22 0.28 1.21 0.23 0.10 0.01 Peak Performance(TOPS) 2.05 2.05 2.05 2.05 0.81 0.74 0.77 0.22 Energy Efficiency(TOPS W) 8.05( 1.00) 8.21( 1.00) 9.31 1.00) 7.29( 1.00) 0.67( 1.00) 3.29( 4.91) 7.65( 11.42) 14.77( 22.04) Area Efficiency(TOPS mm2) 5.53( 1.00) 7.22( 1.00) 7.22( 1.00) 6.15( 1.00) 3.77( 1.00) 1.79( 0.47) 0.72( 0.19) 0.25( 0.07) Ours OPT1 (TPU) OPT1 (Ascend) OPT1 (Trapezoid) OPT1 (FlexFlow) OPT2 (FlexFlow) OPT3 OPT4C OPT4E Frequency(MHz) 1500 1500 1500 1500 1500 2000 2500 2000 Area(um2) 436646 332185 271989 373898 347216 460349 259298 672419 Power(W) 0.37 0.24 0.22 0.38 0.35 0.70 0.51 0.89 Peak Performance(TOPS) 3.07 3.07 3.07 3.07 3.07 1.80 2.25 7.22 Energy Efficiency(TOPS W) 8.41( 1.04) 12.82( 1.56) 13.89( 1.49) 8.08( 1.11) 8.77( 1.20) 2.57( 3.83) 4.41( 6.58) 8.11( 12.10) Area Efficiency(TOPS mm2) 7.04( 1.27) 9.25( 1.28) 11.29( 1.56) 8.22( 1.34) 8.85( 1.44) 3.91( 1.04) 8.68( 2.30) 10.73( 2.85) Reports on timing, power, and area after logic synthesis.",
    "source": "2503.06342v1_Exploring_the_Performance_Improvement_of_Tensor_Pr.pdf",
    "length": 1365,
    "tokens": 601
  },
  {
    "text": "Over multiple iterations of such asynchronous scheduling, the kernel queue for each tile will be of different size creating a load imbalance; how to tackle this? 3. How do we know when to stop executing? To address the ï¬rst two issues, we developed a work-stealing mechanism for each tile. When any of the active tiles are marked ready by the scheduler, the tile employees a state machine to decide where to get work from. Each time the tile ï¬nishes some work, if its remaining work queue (the local work queue size) is less than the average of all other active tiles, it seeks a new kernel to work on. Considering the global control always enqueues any idle tile with work, whenever the tile has no work left, it steals a kernel from the most 899 Authorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply. heavily loaded tiles. We implemented a counter (local kernel counter) to keep track of the size of the remaining local work queue of each of the tile. We also implemented a counter (layer kernel counter) which keeps track of the total kernels to be scheduled for each layer. Whenever all the local work queue counter hits zero along with the layer kernel counter, the control moves to schedule the next layer (or previous layer in backward propagation) for computation. Note that we do not delve into the details of the computa- tional primitives involved in training the DNN as several prior works [15], [16], [80] provide a very detailed accounting of it (for both forward and backward pass) along with the hardware and control requirements. We treat the convolution scheduling (using input stationary and at a kernel level) in a morphable systolic hardware to be the main challenge and explain it. V. IMPLEMENTATION AND EVALUATION We focus our evaluation on urban mobility, i.e. performing single shot object detection for trafï¬c monitoring using the MobileNetV2 [51] model on the urban trafï¬c data set [97]. This is a trafï¬c video dataset containing 62GB of videos recorded from ï¬ve pole-mounted ï¬sh-eye cameras in the city of Bellevue, WA, USA.",
    "source": "Usas.pdf",
    "length": 2138,
    "tokens": 488
  },
  {
    "text": "In the first sub-stage, we train the model using line-level data, where each line of Verilog code is paired with a corresponding natural language comment. The You are an expert in Verilog RTL code design, with extensive experience in optimizing code for performance, power, and area (PPA). Given the following Verilog code, please rewrite it to achieve the same functionality using a different implementation approach, while considering potential improvements to PPA metrics. The provided code is: \"{code}\" Functional Description (a high-level description of the functionality of the Verilog code): \"{functional_description}\" Specification (detailed implementation requirements of the Verilog code): \"{specification}\" Please rewrite the code to retain the same functionality but with a different implementation style. You can use the functional description and specification as a reference, but note that these are not fully accurate. Therefore, treat the provided code as the \"golden reference\" for the intended functionality. I encourage you to propose significant changes that may lead to improvements in PPA after synthesis. However, it is crucial that the rewritten code performs exactly the same as the original. Please provide the rewritten code in the following format: verilog [rewritten code] Note: Output only the rewritten code and do not include any additional explanations or comments. Additionally, ensure that only the implementation of the internal logic of the code is modified; you are forbidden to change the module head declaration. In summary, your task is to: 1. Keep the same functionality as the original code. 2. Significantly change the implementation style. 3. Consider potential improvements to PPA metrics after synthesis, such as optimization for area, timing, or power consumption. In summary, your task is to: 1. Keep the same functionality as the original code. 2. Ensure that the new implementation differs from the provided rewritten code, using a different implementation style. 3. Consider potential improvements to PPA metrics after synthesis, such as optimization for area, timing, or power consumption. In summary, your task is to: 1. Keep the same functionality as the original code. 2. Take into account the discrepancies between the original code and the rewritten code. Use the rewritten code as a reference and make adjustments to ensure the final version retains the intended functionality of the original code. 3. Consider potential improvements to PPA metrics after synthesis, such as optimization for area, timing, or power consumption.",
    "source": "2506.15697v1_DeepRTL2_A_Versatile_Model_for_RTL-Related_Tasks.pdf",
    "length": 2586,
    "tokens": 495
  },
  {
    "text": "However, the solvers are computationally costly, particu- larly for novel devices such as FinFET and gate-all-around transis- tors, which hinders design technology co-optimization (DTCO)[38]. ML has emerged as an effective tool to enhance traditional modeling methods by improving prediction accuracy and reducing computa- tional cost. ML has been applied for the accurate modeling and represen- tation of parameters of devices and EM structures, where param- eters including resonant frequency, bandwidth, and impedance directly impact performance. In [39], ML is applied to predict the current and capacitance of FinFETs based on other device parame- ters. An autoencoder-based approach is utilized to develop a PIN diode model, utilizing unsupervised learning to compress high- dimensional data into a latent space[40], which accelerates the extraction of device parameters. In [41], an ANN is introduced that performs real-time BSIM parameter extraction in nanosheet FETs, while accounting for multiple structural variations. For the modeling of EM structures, an on-chip transformer au- tomatic synthesis (OTAS) flow is proposed in [42] that utilizes Gaussian process regression models to automate the translation of , Zhengfeng Wu, Nnaemeka Achebe, Ziyi Chen, Vaibhav V. Rao, Pratik Shrestha, and Ioannis Savidis system requirements into transformer design parameters, reduc- ing the effort required for impedance matching. Similarly, recent work on deep learning-enabled mmWave power amplifier (PA) and antenna design has shown translations from high-level design specifications to physical layouts[43]. Physics-based device equations have been integrated with ML models to align model predictions with known device behaviors. In [44], analytical equations process variations such as current shifts and threshold voltage shifts are integrated into the learning models. A graph-based compact model (GCM) is proposed in [45] that represents physical parameters, including threshold voltages and channel length dependence, as graph nodes. Implemented in Verilog-A, GCM provides accurate predictions with 300 sample points, while passing industry-standard benchmark tests. GCM is integrated with SPICE simulations, offering a computationally efficient approach to advanced transistor modeling.",
    "source": "2506.00007v1_Emerging_ML-AI_Techniques_for_Analog_and_RF_EDA.pdf",
    "length": 2296,
    "tokens": 480
  },
  {
    "text": "Additional Experimental Details A.4.1. DOES CODESIGN MATTER? During optimization, we track the root mean square (RMS) of the gradient of the objective with respect to (a) the topology parameters and (b) the decoder parameters, in an attempt to understand the relative contributions to the designs. In general (but with some exceptions), we find that the topology derivatives tend to be between 1 and 3 orders of magnitude larger in RMS than the decoder derivatives; to first order then, the interpretation is that the classification accuracy of the system is around 10-1000x more sensitive to changes in the topology than that of the decoder parameters. However, it should not be inferred from this quantitative result that the decoder is not critical to the overall system performance. In fact, optimizing without using the decoder at all degrades the system performance significantly. Our interpretation of the relatively smaller decoder gradient magnitude follows the apparent role of the decoder as a permuting map to account for the arbitrary assignment of class labels to output measurement locations. Then, after initialization, optimizing the decoder amounts to merely breaking symmetry in its weights to achieve a better class label to output region assignment. From that point, the absolute magnitude of any given decoder weight has minimal effect, since only their relative values are responsible for the label output region re-mapping. Optimizing only the decoder but not the topology results in significantly worse performance (around 10 reduction in classification accuracy). But, the fact that the decoder-only performance is significantly better than random indicates that even randomly initialized topologies can be adapted for use as effective classifiers with an appropriately optimized linear decoder; similar to the use of multiple-scattering devices as reservoir networks. A.4.2. COMPUTING RESOURCES All experiments were carried out on a single node Linux server with a 32 CPUs and an Nvidia RTX 3080Ti GPU. For efficiency, we precompute all possible source combinations (derived from the finite number of quantized values) and store them in an in-memory cache, and the wave operator (less the contribution from the topology) is precomputed and stored as a sparse matrix in memory. We execute batched simulations with a batch size of 32, as we determined that batch sizes larger than this only marginally improved performance.",
    "source": "2504.20401v1_Nonlinear_Computation_with_Linear_Optics_via_Sourc.pdf",
    "length": 2448,
    "tokens": 479
  },
  {
    "text": "in 2008 IEEE International Symposium on Workload Characterization, 2008, pp. 141 150. [32] M. Kurek, T. Becker, T. C. P. Chau, and W. Luk, Automating Optimiza- tion of Reconfigurable Designs, in Proceedings of the 2014 IEEE 22nd International Symposium on Field-Programmable Custom Computing Machines, ser. FCCM 14. USA: IEEE Computer Society, 2014, p. 210 213. [33] S.-C. Kao and T. Krishna, Gamma: Automating the HW mapping of DNN models on accelerators via genetic algorithm, in International Conference on Computer-Aided Design, 2020, pp. 1 9. [34] J. Zhang, H. S.-H. Chung, A. W.-L. Lo, and T. Huang, Extended ant colony optimization algorithm for power electronic circuit design, IEEE Transactions on Power Electronics, vol. 24, no. 1, pp. 147 162, 2008. [35] C. Bai, J. Zhai, Y. Ma, B. Yu, and M. D. Wong, Towards automated risc- v microarchitecture design with reinforcement learning, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 1, 2024, pp. 12 20. [36] J. Alastruey, T. Monreal, F. Cazorla, V. Vi nals, and M. Valero, Selection of the Register File Size and the Resource Allocation Policy on SMT Processors, in 2008 20th International Symposium on Computer Archi- tecture and High Performance Computing. Washington, DC, United States: ACM, 2008, pp. 63 70.",
    "source": "2506.06817v1_ASPO_Constraint-Aware_Bayesian_Optimization_for_FP.pdf",
    "length": 1300,
    "tokens": 361
  },
  {
    "text": "Finally, existing methods are typically limited to individual tasks, with a constrained design space and a lack of cross- stage hardware-software co-design, making it difficult to push the boundaries of human-driven design. 4 The development of LLMs and agents has opened up new possibilities for overcoming three key limitations of the conventional automatic design methodologies. First, LLMs can convert informal natural language descriptions into for- mal programming languages, allowing them to automatically generate correct code for tasks ranging from basic functions to entire programs based on natural language specifications. Second, agents built on LLMs can autonomously plan and execute complex tasks and can independently utilize external tools. This capability offers a novel approach for integrating AI techniques with domain-specific tools, which offers new perspectives to achieve fully automated processor chip de- sign. Finally, LLMs possess powerful multi-task abilities and demonstrate strong potential in completing complex planning and reasoning tasks, which form the basis for achieving cross- stage collaboration in hardware-software design. Based on the above analysis, we introduce QiMeng, an innovative paradigm for fully automated hardware and soft- ware design for processor chips. QiMeng consists of three hierarchical layers, as illustrated in Figure 1. The bottom- layer is LPCM, which embeds domain-specialized knowledge in the field of processor chip design. The middle-layer is the Hardware Design Agent and Software Design Agent, which enable the automated design of hardware and software by leveraging the domain knowledge from LPCM. The top- layer focuses on implementing various applications that use the automated design capabilities provided by the Hardware Design Agent and Software Design Agent to address different design requirements for processor chips. These three layers work synergistically, forming a complete system for fully automated hardware and software design of processor chips. However, the realization of QiMeng is not achieved in- stantaneously. Each of the three levels faces its own unique set of challenges, making it difficult to directly establish the complete QiMeng system in a bottom-up way. Specifically, the implementation of LPCM in bottom-layer requires sub- stantial domain-specialized data in hardware software design of processor chips. However, the domain-specialized data is extremely scarce, preventing the training of LPCM. In the middle-layer, the development of Hardware Software Design Agent depends on the domain knowledge provided by LPCM, while also needing to integrate specialized tools for verifying the correctness and evaluating performance.",
    "source": "2506.05007v1_QiMeng_Fully_Automated_Hardware_and_Software_Desig.pdf",
    "length": 2731,
    "tokens": 494
  },
  {
    "text": "We evaluate two embedding models: E5-Base [31], [30], [29], [84], and Mistral-7B. The retrieval uses the IVF-PQ algorithm with: 4M centroids, 50 probes, 5K points per probe. After re-ranking, 20 documents (512 tokens each) are selected, E5_Base A100 E5_Base Large CPU Mistral_7B A100 E5_Base Small CPU Mistral_7B Large CPU Mistral_7B Small CPU 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 Latency (s) Embedding Model RAG:Embedding RAG:Embed transfer RAG:Retrieval Communication Prefill Decode Queue Fig. 9: Understanding the bottleneck of LLM inference pipeline for different embedding models on different HWs. adding 10K context tokens. We use queries from the Azure Conversational dataset. Fig. 9 shows that placing large embedding models on smaller CPUs leads to severe performance bottlenecks, significantly increasing TTFT. Offloading the embedding computation to a faster NPU, like an A100, drastically reduces embedding latency even for large models like Mistral-7B. We also observe that the time spent transferring retrieved context to the prefill client is negligible (under 1 of total runtime), even on modest PCIe bandwidth. Thus, using high compute HW for the embedding stage is more critical than maximizing CPU-GPU bandwidth for context transfer. For large embedding models (e.g., Mistral-7B), embedding time becomes a major bottleneck; offloading it to a high-compute NPU significantly improves performance. CPU-GPU bandwidth is rarely a bottleneck for typi- cal (10K token) context transfers even with PCIe4.0x4, transfer time accounts for less than 1 of total runtime. V. Navigating Design Choices with HERMES HERMES allows us to model different real-world scenarios and as a result make more efficient LLM inference pipelines. A. Impact of Batching Methods in Online Serving Online serving workloads vary significantly in input output characteristics, service level objectives (SLOs), and avail- able hardware resources. As a result, identifying optimal batching strategies is a complex and context-dependent task.",
    "source": "2504.09775v3_Understanding_and_Optimizing_Multi-Stage_AI_Infere.pdf",
    "length": 2019,
    "tokens": 488
  },
  {
    "text": "- 13.3 67 46 115 - 3.4 - 1.4e-1 - [121] 7A35T CNN f32 - - - - 36 1.1 0.1 9.8 - - [133] 7Z020 Ghost-YOLOS i16 8.41G 12.6 58 60 150 29.5 3.0 9.9 3.2e-1 - [141] 7VX690T Q-IORN i8 - 121.5 21 27 200 209.6 6.3 33.2 1.5e-1 6.8 [139] 7VX690T A2NN i8,f32 14.9G - 9 86 200 3047 8.3 368.4 4.9e-3 203.2 F [48] ZU15EG 2D CNN i8 684 K 1.2 5 71 100 7.1 8.4 0.8 9.7e-5 - ZU15EG 3D CNN i8 4.23M 0.1 9 76 100 3.8 8.4 0.5 1.1e-3 - ZU15EG HybridSN i8 101 M 20.5 37 80 100 13.2 8.4 1.6 7.7e-3 - [75] 7Z100 CBFF-SSD i16 - - 57 60 200 452.8 19.5 23.2 4.3e-2 - [76] ZU3EG AP2D-Net i(mix) - - 80 75 300 130.2 5.6 23.3 3.3e-2 30.5 [89] 7VX690T Impr. YOLOv2 i8,i32 379 G 49.4 23 54 200 387 15.0 25.9 9.8e-1 - 7VX690T ResNet-34 i8,i32 7.33G 21.3 23 54 200 182 15.0 12.2 4.0e-2 - 7VX690T VGG16 i8,i32 30.6G 14.7 23 54 200 344 15.0 23.0 8.9e-2 - [126] 7K325T LeNet-5 f32 f32 - 6.6 34 33 100 - - - 2.3e-3 - 7K325T LeNet-5 i8 i8 - 1.7 14 16 100 - - - 2.3e-3 - [131] 7A200T Impr. VGG16 i8 40.9G 14.8 12 29 200 23.1 3.4 6.8 1.8e 0 - 7A200T Impr.",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 1011,
    "tokens": 569
  },
  {
    "text": "URL 2307.13018. Thakur, S., Ahmad, B., Fan, Z., Pearce, H., Tan, B., Karri, R., Dolan-Gavitt, B., and Garg, S. Benchmark- ing large language models for automated verilog rtl code generation. 2023 Design, Automation and Test in Eu- rope Conference and Exhibition (DATE), 2023. doi: 10.23919 DATE56975.2023.10137086. URL https: par.nsf.gov biblio 10419705. Tseng, A., Chee, J., Sun, Q., Kuleshov, V., and De Sa, C. Quip: even better llm quantization with hadamard incoherence and lattice codebooks. In Proceedings of the 41st International Conference on Machine Learning, ICML 24. JMLR.org, 2024. Varambally, B. S. and Sehgal, N. Optimising design ver- ification using machine learning: An open source solu- tion, 2020. URL 02453. Vungarala, D., Nazzal, M., Morsali, M., Zhang, C., Ghosh, A., Khreishah, A., and Angizi, S. Sa-ds: A dataset for large language model-driven ai accelerator design gener- ation, 2024. Wang, A. 10 best data annotation and data labeling tools in 2024: Basicai s blog, 2024. Wang, C., Xu, Y., Peng, Z., Zhang, C., Chen, B., Wang, X., Feng, L., and An, B. keqing: knowledge-based question answering is a nature chain-of-thought men- tor of llm, 2023a. URL 2401.00426. Wang, H., Ma, S., Dong, L., Huang, S., Wang, H., Ma, L., Yang, F., Wang, R., Wu, Y., and Wei, F. Bitnet: Scaling 1- bit transformers for large language models, 2023b. URL Wang, H., Ma, S., and Wei, F. Bitnet a4.8: 4-bit activations for 1-bit llms, 2024a. URL abs 2411.04965.",
    "source": "2504.08852v1_ML_For_Hardware_Design_Interpretability_Challenges.pdf",
    "length": 1466,
    "tokens": 495
  },
  {
    "text": "constraints Quality constraints Operational carbon Embodied carbon Inputs Profiling Carbon Modeling 4 3 2 Figure 1: Overview of FUEL framework. efforts include LLMCarbon (Faiz et al., 2024) and LLMCO2 (Fu et al., 2024), which provide end- to-end carbon modeling frameworks, while LLM- Campass (Zhang et al., 2024) focuses on hardware evaluation for LLM workloads. Profiling studies have run various LLM serving models across dif- ferent hardware and QPS settings (Nguyen et al., 2024; Li et al., 2024c; Patel et al., 2024a), with GreenLLM (Shi et al., 2024) and Sprout (Li et al., 2024b) optimizing carbon emissions based on their profiling. However, none of these studies take a functional unit perspective as we do in this work. LLM serving optimization. Prior work on LLM serving has primarily focused on optimizing per- formance and energy efficiency. Performance im- provements can be categorized into model-level and system-level techniques. Model-side optimiza- tions include quantization (Lin et al., 2024; Fran- tar et al., 2022), sparsification (Frantar and Alis- tarh, 2023), and speculative decoding (Leviathan et al., 2023). System-side approaches involve memory management (Kwon et al., 2023), batch- ing (Agrawal et al., 2024; Yu et al., 2022), and kernel optimizations (Dao et al., 2022). Addition- ally, efforts to enhance energy efficiency include solutions like Splitwise (Patel et al., 2024b) and Dy- namoLLM (Stojkovic et al., 2024). However, they have largely overlooked quality constraints when considering performance and energy efficiency. 3 The Framework FUEL We present FUEL, a Functional Unit-based Evaluation framework for evaluating the environ- ment impact of LLMs. FUEL enables a systematic and comprehensive analysis across various compar- ison configurations (e.g., model size, quantization, and hardware). Inspired by life cycle assessment in environmental sustainability (KlÃ¶pffer and Grahl, 2014), the key insight is to establish a functional unit as a standardized basis for comparison.",
    "source": "2502.11256v2_Unveiling_Environmental_Impacts_of_Large_Language_.pdf",
    "length": 2024,
    "tokens": 479
  },
  {
    "text": "The solution is a CGA column design being 217 fine copper wires in a hex-close pack configuration. Each 80 Î¼m diameter wire contributes to a robust 640 Î¼m copper column that simultaneously could provide: low resistance and low voltage drop of 0.25 mV; total power loss of all CGA columns of only 0.33 W; highs current-carrying capacity without electromigration failure; thermal-mechanical compliance to accommodate differential expansion; elimination of elastoplastic deformation common in solder columns; and sufficient structural integrity for reliable system assembly. To manufacture these columns, continuous copper wire bundles are induction welded at intervals of approximately 4 mm. These welded sections are then cut through their centers and staked into the busbars. Small holes are drilled in the edge of the busbars where the CGA columns are to go. These holes are plastically enlarged by forcing hardened steel spikes into them, displacing the copper sideways. The CGA column is placed into Table 16: CGA column structure Aspect Value Units Diameter of CGA column 640 Î¼m Copper wire diameter 80 Î¼m Hex close pack configuration Number of complete rings 8 rings Number of copper wires 217 wires 38 the expanded copper hole, and the displaced copper is compressed back into place, trapping the CGA columns and forming a conductive path. The CGA columns are precision-trimmed in a dedicated fixture to ensure accurate length and coplanarity. A high-temperature elastomer applied between the welded sections wicks between the 217 individual wires of a CGA column, preventing solder from later infiltrating the bundle during reflow to the WSSCB - thus preserving the critical wire flexibility required for reliable long-term operation. 23.2 PSU PCB Attach process Each PCB undergoes final inspection and final electrical verification testing of voltage regulation and control systems. Verified PCBs are loaded into a precision-aligned mounting jig that maintains their positions without constraining the CGA columns. The jig assembly is dipped approximately 1 mm into a low-temperature tin-lead solder bath (Sn63 Pb37, melting point 183 C), applying a controlled amount of solder to all CGA column tips simultaneously. Alternatively, they may be printed with solder paste. After WSSCB plasma cleaning, the complete PCB array is aligned to the WSSCB, forming all CGA connections simultaneously through low-temperature reflow that protects the attached chips and underfill materials.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2487,
    "tokens": 499
  },
  {
    "text": "[44] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. URL [45] Yongan Zhang, Zhongzhi Yu, Yonggan Fu, Cheng Wan, and Yingyan (Celine) Lin. MG-Verilog: multi-grained dataset towards enhanced llm-assisted verilog generation. In The First IEEE International Workshop on LLM-Aided Design (LAD 24), 2024. [46] Yang Zhao, Di Huang, Chongxiao Li, Pengwei Jin, Ziyuan Nan, Tianyun Ma, Lei Qi, Yansong Pan, Zhenxing Zhang, Rui Zhang, Xishan Zhang, Zidong Du, Qi Guo, Xing Hu, and Yunji Chen. Codev: Empowering llms for verilog generation through multi-level summarization, 2024. URL 13 [47] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, and Zheyan Luo. Llamafactory: Unified efficient fine-tuning of 100 language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 400 410, 2024. 14 A Method Details A.1 Proof of Theorem 2.1 Proof. Observe that the sequence Y X Y forms a Markov chain. By the Data Processing Inequality (DPI), I(Y ; Y ) I(Y ; X). Under the assumption that EY,Y holds almost surely, we have H(Y Y ) 0, and thus I(Y ; Y ) H(Y ) H(Y Y ) H(Y ). It follows that H(Y ) I(Y ; Y ) I(Y ; X) H(Y ) I(Y ; X) H(Y ) H(Y X) 0, meaning Y is determined by X almost surely and hence EY,X holds.",
    "source": "2505.24183v2_CodeV-R1_Reasoning-Enhanced_Verilog_Generation.pdf",
    "length": 1422,
    "tokens": 448
  },
  {
    "text": "However, LLMs inherently capture circuit textual semantics rather than structure. Limited work has explored LLMs for general- purpose circuit representation learning, with key limitations sum- marized as follows: (1) Struggle with netlists. Although LLMs can interpret circuit functionality from RTL code, the gate-level netlists are more flattened and lack informative context, making functional understanding more challenging. (2) Lack of structural encoding. LLMs struggle to capture the circuit structures, limiting their utility for netlist representation learning. In this work, we present NetTAG, a foundation model1 for netlists that captures functional and physical properties across diverse gate types. Unlike previous circuit encoders and LLM solutions focusing on single circuit modality, NetTAG fuses gate text semantics with global graph structures to achieve functional and physical under- standing. Serving as a foundation model, the pre-trained NetTAG generates versatile embeddings for logic gates, register cones, and full circuits. The embeddings can be easily fine-tuned for various downstream tasks, supporting largely different netlist-stage functional and physical tasks across these circuit granularities. As shown in Fig. 1, we propose the following innovative strategies in NetTAG: Preprocess: formulating netlists as text-attributed graphs. This paper is the first to represent netlists in text-attributed graphs (TAGs) format. In the netlist graph, we annotate each gate with functional symbolic logic expressions and physical characteristics as node-level text attributes. Our TAG format combines gate textual attributes with graph topology, moving beyond existing graph-only circuit representation learning. 1The code and pre-trained NetTAG model are available at The pre-trained model enables users to easily generate and fine-tune embeddings for their own netlist tasks. arXiv:2504.09260v1 [cs.AR] 12 Apr 2025 TABLE I: Comparision of state-of-the-art netlist representation learning methods. Method Target Circuit Encoding Methodology Downstream Tasks Cell Type Circuit Type Modality ML Model Pre-Train Target Cross-Stage Align Target Type DeepGate1 2 [25], [26] AIG Comb. Graph GNN Gate N A Gate Func. DeepGate3 [27] AIG Comb. Graph GT Gate Circuit N A Gate Func. FGNN [28], [29] AIG Comb.",
    "source": "2504.09260v1_NetTAG_A_Multimodal_RTL-and-Layout-Aligned_Netlist.pdf",
    "length": 2323,
    "tokens": 491
  },
  {
    "text": "Require: ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘œwith sorted configurations ğ‘ğ‘“ğ‘”ğ‘–for 0 ğ‘– ğµğ‘  Require: Î”ğ‘Ÿğ‘’ğ‘for maximum idle time before decreasing skipping Require: ğ‘šğ‘–ğ‘›ğ‘ğ‘ğ‘for the minimum acceptable accuracy 1: ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘œ ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘œ[ğ‘–] ğ‘–(ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦(ğ‘ğ‘“ğ‘”ğ‘–) ğ‘šğ‘–ğ‘›ğ‘ğ‘ğ‘) {Filter out configurations with accuracy below the user s threshold} 2: ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ_ğ‘ğ‘“ğ‘” ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘œ[0] {Start with no skipping} 3: loop 4: ğ‘Ÿğ‘’ğ‘ ğ‘›ğ‘’ğ‘¤_ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡() {Wait for new inference request} 5: if ğ‘ğ‘¢ğ‘ ğ‘¦() then 6: ğ‘‘ğ‘Ÿğ‘œğ‘_ğ‘–ğ‘›ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’(ğ‘Ÿğ‘’ğ‘) 7: ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ_ğ‘ğ‘“ğ‘” ğ‘–ğ‘›ğ‘ğ‘Ÿğ‘’ğ‘ğ‘ ğ‘’_ğ‘ ğ‘˜ğ‘–ğ‘ğ‘ğ‘–ğ‘›ğ‘”() {Increase number of skipped blocks by one if possible, otherwise returns ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘œ.ğ‘™ğ‘ğ‘ ğ‘¡()} 8: else 9: if ğ‘¡ğ‘›ğ‘œğ‘¤ ğ‘¡ğ‘™ğ‘ğ‘ ğ‘¡ Î”ğ‘Ÿğ‘’ğ‘then 10: ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ_ğ‘ğ‘“ğ‘” ğ‘‘ğ‘’ğ‘ğ‘Ÿğ‘’ğ‘ğ‘ ğ‘’_ğ‘ ğ‘˜ğ‘–ğ‘ğ‘ğ‘–ğ‘›ğ‘”() {Decrease the num- ber of skipped blocks by one if possible, otherwise returns ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘¡ğ‘œ.ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡()} 11: end if 12: ğ‘ğ‘Ÿğ‘œğ‘ğ‘’ğ‘ ğ‘ _ğ‘–ğ‘›ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’(ğ‘Ÿğ‘’ğ‘, ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ_ğ‘ğ‘“ğ‘”) 13: ğ‘¡ğ‘™ğ‘ğ‘ ğ‘¡ ğ‘¡ğ‘›ğ‘œğ‘¤ 14: end if 15: end loop the device is busy, the inference requested cannot be processed and it is dropped (line 6). The rationale behind it follows the MLPerf inference server standard [21] that defines a request as dropped if it arrives at a busy device. Therefore, in such cases, the runtime increases the number of skipped blocks by one (line 7).",
    "source": "2505.17626v1_Leveraging_Stochastic_Depth_Training_for_Adaptive_.pdf",
    "length": 1128,
    "tokens": 895
  },
  {
    "text": "Results show that most of the testing accuracy losses are lower than the predefined tolerance, with only one case slightly higher, suggesting that purity threshold pruning effectively reduces model complexity while ensuring controlled accuracy degradation. Models pruned with higher tolerance also follow this trend, though we omitted the statistics here for clarity. D. Experiment 3: Tree Mapping Scheme After validating purity threshold pruning, we estimate the effectiveness of the proposed data placement strategies. Fig. 7 presents the results of implementing ODR and SPC on XGBoost and unpruned Random Forest. Since unpruned Random Forest is usually more complex than XGBoost, the improvement of applying ODR and SPC on Random Forest is often more obvious. For energy-efficient mapping, ODR presents more than 50 reduction in all cases. Since unique conditions of the models trained on CreditAp- proval and Letter datasets is an order of magnitude fewer than in other datasets, the improvement observed in the Random Forest trained on these two datasets is relatively limited. For space-efficient mapping, SPC exhibits notable improvement, with over 30 reduction in every case, while FR performs worse than the baseline in half of the cases. The underlying reason is that FR only eliminates completely redundant path segments within a TCAM, whereas significant redundancy is still left behind. Thus, FR beats naive independent mapping only when the redundancy within a single tree is overwhelm- ing. On the other hand, by greedily clustering similar paths together, SPC minimizes redundancy while incurring less runtime computational overhead, consistently outperforming FR and naive independent mapping in all cases. The per- formance of applying SPC to the Random Forest trained on the CreditApproval dataset is nearly optimal (SPC requires 99 TCAMs, while the theoretical minimum is 98 for 6246 paths). However, the exceptionally strong performance of naive inde- pendent mapping on this model limits the potential for further improvement. Experimental results show that implementing the tree mapping scheme alone yields 1.46 to 21.30 better space efficiency, highlighting the effectiveness of both ODR and SPC. E. Experiment 4: Number of Trees Prior experiments demonstrate that RETENTION is ex- tremely effective in minimizing CAM capacity requirement.",
    "source": "2506.05994v1_RETENTION_Resource-Efficient_Tree-Based_Ensemble_M.pdf",
    "length": 2363,
    "tokens": 451
  },
  {
    "text": "Load Value Injection (LVI) [54] is a technique that generalizes injection-based attacks to the memory hier- archy by injecting attacker-controlled values into a victim s transient execution. Downfall [32] is a new class of transient execution attacks that exploit the gather instruction on x86 CPUs to leak sensitive data across security boundaries, in- cluding user-kernel, process, and virtual machine isolation, as well as trusted execution environments. 3.2 Reinforcement Learning In RL, the objective is for an agent to learn a policy Ï€Î¸(a s), parameterized by Î¸, which maximizes the expected cumu- lative reward through its chosen actions in an environment. The policy gradient method [48] computes the gradient of the expected reward with respect to the policy parameters, allowing the agent to directly update the policy by following the gradient. Formally, the objective function J(Î¸) is defined as: J(Î¸) EÏ€Î¸ \" T t 0 rt , where rt is the reward at time step t, and the expectation is over the trajectories induced by the policy Ï€Î¸. The policy is updated by adjusting Î¸ in the direction of the gradient Î¸J(Î¸) using gradient ascent. One of the major challenges with vanilla policy gradient methods is the high variance of the gradient estimates, which can lead to unstable learning. Additionally, large updates to the policy parameters Î¸ can cause dramatic changes to the policy, potentially leading to performance col- lapse. Trust Region Policy Optimization (TRPO) [43] was proposed to address this issue by enforcing a constraint on the size of policy updates using a trust region. TRPO introduces the following constrained optimization problem: max Î¸ EÏ€Î¸ Ï€Î¸(a s) Ï€Î¸old(a s) Ë†A(s,a) subject to Es DKL Ï€Î¸old Ï€Î¸ Î´ where DKL is the Kullback-Leibler (KL) divergence, Ë†A(s,a) is the advantage estimate, and Î´ is a small positive value controlling the step size. However, TRPO is computationally expensive due to the need for second-order optimization to enforce the KL-divergence constraint.",
    "source": "2502.14307v1_Î¼RL_Discovering_Transient_Execution_Vulnerabilitie.pdf",
    "length": 1996,
    "tokens": 479
  },
  {
    "text": "These technologies have become an important part of our daily life, in the form of creative photography, content creation, gaming, online shopping, virtual touring, and educational and non-educational training, etc. For ex- ample, one of the earliest AR games, PokÃ©mon GO (launched in July 2016), had a cumulative download of over 1 Billion, and gener- ated about 900 Million in revenue by late 20191. Moreover, these AR infotainment applications have helped many of us through the recent global pandemic by bringing us the liveliness of the virtual outdoors, while we were confined to our homes, and more AR capa- ble mobile devices penetrating the market with cheaper price tags have made AR applications pervasive and made the virtual world easily accessible for users on the tip of their fingers. However, even the state-of-the-art mobile devices with high bandwidth cannot meet the heavy compute and real-time demands of the AR applications, leading to very low quality of service (QoS) in some cases as low as 1 frame per second (fps) [19, 54]. Further, 1To give a quantitative estimation of the popularity of the game, a PokÃ©mon GO event at Safari Zone New Taipei City, Taiwan in October 2019 had a total of 327,000 attendees and they walked around 4.5 million kilometers to catch 50 Million PokÃ©mons [5]. 494 MICRO 21, October 18 22, 2021, Virtual Event, Greece Shulin and Haibo, et al. the limited battery capacity prevents users from enjoying their AR devices for extended periods of time. To meet the heavy compute demands of these applications, most of AR applications are run using high-end desktop server-class GPUs [18, 55], or specialized hardware accelerators [35] on cloud platforms [16, 27]. However, since most of these applications are now running on low-power mobile devices, and frequent communi- cation of data to and from cloud via wireless medium is inefficient, optimization of an AR pipeline to maximize the compute and en- ergy efficiency, while providing adequate QoS, at an edge device is an architectural challenge. Furthermore, existing AR headsets are typically equipped with multiple sensors for head orientation, eye tracking, motion detection, etc., to provide an interactive and life- like experience. These sensor inputs play a major role in deciding which portions of the 3D voxels need to be rendered for the user to view.",
    "source": "HoloAR.pdf",
    "length": 2363,
    "tokens": 494
  },
  {
    "text": "Task-2.5: Runtime Support Our runtime support system will serve as the cohesive glue that efficiently manages system resources, models, and data in real-time with minimal overhead. It effectively addresses the operational efficiency and response quality within the constraints of stringent SLOs and a minimum accuracy rate. By integrating expert affinities (Expert Expert, Expert Data, Expert Router, and Expert Composition Function) detailed in Table 2, the system will optimize responsiveness and computational efficiency. Grouping experts in close memory proximity is expected to enhance cache efficiency and reduce latency. Data locality should benefit from prefetching and caching critical data, which would lower transfer delays and potentially speed up training and inference. Intelligent routing algorithms are proposed to efficiently distribute queries based on real-time performance data and expert availability. Custom composition functions are anticipated to further speed up the integration of diverse expert outputs, thereby improving overall response times. This unified strategy ensures rapid, accurate query responses and high throughput within the infrastructure s memory and computational constraints. Task-2.6: Fault-Tolerant Expert Training Fault-tolerance of monolithic LLM models has been a major concern due to the long-running training jobs on a large number of GPUs [27, 149, 165]. The proposed EoE model can provide inherent fault-tolerance since each expert can be trained independently, and is thus exposed to hardware failures in a reduced time- frame. To further minimize the impact of failures, we will investigate known fault-tolerance techniques from the distributed computing domain to support graceful degradation of training. A few techniques we plan to study include: i) Exploiting Redundancy: Selectively training multiple experts on similar over- lapping tasks (which are deemed critical to performance accuracy) provides a fallback mechanism in case one of them fails.",
    "source": "NSF_LLM_Medium_Proposal.pdf",
    "length": 2009,
    "tokens": 365
  },
  {
    "text": "544 584, 2024. [3] R. V. W. Putra and M. Shafique, Fspinn: An optimization framework for memory-efficient and energy-efficient spiking neural networks, IEEE Trans. on Computer-Aided Design of Integrated Circuits and Systems (TCAD), vol. 39, no. 11, pp. 3601 3613, 2020. [4] A. Basu et al., Spiking neural network integrated circuits: A review of trends and future directions, in CICC, 2022, pp. 1 8. [5] R. V. W. Putra et al., Embodied neuromorphic artificial intelligence for robotics: Perspectives, challenges, and research development stack, in ICARCV, 2024, pp. 612 619. [6] B. Vogginger et al., Neuromorphic hardware for sustainable ai data centers, arXiv preprint arXiv:2402.02521, 2024. [7] BrainChip. Akida neural processor soc. [Online]. Available: https: brainchip.com akida-neural-processor-soc [8] SynSense. Dynap-cnn: The world s first fully scalable, event- driven neuromorphic processor with up to 1m configurable spiking neurons and direct interface with external dvs. [Online]. Available: [9] J. Dupeyroux et al., Neuromorphic control for optic-flow-based landing of mavs using the loihi processor, in ICRA. IEEE, 2021, pp. 96 102. [10] S. Stroobants, J. Dupeyroux, and G. De Croon, Design and implemen- tation of a parsimonious neuromorphic pid for onboard altitude control for mavs using neuromorphic processors, in ICONS, 2022, pp. 1 7. [11] H. Patel et al., Bringing touch to the edge: A neuromorphic processing approach for event-based tactile systems, in AICAS, 2023, pp. 1 5. [12] S. Venkatachalam et al., Realtime person identification via gait analysis using imu sensors on edge devices, in ICONS, 2024, pp. 371 375.",
    "source": "2504.00957v2_Enabling_Efficient_Processing_of_Spiking_Neural_Ne.pdf",
    "length": 1642,
    "tokens": 476
  },
  {
    "text": "However, these devices take a throughput-ï¬rst approach, to minimize the time consumption and seldom optimize power consumption ï¬rst. This has lead to a global concern of the energy and consequently carbon-footprint of the DNN training [21], [57], [67], [89]. Furthermore, these accelerators have been designed to operate under constantly available power. Although our pro- posed representation learning ( III) and micro-proï¬ler ( III-C) help us ï¬nd a better training conï¬guration that can minimize the compute if deployed in the aforementioned accelerators, it does not solve sustainability: That is, with variable solar power, can we scale compute alongside power to continue to make forward progress , even when minimum amount of power is available. The systolic array structure of the DNN accelerators is well suited for this as we can change the com- pute size, as well as the number of memory channels feeding to those compute units as per the power availability. However, we need to be innovative in terms of designing and placing the compute hierarchy to ensure minimum data movement and re-computations when compute scaling. The hardware design of Us. as (Fig. 5a) incorporates all the aforementioned points. Note that, Us. as introduces a design philosophy for building a morphable hardware, and it can easily be adapted by any of the systolic array based commercial off the shelf (or research prototype) DNN training accelerators.",
    "source": "Usas.pdf",
    "length": 1440,
    "tokens": 310
  },
  {
    "text": "M2-M7 refers to approximate multipliers namely, KV8 (0.0018 MAE), KV9 (0.0064 MAE), KVP (0.051 MAE), L2J (0.081 MAE), L2L (0.23 MAE), and L2N (0.52 MAE) in Evoapprox8b [15] library. Skipping Threshold Multipliers Accuracy Ea L5 (5, 0, 10, 15, 70) (M2, M1, M3, M6, M7) 94 2 ÂµW (5, 0, 5, 10, 60) (M3, M1, M3, M6, M7) 94 3 ÂµW (5, 0, 5, 20, 50) (M2, M1, M2, M6, M7) 95 4 ÂµW R1 (0, 5, 5, 10, 15, 70) (M1, M2, M2, M5, M6, M7) 88 26 ÂµW (0, 5, 5, 10, 20, 70) (M1, M2, M2, M3, M4, M5) 86 20 ÂµW (0, 5, 10, 15, 15, 60) (M4, M1, M5, M6, M7) 87 17 ÂµW R2 (0, 5, 10, 15, 15, 50) (M1, M2, M2, M3, M3, M7) 81 38 ÂµW (0, 5, 10, 15, 15, 70) (M1, M2, M2, M4, M4, M5) 89 35 ÂµW (0, 5, 10, 30, 35, 75) (M1, M2, M3, M3, M6, M7) 88 34 ÂµW R3 (0, 10, 10, 15, 25, 55) (M1, M2, M2, M3, M6, M7) 91 42 ÂµW (0, 15, 20, 25, 30, 60) (M1, M2, M3, M4, M5, M6) 89 40 ÂµW (0, 15, 30, 30, 35, 35) (M1, M2, M3, M3, M6, M7) 90 41 ÂµW Table II: Comparing accuracy and energy efficiency of AL- WANN [5] and CGP [6] with our XAI-NAS for ResNet-14 Perf. Metrics ALWANN [5] CGP [6] XAI-NAS Accuracy 85.55 83.98 92 Energy 19.76 ÂµJ 14.88 ÂµJ 7.88 ÂµJ CIFAR10 dataset.",
    "source": "2503.16583v1_Explainable_AI-Guided_Efficient_Approximate_DNN_Ge.pdf",
    "length": 1113,
    "tokens": 604
  },
  {
    "text": "To address the capacity limitation inherent in digital CIM architectures, CIMFlow implements a systematic partitioning strategy to divide the model into multiple execution stages. As detailed in Alg. 1, the model partitioning phase employs a dynamic programming (DP) based approach that optimizes workload distribution across available cores. The algorithm incorporates a state compression optimization that encodes all the dependency closures in the DAG as bitmasks, significantly reducing both space complexity and computational overhead. Each dependency closure represents a self-contained set of operators whose dependencies are fully enclosed within the set, serving as basic building blocks for candidate partitions. The compiler derives candidate partitions through set op- erations on these dependency closures, and performs core mapping optimization for each partition. This process involves strategically duplicating operator weights across clusters of cores when deemed beneficial by the cost estimation model. To balance parallel execution benefits against communication costs, the estimation model accounts for both computation costs and data transfer overheads across inter- and intra-cluster communications. These cost assessments and their correspond- ing optimal mapping configurations are then used to guide the DP-based partition selection. The final phase of CG-level optimization focuses on inter- core scheduling and intermediate representation (IR) genera- tion. The scheduler orchestrates data movement through the NoC interconnection, facilitating the inter-operator pipelines across different clusters. For each core, the compiler gen- erates an optimized operation sequence incorporating both partitioning decisions and mapping destinations, establishing the foundation for OP-level optimizations. OP-level Optimization. Following CG-level workload dis- tribution, the compiler performs fine-grained operator transfor- mations to maximize hardware efficiency. This process involves a structured approach that first establishes an ideal mapping in a constraint-free virtual space, and then adapts this mapping to actual hardware resource constraints. The virtual mapping phase begins by analyzing the di- mensional structure of each operator, transforming complex nested loops into a simplified version that aligns with the CIM array structure. This transformation process maps the software-level weight dimensions onto a two-dimensional ar- ray representation. By temporarily abstracting away physical constraints, the compiler explores the optimal weight data layout strategies, including the image-to-column (im2col) transformation commonly employed in DNN acceleration. The physical mapping phase then adapts the idealized representation to actual hardware constraints through a se- ries of optimization passes implemented within the MLIR infrastructure.",
    "source": "2505.01107v1_CIMFlow_An_Integrated_Framework_for_Systematic_Des.pdf",
    "length": 2884,
    "tokens": 494
  },
  {
    "text": "22. 31 C.2.2 Optimized Hardware FSM-Based Code Example void test(float Adyn [12][12] , float Bdyn [12][4] float Kinf [4][12] , float x[NHORIZON 1][12][1] , float d[ NHORIZON ][4][1] , float u[NHORIZON ][4][1]) { static elem_t B_u [12][1]; gemmini_extended_config_ex (1, 0, 0, 1, false , false); gemmini_extended3_config_ld (4, 1.0, false , 1); gemmini_extended3_config_ld (4, 1.0, false , 2); for (int i 0; i NHORIZON; i ) { gemmini_extended_config_st (4, 0, -1.0); gemmini_extended3_config_ld (48, 1.0, false , 0); gemmini_loop_ws (1, 1, 3, 0, 3, 0, Kinf , x[i], d[i], u[i], 12, 1, 1, 1, false , false , false , false , true , 0, 1, 1, false); gemmini_fence (); if (i NHORIZON - 1) { gemmini_extended_config_st (4, 0, 1.0); gemmini_extended3_config_ld (16, 1.0, false , 0); gemmini_loop_ws (3, 1, 1, 0, 3, 0, Bdyn , u[i], NULL , B_u , 4, 1, 1, 1, false , false , false , false , false , 0, 1, 1, false); gemmini_fence (); gemmini_extended3_config_ld (48, 1.0, false , 0); gemmini_loop_ws (3, 1, 3, 0, 3, 0, Adyn , x[i], B_u , x[i 1], 12, 1, 1, 1, false , false , false , false , true , 0, 1, 1, false); gemmini_fence (); } } } Figure 24: Example of the TinyMPC primal update forward pass from Fig. 22, hand-optimized and using the hardware FSM, from the experiments.",
    "source": "2505.18574v2_Autocomp_LLM-Driven_Code_Optimization_for_Tensor_A.pdf",
    "length": 1267,
    "tokens": 524
  },
  {
    "text": "However, in some pathological cases, the error at times goes close to 60 , and we believe them to be generated artifacts which are common side effects of the GANs[11]. Our experi- ments suggests that inference on the GAN recovered signal is almost as good as (about 2 4 difference in accuracy) the inference on the recovered cluster signal. The recovery policy can be implemented as a simple generator network in the host. Although, the training of the GAN is complex and involves multiple networks as well as hyper-parameters tun- ing, the generator network itself is very small (few hundred thousands of parameters depending on the sensor data). 4 DESIGN IMPLEMENTATION OF SEEKER By leveraging the coreset construction techniques discussed in Section 3, we design Seeker: A synergistic sensor host ecosys- tem. Figure 5 gives a pictorial representation of the overall design of Seeker and its various components. Seeker lever- ages the concept of NVP, and employs a flexible store and execute method using the state of the art ReRAM crossbar architecture [47] to perform inference at the edge. It aug- ments the sensor nodes with two different quantized DNNs (16 bit and 12 bit) to increase the number of completed in- ferences at the sensor node itself. Prior studies [56, 64, 68] and our empirical analysis on the quantization vs accuracy trade-offs (see Fig. 2c) indicate the 16 and 12bit precision to maximize the accuracy of the inferences while minimizing the energy consumption. Moreover, we also implement the memoization option so that it does not have to repeat infer- ences if it encounters similar data, thereby saving substantial energy as well as delivering results with extremely low la- tency. However, even with all these optimizations, due to the fickle nature of EH, Seeker cannot finish all the inferences at the edge and must communicate with a host device. To minimize the data communication overhead between the sensor-node and the host device, Seeker utilizes coresets to build representative, yet compressed, forms of the data. To cater towards the fickle EH budget, we use the two dif- ferent coreset construction techniques, described in Section 3: a cheaper, less accurate formation (importance sampling) and a more expensive, yet accurate formation (K-means).",
    "source": "Seeker.pdf",
    "length": 2290,
    "tokens": 486
  },
  {
    "text": "We first describe the disaggregated memory pooling foundation ( 4.4.1), which leverages the high- bandwidth UB plane to build a disaggregated memory pool with unified memory access. We then introduce two key caching services built atop this pool: Context Caching ( 4.4.2) and Model Caching ( 4.4.3), both delivered via Huawei Cloud s elastic memory service (EMS) [26]. 4.4.1 Disaggregated Memory Pooling At the heart of EMS caching services is a logically disaggregated memory pool, composed of CPU-attached DRAM aggregated across nodes within a CloudMatrix384. This pool acts as a unified, high-performance memory substrate for caching historical KV cache and model parameters. A distinguishing characteristic of this memory pool is its deep integration with the UB network Serving Large Language Models on Huawei CloudMatrix384 33 CPU AI Process AI Process MP SDK Caching SDK AI Workflow MP Server Process DRAM Node 0 NPU NPU CPU AI Process AI Process MP SDK Caching SDK AI Workflow MP Server Process DRAM Node N-1 NPU NPU UB Plane VPC Plane The Data Plane: The Ctrl Plane: Node X EVS SSD EVS SSD MP Server Process Data Index Mem Access Mem Mgr Mem Tiering CPU MP Controller Process Fig. 19. The deployment architecture of the UB-driven disaggregated memory pool in EMS. plane, enabling efficient, unified memory access to this distributed DRAM and allowing NPUs to rapidly retrieve necessary data regardless of its physical location, facilitating a peer-to-peer serving architecture as presented in 4.1. The design s efficacy is critically driven by the following UB s hardware capabilities: 1) High-Speed Peer-to-Peer Fabric: The UB network enables fast inter-node data transfers, allowing any NPU or CPU to access DRAM on other nodes efficiently; 2) DMA over UB: Zero-copy data transfers are enabled via direct memory access (DMA), bypassing CPU mediation and cutting transfer latencies; 3) Low-Level Memory Primitives: The UB protocol exposes primitives for remote memory registration and access, allowing the software stack to maintain a global memory view.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2064,
    "tokens": 461
  },
  {
    "text": "[44] PJ Joseph, Kapil Vaswani, and Matthew J Thazhuthaveetil. 2006. A Predictive Performance Model for Superscalar Processors. In MICRO. [45] PJ Joseph, Kapil Vaswani, and Matthew J Thazhuthaveetil. 2006. Construction and Use of Linear Regression Models for Processor Performance Analysis. In HPCA. [46] Ajay Joshi, Aashish Phansalkar, Lieven Eeckhout, and Lizy Kurian John. 2006. Measuring Benchmark Similarity using Inherent Program Characteristics. IEEE TC (2006). [47] Sagar Karandikar, Howard Mao, Donggyu Kim, David Biancolin, Alon Amid, Dayeol Lee, Nathan Pemberton, Emmanuel Amaro, Colin Schmidt, Aditya Chopra, et al. 2018. FireSim: FPGA-Accelerated Cycle-Exact Scale-Out System Simulation in the Public Cloud. In ISCA. [48] Tejas S. Karkhanis and James E. Smith. 2004. A First-Order Superscalar Processor Model. In ISCA. [49] Tejas S. Karkhanis and James E. Smith. 2007. Automated Design of Application Specific Superscalar Processors: An Analytical Approach. In ISCA. [50] Aviral Kumar, Amir Yazdanbakhsh, Milad Hashemi, Kevin Swersky, and Sergey Levine. 2022. Data-Driven Offline Optimization for Architecting Hardware Accelerators. In ICLR. [51] Benjamin C Lee and David M Brooks. 2006. Accurate and Efficient Regression Modeling for Microarchitectural Performance and Power Prediction. In ASPLOS. [52] Benjamin C Lee and David M Brooks. 2007. Illustrative Design Space Studies with Microarchitectural Regression Models. In HPCA. [53] Jiangtian Li, Xiaosong Ma, Karan Singh, Martin Schulz, Bronis R de Supinski, and Sally A McKee. 2009. Machine Learning Based Online Performance Prediction for Runtime Parallelization and Task Scheduling. In ISPASS. [54] Lingda Li, Thomas Flynn, and Adolfy Hoisie. 2023.",
    "source": "2503.23076v1_Concorde_Fast_and_Accurate_CPU_Performance_Modelin.pdf",
    "length": 1717,
    "tokens": 487
  },
  {
    "text": "Broadcast Activation Flow: Unlike conventional horizontal activation pipeline flow, a single FP4 activation value enters simultaneously at the PEs of all 8,208 (8,192 plus 16 spares) columns. While this is a little more complex in hardware than systolically pumping the activations from left to right through the array, it is worth the extra hardware complexity to avoid the delay in activation availability, and the complexity of skewed data. The activation broadcast is accomplished via a 8-level fan-out tree of latches, distributing one activation value across all columns each clock cycle. The 32,768 batches of 24,576 activations are entered into all columns simultaneously at the 12 GHz CASCADE array clock frequency, using 24,576 activation HILTs and 24,576 activation broadcast latch trees. The broadcast latch tree, shown in Table 7, is used instead of a bus, even though the simpler bus structure would be functionally equivalent. A bus would result in significant (and insurmountable, in TSMC A16 or A14) propagation delay, IR drop, fan-out and ground bounce difficulties operating at the SLD s 12 GHz clock frequency. 13.5 Advantages of CASCADE This full-array column-oriented approach offers critical advantages: 1. Simplified Accumulation: Final results accumulate automatically without complex sharding of submatrices and stitching accumulation processes. 2. Minimized Inter-Chip Communication: In most circumstances, no partial sums need to be transferred between chips during computation. This dramatically reduces chip-to-chip bandwidth requirements compared to traditional architectures. 3. Reduced Output Bandwidth: With only complete sums output after 33,260 cycles, the output data rate is vastly lower than systems that must transfer partial sums. 4. Memory Efficiency: Weights reside directly within the CASCADE array, eliminating the need for duplicate weight storage in cache SRAMs. Weights are loaded into the CASCADE arrays asynchronously using the HBM4 data paths or transferred between TRIMERA stacks at 39 TB s. 25 5. Superior Fault Tolerance: With no cross-column communication, the CREST redundancy system can 6. independently validate and substitute spare CASCADE columns for any detected faults, maintaining computational throughput despite silicon defects.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2293,
    "tokens": 462
  },
  {
    "text": "Beyond similarity, functional inversion is another key relationship for tasks like logic propagation and netlist rewriting. Based on these observations, we define three sets of regularity tests that can be automatically derived from Liberty files. The cell representation learning problem can then be formulated as learning vector space representations that maximize accuracy on these regularity tests. Our approach assumes well-documented Liberty files with consistent pin naming, as seen in the ASAP library used in this study. Consequently, input pin reordering is not considered in the regularity tests. Details of the regularity tests are elaborated as follows. A. Inverting Functionality Tests This test set evaluates inverting functionality relationships among cell types. A cell type refers to a group of standard cells with the same functionality but differing in driving strengths, voltage thresholds, or layout implementations. Two cell types with identical input pin names are considered to have an inverting functionality relationship if their outputs always complement each other, such as BUF (buffer) and INV (inverter). After identifying all inverting functionality pairs, we design tests to evaluate these relationships. For instance, as shown in Fig. 1(a), given two pairs, (BUF, INV) and (AND2, NAND2), two tests are created: (BUF vs. INV) (AND2 vs. ?) (AND2 vs. NAND2) (BUF vs. ? ). More examples can be found in Table I. Using linear algebraic operations on cell vectors, we assess whether the target vector 2 (e.g., vector(NAND2)) ranks among the top-K closest vectors to the inferred vector (e.g., vector(INV) - vector(BUF) vector(AND2)). The resulting top-K accuracy indicates how well the learned cell representations capture inverting functionality relationships. B. Functional Similarity Tests This test set evaluates functional similarity among cell types with identical input pins. To simplify the analysis, we focus on single- output cells, which constitute the majority in the ASAP7 library used in our experiments. Future work will extend functional similarity evaluation to individual output pins. Functional similarity between two cells is computed by comparing their truth tables, as shown in Fig. 2. It is defined as the ratio of matching output values to the total number of input combinations.",
    "source": "2503.22900v1_Learning_Library_Cell_Representations_in_Vector_Sp.pdf",
    "length": 2331,
    "tokens": 466
  },
  {
    "text": "Note that FP8 NVFP4 refers to the datapath which supports FP8 weights and NVFP4 activations. Configuration Area (ğ‘¢ğ‘š2) FP8 Datapath 2995 NVFP4 Datapath 1811 FP8 NVFP4 Datapath 2669 NVFP4 FP8 Datapath 2630 FGMP Datapath 10356 FGMP PPU 8848 hardware. Furthermore, it is important to note that datapath area is often a small portion of the area of a DNN accelerator. For example, the datapath is only 11 of the area of a processing element in [27]. The processing elements are an even smaller portion of full system area, which tends to be memory-dominated. Additionally, the PPU has an 85 area overhead relative to the FGMP datapath with 16 lanes. However, the PPU overhead can be amortized by increasing the number of lanes, or by sharing a PPU across multiple PEs. Given ğ‘ƒPEs with ğ¿vector lanes per PE and ğ‘ˆPPUs, and assuming block size of 16, the time for the datapaths to process an (ğ‘€by ğ¾) (ğ¾by ğ‘) matrix (assuming a balanced pipeline) would be ğ‘€ ğ¿ ğ¾ 16 ğ‘ ğ‘ƒcycles, whereas the time for PPU processing would be ğ‘€ 16 ğ‘ ğ‘ˆcycles. For a typical Llama-2-7B matrix multiplication with 4K context length (4096 by 4096 4096 by 4096), a single PPU would be able to support up to 256 16-lane PEs without stalling; the area overhead of the PPU is therefore minimal when the cost is amortized across several PEs. 6 Conclusion In this paper, we propose a post-training methodology for fine- grained mixed-precision quantization with both weights and ac- tivations. This approach allows us to retain the performance of the base model with only a small percentage of blocks retained in higher precision. This is achieved by leveraging sensitivity infor- mation with respect to the final model output to determine which blocks should be preferentially retained in higher precision in order to preserve model accuracy.",
    "source": "2504.14152v1_FGMP_Fine-Grained_Mixed-Precision_Weight_and_Activ.pdf",
    "length": 1802,
    "tokens": 488
  },
  {
    "text": "5345 5353, 2024. [9] C. Lammie, W. Xiang, B. Linares-Barranco, and M. R. Azghadi, MemTorch: An Open-source Simulation Framework for Memris- tive Deep Learning Systems, Neurocomputing, 2022. [10] M. J. Rasch, D. Moreda, T. Gokmen, M. Le Gallo, F. Carta, C. Goldberg, K. El Maghraoui, A. Sebastian, and V. Narayanan, A flexible and fast pytorch toolkit for simulating training and infer- ence on analog crossbar arrays, in 2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS), 2021, pp. 1 4. [11] T.-H. Wen, J.-M. Hung, W.-H. Huang, C.-J. Jhang, Y.-C. Lo, H.-H. Hsu, Z.-E. Ke, Y.-C. Chen, Y.-H. Chin, C.-I. Su, W.-S. Khwa, C.-C. Lo, R.-S. Liu, C.-C. Hsieh, K.-T. Tang, M.-S. Ho, C.-C. Chou, Y.-D. Chih, T.-Y. J. Chang, and M.-F. Chang, Fusion of memristor and digital compute-in-memory processing for energy-efficient edge computing, Science, vol. 384, no. 6693, pp. 325 332, 2024. [12] J. Souto, G. Botella, D. Garc Ä±a, R. Murillo, and A. del Barrio, Neuromorphic circuit simulation with memristors: Design and evaluation using memtorch for mnist and cifar, 2024. [13] S. Jain, A. Sengupta, K. Roy, and A. Raghunathan, Rxnn: A framework for evaluating deep neural networks on resistive cross- bars, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 40, no. 2, pp. 326 338, 2021. [14] Y. LeCun and C. Cortes, MNIST handwritten digit database, 2010.",
    "source": "2505.24721v1_Running_Conventional_Automatic_Speech_Recognition_.pdf",
    "length": 1422,
    "tokens": 497
  },
  {
    "text": "We propose data-driven solutions to address the problem. Specifically, we learn a map, Ë†Î¾ : G, [k] 2V , that takes as input a graph G and a component type Ï• [k]. This mapping returns a subset of vertices (the power set of vertices V , represented as 2V ) that indicate the connections for the ports of components within graph G. We provide a visual illustration of this problem in Figure 2 and 3. 6 DGCNN Link Prediction SEAL (Subgraphs, Embeddings, and Attributes for Link Prediction) Input Netlist Graph ğ‘® Enclosing subgraph extraction ğ‘®ğŸ ğ‘®ğ’ Netlist Babel Fish Figure 3: Architecture of our circuit link prediction architecture with SEAL. Netlists are converted into graph representations and processed by SEAL, which extracts and encloses subgraphs to generate training samples for GNN-based link prediction. We implement DGCNN as the GNN architecture. 7 3. Link prediction through GNNs In Problem 2.2, we are given a graph G containing various node types and a distinguished node v. The objective of the problem is to predict the neighborhood of v within G. This problem can be simplified by determining whether an edge should exist between v and each neighboring node u in G. The goal is to assess whether two nodes in the graph will likely be connected by an edge based on the other nodes connectivity. Therefore, this connection test can be framed as a link prediction problem. We select the SEAL framework [14], which we illustrate in Figure 3. The idea is to enclose subgraphs around the links in a graph representing positive class instances to extract training data. For a pair of nodes (x, y), an enclos- ing subgraph is defined as a subgraph that includes the h-hop neighborhood of both x and y. In practice, this method is effective for the link prediction task. This work proposes a node labeling approach denoted as Double-Radius Node Labeling (DRNL) to label nodes within the subgraphs. DRNL aims to identify the different roles of nodes while preserving structural information. We adopt the SEAL framework for link prediction in circuit graphs and train it using DRNL one-hot encoding combined with the original component node features.",
    "source": "2504.10240v2_GNN-ACLP_Graph_Neural_Networks_based_Analog_Circui.pdf",
    "length": 2154,
    "tokens": 476
  },
  {
    "text": "Cloud Serv- ing Benchmark (YCSB) suite [115], and MLPerf Storage3 [116] from real enterprise and datacenter environments. The average workload size is approximately 50000x the fast device capacity. We choose these workloads to represent diverse I O access patterns with dif- ferent read and write ratios, I O request sizes, and inter-request times. Table 4 reports the characteristics of the chosen workloads. In our evaluation, each workload runs in a separate thread. Table 4: Characteristics of the evaluated I O traces Benchmark Suite Traces Read Avg. Request Size (KB) Avg. Inter- Request Time (ğœ‡s) SYSTOR17 [113] LUN0 0.2 31.7 1163.9 LUN1 0.3 34.2 1864.1 LUN2 7.6 31.1 1418.9 LUN3 3.5 42.7 734.5 LUN4 0.5 26.3 823.1 RocksDB [114] ssd-00 79.9 108.9 66.4 ssd-01 73.5 75.1 40.7 ssd-02 79.9 7.5 3.3 ssd-03 79.9 9.5 3.5 ssd-04 79.9 7.8 3.6 YCSB [115] YCSB-B 51.3 45.9 9.3 YCSB-C 47.6 54.6 6.5 YCSB-D 55.9 36.1 8.5 YCSB-E 52.1 46.6 9.6 YCSB-F 49.5 53.1 6.6 MLPerf Storage [116] ResNet50 80.0 172.6 500.1 CosmoFlow 83.4 180.1 1023.8 To evaluate Harmonia under real-world scenarios, we generate six multi-programmed workloads by running multiple workloads concurrently (see Table 5). We choose these multi-programmed workloads based on three key factors: (1) combination of read- intensive, write-intensive and mixed workloads, (2) number of 3We generated the traces by running MLPerf applications on our evaluated system.",
    "source": "2503.20507v2_Harmonia_A_Multi-Agent_Reinforcement_Learning_Appr.pdf",
    "length": 1420,
    "tokens": 473
  },
  {
    "text": "16 1.0 1.4 1.8 2.2 Skewness 0 5 10 15 20 Latency (ms) Attn Layers Attn Comm FFN FFN Comm (a) Baseline latency with no predic- tion (interconnect NVLink). 0.6 0.7 0.8 0.9 1.0 Normalized Latency No Prediction Dist-Only Prediction Token-To- Expert Prediction (Accuracy) 0.5 0.95 0.5 0.95 0.5 0.95 0.5 0.95 1.0 1.4 1.8 2.2 Skewness 0.0 0.5 Attn Layers Attn Comm FFN FFN Comm Overhead (b) Latency of different prediction strategies and accuracies (intercon- nect NVLink). 1.0 1.4 1.8 2.2 Skewness 0 5 10 15 20 Latency (ms) Attn Layers Attn Comm FFN FFN Comm (c) Baseline latency with no predic- tion (interconnect PCIe). 0.6 0.7 0.8 0.9 1.0 Normalized Latency No Prediction Dist-Only Prediction Token-To- Expert Prediction (Accuracy) 0.5 0.95 0.5 0.95 0.5 0.95 0.5 0.95 1.0 1.4 1.8 2.2 Skewness 0.0 0.5 Attn Layers Attn Comm FFN FFN Comm Overhead (d) Latency of different prediction strategies and accuracies (intercon- nect PCIe). Figure 9: Simulated prefill latency for a single layer of Switch Transformer model [7] under different prediction strategies and interconnect types. Workload sizes and hardware configurations are the same as Figure 6. For illustration purposes, overhead 0.5 of original latency is omitted. 17",
    "source": "2506.07366v1_MoE-GPS_Guidlines_for_Prediction_Strategy_for_Dyna.pdf",
    "length": 1219,
    "tokens": 394
  },
  {
    "text": "IV. EVALUATION AND DISCUSSION To evaluate our agentic AI framework for Verilog code generation, we used the VerilogEval benchmark suite [24]. Recognizing that refinement loops require a nuanced evalu- ation, we introduced the metric which combines with Average Refinement Cycles (ARC) (details in subsequent subsections). These benchmarks standardize the assessment of syntactic correctness, functional accuracy, and iterative efficiency. A. Experimental Setup Benchmarks: VerilogEval benchmark consists of two parts: 1) VerilogEval Machine and 2) VerilogEval Human with 143 and 156 problems respectively. They all sourced from the HDLbits platform1, covering a wide range of Verilog codes from combinational circuits to complex sequential circuits. In addition, for the metric, we extend the VerilogEval platform by incorporating the average number of refinement cycles into the evaluation framework. Experimental setup: For the comparison purposes, we im- plemented the proposed design using multiple baseline LLMs including GPT-4o-mini, Claude3.5-Sonnet, CodeLlama-70b- Instruct, DeepSeek-Coder-V2-Instruct and PyraNet. We com- pared the results with other SOTA works including PyraNet [13], Origen [18], AutoVCoder [17], CodeV [14], and also with the commercial LLMs without any agents. All the exper- imental results can be found in Tables I and II. It is important to note that in all implemented designs, the Supervisor and Prompt Engineer agents consistently employ the gpt-4o-mini model, while the Verilog and Testbench Generator agents utilize different LLMs. B. Shortcoming and Solution One major challenge was selecting an appropriate evaluation metric. Traditional approaches typically rely on the metric. However, applying to architectures that em- ploy an iterative refinement loop can be misleading, since a single task may require multiple attempts before reaching a correct output.",
    "source": "2503.16514v3_VeriMind_Agentic_LLM_for_Automated_Verilog_Generat.pdf",
    "length": 1899,
    "tokens": 418
  },
  {
    "text": "For example, software architecture is often represented as ASTs, chip logic architecture as DFGs, and chip circuit architecture as CFGs. These graph data are essential for processor chip design. As a result, LPCM is specifically designed as a multimodal architecture, capable of understanding, representing, and generating graph data, enabling more effective capacities of learning and presenting processor chip domain knowledge, as shown in Figure 2 left. Specifically, the input to LPCM consists of two modalities: textual descriptions and graphical illustrations of requirement specifications. There are two critical issues in understanding and representing graph data: feature representation and feature alignment. A straightforward approach is to represent the graph data in a special textual format and concatenate it with the textual tokens before feeding it into the model. However, this approach serializes the graph s topological struc- ture, potentially causing nodes that are topologically close in the graph to be positioned far apart in the sequence, thus losing topological information of the graph data. To better preserve the graph s topological information, Graph Neural Networks (GNNs) [13] can be used to encode the graph data and generate its embedding. Contrastive learning can then be applied to align the features of the graph embedding with the corresponding text embedding. Once feature alignment is achieved, the graph embedding is concatenated with the other textual tokens and fed into LPCM. The output of LPCM also encompasses two modalities: text and graph. The text modality includes both the software and hardware code for processor chip design, while the graph modality includes generated diagrams, such as software ar- chitecture diagrams, chip circuit diagrams, and chip layouts. The process of generating a graph is closely tied to its representation. If the graph data is directly represented by a special textual format, LPCM can also directly output the graph in this format. However, this representation method may risk losing topological information, which could compromise the accuracy of the generated graphs. To better preserve the topological information of the graph, LPCM can first output the graph s embedding, which is then mapped to a graph structure using specialized graph generation models, such as diffusion models like GRAPHARM [14] or generative GNNs like GPT-GNN [15].",
    "source": "2506.05007v1_QiMeng_Fully_Automated_Hardware_and_Software_Desig.pdf",
    "length": 2426,
    "tokens": 471
  },
  {
    "text": "As shown in Figure 16, MP models have similar trends, and the optimization impact is mostly proportional to their compute-per- lookup ratio (Table 1). 8.2 Impact of Model-Specific Optimizations As discussed in Section 7.4, Ember lends itself to model-specific optimizations. Figure 18 shows that, in block-sparse attention mech- anisms (Section 2.2.2), loading highly-reused embedding blocks from L2 rather than LLC filters 67 74 of the embedding reads and 50 65 of the overall accesses, which include non-temporal loads for indexes, with larger reductions on larger block sizes with more intrinsic reuse. By directly writing data with the store streams instead of going through the core, Ember enables efficient gather operations with low resource utilization. 8.3 Comparison with Hand-optimized Code Figure 19 shows the performance of DAE code automatically gen- erated and optimized by Ember (emb-opt3) compared to hand- optimized code (ref-dae) for all model classes in Table 1. Besides all optimizations discussed in Section 7, hand-optimized code also includes low-level, CPU-specific optimizations to improve callback invocations. These CPU-specific optimizations include, for instance, (1) reordering the if-cases of multi-callback code (like the code in Figure 14d) according to their taken frequency or (2) set the val- ues of control tokens to be directly used in compute code (e.g. to increment variables). Overall, these CPU-specific optimizations primarily affect multi- callback code such as MP, SLS, and SpMM, yielding performance improvements of up to 5 , with an average geometric mean im- provement of 1 . The limited impact of these low-level optimiza- tions arises because, as discussed in Section 8.1, the optimizations introduced in Section 7 already push the architecture close to its limits, leaving little room for further gains. Nevertheless, because these low-level optimizations are highly CPU-specific, we chose not to integrate them into Ember, which is designed to provide a more general solution for a larger class of architectures. Ultimately, we believe that the optimizations presented in Sec- tion 7 are sufficient to fully unlock the potential of general DAE architectures at no programmability cost. 9 Related Work Embedding operations are becoming increasingly critical in several machine learning models, and DAE architectures a more widely adopted solution for similar irregular workloads.",
    "source": "2504.09870v1_Ember_A_Compiler_for_Efficient_Embedding_Operation.pdf",
    "length": 2431,
    "tokens": 502
  },
  {
    "text": "URL forum?id ia5XvxFUJT. Yin, X., Ni, C., and Wang, S. Multitask-based evaluation of open-source llm on software vulnerability. IEEE Transac- tions on Software Engineering, 50(11):3071 3087, 2024. doi: 10.1109 TSE.2024.3470333. Zhang, T., Ladhak, F., Durmus, E., Liang, P., McKeown, K., and Hashimoto, T. B. Benchmarking large lan- guage models for news summarization. Transactions of the Association for Computational Linguistics, 12:39 57, 2024a. doi: 10.1162 tacl a 00632. URL https: aclanthology.org 2024.tacl-1.3 . Zhang, Y., Yu, Z., Fu, Y., Wan, C., and Lin, Y. C. MG- Verilog: multi-grained dataset towards enhanced llm- assisted verilog generation. In The First IEEE Interna- tional Workshop on LLM-Aided Design (LAD 24), 2024b. Zhong, R., Du, X., Kai, S., Tang, Z., Xu, S., Zhen, H.-L., Hao, J., Xu, Q., Yuan, M., and Yan, J. Llm4eda: Emerg- ing progress in large language models for electronic de- sign automation, 2023. URL abs 2401.12224. Ziegler, M. M., Bertran, R., Buyuktosunoglu, A., and Bose, P. Machine learning techniques for taming the complexity of modern hardware design. IBM Journal of Research and Development, 61(4 5):13:1 13:14, 2017. doi: 10. 1147 JRD.2017.2721699. 14",
    "source": "2504.08852v1_ML_For_Hardware_Design_Interpretability_Challenges.pdf",
    "length": 1195,
    "tokens": 402
  },
  {
    "text": "Specifically, we measure prefill throughput and time-to-first-token (TTFT) using inputs with a 4K token length and a batch size containing 16K total tokens per NPU. To evaluate the performance under varying cache hit rates, we adjust the token reuse rate, which controls the proportion of historical KV prefixes reused. A central goal of this study is to compare EMS performance under two network configurations: one utilizing the high-bandwidth UB interconnect, and the other falling back to the slower VPC network plane for cache access. Figure 23 illustrates the performance trends as a function of the token reuse rate for these different EMS configurations. As shown in Figure 23a, there is a strong positive correlation between throughput and the reuse rate for both network configurations. For EMS with UB, increasing the reuse rate from 12.5 to 50 resulted in a 1.42 increase in prefill throughput. At a 90 reuse rate, the throughput improved by 2.28 over the baseline without EMS. This substantial improvement occurs because a higher reuse rate translates to a larger portion of the input sequence s KV cache 46 12.5 25 50 75 90 Token Reuse Rate 0 4000 8000 12000 16000 Prefill Throughput (tokens s) EMS with UB EMS with VPC Without EMS (a) Prefill throughput. 12.5 25 50 75 90 Token Reuse Rate 0 500 1000 1500 2000 2500 3000 TTFT (ms) EMS with UB EMS with VPC Without EMS (b) TTFT. Fig. 23. The overall prefill throughput and TTFT using EMS-Context Caching with different configurations. being loaded directly from the EMS cache rather than being recomputed, significantly reducing the computational load on prefill NPUs. Furthermore, when comparing the two network configurations, EMS with UB consistently outperforms EMS with VPC. Using the UB plane improves prefill throughput by up to 1.52 . This gain is directly attributable to the significantly higher bandwidth and lower latency of the UB plane, which accelerates the loading of KV cache blocks from the distributed EMS cache to the NPUs. Concurrently, TTFT significantly decreases as the token reuse rate increases, as depicted in Figure 23b.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2111,
    "tokens": 458
  },
  {
    "text": "[24] C. Chen, S. Borgeaud, G. Irving, J.-B. Lespiau, L. Sifre, and J. Jumper, Accelerating large language model decoding with speculative sam- pling, arXiv preprint arXiv:2302.01318, 2023. [25] P. Patel, E. Choukse, C. Zhang, A. Shah, I. Goiri, S. Maleki, and R. Bianchini, Splitwise: Efficient generative llm inference using phase splitting, in 2024 ACM IEEE 51st Annual International Symposium on Computer Architecture (ISCA). IEEE, 2024, pp. 118 132. [26] C. Hu, H. Huang, L. Xu, X. Chen, J. Xu, S. Chen, H. Feng, C. Wang, S. Wang, Y. Bao et al., Inference without interference: Disaggre- gate llm inference for mixed downstream workloads, arXiv preprint arXiv:2401.11181, 2024. [27] Z. Zhou, X. Ning, K. Hong, T. Fu, J. Xu, S. Li, Y. Lou, L. Wang, Z. Yuan, X. Li et al., A survey on efficient inference for large language models, arXiv preprint arXiv:2404.14294, 2024. [28] R. Qin, Z. Li, W. He, M. Zhang, Y. Wu, W. Zheng, and X. Xu, Moon- cake: A kvcache-centric disaggregated architecture for llm serving, arXiv preprint arXiv:2407.00079, 2024. [29] F. Wang, S. Fathizadan, F. Ju, K. Rowe, and N. Hofmann, Print surface thermal modeling and layer time control for large-scale additive manu- facturing, IEEE Transactions on automation science and engineering, vol. 18, no. 1, pp. 244 254, 2020. [30] B. Pang, X. Xie, Y. Song, and L. Luo, Surgery scheduling under case cancellation and surgery duration uncertainty, IEEE Transactions on Automation Science and Engineering, vol. 16, no. 1, pp. 74 86, 2018.",
    "source": "2502.15763v1_Hybrid_Offline-online_Scheduling_Method_for_Large_.pdf",
    "length": 1509,
    "tokens": 494
  },
  {
    "text": "These components were integrated into a distill- then-RL two-stage training pipeline to develop CodeV- R1 [51], a reasoning-enhanced Verilog generation LLM that is capable of thinking and test-time scaling. As shown in Table III, CodeV-R1 achieves 68.6 and 72.9 on VerilogEval v2 and RTLLM v2, respectively, outperforming previous state-of-the-art models by 12 to 21 , and matching or even exceeding the performance of the 671B DeepSeek-R1. C. Automated OS Configuration Optimization The operating systems (OS) act as a crucial bridge be- tween processors and higher-level software, playing a vital role in maximizing the performance of the processor chips. The widely used open-source OS Linux, designed to meet the diverse requirements of different application scenarios and processors, consists of over 20 million lines of code contributed by developers around the world, making it one of the most complex software projects to date. This vast codebase presents significant opportunities for optimization, and there is a pressing need to tailor or optimize the OS for specific processors and application scenarios to fully unleash the potential of the entire computer system. However, customizing or optimizing an OS involves three main challenges. First, the complexity of the task is extremely high. Even just optimizing the OS kernel involves over 15,000 interdependent configuration options [52], [53], which are beyond the capability of conventional optimization methods. Second, the cost of evaluating each configuration is high, as compiling, installing, and testing the OS can take up to 1 to 2 hours [54], which limits the feasibility of data-driven methods like neural networks. Third, the optimization process is highly sensitive, where even a small error could prevent the OS from booting properly and make debugging extremely difficult. To address these challenges, we leverage the LLM-guided performance feedback loop from our Software Design Agent to develop an automated OS configuration optimization method, AutoOS [26], which can generate optimized kernel configurations without manual intervention and surpass the performance achieved by hardware vendors manual opti- mizations. To achieve this, we introduce an observe-prune- propose-act-correct feedback loop, which leverages the prior knowledge embedded in LLMs to eliminate irrelevant configu- ration options that do not contribute to performance optimiza- tion and might cause booting issues, significantly reducing the search space for customization.",
    "source": "2506.05007v1_QiMeng_Fully_Automated_Hardware_and_Software_Desig.pdf",
    "length": 2527,
    "tokens": 499
  },
  {
    "text": "module beh_vlog_ff_ce_clr_v8_2 ( Q, C, CE, CLR, D ); parameter INIT 0; localparam FLOP_DELAY 100; output Q; input C; input CE; input CLR; input D; reg Q; initial Q 1'b0; always (posedge C) if (CLR) Q 1'b0; else if (CE) Q FLOP_DELAY D; endmodule module pcie_7x_v1_3_fast_cfg_init_cntr ( parameter PATTERN_WIDTH 8, parameter INIT_PATTERN 8'hA5, parameter TCQ 1 ) ( input clk, input rst, output reg [PATTERN_WIDTH-1:0] pattern_o ); always (posedge clk) begin if(rst) begin pattern_o TCQ {PATTERN_WIDTH{1'b0}}; end else begin if(pattern_o ! INIT_PATTERN) begin pattern_o TCQ pattern_o 1; end end end endmodule The module pcie_7x_v1_3_fast_cfg_init_cntr implements a counter that operates on the rising edge of the clk signal, resetting pattern_o to zeros on rst assertion, incrementing it by 1 on deassertion if not equal to INIT_PATTERN , with a delay specified by TCQ . module Event_Pulse( input in, input clk, output rising_edge, output falling_edge, output both_edges ); reg [1:0] reg_i 2'b0; assign rising_edge ( reg_i[1]) reg_i[0]; assign falling_edge reg_i[1] ( reg_i[0]); assign both_edges (( reg_i[1]) reg_i[0]) (reg_i[1] ( reg_i[0])); always (posedge clk) begin reg_i[0] in; reg_i[1] reg_i[0]; end endmodule The module Event_Pulse implements a detector for rising and falling edge transitions of the input signal, generating separate outputs for rising edges, falling edges, and both edges based on the sampled input signal at the rising clock edge.",
    "source": "2502.15832v1_DeepRTL_Bridging_Verilog_Understanding_and_Generat.pdf",
    "length": 1455,
    "tokens": 508
  },
  {
    "text": "Recomputation is a viable strategy for short KV caches especially when cache reuse is limited. VI. Related Work Prior works leverage the predictability of DNN training iterations [86], [87], [88], [89], [90] to model training perfor- mance. Several simulation frameworks have been proposed to model LLM systems. LLMCompass[44] and GenZ[45] provide detailed modeling capabilities for single-client configurations. Vidur [14] supports multi-client simulation but assumes client homogeneity and is restricted to modeling existing system configurations. It lacks support for heterogeneous client setups, disaggregated hardware for prefill and decode stages, or specu- lative scenarios involving future system architectures. Similarly, LLMServingSim [13] does not support chunked prefill or disaggregated batching, which are increasingly common in production-grade LLM deployments. Splitwise-sim [15] models three pools for hardware clients representing prefill, decode and mixed pool. Similar to LLMServingsim it doesn t model chunked batching. Consequently, both Vidur, LLMServingSim and Splitwise-sim fall short in modeling advanced, multi- stage LLM inference pipelines. In contrast, HERMES is the first simulator designed to support end-to-end modeling of real-world LLM inference pipelines across heterogeneous HW configurations. VII. Conclusion Modern LLM inference pipelines demand simulators that can model complex, heterogeneous workflows something existing tools fail to provide. We present HERMES a high- fidelity, event-driven simulation framework that captures the full spectrum of inference stages across diverse hardware setups. HERMES supports flexible batching, multi-model execution, and detailed memory modeling, enabling accurate evaluation of architectural trade-offs. Through case studies, we show how HERMES offers actionable insights into KV cache design, batching strategies, and hardware-software co- design. Looking ahead, HERMES can be extended for exploring optimal configuration of future chips, developing new adaptive schedulers and simulating future multi-agent LLM deployments. VIII. Acknowledgment This work was supported in part by CoCoSys, one of seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA.",
    "source": "2504.09775v3_Understanding_and_Optimizing_Multi-Stage_AI_Infere.pdf",
    "length": 2277,
    "tokens": 476
  },
  {
    "text": "0 ] \" R e t r i e v a l i n f o : USED_PORT: remain 0 0 32 0 OUTPUT NODEFVAL \" remain [ 3 1 . . 0 ] \" R e t r i e v a l i n f o : CONNECT: aclr 0 0 0 0 a c l r 0 0 0 0 R e t r i e v a l i n f o : CONNECT: clock 0 0 0 0 clock 0 0 0 0 R e t r i e v a l i n f o : CONNECT: denom 0 0 32 0 denom 0 0 32 0 R e t r i e v a l i n f o : CONNECT: numer 0 0 32 0 numer 0 0 32 0 R e t r i e v a l i n f o : CONNECT: q u o t i e n t 0 0 32 0 quotient 0 0 32 0 R e t r i e v a l i n f o : CONNECT: remain 0 0 32 0 remain 0 0 32 0 R e t r i e v a l i n f o : GEN_FILE : TYPE_NORMAL div_unsigned . v TRUE R e t r i e v a l i n f o : GEN_FILE : TYPE_NORMAL div_unsigned . inc TRUE R e t r i e v a l i n f o : GEN_FILE : TYPE_NORMAL div_unsigned . cmp TRUE R e t r i e v a l i n f o : GEN_FILE : TYPE_NORMAL div_unsigned . b s f TRUE R e t r i e v a l i n f o : GEN_FILE : TYPE_NORMAL d i v _ u n s i g n e d _ i n s t . v TRUE R e t r i e v a l i n f o : GEN_FILE : TYPE_NORMAL div_unsigned_bb . v TRUE R e t r i e v a l i n f o : GEN_FILE : TYPE_NORMAL div_unsigned_syn . v TRUE R e t r i e v a l i n f o : LIB_FILE : lpm Summarized Problem for The Code You need to design a Verilog module that performs unsigned division on two 32-bit numbers.",
    "source": "2505.24183v2_CodeV-R1_Reasoning-Enhanced_Verilog_Generation.pdf",
    "length": 1228,
    "tokens": 484
  },
  {
    "text": "Despite optimizations in logic regions such as OPT1, OPT2, and OPT3, the increase in clock network power consumption at high frequencies exceeds the increase in combined logic power consumption. Therefore, when the frequency increases to a certain threshold, the energy efficiency will decrease. Designers can reduce power consumption at high frequen- cies by minimizing the register area within the logic design. This consideration is reflected in designs such as OPT4C and OPT4E, which reduces the need for input and output DFFs while balancing the logic and DFFs regions. Ultimately, OPT4E enables significant computational density while main- taining energy efficiency, as demonstrated in Figure 9(B) and (D). C. Array-level comparison with state of the art 1) Configuration setup: In the experimental deployment of the PE array, since EDA tools require constraint files to be read before synthesis, it is necessary to use predefined delays to constrain the clock. To this end, we thoroughly tested the frequency range for each PE design, as shown in Figure 9, aiming to determine the optimal clock frequency for each configuration (achieving better energy and area efficiency). From a detailed observation of Figure 9(A), the frequency range of the TPU-like MAC spans from 500 MHz to 1.5 GHz. Beyond 1.5 GHz, timing violations occur, preventing normal operation. Only design 5 (OPT4C) can reach 3.0 GHz, but higher frequencies do not always lead to better synthesis performance. As shown in Figures 9(C) and 9(D), the TPU-like MAC- based design achieves peak area and energy efficiency at 1.0 GHz. The frequency limit of the PE using the OPT1 design is 2.0 GHz, but its synthesis performance is optimal at 1.5 GHz. Similarly, we identified the optimal frequency points for OPT3, OPT4C, and OPT4E, which were then used as clock constraints for synthesizing and testing the TPEs. 2) Comparision with classical TPE architecture: As de- picted in Table VII, we implement the OPT1 on conventional architectures such as TPU (systolic array), Ascend (3D-Cube), Trapezoid (multiplier-adder tree), and FlexFlow (2D-Matrix). For FlexFlow (2D-Matrix), OPT2 is employed.",
    "source": "2503.06342v1_Exploring_the_Performance_Improvement_of_Tensor_Pr.pdf",
    "length": 2163,
    "tokens": 484
  },
  {
    "text": "63 330 int4 Prefetch 1 0x78E26A27 0xA4D89ED9 Aggregation Engine (AGE): performing permutation-invariant aggregation functions over all neighbours of a node through a Network-on-Chip architecture. Aggregation Buffer (ABF): storage element containing aggregated feature embeddings generated by the AGE. Feature Transformation Engine (FTE): computing the updated feature embeddings for each node by performing a matrix multiplication between weights in the Weight Bank and aggregation results in the Aggregation Buffer. 3.1 EVENT-DRIVEN PROGRAMMING THROUGH THE NODE INSTRUCTION DECODER Communication between AMPLE and the host is handled by the Node Instruction Decoder (NID), which is a memory-mapped register bank comprised of a configurable number of nodeslots. As shown in Table 2, each nodeslot contains the information required to perform a node s aggregation and transformation steps, and a state machine is maintained indicating each node s state. Algorithm 1 shows how work can be offloaded by the host, which runs concurrently with the accelerator. First, the NID is programmed with a number of global and layer-wise parameters, including node feature counts and aggregation functions. Subsequently, the host programs the nodeslots and updates values in the mask available nodeslots {0, 1}n where n is the number of nodeslots. While a node is programmed, the accelerator performs aggregation and transformation over previously-programmed nodes. The available nodeslots mask is then deasserted independently by the accelerator when the computation is finished. Thus, the accelerator supports a node-wise, event-driven computation paradigm. Note that 1 and 0 indicate a mask full of ones and zeros, respectively. Algorithm 1 Host programming pseudocode Require: global parameters P, layers L, nodes V nid register bank.global parameters P available nodeslots 1 for layer in L do nid register bank.layer config layer layer.prefetch layer weights() while V do if available nodeslots !",
    "source": "2502.21196v1_AMPLE_Event-Driven_Accelerator_for_Mixed-Precision.pdf",
    "length": 1987,
    "tokens": 425
  },
  {
    "text": "In contrast, our method employs a convolutional autoencoder that compresses both spatial and temporal domains, resulting in a higher compression ratio. Additionally, [54] utilizes 16-bit input data samples and 10-bit compressed outputs, achieving an overall CR of 19.2 (96 16 (8 10)). Their design was implemented using 180-nm CMOS process technology with an area of 0.002 mm2 per channel. To benchmark our results, we use the same dataset [53] as [54], and it s evident that our SNDR and R2 scores are superior, even at a high CR of 150 with a dynamic xii power of 32.39 ÂµW per channel for the DS-CAE1 model at 2 MHz. Additionally, several FPGA-based implementations have been proposed in the literature. Turcotte et al. [62] employs a four-level discrete wavelet transform (DWT) to compress neural data. Their system, comprising a spike detection core, threshold estimation core, and wavelet compression, was im- plemented on a Xilinx Spartan-6 FPGA. This design achieves a CR of 4.17 with an SNDR of 17 dB consuming power of 5 mW per channel. Shrivastwa et al. [63] utilize a combination of compressed sensing and neural networks to compress and reconstruct ECoG signals, respectively. Their design, imple- mented on a Xilinx Virtex-7 FPGA with 285.5k LUTs and 22.18k registers, achieves a CR of up to 4. In practical BCI applications where subsequent decoding and processing can reasonably tolerate reconstruction and wireless transmission errors, employing lossy methods such as CAE, AE, or CS with a higher CR presents a more feasible approach. Specifically, the proposed CAE-based compression scheme achieves a significantly higher compression ratio than previous methods due to compression in both spatial and temporal domains while maintaining good SNDR and R2 scores. On the contrary, lossless compression schemes involve reducing the dynamic range of neural signals and encoding them using Huffman coding. While this approach reconstructs the original signal accurately, it typically achieves a very low compression ratio. Additionally, various spike compression methods dedicated to high-density brain-implantable microsystems have been proposed [32] [35], [64]. Chen et al.",
    "source": "2504.06996v1_Neural_Signal_Compression_using_RAMAN_tinyML_Accel.pdf",
    "length": 2186,
    "tokens": 488
  },
  {
    "text": "117 126. [63] S. Liu, H. Fan, and W. Luk, Design of fully spectral cnns for efficient fpga-based acceleration, IEEE Transactions on Neural Networks and Learning Systems, 2022. [64] S. O. Ayat, M. Khalil-Hani, A. A.-H. Ab Rahman, and H. Abdellatef, Spectral-based convolutional neural network without multiple spatial- frequency domain switchings, Neurocomputing, vol. 364, pp. 152 167, 2019. [65] T. Watanabe and D. F. Wolf, Image classification in frequency domain with 2srelu: a second harmonics superposition activation function, Applied Soft Computing, vol. 112, p. 107851, 2021. [66] A. Lavin and S. Gray, Fast algorithms for convolutional neural networks, in Proc. CVPR, 2016, pp. 4013 4021. [67] Y. Liang, L. Lu, Q. Xiao, and S. Yan, Evaluating fast algorithms for convolutional neural networks on fpgas, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 39, no. 4, pp. 857 870, 2019. [68] J. Yepez and S.-B. Ko, Stride 2 1-D, 2-D, and 3-D Winograd for convolutional neural networks, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 28, no. 4, pp. 853 863, 2020. [69] M. Yang, S. Cao, W. Zhang, Y. Li, and Z. Jiang, Loop-tiling based compiling optimization for cnn accelerators, in 2023 IEEE 15th International Conference on ASIC (ASICON), 2023, pp. 1 4. [70] H. Huang, X. Hu, X. Li, and X. Xiong, An efficient loop tiling framework for convolutional neural network inference accelerators, IET circuits, devices systems, vol. 16, no. 1, pp. 116 123, 2022. [71] S. Liu et al., Optimizing cnn-based segmentation with deeply cus- tomized convolutional and deconvolutional architectures on fpga, ACM Trans.",
    "source": "2505.13461v1_FPGA-based_Acceleration_for_Convolutional_Neural_N.pdf",
    "length": 1665,
    "tokens": 500
  },
  {
    "text": "YouTube, 2024. Available at: be gofI47kfD28?t 685 [Accessed: 10 24 2024]. [31] Reetuparna Das, Onur Mutlu, Thomas Moscibroda, and Chita R Das. Aergia: Exploiting packet latency slack in on-chip networks. ACM SIGARCH computer architecture news, 38(3):106 116, 2010. [32] Sarkar Snigdha Sarathi Das, Arzoo Katiyar, Rebecca Passonneau, and Rui Zhang. CONTaiNER: Few- shot named entity recognition via contrastive learning. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6338 6353, Dublin, Ireland, May 2022. Association for Com- putational Linguistics. [33] Sarkar Snigdha Sarathi Das, Ranran Haoran Zhang, Peng Shi, Wenpeng Yin, and Rui Zhang. Unified low-resource sequence labeling by sample-aware dynamic sparse finetuning. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natu- ral Language Processing, pages 6998 7010, Singapore, December 2023. Association for Computational Linguistics. [34] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024. [35] Shichen Dong, Wen Cheng, Jiayu Qin, and Wei Wang. Qaq: Quality adaptive quantization for llm kv cache, 2024. [36] Xiaobin Dong, Kurt Keutzer, and Cong Zhang. Ramulator: A cycle accurate dram simulator. In Proceedings of the 22nd ACM International Symposium on High-Performance Parallel and Distributed Com- puting, pages 151 160. ACM, 2014.",
    "source": "NSF_LLM_Medium_Proposal.pdf",
    "length": 1623,
    "tokens": 458
  },
  {
    "text": "As shown in Fig. 6b b , similar to the Inter- Holo pipeline, the additional pose estimation step also sits between the inputs and the original hologram processing, and thus has to be efficient without introducing much overhead. Our profiling on the edge GPU prototype [36] shows that Kimera-VIO takes, on av- erage, 13.75ms latency to execute, which is less than 1 of the total hologram processing time. Therefore, the overhead introduced due to the additional pose estimation step is negligible compared to the baseline latency, thereby opening up opportunities for significant energy savings and performance speedup as demonstrated later in Sec. 5. With the help of the pose estimation, now the AR hologram pipeline has the knowledge about the range size of each object as well as its relative distance from the user. Next, as shown in Algo. 3, for each of the objects, a corresponding approximation factor (Î²) can be determined based on these insights. Similarly, the original hologram engine can still be reused without any reprogramming, except for the first argument, i.e., the number of depth planes for this particular object, as shown in Line 5 of Algo. 3. Inter-Intra-Holo: It is to be noted that, when the user eye track- ing and pose estimation are available simultaneously for hologram processing, the Inter-Holo and Intra-Holo schemes can be both ap- plied to achieve maximum amount of energy savings and perfor- mance benefits. In this paper, we refer to this combined scheme as Inter-Intra-Holo. In this scheme, we first identify the objects in- side outside the RoF (Inter-Holo), and then approximate each of them based on its shape and distance (Intra-Holo). Note that since the other option first Intra-Holo, then Inter-Holo is theoreti- cally identical to the proposed Inter-Intra-Holo, we skip its detailed discussion due to space limitation. 4.5 Design and Implementation Optimization Choices: Our main goal in this paper is to reduce the amount of hologram computation by appropriate approxima- tion, in order to speed up hologram processing, to satisfy the real- time requirement as well as to reduce the energy consumption and prolong the battery life of the AR device, while maintaining the QoS.",
    "source": "HoloAR.pdf",
    "length": 2221,
    "tokens": 490
  },
  {
    "text": "To the best of our knowledge, there is no model that has been specifically trained for RTL embedding, despite its critical role in optimiz- ing hardware design workflows. DeepRTL2 is the first model explicitly designed for RTL embedding- based tasks, outperforming general-purpose text embedding models on our benchmarks. 3 Dataset Previous research has predominantly focused on generation-based tasks, resulting in a notable gap in available datasets for the embedding-based tasks considered in this paper. Moreover, the availability of RTL code is limited even for generation-based tasks, due to the proprietary nature of hardware designs. To fill this gap, we have curated a compre- hensive dataset tailored to support both generation- and embedding-based tasks at the RTL stage. Fur- thermore, we have established new benchmarks specifically for the embedding-based tasks, which have been largely neglected in previous work. 3.1 Generation-Based Tasks 3.1.1 RTL Code Generation RTL code generation involves automatically syn- thesizing RTL code from user-defined natural lan- guage descriptions, streamlining hardware design and enabling a more accessible development pro- cess. To construct a high-quality dataset for this task, we follow the data construction pipeline proposed in DeepRTL (Liu et al., 2025), given its demonstrated effectiveness in generation-based tasks. The process begins by collecting .v files from GitHub1 using the keyword Verilog. Each file is then segmented into individual Verilog mod- ules, with each module representing a distinct func- tional unit. To ensure dataset quality and reduce redundancy, we remove modules that are predomi- nantly composed of comments or lack structurally complete module and endmodule declarations. Ad- ditionally, we apply MinHash and Jaccard similar- ity metrics (Yan et al., 2017) to eliminate duplicates. To further refine the dataset, we employ the Stagira Verilog parser (Chen et al., 2023) to filter out mod- 1 Original Code Refined Code Specification Line Comment Commented Code Functional Description Clean Add Comment Extract Summarize Distill User Query Rephrase Figure 2: The annotation process for the RTL code generation understanding dataset.",
    "source": "2506.15697v1_DeepRTL2_A_Versatile_Model_for_RTL-Related_Tasks.pdf",
    "length": 2220,
    "tokens": 469
  },
  {
    "text": "[Problem] Make a decade counter that counts 1 through 10, inclusive. The reset input is active high synchronous, and should reset the counter to 1. This Verilog module, named top_module , has the interface designed as follows: Signal Name Direction Width Description - - clk Input 1 Clock signal that triggers the counter on its rising edge reset Input 1 Active-high synchronous reset signal to initialize the counter q Output 4 4-bit register output representing the current count (1-10) demonstration4 Please write a description about the code snippet and a problem such that the given code snippet is exactly the answer to this problem. [The Code Snippet] verilog module top_module ( input clk , input r e s e t , output reg [ 4 : 0 ] q ) ; l o g i c [ 4 : 0 ] q_next ; always ( q ) begin q_next q [ 4 : 1 ] ; q_next [ 4 ] q [ 0 ] ; q_next [ 2 ] q [ 0 ] ; end always ( posedge clk ) begin i f ( r e s e t ) q 5 h1 ; e l s e q q_next ; end endmodule [Description] The top module has 3 inputs and 1 output, where the inputs are clk, reset, and output is q. The module has 2 always blocks to define the state transition of q and the logic description. The state transitions are defined in the first always block, which is triggered when q changes. In the first always block, q_next is assigned with q[4:1], which is the value of q except the LSB bit. Then, q_next[4] is assigned with q[0], which is the LSB bit. Lastly, q_next[2] is xored with q[0]. The second always block is triggered at positive edge of clk. If reset is active, q is assigned with 5 h1, which is the reset value. If reset is inactive, q is assigned with q_next, which is the state transition. The port connections of instantiated modules are shown above. The module takes clk, reset, and q as input. q is a 5 bit output, which is assigned with 5 h1 at reset and q_next at positive edge of clk.",
    "source": "2505.24183v2_CodeV-R1_Reasoning-Enhanced_Verilog_Generation.pdf",
    "length": 1863,
    "tokens": 484
  },
  {
    "text": "If an operation needs to be performed on a certain variable, the corresponding nodes will be connected through some directed edges. 3.3 Graph Neural Network A simple graph neural network extracts the features of each node during the training process and aggregates the messages from its neighbors to update the node features. Specifically, at GNN s l-th layer, node v s feature representation hl v will be updated by aggregating (AGG) node v s feature representation hl 1 v in the previous layer and all the neighbor messages hl 1 w , where w N(v) and N(v) is a neighbor set of node v. To make this process learnable, the result obtained above needs to be multiplied by the learnable weight matrix W l 1. hl v Ïƒ W l 1hl 1 v W lÏˆ hl 1 w , w N(v) (2) where Ïƒ is a activation function to provide non-linearity and Ïˆ is aggregation function like mean, W l 1 and W l are learnable matrices. 4 Running Title for Header 3.4 Gumbel-softmax Estimator In CoGNNs [1], the action network dynamically adjusts the node states based on task-specific characteristics, enabling adaptive reconstruction of the computational graph topology. Specifically, the action network performs a node-level classification task to predict discrete state assignments, where the optimal state is selected via argmax over the predicted probabilities. However, since node states follow a categorical distribution, this prediction task is inherently non- differentiable. To address this, CoGNNs employs the Gumbel-Softmax estimator [31], which provides a continuous relaxation of the discrete state selection process, thereby enabling end-to-end gradient-based optimization. This approach ensures both the dynamic adaptability of the graph structure and the trainability of the overall framework. This estimator approximates the stochastic action sampling in a differentiable and continuous manner. We consider a set of actions Î¥, which can be represented as a probability vector p R Î¥ and the vector p stores the probability values of different actions. Let q be a categorical variable with class probabilities p1, p2, ..., p Î¥ and we assume that the categorical samples are encoded as k-dimensional one-hot vectors.",
    "source": "2504.19649v2_Intelligent4DSE_Optimizing_High-Level_Synthesis_De.pdf",
    "length": 2181,
    "tokens": 477
  },
  {
    "text": "14 0.100 0.075 0.050 0.025 0.000 0.025 0.050 0.075 0.100 Memory Usage Difference ( ) with Std Dev 38 34 30 26 40 1 27 24 37 14 13 9 29 39 12 10 11 4 19 5 8 28 2 16 33 3 22 7 36 31 17 6 32 25 23 21 18 File Less memory than Ground Truth More memory than Ground Truth -0.0800 0.01 -0.0745 0.01 -0.0587 0.02 -0.0480 0.01 -0.0427 0.01 -0.0427 0.01 -0.0426 0.02 -0.0162 0.02 -0.0160 0.02 -0.0107 0.02 -0.0107 0.01 -0.0106 0.01 -0.00534 0.01 -0.00534 0.01 -0.00318 0.01 0.00457 0.01 0.00514 0.01 0.00527 0.01 0.00533 0.01 0.0105 0.01 0.0128 0.01 0.0160 0.01 0.0206 0.01 0.0213 0.01 0.0320 0.01 0.0340 0.01 0.0376 0.01 0.0479 0.01 0.0534 0.01 0.0534 0.02 0.0534 0.01 0.0543 0.01 0.0586 0.01 0.0640 0.01 0.0641 0.02 0.0748 0.01 0.0801 0.01 4 2 0 2 4 6 Execution Time Difference ( ) with Std Dev 3 37 29 22 5 36 4 12 40 2 14 18 25 7 26 9 32 8 19 11 39 34 10 38 27 33 1 23 17 24 31 28 30 13 16 21 6 File Faster than Ground Truth Slower than Ground Truth -3.14 0.58 -2.80 0.60 -2.03 0.77 -1.93 0.35 -1.53 0.62 -1.52 0.67 -1.27 0.65 -1.18 0.53 -1.01 0.70 -0.51 0.70 -0.48 0.85 -0.26 0.66 -0.26 0.68 -0.25 0.69 0.00 0.73 0.15 0.19 0.22 0.63 0.23 0.09 0.26 0.60 0.48 0.77 0.50 0.62 0.53 0.43 1.18 0.62 1.31 0.72 1.55 0.66 1.81 0.54 1.81 0.62 2.06 0.67 2.07 0.65 2.09 0.60 2.17 0.61 2.55 0.73 2.58 0.71 2.70 0.88 3.08 0.75 3.71 0.69 4.95 0.57 Figure 11: Comparison of memory usage (left) and execution time (right) between predicted and ground truth HIP programs, measured via compilation and runtime profiling. A.3 CASS Domain Coverage To obtain the domain-level breakdown shown in Figure 3, we developed a static analysis pipeline that categorizes each source file based on its content. The classification is performed by matching the file s text against curated sets of domain-specific keywords corresponding to seven high-level categories: general compute, simulation, data structure, machine learning, graphics, cryptography, and scientific computing.",
    "source": "2505.16968v3_CASS_Nvidia_to_AMD_Transpilation_with_Data_Models_.pdf",
    "length": 1940,
    "tokens": 769
  },
  {
    "text": "2.2.1 Classic Optimization Algorithms Algorithms based on gra- dient descent are computationally efficient and are often used for problems with differentiable cost functions [17]. However, the al- gorithms are limited by potential convergence to local minima, especially in non-convex design spaces. Optimization utilizing heuristic-based evolution algorithms, in- cluding simulated annealing, particle swarm optimization, and ge- netic algorithms [18], allow for a global exploration of the design space. Genetic algorithms apply principles of evolution to itera- tively refine populations of design candidates through crossover and mutation operations [18]. 2.2.2 Surrogate-Assisted Optimization Algorithms that perform surrogate-assisted optimization, including Bayesian optimization (BO) [19, 20], more efficiently explore the design space by utilizing surrogate models like Gaussian processes to approximate expensive cost functions. The search process, therefore, proceeds with fewer costly evaluations of the design space. Reinforcement learning (RL) [15, 21] has been explored for opti- mization in a sequentially executed decision-making flow. In RL, an agent iteratively interacts with the circuit simulation environ- ment and refines actions to maximize cumulative rewards based on results returned from the simulation framework. Emerging ML-AI Techniques for Analog and RF EDA , 2.3 Large Language Models for Analog EDA Since 2023, large language models (LLMs) have emerged as promis- ing tools to address the challenges in design [22 24]. LLMs excel in processing unstructured inputs, such as textual specifications, and translating high-level requirements into actionable design strate- gies. LLMs have been applied to recognize patterns in circuit data, predict relationships between parameters and performance, gen- erate topologies, and derive sizing specifications based on prior knowledge [22 24]. In addition, LLMs integrate with optimization algorithms including BO and RL to iteratively determine and refine solutions that meet design objectives [22, 23]. LLMs also provide an intuitive and accessible interface, lowering the barrier to entry for EDA tool users. 3 Data-Driven and Heuristic Approaches for Analog Circuit Synthesis and Physical Design Analog and RF circuit synthesis and physical design involve a di- verse set of tasks, which include defining design constraints, gener- ating topologies, modeling devices, sizing circuits, and optimizing layouts.",
    "source": "2506.00007v1_Emerging_ML-AI_Techniques_for_Analog_and_RF_EDA.pdf",
    "length": 2485,
    "tokens": 496
  },
  {
    "text": "Current approaches like Vaire Computing's resonator- based design are incompatible with ZettaLith's density requirements due to large physical footprints (e.g., 34 Î¼m diameter resonator). Should breakthroughs make reversible logic dense and practical, ZettaLith's modular SLD approach could potentially integrate it, but this is considered a long-term prospect. 44 25.7 Transition metal dichalcogenides (TMDs) TMDs and other 2D materials beyond graphene represent another active research frontier. Commercial viability for complex logic is uncertain, likely post-2030. Seminal work includes (Radisavljevic et al., 2011), with a recent perspective in (Li et al., 2024). 25.8 Tunneling field-effect transistors (TFETs) TFETs operate based on quantum mechanical tunneling, allowing them to potentially achieve subthreshold slopes steeper than the 60 mV decade Boltzmann limit of MOSFETs. TFETs could offer substantial power savings if ON-current and integration challenges are resolved. Foundational work includes (Zhang et al., 2006), with recent III-V analysis in (Chen et al., 2024). 25.9 Superconducting electronics Logic families like Rapid Single Flux Quantum (RSFQ) operate using quantized magnetic flux pulses. Requires cryogenic cooling (typically 4K), adding significant system complexity, cost, and infrastructure overhead. Furthermore, cryogenic cooling is fundamentally incompatible with 2-PIC cooling used in JETSTREAM, and the entire packaging, power supply, and cooling technologies of ZettaLith would need to be re-imagined. Seminal work: (Likharev and Semenov, 1991); recent review: (Shibata et al., 2023). 25.10 Graphene transistors While graphene exhibits exceptional electron mobility, its lack of a natural bandgap makes it inherently difficult to use for conventional digital logic (achieving a high ON OFF current ratio is problematic). Graphene is currently more promising for analog RF applications or as an advanced interconnect material (replacing copper) rather than as a direct replacement for silicon in ZettaLith's logic PEs. Foundational work: (Schwierz, 2010); recent perspective: (Mak et al., 2024).",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2131,
    "tokens": 478
  },
  {
    "text": "However, this model relies on the range of input values from all convolu- tional layers, limiting its applicability in real-time scenarios due to the computational overhead of analyzing all layers. B. Explainable Artificial Intelligence Neural networks are highly complex and often considered black-box models because their inner workings are not readily interpretable by humans. XAI bridges this gap by providing insights, explanations, and interpretability to the functioning of neural networks. A comprehensive survey of existing XAI techniques can be found in [8], most of which are related to explaining deep learning models, known as a post-hoc explana- tion. Most of the algorithms used in post-hoc explanations can be grouped into gradient and perturbation-based approaches. Several tools are available for post-hoc explanations through calculating feature or neuron importance in neural networks, including LIME, Captum, and SHAPly [19] [21] etc. The Captum tool, which we use in this paper, supports the identi- fication of neuron importance in DNN models. This capability distinguishes it from LIME and SHAPly, which primarily focus on identifying feature importance in DNN models. C. Multi-pod approximate systolic array accelerator A monolithic systolic array employs a two-dimensional grid of multiply-and-accumulate (MAC) units, which process the data from the neural network layers in a parallel fashion. How- ever, such a grid may not be fully utilized when confronted with small layers. Indeed, a significant performance disparity among layers may lead to wait times for processing the computationally intensive layers. Modern multi-pod systolic array accelerators like Google TPU v3 and v4 [12] address these challenges by clustering a few coarse-grain systolic array pods connected via shared memory on a single die. They partition the weight matrix W into r c tiles matching the rows r and columns c of the systolic array pods. Yet, the activation matrix X is partitioned with a size of r along the second dimension only. Yuzuguler et al. proposed full data- level parallelism through partitioning X into r r tiles [22]. Specifically, the weight and activation matrices are mapped to the systolic array pods such that their MAC units perform xij wjk yimk yijk in every time slice.",
    "source": "2503.16583v1_Explainable_AI-Guided_Efficient_Approximate_DNN_Ge.pdf",
    "length": 2301,
    "tokens": 487
  },
  {
    "text": "3, is responsible for generating token embed- dings, positional encodings, and segment embeddings using learned dictionaries. These embeddings are then summed to produce the final input encoding fed into the model. The total parameter count for the embedder, Wemb, is calculated as the sum of the sizes of the token embedding matrix (v d), the positional embedding matrix (â„“ d), and segment embedding matrix (2d): Wemb d (v â„“ 2), (1) where v is the vocabulary size, â„“is the sequence length, and d is the embedding dimension. The maximum activation size, Aemb, results from storing token, positional, and segment embeddings as matrices of size â„“ d during inference. These embeddings are summed in pairs, leading to: Aemb 2d â„“. (2) The embedding operations required to compute the output of this layer involve â„“ (4d 2) 2d memory accesses and â„“ 2d summations. Attention The standard Attention Mechanism [42] allows models to selectively focus on the most relevant parts of an input sequence.Initially designed for machine translation, attention assigns vary- ing weights to tokens based on their relevance to the task or context, enabling models to capture dependencies between distant words. The self-attention variant computes relationships within a sequence by enabling each token to attend to all others, creating contextualized representations that encode both local and global depen- dencies. The input is processed through three fully connected layers to produce the Query, Key, and Value matrices, each with size d2. This step generates an activation size of 4 d â„“and requires: i) 6â„“ d2 memory accesses, ii) 3â„“ d2 summations, and iii) 3â„“ d2 multiplications. The Query and Key matrices are then multiplied to compute a Weight matrix of size â„“ â„“for each of the h attention heads. Due to the quadratic growth in the Weight matrix, the context length has a significant impact on activation size, which increases to 3d â„“ â„“2 h. This step also adds: i) 2â„“2 d h memory accesses, ii) â„“2 d h summations, and iii) â„“2 d h multiplications.",
    "source": "2502.10001v1_EmbBERT-Q_Breaking_Memory_Barriers_in_Embedded_NLP.pdf",
    "length": 2031,
    "tokens": 501
  },
  {
    "text": "Tesla P100 GPU Accelerator. data-center tesla-p100 , 2016. Accessed: 2025-01-29. [7] NVIDIA Corporation. Tesla V100 GPU Accelerator Datasheet. content technologies volta pdf tesla-volta-v100-datasheet-letter-fnl-web.pdf, 2017. Accessed: 2025-01-29. [8] NVIDIA Corporation. NVIDIA T4 Virtualization Datasheet. com content dam en-zz Solutions design-visualization solutions resources documents1 Datasheet_NVIDIA_T4_Virtualization.pdf, 2021. Accessed: 2025-01-29. [9] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024. [10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 14 [11] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019. [12] Raspberry Pi Foundation. Raspberry Pi 4 Model B. products raspberry-pi-4-model-b , 2019. Accessed: 2025-01-29. [13] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. Advances in neural information processing systems, 28, 2015. [14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.",
    "source": "2502.08141v1_LowRA_Accurate_and_Efficient_LoRA_Fine-Tuning_of_L.pdf",
    "length": 1558,
    "tokens": 487
  },
  {
    "text": "NeuroSim V1.5 provides an integrated framework to evaluate these designs through two main com- ponents shown in Fig. 1 working jointly: 1) A behavioral simulator that maps and quantizes neu- ral networks for ACIM implementation and evaluates inference accuracy while accounting for hardware non- idealities. 2) A hardware analyzer that estimates system-level energy, latency, and area through detailed circuit models of array peripherals and interconnects. In this update, our first contribution is a comprehensive re-design of the behavioral simulator. Second, we update the hardware analyzer to support the emerging nvCap-based CIM, complete with circuit models for the new charge- domain compute mode. In the behavioral simulator, we have fundamentally enhanced how we model compute-in-memory operations for accuracy estimation. A key approach is that we leverage pre-characterized statistical models derived from SPICE simulations or silicon measurements to ensure ac- curacy, while utilizing GPU-accelerated PyTorch operations to achieve the throughput needed for large-scale inference simulation. This separation allows us to maintain high fidelity while dramatically improving simulation efficiency. The behavioral simulator supports two primary simulation modes: Device expert mode: Model memory array physics directly, simulating conductance variations, stuck faults, and temporal drift to isolate the effect of memory technologies on network accuracy. Circuit expert mode: Model memory arrays with a sta- tistical approach using pre-characterized noise distributions from SPICE simulations or silicon measurements, enabling rapid evaluation of aggregated non-idealities from devices and circuits. Through integration with TensorRT s post-training quanti- zation flow, NeuroSim V1.5 automatically maps pre-trained neural networks to the corresponding ACIM architecture. The following sections detail the quantization methodol- ogy, array modeling approach, and specific techniques for incorporating device and circuit-level non-idealities into the simulation framework. B. Neural Network Quantization and Mapping 1) Quantization: NeuroSim V1.5 adopts NVIDIA s Ten- sorRT quantization infrastructure for converting neural net- works from floating-point to integer representations. This differs from previous NeuroSim versions which used in-house WAGE quantization [59] - an integer-training method that requires retraining the network.",
    "source": "2505.02314v1_NeuroSim_V15_Improved_Software_Backbone_for_Benchm.pdf",
    "length": 2443,
    "tokens": 468
  },
  {
    "text": "After the pattern between left eye and right eye is captured, an external signal is propagated to the OCE to bypass the further original projection computations. Consequently, the projection computation results (Result[2 : n].R) for the remaining rows of the right eye can be easily reconstructed by adding Result[2 : n].L and the Î”. We prototyped our proposed EA and AE design blocks using System Verilog in Xilinx Vivado 2019.2 [58], targeting the Xil- inx Zynq-7000 SoC ZC706 board running at 100MHz (same as state-of-the-art EVR [28]). The evaluation shows that our EA and AE designs consume only 2mW and 65mW, respectively, and are able to deliver around 100 fps, which is more than sufï¬cient for the current VR application requirements. V. EVALUATION We compare our proposed EA and AE designs with six different VR streaming setups, by evaluating the computation and the total energy consumption. In this section, we ï¬rst describe the experimental platforms, datasets and measurement tools used in this study. We then analyze the results measured using these platforms. A. VR Design Conï¬gurations We evaluate the following six conï¬gurations of VR stream- ing to demonstrate the effectiveness of D ej a View: Baseline (SW): We use a mobile GPU [36] to evaluate the baseline VR video streaming. This GPU is commonly used in contemporary VR devices (Oculus [39], Magic Leap [16], and GameFace [47], etc.). Note that, with this setup, the projection computation is triggered for each frame, and also per projection computation invocation includes computation of two projection matrices for the two eyes. PTU (HW): A recent optimized solution [28] utilizes a more energy-efï¬cient hardware accelerator, i.e., Projec- tive Transformation Unit (PTU), to process the compute- intensive projection operations. This is the most recent VR design that uses an FPGA for accelerating the computation. We consider this design as the state-of-the-art. However, PTU only optimizes the energy per compute through accel- eration, with exactly the same amount of computations as in the baseline design. In contrast, as explained earlier in Sec.",
    "source": "DejaView.pdf",
    "length": 2129,
    "tokens": 492
  },
  {
    "text": "Award 2338418 (CAREER: Trustworthy Human-Centered Summarization); PI: Zhang; duration 09 15 24-08 31 29; amount 546,000. Intellectual Merit: This research advances trustwor- thy summarization by centering design, development, and deployment on humans in terms of user preferences for controllability, social perspectives for fairness, and human knowledge for factuality. Broader Impacts: This project initiates several aspiring education and outreach activities supported by project research outcomes to involve, mentor, and empower female, underrepresented, disabled, and interdisciplinary students. Major Results: No publication yet as the project started in September 2024. References [1] NVIDIA A100 Tensor Core GPU., 2023. a100 . [2] NVIDIA H100 Tensor Core GPU, 2023. technologies hopper-architecture . [3] Exploring llms - real-world case studies in ai-generated art literature. tome01.com exploring-llms-real-world-case-studies-in-ai-generated-art- literature, 2024. Accessed: 2024-10-22. [4] The state of ai in early 2024: Gen ai adoption spikes and starts to generate value. mckinsey.com capabilities quantumblack our-insights the-state-of-ai, Febru- ary 2024. Accessed: 2023-10-20. [5] Dennis Abts, Garrin Kimmell, Andrew Ling, John Kim, Matt Boyd, Andrew Bitar, Sahil Parmar, Ibrahim Ahmed, Roberto DiCecco, David Han, John Thompson, Michael Bye, Jennifer Hwang, Jeremy Fowers, Peter Lillian, Ashwin Murthy, Elyas Mehtabuddin, Chetan Tekur, Thomas Sohmers, Kris Kang, Stephen Maresh, and Jonathan Ross. A software-defined tensor streaming multiprocessor for large-scale machine learning. In Proceedings of the 49th Annual International Symposium on Computer Architecture, pages 567 580, 2022. [6] Eleni Adamopoulou and Lefteris Moussiades. An overview of chatbot technology. In IFIP interna- tional conference on artificial intelligence applications and innovations, pages 373 383. Springer, 2020.",
    "source": "NSF_LLM_Medium_Proposal.pdf",
    "length": 1909,
    "tokens": 483
  },
  {
    "text": "In particular, for M 32, MUXORNet-11 with GLT even achieves a slightly higher accuracy than the baseline model with gamma-inversed input. Figure 4 shows the curves of the encoding bit-count level (from 1 to M) as a function of the thresholds learned on gamma-inversed dataset. It is shown that although being linearly initialized, our GLT can successfully learn proper nonlinear curves (i.e., global tone mapping), providing a higher accuracy and being well- suited to a real-world deployment scenario. It thus suggests the possibility of using GLT to perform inference directly from analog data, bypassing all the image rendering stages. TABLE II: Accuracy ( ) on original STL-10 dataset. Input encoding method planes (M) VGG-Small MUXORNet-11 FP Bin. FP Bin. Baseline integer (8-bit) 32-b 83.02 79.95 84.24 79.74 Base-2 fixed-point [8] 8 78.32 73.15 80.06 75.83 Fixed Linear Thermometer [11] 8 79.35 76.40 81.39 77.43 16 80.63 77.49 82.17 78.47 32 81.50 77.81 82.51 78.90 Ours GLT 8 79.93 77.47 81.62 78.28 16 80.87 78.02 82.73 78.60 32 81.40 78.24 82.79 79.06 TABLE III: Accuracy ( ) on gamma-inversed STL-10 dataset. Input encoding method planes (M) VGG-Small MUXORNet-11 FP Bin. FP Bin.",
    "source": "2505.13462v1_End-to-end_fully-binarized_network_design_from_Gen.pdf",
    "length": 1191,
    "tokens": 349
  },
  {
    "text": "[41] H. Liu, H. Yuan, Q. Liu, J. Hou, and J. Liu, A comprehensive study and comparison of core technologies for mpeg 3-d point cloud compression, IEEE Transactions on Broadcasting, pp. 701 717, 2020. [42] M. Liu, Robotic online path planning on point cloud, IEEE transactions on cybernetics, vol. 46, no. 5, pp. 1217 1228, 2015. [43] Marek Kowalski, Jacek Naruniec, LiveScan3D-Hololens, , 2020. [44] Marek Simonik, Record3D-Record your own 3D Videos! , 2021. [45] S. Martin, M. Stefan, A. Karl et al., Complex-yolo: real-time 3d objectdetection on point clouds, in Computer vision and pattern recognition, 2018. [46] Mayank Raj, Point Clouds and its signiï¬cance in AR, https: bit.ly 3uknBjT , 2020. [47] D. Meagher, Geometric modeling using octree encoding, Computer Graphics and Image Processing, pp. 129 147, 1982. [48] R. Mekuria, K. Blom, and P. Cesar, Design, implementation, and evaluation of a point cloud codec for tele-immersive video, IEEE Transactions on Circuits and Systems for Video Technology, pp. 828 842, 2017. [49] G. Meynet, Y. Nehm e, J. Digne, and G. Lavou e, Pcqm: A full- reference quality metric for colored 3d point clouds, in 2020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX), 2020, pp. 1 6. [50] A.-M. S. Mohamed, Potential of 3d laser point cloud data usage for the tourism industry, in The International Conference on Civil and Architecture Engineering, vol. 12, no. 12th International Conference on Civil and Architecture Engineering. Military Technical College, 2018, pp. 1 13. [51] C. Moreno, Y. Chen, and M. Li, A dynamic compression technique for streaming kinect-based point cloud data, in 2017 International Conference on Computing, Networking and Communications (ICNC). IEEE, 2017, pp. 550 555.",
    "source": "PCcompress.pdf",
    "length": 1763,
    "tokens": 496
  },
  {
    "text": "The 1 bit full adders used are CLRCL, used for its 10T design, high speed and suitability for GAAFET process nodes. CLRCL directly uses pass-transistor structures to convey signals, often resulting in fewer intermediate nodes storing charge. Hence, CLRCL can achieve higher speed, provided the pass-transistor network is optimally sized, and threshold drops are mitigated. This requires careful transistor scaling to ensure clean output levels. Alternative well-known 10T alternative designs include 13A and SERF. Newer full adder circuits specifically designed for GAAFET may emerge, and these should also be considered. There is no direct reset of the accumulator. Reset should not be required for normal operation, but if it is required for testing a zero condition can be flowed down the CASCADE column. The Activation clock and Accumulation clock are separate, allowing them to be carefully phased to present the multiply result and the partial sum input to the adder simultaneously, almost doubling the effective cycle time. The accumulator is a D latch. Timing closure would likely be easier if it were an edge triggered flip flop. However, this would add another 48 transistors to the PE, and therefore significantly reduce performance of ZettaLith, so it should be avoided by extensive optimization of the PE. The circuit is specifically designed for 12 GHz operation, instead of as fast as possible . If timing closure can t be achieved at 12 GHz, the operating frequency can be reduced, or a pipeline register can be added to the PE. These decisions should be made after optimization, layout and SPICE simulation of the PE, using the PDK appropriate to the node chosen.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 1680,
    "tokens": 342
  },
  {
    "text": "Figures 8(b) and 9 (b) present the end- to-end energy efficiency, normalized to A100 AttAcc system, for the creative-writing and general-qa datasets. PAPI im- proves average energy efficiency by 3.4 and 3.1 for these datasets, respectively, over A100 AttAcc. This is because 0 0.5 1 1.5 2 2.5 3 4 16 64 4 16 64 4 16 64 4 16 64 4 16 64 4 16 64 4 16 64 4 16 64 4 16 64 spe 1 spe 2 spe 4 spe 1 spe 2 spe 4 spe 1 spe 2 spe 4 Speedup A100 AttAcc A100 HBM-PIM AttAcc-only PAPI 0 1 2 3 4 5 4 16 64 4 16 64 4 16 64 4 16 64 4 16 64 4 16 64 4 16 64 4 16 64 4 16 64 spe 1 spe 2 spe 4 spe 1 spe 2 spe 4 spe 1 spe 2 spe 4 Energy Efficiency A100 AttAcc A100 HBM-PIM AttAcc-only PAPI (b) End-to-end Energy Efficiency Improvement (a) End-to-end Performance Speedup LLAMA-65B GPT-3 66B GPT-3 175B Batch size Speculation length 1 Speculation length 2 Speculation length 4 Speculation length 1 Speculation length 2 Speculation length 4 Speculation length 1 Speculation length 2 Speculation length 4 Batch size LLAMA-65B GPT-3 66B GPT-3 175B Speculation length 1 Speculation length 2 Speculation length 4 Speculation length 1 Speculation length 2 Speculation length 4 Speculation length 1 Speculation length 2 Speculation length 4 Figure 8: End-to-end speedup (top) and energy efficiency (bottom) comparisons of four evaluated designs on the Dolly creative- writing dataset. Values are normalized to A100 AttAcc.",
    "source": "2502.15470v2_PAPI_Exploiting_Dynamic_Parallelism_in_Large_Langu.pdf",
    "length": 1392,
    "tokens": 394
  },
  {
    "text": "Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E Gonzalez, and Ion Stoica. 2024c. From crowdsourced data to high- quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939. Yueying Lisa Li, Omer Graif, and Udit Gupta. 2024d. Towards carbon-efficient llm life cycle. In Proceed- ings of the 3rd Workshop on Sustainable Computer Systems (HotCarbon). Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei- Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2024. Awq: Activation-aware weight quantization for on- device llm compression and acceleration. Proceed- ings of Machine Learning and Systems, 6:87 100. Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958. Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Ju- jie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. 2024a. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451. Jiachen Liu, Zhiyu Wu, Jae-Won Chung, Fan Lai, Myungjin Lee, and Mosharaf Chowdhury. 2024b. Andes: Defining and enhancing quality- of-experience in llm-based text streaming services. arXiv preprint arXiv:2404.16283. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2024c. Is your code generated by ChatGPT really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems (NeurIPS). Diptyaroop Maji, Prashant Shenoy, and Ramesh K Sitaraman. 2022.",
    "source": "2502.11256v2_Unveiling_Environmental_Impacts_of_Large_Language_.pdf",
    "length": 1590,
    "tokens": 475
  },
  {
    "text": "Avail- able: consumption-how-green-is-the-chip-industry-latest-stats [5] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski, L. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk, and T. Hoefler, Graph of thoughts: solving elaborate problems with large language models, in Proceedings of the AAAI Conference on Artificial Intelligence, 2024. [6] T. Cai, Y. Li, Z. Geng, H. Peng, J. D. Lee, D. Chen, and T. Dao, Medusa: Simple llm inference acceleration framework with multiple decoding heads, in Proceedings of the International Conference on Machine Learning (ICML), 2024. [7] G. I. Chaudhry, E. Choukse, I nigo Goiri, R. Fonseca, A. Belay, and R. Bianchini, Towards Resource-Efficient Compound AI Systems, in arxiv.org, 2025. [8] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A.",
    "source": "2506.04301v1_The_Cost_of_Dynamic_Reasoning_Demystifying_AI_Agen.pdf",
    "length": 989,
    "tokens": 357
  },
  {
    "text": "Edge computing devices based on field-programmable gate arrays (FPGAs) [17 21] offer a promising path for moving part or all of the PD-AI based computational MRI processing pipeline closer to the MRI sensors, thereby reducing data transmission and storage requirements substantially. These applications may range from data reduction techniques [22] to the actual PD-AI reconstruction algorithm [6]. In this study, we sought to establish the feasibility of a PD- AI reconstruction algorithm suitable for FPGA implementation and validate its performance against its commonly used coun- terpart. In particular, we consider both quantization effects and elimination of fast Fourier transforms (FFTs) and inverse FFTs (IFFTs) between the image and frequency domains, the latter of which is the measurement domain in MRI. For the latter, we focus on the case of equispaced sub-sampling with no calibration lines that is ubiquitous in echo-planar imaging based acquisitions such as fMRI [9, 23]. This allows to reformulate the data fidelity operation without the need for multiple FFTs and IFFTs, which is critical for FPGA implementations because the precision limitations can cause significant accuracy loss. For the former, we assess how 8-bit quantization in the convolutional neural network (CNN), suit- able for FPGA implementation, affects the performance of the PD-AI neural network. Our results show that PD-AI methods can be implemented with sufficient quantization and without repeated FFTs IFFTs, suitable for using FPGAs on edge com- puting devices such as the sensors, for many high-resolution MRI applications that lead to a large amount of raw data. II. METHODS A. MRI Acquisition Model MRI acquisitions collect data in the frequency domain, also referred to as the k-space. This is done using multiple receiver coils, each of which is sensitive to different parts of the imaging volume [16]. Due to the aforementioned resolution, scan time and SNR trade-off, MRI acquisitions are typically performed by sub-sampling the data, i.e. taking fewer measurements at a sub-Nyquist rate [9]. In this case, the forward model is given as: yk â„¦ Pâ„¦FNCkx nk, k {1, . . .",
    "source": "2506.03183v1_Edge_Computing_for_Physics-Driven_AI_in_Computatio.pdf",
    "length": 2167,
    "tokens": 474
  },
  {
    "text": "The proposed approach uses loop tiling-based computation decomposition, model duplication within the RCA, and inter-layer pipelining to reduce RCA activation thresholds and more closely track execution costs with dynamic power in- come. Experimental results show that ResiRCA and ResiSchedule achieve average speedups and energy efï¬ciency improvements of 8 and 14 respectively compared to a baseline RCA with intermittency-unaware scheduling. Keywords-Energy harvesting, ReRAM crossbar, CNN, Recon- ï¬gurable hardware, Loop tiling, Computation scheduling I. INTRODUCTION In recent years, inference tasks, such as convolutional neural networks (CNNs), have been integrated into an increasing number of embedded applications to process edge-device collected data locally [1]. Such integration grants IoT devices an important degree of independence from remote servers, which can be critical in deployments with challenging communication environments. However, continuing this trend onto ultra-low- power (ULP) IoT nodes presents clear design challenges due to the mismatch between the performance and computation requirements of CNNs and the limited resources of ULP platforms. Such platforms often already operate at their limits just in order to transmit sensed data at acceptable quality of service (QoS) rates for deployment-viable battery lifetimes, and may not have additional resources available for further computation. For many inference tasks, it is known that multiplication- and-accumulation (MAC) is the dominant operation type. In CNNs, for instance, MACs between the feature map data and kernel weights comprise nearly 90 of the total operations [2], [3]. Resistive random-access memory (ReRAM) crossbars are regarded as a promising mechanism for accelerating CNNs with high energy-efï¬ciency as they can perform MAC operations through analog current summation and can retain model parameters in memory during inactive periods with extremely low power overheads [3], [4], [5], [6], [7], [8], [9], [10]. In the remainder of the paper, we may shorten the term ReRAM crossbars to ReRAMs. Despite the obvious potential synergy between ReRAM crossbar-based CNN accelerators (RCAs) and IoT applications needing CNN inference, there can remain substantial challenges in efï¬ciently performing inference on an IoT device if it does not have either a high power or high stability power source.",
    "source": "ResiRCA.pdf",
    "length": 2394,
    "tokens": 498
  },
  {
    "text": "[46] Ali Tariq, Austin Pahl, Sharat Nimmagadda, Eric Rozner, and Siddharth Lanka. 2020. Sequoia: Enabling quality-of-service in serverless com- puting. In Proceedings of the 11th ACM Symposium on Cloud Computing. 311 327. [47] Prashanth Thinakaran, Jashwant Raj Gunasekaran, Bikash Sharma, Mahmut Taylan Kandemir, and Chita R. Das. 2017. Phoenix: A Constraint-Aware Scheduler for Heterogeneous Datacenters. In 2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS). 977 987. [48] Prashanth Thinakaran, Jashwant Raj Gunasekaran, Bikash Sharma, Mahmut Taylan Kandemir, and Chita R. Das. 2019. Kube-Knots: Re- source Harvesting through Dynamic Container Orchestration in GPU- based Datacenters. In 2019 IEEE International Conference on Cluster Computing (CLUSTER). 1 13. 8891040 [49] Guido Urdaneta, Guillaume Pierre, and Maarten Van Steen. 2009. Wikipedia workload analysis for decentralized hosting. Computer Networks (2009). [50] Liang Wang, Mengyuan Li, Yinqian Zhang, Thomas Ristenpart, and Michael Swift. 2018. Peeking Behind the Curtains of Serverless Plat- forms. In ATC. [51] Hailong Yang, Quan Chen, Moeiz Riaz, Zhongzhi Luan, Lingjia Tang, and Jason Mars. 2017. PowerChief: Intelligent power allocation for multi-stage applications to improve responsiveness on power con- strained CMP. In Computer Architecture News. [52] Yiming Zhang, Jon Crowcroft, Dongsheng Li, Chengfen Zhang, Huiba Li, Yaozheng Wang, Kai Yu, Yongqiang Xiong, and Guihai Chen. 2018. KylinX: a dynamic library operating system for simplified and efficient cloud virtualization. In 2018 USENIX Annual Technical Conference. 173 186. 167",
    "source": "kraken.pdf",
    "length": 1638,
    "tokens": 454
  },
  {
    "text": "8(a.2), respectively. Latency is stable around 41ms across 100 iterations of experiments (see A ), which leads to 24FPS throughput (see B ), thereby meeting the design requirement of maximum 50ms latency. Such low latency and high throughput mainly come from the selected mapping strat- egy, which considers maximizing hardware resources to fully map the entire SNN model on the Akida s NPU fabrics, hence avoiding the time-consuming execution of network partitions. Meanwhile, the results for power and energy consumption are presented in Fig. 8(a.3) and Fig. 8(a.4), respectively. Overall, power consumption is about 215mW (see C ), and energy consumption is about 9mJ (see D ), thereby meeting the design requirement of maximum 250mW power. Such low power and low energy consumption come from the sparse spike-driven computation, and the selected mapping strategy that optimizes data movements through efficient NPU allocation, and hence minimizing power consumption for the respective operations. 6 B. Real-Time Object Detection in Video Streaming Network Selection: Our compatibility analysis (from Sec- tion III-A) leads to the selection of Spiking-YOLOv2 [17] due to the following reasons. The network size (i.e., 3MB) meets the memory budget of Akida chip (i.e., 8MB). The number of NPUs required for a complete computation of the network (i.e., 71 NPUs) meets the NPU budget of an Akida chip (i.e., 80 NPUs). Accuracy: We perform inference using with 2500 iterations of object presentation (i.e., person and car). Screenshots of the real-time object detection in video streaming are presented in Fig. 9. The experimental results show that, running Spiking- YOLOv2 on the Akida processor can achieve 94.44 accuracy for detecting the presented objects. This high accuracy is due to the training process that utilizes accurate backpropagation in ANN domain, and the conversion process that accurately translates the ANN model into a representative SNN model. Latency, Throughput, Power and Energy Consumption: The experimental results for latency and throughput are shown in Fig. 8(b.1) and Fig. 8(b.2), respectively.",
    "source": "2504.00957v2_Enabling_Efficient_Processing_of_Spiking_Neural_Ne.pdf",
    "length": 2123,
    "tokens": 474
  },
  {
    "text": "ÂµRL: Discovering Transient Execution Vulnerabilities Using Reinforcement Learning M. Caner Tol Worcester Polytechnic Institute Kemal Derya Worcester Polytechnic Institute Berk Sunar Worcester Polytechnic Institute Abstract We propose using reinforcement learning to address the chal- lenges of discovering microarchitectural vulnerabilities, such as Spectre and Meltdown, which exploit subtle interactions in modern processors. Traditional methods like random fuzzing fail to efficiently explore the vast instruction space and often miss vulnerabilities that manifest under specific conditions. To overcome this, we introduce an intelligent, feedback-driven approach using RL. Our RL agents interact with the proces- sor, learning from real-time feedback to prioritize instruction sequences more likely to reveal vulnerabilities, significantly improving the efficiency of the discovery process. We also demonstrate that RL systems adapt effectively to various microarchitectures, providing a scalable solution across processor generations. By automating the exploration process, we reduce the need for human intervention, enabling continuous learning that uncovers hidden vulnerabilities. Ad- ditionally, our approach detects subtle signals, such as tim- ing anomalies or unusual cache behavior, that may indicate microarchitectural weaknesses. This proposal advances hard- ware security testing by introducing a more efficient, adaptive, and systematic framework for protecting modern processors. When unleashed on Intel Skylake-X and Raptor Lake mi- croarchitectures, our RL agent was indeed able to generate instruction sequences that cause significant observable byte leakages through transient execution without generating any Âµcode assists, faults or interrupts. The newly identified leaky sequences stem from a variety of Intel instructions, e.g. in- cluding SERIALIZE, VERR VERW, CLMUL, MMX-x87 transitions, LSL RDSCP and LAR. These initial results give credence to the proposed approach. 1 Introduction In the past two decades, our computing systems have evolved and grown at an astounding rate. A side effect of this growth has been increased resource sharing and, with it, erosion of isolation boundaries. Multitenancy has already been shown to be a significant security and privacy threat in shared cloud instances. VM boundaries can be invalidated either by soft- ware or hardware bugs [10,21,33,54] or by exploiting subtle information leakages at the hardware level [34]. Microarchitectural Threats.",
    "source": "2502.14307v1_Î¼RL_Discovering_Transient_Execution_Vulnerabilitie.pdf",
    "length": 2513,
    "tokens": 477
  },
  {
    "text": "Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation. arXiv preprint arXiv:2503.20552 (Mar 2025). [38] Heng Liao, Bingyang Liu, Xianping Chen, Zhigang Guo, Chuanning Cheng, Jianbing Wang, Xiangyu Chen, Peng Dong, Rui Meng, Wenjie Liu, Zhe Zhou, Ziyang Zhang, Yuhang Gai, Cunle Qian, Yi Xiong, Zhongwu Cheng, Jing Xia, Yuli Ma, Xi Chen, Wenhua Du, Shizhong Xiao, Chungang Li, Yong Qin, Liudong Xiong, Zhou Yu, Lv Chen, Lei Chen, Buyun Wang, Pei Wu, Junen Gao, Xiaochu Li, Jian He, Shizhuan Yan, and Bill McColl. 2025. UB-Mesh: a Hierarchically Localized nD-FullMesh Datacenter Network Architecture. arXiv preprint arXiv:2503.20377 (Mar 2025). [39] Meta AI. 2025. Llama 4: Multimodal Intelligence at Scale. Accessed: 2025-04-28. [40] NVIDIA Corporation. 2024. CUDA Toolkit. Accessed: June 10, 2025. [41] NVIDIA Corporation. 2025. NVIDIA Dynamo Open-Source Library Accelerates and Scales AI Reasoning Mod- els. models. Accessed: 2025-04-23. [42] ONNX Community. 2019. ONNX: Open Neural Network Exchange. Accessed: 2025-05-31. [43] ONNX Runtime. 2025. CANN Execution Provider. maintained CANN-ExecutionProvider.html. Accessed: June 10, 2025. [44] OpenAI. 2024. Introducing SimpleQA. Accessed: 2025-06-14. [45] OpenAI. 2025. Introducing GPT-4.5. Accessed: 2025-04-28.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 1322,
    "tokens": 430
  },
  {
    "text": "For instance, the model often selects different temporary registers (e.g., r3 instead of r2) or reorders commutative operands without altering the underlying operation. It also adjusts stack frame offsets or memory allocation sizes, provided that the modifications do not violate data dependencies or correctness constraints. These variations suggest that the model is not merely memorizing instruction patterns but is instead learning high-level register-to-variable mappings and instruction equivalence classes. This flexibility enables generalization beyond the exact reference format and increases robustness to minor program transformations. Prog ID Edit Dist Combined Patterns and Examples P128 78 Multiple Optimization Patterns: Groud truth: mul r1, r2, r3 Predicted: lsl r1, r2, 2; add r1, r1, r2 P113 74 Memory and Instruction Patterns: Ground truth: str r1, [fp, -12] mov r3, r2 add r3, r3, 4 Predicted: str r1, [fp, -8] add r2, r2, 4 Table 8: Complex Variation Patterns with Multiple Differences Furthermore, Table 8 presents more substantial structural rewrites that nonetheless retain functional fidelity. These include compound transformations such as converting multiplications into equivalent shift-add sequences, or restructuring memory operations while preserving access order and scope. In one example, a multiplication instruction is replaced with a pair of shift and add instruc- tions demonstrating the model s awareness of performance-equivalent alternatives. In another case, memory writes and register arithmetic are reordered while maintaining the intended result, revealing the model s competence in preserving state consistency across instruction sequences. While these examples have higher edit distances, they exemplify a deeper form of equivalence: one grounded in operational semantics rather than surface-level syntax. The ability to produce such alternative forms underscores the potential of language models to reason compositionally about program structure and to synthesize diverse yet correct outputs for the same task. In contrast, Table 6 presents failure cases where minor syntactic deviations result in critical semantic errors. These include incorrect immediate values, 13 register mismanagement, and mismatched memory offsets that compromise program correctness despite appearing superficially similar to the ground truth. Together, Tables 7, 8, and 6 reveal that syntactic deviation does not necessarily imply failure.",
    "source": "2506.14606v1_Guaranteed_Guess_A_Language_Modeling_Approach_for_.pdf",
    "length": 2463,
    "tokens": 472
  },
  {
    "text": "1b). Therefore, we induce a delay (no-op cycles in Fig. 3) between one sensor ï¬nishing an inference and the next sensor starting the next one, so that each of the sensors get more time to accumulate more energy prior to attempting an inference. We refer to this policy that stretches the basic round-robin policy as extended round-robin (ER-r). Using ER-r, we can complete more total inferences, 1This can also be extended to larger numbers of sensors and modalities but this design is limited by the accuracy of individual sensors. Moreover, since all sensors are not equally capable of classifying each activity with same accuracy (Fig. 2), ER-r might lead to lower accuracy in many cases. A better approach is to prioritize performing inferences on the sensor that has the highest local accuracy for the current activity. However, this poses a chicken and egg problem to know which sensor is the best for classifying an activity we need to know what activity is going to be performed beforehand. However, while perfect future knowledge remains impossible, in the context of HAR, we can anticipate the next activity from the previous activity with high conï¬dence. Intuitively, human activities do not usually stop abruptly, i.e. if a person is walking and has taken a step, there is a high probability that the person will continue walking rather than immediately switch to another activity. Therefore, to classify the next possible activity, we activate the sensor which is most accurate for classifying the anticipated activity. This motivates us to develop an activity-aware scheduling (AAS) policy which aims to activate the best suited sensor for the anticipated activity. B. Activity Aware Scheduling To enable the activity awareness we keep a small lookup table of accuracy of all the sensors over all the classes. However, accuracy being a ï¬‚oating point number, is expensive in terms of energy to store and lookup. To minimize this overhead, instead of storing the accuracy, we store the rank of the sensors for individual activities. After a sensor detects an activity, it anticipates the next activity to be the current classiï¬ed activity, looks up for the best sensor, and signals to activate it for the upcoming inference. However, this leads to another potential issue - what if the current inference is running on the best sensor, and the sensor does not have enough energy to run the next inference?",
    "source": "Origin.pdf",
    "length": 2416,
    "tokens": 496
  },
  {
    "text": "USENIX Association. [155] The New York Times. Amazon, google, microsoft turn to nuclear energy. https : www.nytimes.com 2024 10 16 business energy- environment amazon- google- microsoft-nuclear-energy.html, 2024. Accessed: 2024-10-20. [156] Prashanth Thinakaran, Jashwant Raj Gunasekaran, Bikash Sharma, Mahmut Taylan Kandemir, and Chita R. Das. Kube-knots: Resource harvesting through dynamic container orchestration in gpu- based datacenters. In 2019 IEEE International Conference on Cluster Computing (CLUSTER), pages 1 13, 2019. [157] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998 6008, 2017. [158] Shyam Venkataramani, Suresh Cherian, Sergio DeRose, Karthik Sankaralingam, and Sek Ching Wong. Garnet: A detailed network-on-chip simulator. In Proceedings of the 2012 International Sym- posium on High Performance Computer Architecture (HPCA), pages 507 518. IEEE, 2012. [159] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353 355, 2018. [160] Bo Wang, Maria Liakata, Arkaitz Zubiaga, and Rob Procter. A hierarchical topic modelling approach for tweet clustering. In Social Informatics: 9th International Conference, SocInfo 2017, Oxford, UK, Septem- ber 13-15, 2017, Proceedings, Part II 9, pages 378 390. Springer, 2017. [161] Li Wang, Wei Zhang, and Mei Huang. Dynamic resource allocation for large-scale distributed training of deep learning models. In Proceedings of the ACM Symposium on Cloud Computing, pages 789 800. ACM, 2020.",
    "source": "NSF_LLM_Medium_Proposal.pdf",
    "length": 1851,
    "tokens": 497
  },
  {
    "text": "Both agents inference networks have an input layer with 7 neurons (for 7 the state features listed in Table 1), one 10-neuron hidden layer and an output layer with 2 neurons representing the agent s actions (for HSS with two storage devices). Each inference requires 90 MAC operations (7 10 10 2). On our evaluated system (see Table 3), these MAC operations consume 90 CPU cycles (240ğ‘›ğ‘ ) per core, lower than the I O read latency of a high-end SSD ( 3ğœ‡s) [47, 49]. Training latency. In Harmonia, RL agent training is separated from inference and executed in the background. Hence, training latency does not impact I O request latency. Harmonia computes 184,320 MAC operations for each training step. We use 16 batches per training step, where each batch of 128 training samples requires (128 7 10 128 10 2) MAC operations. This computation takes 53ğœ‡s on our evaluated system with 200,000 cycles per core. 4.6.2 Storage Overhead. Harmonia s storage overhead includes (1) RL agents neural networks, (2) experience buffers, (3) migration queue, (4) migration agent s reward, and (5) address mapping in- formation. First, neural network weights are represented using a half-precision floating-point format. With 90 16-bit weights, each network requires 1.5 KiB of memory. The four networks (training and inference networks for two agents) take a negligible 6 KiB of memory, so we store their weights in the CPU caches. Second, to train each RL agent, we use an experience buffer located in the DRAM that consumes 100 KiB to store 1000 experiences. Hence, the total storage overhead of two experience buffers is 200 KiB. Overall, Harmonia s RL agents require 206 KiB of memory. Third, the migra- tion queue holds the logical block address (32 bits) and target device ID (4 bits) for each queue entry. The migration queue size is 10 in our experiments, requiring 50 bytes of DRAM. Fourth, we store the latencies of 50 incoming I O requests temporarily to assign a delayed reward to the migration agent, which consumes 100B. Fifth, we maintain address mapping information for HSS devices [107]. Harmonia requires 32 bits to store the state features for each page (see Table 1).",
    "source": "2503.20507v2_Harmonia_A_Multi-Agent_Reinforcement_Learning_Appr.pdf",
    "length": 2170,
    "tokens": 493
  },
  {
    "text": "3.4. Our Goal Our goal is to design a versatile computing platform that caters to the varying parallelization levels in real-world LLM in- ference with different and dynamically changing computation and memory demands. To this end, we propose (1) a heteroge- neous architecture that integrates memory-centric PIM units and computation-centric GPU and host CPU, each offering distinct computation throughput and memory bandwidth char- acteristics, and (2) a parallelism-aware scheduling technique that adapts to runtime variations in parallelization and intel- ligently and dynamically assigns FC and attention kernels to the most appropriate hardware units in our platform. 4. PAPI: Overview Given that LLM inference exhibits varying parallelization levels during runtime, an intelligent dynamic scheduling policy is necessary to identify the most suitable computing hardware for a given kernel at a given time. The key challenge is to design a kernel offloading and allocation scheme that monitors dynamic parallelism online at low cost (in terms of latency and energy consumption) and selects the best-fit computing hardware to fully and efficiently utilize the available hardware resources. 4.1. PAPI: Key Components We propose the PAPI architecture and framework. Figure 5 shows the overview of the PAPI framework. PAPI has three key components explained next. Heterogeneous Architecture. We propose a heterogeneous architecture to effectively cater to both compute-bound and memory-bound kernels of LLMs. This architecture includes (1) a host CPU, (2) a high-performance processor with PIM mem- ory units (FC-PIM), and (3) physically separated (i.e., disaggre- gated) PIM units (Attn-PIM). The high-performance processor includes processing units (hereafter referred to as PUs), e.g., GPU tensor cores [53], PIM memory units (i.e., HBM-based PIM devices), and a hardware scheduler. In our evaluation, we use GPU tensor cores for the PUs, but any other high-performance processor designed for compute-bound kernels (e.g., TPU [54] or NPU [55]) could also be used for this design. The host CPU sends instructions to the high-performance processor and the physically separate Attn-PIM devices, which are disaggregated from the high-performance processor. Hybrid PIM Units.",
    "source": "2502.15470v2_PAPI_Exploiting_Dynamic_Parallelism_in_Large_Langu.pdf",
    "length": 2274,
    "tokens": 492
  },
  {
    "text": "Synergistic and Efficient Edge-Host Communication for Energy Harvesting Wireless Sensor Networks Cyan Subhra Mishra, Jack Sampson, Mahmut Taylan Kandmeir, Vijaykrishnan Narayanan, Chita R Das {cyan, jms1257, mtk2, vijaykrishnan.narayanan, The Pennsylvania State University ABSTRACT There is an increasing demand for intelligent processing on ultra-low-power internet of things (IoT) device. Recent works have shown substantial efficiency boosts by execut- ing inferences directly on the IoT device (node) rather than transmitting data. However, the computation and power de- mands of Deep Neural Network (DNN)-based inference pose significant challenges in an energy-harvesting wireless sen- sor network (EH-WSN). Moreover, these tasks often require responses from multiple physically distributed EH sensor nodes, which impose crucial system optimization challenges in addition to per-node constraints. To address these chal- lenges, we propose Seeker, a hardware-software co-design approach for increasing on-sensor computation, reducing communication volume, and maximizing inference comple- tion, without violating the quality of service, in EH-WSNs co- ordinated by a mobile device. Seeker uses a store-and-execute approach to complete a subset of inferences on the EH sensor node, reducing communication with the mobile host. Fur- ther, for those inferences unfinished because of the harvested energy constraints, it leverages task-aware coreset construc- tion to efficiently communicate compact features to the host device. We evaluate Seeker for human activity recognition, as well as predictive maintenance and show 8.9 reduc- tion in communication data volume with 86.8 accuracy, surpassing the 81.2 accuracy of the state-of-the-art. 1 INTRODUCTION Innovations in low-power computing, artificial intelligence, and communication technologies have given rise to the gen- eration of intelligently connected devices that constitute the Internet of Things (IoT). Wireless sensor networks (WSNs), one of the prominent classes of IoT deployments, is currently dominating and expected to be pervasive impacting many application spaces [13] including, but not limited to, body area network [22, 47], industrial monitoring [34], predic- tive maintenance [70], commercial satellites[15] and smart farming [61].",
    "source": "Seeker.pdf",
    "length": 2308,
    "tokens": 499
  },
  {
    "text": "This has a significant economic (users, products and data generating dollar value) as well as environmental (battery and e-waste) impact (Mishra et al., 2024). In fact, advances in EH has lead to a staggering development in intermittently powered battery-free devices (Maeng Lucia, 2018; Gobieski et al., 2019; Qiu et al., 2020; Saffari et al., 2021; Afzal et al., 2022). A typical EH setup consists of 5 components, namely, energy capture (solar panel, thermocouple, etc), power conditioning, voltage regulation (buck or boost converter), energy storage (super capacitor) and compute unit (refer Appendix B for details about each of them). To cater towards the sporadic 2 power income and failures, an existing body of works explores algorithms, orchestration, compiler support, and hardware development (Yang et al., 2017, 2018; Mendis et al., 2021; Maeng Lucia, 2018; Gobieski et al., 2018; Qiu et al., 2020; Islam et al., 2022; Mishra et al., 2024, 2021; Ma et al., 2016, 2017; Liu et al., 2015). Most of these works rely on software checkpointing (static and dynamic (Maeng Lucia, 2018), refer Appendix C) to save and restore, while some of the prior works developed nonvolatile hardware (Ma et al., 2016, 2017) which inherently takes care of the checkpointing. Considering the scope of these initiatives, it is crucial to acknowledge that, despite the substantial support for energy harvesting and intermittency management, developing intermittency-aware applications and hardware necessitates multi-dimensional efforts that span from theoretical foundations to circuit design. Intermittent DNN Execution Training: As the applications deployed on such EH devices demand analytics, executing DNNs on EH devices and EH-WSNs have become prominent (Lv Xu, 2022; Gobieski et al., 2019; Qiu et al., 2020; Mishra et al., 2021). However, due to computational constraints, limited memory capacity and restricted operating frequencies, many of these applications fail to complete inference execution with satisfactory SLOs, despite comprehensive software and hardware support (Mishra et al., 2021).",
    "source": "NexUME.pdf",
    "length": 2094,
    "tokens": 465
  },
  {
    "text": "Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU 1st Petar JakuÅ¡ Faculty of Electrical Engineering and Computing Zagreb, Croatia 2nd Hrvoje DÅ¾apo Faculty of Electrical Engineering and Computing Zagreb, Croatia Abstract This paper presents a keyword spotting (KWS) system implemented on the NXP MCXN947 microcontroller with an integrated Neural Processing Unit (NPU), enabling real-time voice interaction on resource-constrained devices. The system combines MFCC feature extraction with a CNN classifier, optimized using Quantization Aware Training to reduce model size with minimal accuracy drop. Experimen- tal results demonstrate a 59 speedup in inference time when leveraging the NPU compared to CPU-only execution, achieving 97.06 accuracy with a model size of 30.58 KB, demonstrating the feasibility of efficient, low-power voice interfaces on embedded platforms. Index Terms keyword spotting, microcontroller, MCU, Neural Processing Unit, NPU, edge AI, QAT I. INTRODUCTION With the growing adoption of IoT devices, keyword spotting (KWS) has become essential for voice-controlled interaction in smart assistants and embedded systems. However, implementing KWS on resource-constrained devices remains challenging due to limited processing power, memory constraints, and strict energy require- ments, demanding optimized solutions for microcontroller deployment. A typical keyword spotting system consists of signal acquisition, feature extraction and a neural network [7]. Signal acquisition in keyword spotting systems is typically performed using a microphone, most often a MEMS microphone, which converts pressure variations into electrical signals. The raw audio signal often con- tains noise and amplitude variations, requiring signal preprocessing to improve robustness and accuracy. Com- mon noise reduction techniques include adaptive noise cancellation (ANC) [9] and beamforming [10]. Feature extraction transforms the input signal into a compact and informative representation suitable for classification. Widely used methods include Mel-frequency cepstral coefficients (MFCC) [1], [11] and RASTA-PLP [8], [18]. To reduce the computational complexity of the preprocessing stage, simplified approaches such as direct application of the Fast Fourier Transform (FFT) have been proposed [2]. For ultra-low-power applications, analog front-end approaches and featureless techniques have been explored [3], [14], [17].",
    "source": "2506.08911v1_Implementing_Keyword_Spotting_on_the_MCUX947_Micro.pdf",
    "length": 2459,
    "tokens": 494
  },
  {
    "text": "4: MEMHD Accuracy Heatmap from 64x64 to 1024x1024. hardware resources by adjusting dimensions and columns. Generally, higher dimensions correlate with improved accuracy, which is likely related to the quality of encoding. For MNIST and FMNIST, we observe that both higher dimensions and more columns (i.e., using more class vectors) correlate with improved accuracy. However, ISOLET dataset exhibits a different pattern, showing peak performance when using 128-256 columns. This divergence in optimal structure reflects the unique characteristics of each dataset. ISOLET, with its small sample size per class (approximately 240 samples per class) and large number of classes, shows peak performance with fewer columns. Using too many columns for ISOLET could lead to overfitting, potentially causing some class vectors to represent outliers rather than general class features. Conversely, MNIST and FMNIST (approximately 6000 samples per class), being larger and more diverse, benefit from additional class vectors without such overfitting concerns. These findings highlight the importance of adapting the AM structure to both the dataset characteristics and available computational resources. D. Clustering-based Initialization Our clustering-based initialization significantly outperforms random sampling in accuracy. Fig. 5 shows that for MNIST (512x512) and ISOLET (1024x256), clustering achieves 8.69 and 19.95 higher initial accuracies respectively, leading to improved final accuracies and faster convergence (within 10- 20 epochs compared to 30-40 epochs for random sampling). 80 85 90 95 100 1 11 21 31 41 51 Accuracy ( ) Epoch 8.69 (a) MNIST 512x512 60 70 80 90 100 1 11 21 31 41 51 Accuracy ( ) Epoch (b) ISOLET 1024x256 19.95 0.8 0.3 Clustering Random Sampling Fig. 5: Accuracy comparison between clustering and random sampling. 70 75 80 85 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Accuracy ( ) R (a) FMNIST 84 85 86 87 88 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Accuracy ( ) R (b) ISOLET 512x512 512x64 Fig. 6: Accuracy for different initial cluster ratios R (0.1 to 1.0).",
    "source": "2502.07834v1_MEMHD_Memory-Efficient_Multi-Centroid_Hyperdimensi.pdf",
    "length": 2080,
    "tokens": 494
  },
  {
    "text": ". . P(C x, Ï‰) x Rd 3 aKHgrmA \" AB83icbVDLSgNBEJz1GeMr6tHLYBA8hV2R6DHoRW8RzAOyS5id9CZDZh M9IphyW948aCI V3 Gm3 jbLIHTSxoKq6e7yEyk02va3tbK6tr6xWdoqb s7u1XDg7bOk4VhxaPZay6PtMgRQtFCihmyh goS h49vcr zCEqLOHrASQJeyIaRCARnaCTXRXjC7C5KUpz2K1W7Zs9Al4lTkCop0OxXvtxBzNMQIuSa d1z7AS9jCkUXMK07KYaEsbHbAg9QyMWgvay2c1TemqUAQ1iZSpCOlN T2Qs1HoS qYzZDjSi14u uf1Ugyu vEzkL0HE54uCVFKMaR4AHQgFHOXEMaVMLdSPmKcTQxlU0IzuLy6R9XnPqtfr9RbVxXcRIsfkhJwRh1 ySBrklTdIinCTkmbySNyu1Xqx362PeumIVM0fkD6zPH7h6kiY latexit Input Source Geometry Simulation 4 bt20O2vpg4PHeDPzgpQzpR3n2yptbG5t75R3K3v7B4dH1eOTjkoySdGjCU9kLyAKORPoaY59lKJA4doPJ7dzvPqFULBGPepqiH5ORYBGjRBvJu2PIw2G15tSdBex14hakBgXaw rXIExoFqPQlBOl q6Taj8nUjPKcVYZApTQidkhH1DBYlR fni2Jl9YZTQjhJpSmh7of6eyEms1DQOTGdM9FitenPxP6 f6ejaz5lIM42CLhdFGbd1Ys8 t0Mm kWo NYRQycytNh0TSag2 VRMCO7qy uk06i7zXrzoVFr3RxlOEMzuESXLiCFtxDGzygwOAZXuHNEtaL9W59LFtLVjFzCn9gf4AqC2OmQ latexit Field Positional Encoder SiO2 Freespace Dipole Source (Inactive) Dipole Source (Active) Positional Encoding Decoder Ï‰d Rd (k k) b s(x) A(Ï‰g) (Section 2.3) (Section 2.2.1) O rvF3Z2d3br5oHhx0Zp4LQNol5Lo lpSziLYVU5x2E0Fx6HP64E u8 rDIxWSxdG9mibUC EoYgEjWGlrYFbrd5TkiFzLtc7PBmbNtuy50Co4BdSgUGtgfvWHMUlDGinCsZQ9x06Ul2GhGOF0VumnkiaYTPCI9jRGOKTSy aLz9CpdoYoiIV kUJz9 dEhkMp6GvO0OsxnK5lpv 1XqpCi69jEVJqmhEFh8FKUcqRnkKaMiEPptPNWAimN4VkTEWmCidVUWH4CyfvAod13IaVuPWrTWvijKcAwnUAcHLqAJN9CNhBI4Rle4c14Ml6Md Nj0Voyipkj CPj8wc6TJGE latexit (Section 2.2.4) QUFR1093lRYJrsO1va25 YXFpObOSXV1b39jMbW3XdBgryqo0FKFqeEQzwSWrAgfBGpFiJPAEq3v9i6Ffv2dK81DewiBirYB0Jfc5JWCkdu7osuAGBHqenzykZ9iFHgPS7hxgl0s8drzkJr1L sbjAdO4n7Zzebtoj4BniTMheTRBpZ37dDshjQMmgQqidOxI2glRAGngqVZN9YsIrRPuqxpqCRmTysZPZfifaN0sB8qUxLwSP09kZBA60Hgmc7huXraG4r ec0Y NWwmUA5N0vMiPB YQD5PCHa4YBTEwhFDFza2Y9ogiFEyeWROCM 3yLKkdFp1SsXR9nC fT LIoF20hwrIQSeojK5QBVURY oGb2iN vJerHerY9x65w1mdlBf2B9 QBI0aGI latexit D(x; Ï‰d) Rk k R(e(x; Ï‰g)) N T0zGxmLju sLi0nFtZvdJhrDhUeChDdeMyDVIEUEGBEm4iBcx3JVy7ndOef30HSoswuMRuBHWftQLhCc7QSI3cec1n2Ha9BNKtEb1PD2kN24Cs0dqmR3SkH5vISL5NdpxUj41sN3J5u2D3Qf8SZ0jyZIhyI dUa4Y89iFALpnWVceOsJ4whYJLSLO1WEPEeIe1oGpowHzQ9aR cko3jdKkXqjMC5D21fGJhPlad3XJHsr6t9eT zPq8boHdQTEUQxQsAH3mxpBjSXn 0KRwlF1DGFfC7Ep5mynG0bScNSU4v0 S652C06xULzYy5dOh nVkyDrZIFvEIfukRM5ImVQIJw kmbyRd vRerU rM9BdMIazqyRH7C vgEHFKqg latexit e(x; Ï‰g) A(Ï‰g) 1s(x) Softmax( Ë†f(x; Ï‰)) 1v x82 cJFto4oGBwzn3MvecIJHCoOt O2vrG5tb24Wd4u7e sFh6ei4ZeJUM95ksYx1J6CGS6F4EwVK3k0p1EgeTsY38z89hPXRsTqAScJ9yM6VCIUjKVWncpJin2S2W34s5BVomXkzLkaPRLX71BzNKIK2SGtP13AT9jGoUTPJpsZcanlA2pkPetVTRiBs m187JedWGZAw1vYpJHP190ZGI2MmUWAnI4ojs zNxP 8borhlZ8J ZRNxRYfhakGJNZdDIQmjOUE0so08LeStiIasrQFlS0JXjLkVdJq1rxapXafbVcv87rKMApnMEFeHAJdbiFBjSBwSM8wyu8ObHz4rw7H4vRNSfOYE cD5 ANXj1A latexit Output x1 W bYBFclZki1WXRjcsK9gGdoWTSTBuayYQkI5ahv HGhSJu Rl3 o2ZdhbaeiBwOde7skJWfauO63s7a sbm1Xdop7 7tHxWjo47OkVoW2S8ET1QqwpZ4K2DTOc9qSiOA457YaT29zvPlKlWSIezFTSIMYj wSJGsLGS78fYjMoe5oN6oNK1a25c6BV4hWkCgVag8qXP0xIGlNhCMda9z1XmiDyjDC6azsp5pKTCZ4RPuWChxTHWTzDN0bpUhihJlnzBorv7eyHCs9TQO7WSeUS97ufif109NdB1kTMjUEWh6KUI5Og vA0ZIoSw6eWYKYzYrIGCtMjK2pbEvwlr 8Sjr1mteoNe4vq82bo4SnMIZXIAHV9CEO2hBGwhIeIZXeHNS58V5dz4Wo2tOsXMCf B8 gAvdpHM latexit x2 d uBovgqiQi1WXRjcsK9gFNKJPJpB06mYSZiVhCf8ONC0Xc jPu BsnaRbaemDgcM693DPHTzhT2ra rcra sbmVnW7trO7t39QPzqTiVhHZJzGM58LGinAna1UxzOkgkxZHPad f3uZ 5FKxWLxoGcJ9SI8 FixkBGsjuW6E9cQPs6f5KBjVG3bTLoBWiVOSBpTojOpfbhCTNKJCE46VGjp2or0MS80Ip OamyqaYDLFYzo0VOCIKi8rMs RmVECFMbSPKFRof7eyHCk1CzyzWSeUS17ufifN0x1eO1lTCSpoIsDoUpRzpG eQEoYJISzWeGYCKZyYrIBEtMtKmpZkpwlr 8SnoXTafVbN1fNto3ZR1VOIFTOAcHrqANd9CBLhBI4Ble4c1KrRfr3fpYjFascucY sD6 AF7PpH latexit xd Figure 2.",
    "source": "2504.20401v1_Nonlinear_Computation_with_Linear_Optics_via_Sourc.pdf",
    "length": 3560,
    "tokens": 2470
  },
  {
    "text": "The Hopper architecture [38] is the fourth generation of NVIDIA GPUs augmented with tensor cores to accelerate deep learning workloads via 4 4 8 mixed-precision, and 4 4 4 single-precision, matrix multiplication instructions. Recent work showcases the benefits of explicit tensor core program- ming via the Warp Matrix Multiply Accumulate (WMMA) TABLE X MATRIX EXTENSIONS SUMMARY ISA Tile Size FP32 Tile 2D Memory Dedicated Name ( Bits) Shape ( Elements) Instructions Registers OpenPower [9] 512 4 4 No Yes MME [8] 512-16384 Agnostic Yes Yes SME [10] V LEN 2 Agnostic (Square) Yes Yes Tensor Cores [38] 512 4 4 Yes - AMX [5] 8192 16 16 Yes Yes SiFiveInt [7] 512-16384 4 (V LEN 128) No No MTE (Section III) Agnostic Agnostic Yes No API, which exposes tile shape configuration, matrix load s- tores, and multiplication, to accelerate cross-correlation [61] and convolutions [62]. Developing efficient kernels for tensor cores relies on adapting the application to the predetermined geometry in the micro-architecture and managing the GPU memory subsystem, which incentives the use of kernel li- braries such as cuDNN [63] maintained by the vendors. Table X summarizes the main aspects of the matrix ISAs outlined in this section and Section III-E. The MTE approach is the first matrix ISA that is agnostic to both the tile shape and tile size to decouple the ISA from the underlying microarchi- tecture. In addition, MTE does not require dedicated registers to store matrix tiles as it relies on the vector register file, and efficiently vectorizes GEMMs across the three dimensions M, N, and K. MTE enables the seamless interplay of vector and matrix instructions, which supports software kernel fusion between GEMM-based workloads and common post-operation like batchnorm and activations. VIII. CONCLUSION This paper demonstrates the limitations of existing vector and matrix ISAs when dealing with GEMM-based work- loads.",
    "source": "2507.03522v1_A_Flexible_Instruction_Set_Architecture_for_Effici.pdf",
    "length": 1922,
    "tokens": 456
  },
  {
    "text": "Gualandi, S. Malucelli, F. Exact solution of graph coloring problems via constraint pro- gramming and column generation. INFORMS Journal on Computing 24, 81 100 (2012). 16 Extended Data Invalid States Ising Valid States Ising Valid States Vectorized 0 10 20 30 40 50 60 70 Energy anna Invalid States Ising Valid States Ising Valid States Vectorized 0 10 20 30 40 50 Energy david Invalid States Ising Valid States Ising Valid States Vectorized 0 20 40 60 80 100 Energy games120 Invalid States Ising Valid States Ising Valid States Vectorized 0 2 4 6 8 10 12 14 Energy myciel3 Invalid States Ising Valid States Ising Valid States Vectorized 0 5 10 15 20 25 Energy myciel4 Invalid States Ising Valid States Ising Valid States Vectorized 0 10 20 30 40 50 Energy myciel5 Invalid States Ising Valid States Ising Valid States Vectorized 0 20 40 60 80 100 Energy myciel6 Invalid States Ising Valid States Ising Valid States Vectorized 0 50 100 150 200 Energy myciel7 Invalid States Ising Valid States Ising Valid States Vectorized 50 75 100 125 150 175 200 Energy queen11_11 Invalid States Ising Valid States Ising Valid States Vectorized 100 150 200 250 Energy queen13_13 Invalid States Ising Valid States Ising Valid States Vectorized 0 10 20 30 40 50 60 Energy queen5_5 Invalid States Ising Valid States Ising Valid States Vectorized 0 10 20 30 40 50 Energy queen6_6 Invalid States Ising Valid States Ising Valid States Vectorized 10 20 30 40 50 60 70 80 Energy queen7_7 Invalid States Ising Valid States Ising Valid States Vectorized 0 20 40 60 80 100 120 Energy queen8_12 Invalid States Ising Valid States Ising Valid States Vectorized 20 40 60 80 100 Energy queen8_8 Invalid States Ising Valid States Ising Valid States Vectorized 20 40 60 80 100 120 Energy queen9_9 Figure S1: Valid and invalid energy states explored while solving the graph coloring problem instances 40 on Ising and Vectorized mapping framework.",
    "source": "2505.20250v1_Efficient_Optimization_Accelerator_Framework_for_M.pdf",
    "length": 1913,
    "tokens": 439
  },
  {
    "text": "Meanwhile, pairwise sum- mation is efficient to implement but suffers from large error in narrow floating-point formats. Figure 3 illustrates the need for high-precision accumulation of FP8 dot products. Using several summation algorithms, we perform dot products between two Gaussian vectors in FP8 precision (4-bit mantissa accumulator) and plot the numerical error relative to the baseline FP32 accumulation (24-bit mantissa accumulator). Sequen- tial summation loses all accuracy after only 200 sums. Pairwise summation is significantly more accurate than sequential summa- tion but still exhibits up to 50 error for longer dot products. In Section 5.2, we discuss how to accumulate FP8 mantissas in low- precision for a majority of sums while attaining numerical accuracy on-par with FP32 accumulation. 3 Analysis of Dot-Product Overflows We begin by providing an analytical framework for reasoning about overflows. We define two types of integer overflow and discuss multiple algorithms for avoiding them. Definition 3.1 (Transient Overflow). Overflow that may occur at any point during the sequential summation of ğ‘˜integers ğ‘‹ {ğ‘¥1,ğ‘¥2, ...,ğ‘¥ğ‘˜} when using a ğ‘-bit accumulator. Definition 3.2 (Persistent Overflow). Overflow that occurs when the final sum ğ‘¦ Ãğ‘˜ ğ‘– 1 ğ‘¥ğ‘–overflows a ğ‘-bit accumulator. Note that transient overflows may occur even when there is no persistent overflow. We aim to minimize these transient overflows. 3.1 Avoiding All Overflows Several prior works aim to avoid both persistent and transient over- flows entirely by retraining the neural network such that partial sums are always within the accumulator bounds. A2Q [11] and 200 400 600 800 1000 Dot Product Length 0 10 20 30 40 50 60 70 80 90 100 Relative Error ( ) Relative Error of FP8 Gaussian Vector Dot Products for Different Summations Algos Sequential Pairwise MGS w Narrow Accum Figure 3: Error, relative to FP32 precision, of Gaussian vec- tor dot products performed in FP8 precision. We execute each algorithm using solely a narrow accumulator and clip par- tial sums upon overflow.",
    "source": "2504.09072v1_MGS_Markov_Greedy_Sums_for_Accurate_Low-Bitwidth_F.pdf",
    "length": 2070,
    "tokens": 490
  },
  {
    "text": "[38] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang et al., Qwen technical report, arXiv preprint arXiv:2309.16609, 2023. [39] B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J. Zhang, B. Yu, K. Lu et al., Qwen2.5-coder technical report, arXiv preprint arXiv:2409.12186, 2024. [40] M. Liu, T.-D. Ene, R. Kirby, C. Cheng, N. Pinckney, R. Liang, J. Alben, H. Anand, S. Banerjee, I. Bayraktaroglu et al., Chipnemo: Domain- adapted llms for chip design, arXiv preprint arXiv:2311.00176, 2023. [41] S. Liu, W. Fang, Y. Lu, Q. Zhang, H. Zhang, and Z. Xie, Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source dataset and lightweight solution, arXiv preprint arXiv:2312.08617, 2023. [42] Z. Pei, H.-L. Zhen, M. Yuan, Y. Huang, and B. Yu, Betterv: Con- trolled verilog generation with discriminative guidance, arXiv preprint arXiv:2402.03375, 2024. [43] M. Liu, Y.-D. Tsai, W. Zhou, and H. Ren, Craftrtl: High-quality synthetic data generation for verilog code models with correct-by- construction non-textual representations and targeted code repair, arXiv preprint arXiv:2409.12993, 2024. [44] D. Guo, D. Yang, H. Zhang, J.",
    "source": "2506.05007v1_QiMeng_Fully_Automated_Hardware_and_Software_Desig.pdf",
    "length": 1189,
    "tokens": 459
  },
  {
    "text": "For DSP and FF resource prediction, CoGNNs(Î², Î±) 11 Running Title for Header Figure 5: Reduction rates of RMSE loss achieved by CoGNNs(Î²,Î±) over other CoGNNs (For FF, we present the reduction rates of RMSE loss achieved by CoGNNs(Î²,Î²) over other CoGNNs). The bar chart quantifies each model s prediction error reduction ratios (PERR) relative to the top-performing model, while the line chart tracks the evolving PERR trend across evaluation metrics. Table 3: RMSE loss of CoGNNs(Î²,Î±) and SOTA models on unseen applications. The top models are marked in bold. Model Latency LUT DSP FF BRAM All GNN-DSE[32] 0.5359 0.0762 0.1253 0.0632 0.0515 0.8521 HGP SAGE GF[15] 0.9519 0.0152 0.0270 0.0895 0.0362 1.1197 IronMan-Pro[20] 0.6778 0.0081 0.0161 0.0399 0.0326 0.7745 PNA-R[12] 1.3728 0.0144 0.0307 0.0811 0.0610 1.5600 PowerGear[14] 1.3956 0.0177 0.0290 0.0908 0.0432 1.5764 CoGNNs(Î²,Î±) 0.3557 0.0039 0.0075 0.0152 0.0244 0.4067 and CoGNNs(Î², Î²) deliver the best results. For the prediction of LUT and BRAM, CoGNNs(Î², Î±)and CoGNNs(Î², Î²) achieve the smallest prediction errors. Although CoGNNs(Î², Î±) has a slightly higher prediction error (0.0006) than CoGNNs(Î², Î²) in FF prediction, its overall error is 0.083 lower than CoGNNs(Î±, Î±). Thus, we conclude that CoGNNs(Î², Î±) exhibits the best CoGNNs combination on the GNN-DSE dataset. As empirically validated in Fig.",
    "source": "2504.19649v2_Intelligent4DSE_Optimizing_High-Level_Synthesis_De.pdf",
    "length": 1361,
    "tokens": 490
  },
  {
    "text": "Since the bottom MLP layer and the embedding layer, which together determine cfnt, are processed in parallel, the cost function is designed to minimize the sum of the maximum cost between these two independently processed layers and the cost of the top MLP layer. Device Allocation The constraints for assigning cores ca- pable of computing the MLP and embedding layers are as follows: 1 X m dm M 1 m M (4) dm {0, 1} m M (5) dm represents the type of hardware core to be mapped onto the FPGA chip of the SmartSSD. Specifically, when dm 0, the constraint ensures that an MLP core is mapped, whereas when dm 1, it enforces the mapping of an EMB core. Equation 4 controls the allocation of heterogeneous cores, preventing all SmartSSDs from being exclusively assigned to either EMB cores or MLP cores. This allocation ensures that all layers of the DLRM can be effectively accelerated within the multi-SmartSSD system. Embedding Table Allocation To enable model paral- lelism by partitioning the EMB parameters across SmartSSDs mapped to EMB cores, the following constraints are defined: X m pmj 1 j J (6) X j pmj dm X j pmj m M (7) pmj {0, 1} m M (8) pmjis a binary variable that indicates whether EMB j is assigned to SmartSSD m. In Equation 6, a constraint is imposed to ensure that each specific EMB is assigned to its corresponding SmartSSD. Equation 7 ensures that EMBs are only assigned to SmartSSDs that have been allocated EMB cores, serving as a constraint to prevent the incorrect assignment of EMBs to SmartSSDs with MLP cores. By utilizing these constraints, SCRec achieves model parallelism in the embedding layer through table-wise splitting of EMBs in the SRM. Three-level Sharding The EMBs assigned to SmartSSDs are split row-wise based on their access frequency patterns and are fetched into the FPGA s DRAM, BRAM, and SSD memory devices.",
    "source": "2504.00520v1_SCRec_A_Scalable_Computational_Storage_System_with.pdf",
    "length": 1853,
    "tokens": 453
  },
  {
    "text": "Dataset Samples Features Classes Attribute Paths Avg. Length Unique Cond. Size (MB) Redundancy ( ) Adult [36] 48842 14 2 medium 206152 16.91 29436 723.40 99.94 CreditApproval [37] 690 15 2 small 6246 7.39 1934 1.44 99.61 DryBean [38] 13611 16 7 multi-class 52663 12.11 41752 262.12 99.97 Letter [39] 20000 16 26 multi-class 190377 15.59 421 9.55 96.30 Wine [40] 4898 11 11 multi-class 107772 13.53 5467 70.24 99.75 comparisons performed in parallel on a column basis. This high degree of parallelism makes CAM particularly well-suited for applications requiring rapid lookups, such as network routing, database indexing, and cache systems. Ternary content-addressable memory (TCAM) [35] extends the functionality of binary CAM by introducing a third state, don t care, denoted as X. Bits set to the X state are treated as matches during comparison, enabling greater flexibility in search operations. Since each root-to-leaf path in a tree- based model represents a sequence of conditions an input must satisfy to reach the corresponding prediction, TCAM can efficiently traverse these paths, making it a compelling solution for tree-based model acceleration. Prior work [22] proposed mapping each path to a TCAM row, with each column representing a unique condition. Unencountered conditions within a path are assigned the X state as they do not affect the traversal process. When an input instance is received, it is first encoded into a binary sequence that aligns with the condition order stored in TCAM. This encoded sequence is then fed into TCAM for in-memory search. The middle part of Fig. 1 illustrates how TCAM serves as a decision tree accelerator. The decision tree at the left part of Fig. 1 is mapped to the TCAM, where four unique conditions are assigned to separate columns, and each path is represented as a row. Conditions are encoded as 0, 1, and X, corresponding to False, True, and don t care, respectively. Once an input is received, the features are encoded into a binary sequence representing the condition check results, and each bit is then delivered to the corresponding column to perform in-memory search.",
    "source": "2506.05994v1_RETENTION_Resource-Efficient_Tree-Based_Ensemble_M.pdf",
    "length": 2133,
    "tokens": 500
  },
  {
    "text": "Average post-attack test accuracy of the GCN and GAT models on the Cora and PubMed datasets under the GBFA attack at the minimum BER that results in a performance drop. the attacker to reduce the model s accuracy at the defined BER. ASR quantifies the proportion of test samples whose predicted labels are altered as a result of an induced bit-flip fault. B. Layer Sequence Prediction Performance In this section, we assess the accuracy of layer sequence prediction using the layer prediction error rate (LER) defined in Equation 5 [10]. LER is computed as the mean normalized edit distance between the predicted and actual sequence. LER ED(L, L ) L (5) ED(L, L ) represents the edit distance between the pre- dicted layer sequence L and the ground-truth layer sequence L . It is defined as the minimum number of insertions, deletions, and substitutions needed to transform L to L . The term L denotes the length of the ground-truth layer sequence. Table IV represents the LER of GNN models. C. Evaluating the effectiveness of GBFA on various GNN models Figure 5 illustrates the average post-attack test accuracy of GCN and GAT models on Cora and PubMed datasets under the GBFA at minimum BER that result in a performance drop. The results indicate that the BER value varies across models and layers. However, deeper layers in both models are more vulnerable to GBFA attacks. For example, GBFA can achieve 20 accuracy drop by injecting faults in layer three with minimum BER. The summary of evaluation of GBFA at BER of 1e-1 is presented in table V. The results indicate that deeper layers are more susceptible to the attack. Moreover, at a BER of 1e-1, GBFA achieves an average ASR of 75 on GCN and GAT using the Cora dataset by focusing on layer three. Table VI presents the effectiveness of the GBFA attack on the GraphSAGE model using the Cora and PubMed datasets. For both datasets, flipping only one vulnerable bit in layer 2 is sufficient to degrade the accuracy. Moreover, GBFA achieves an ASR of 88 on the Cora dataset and 58 on the PubMed dataset by targeting layer 2 at a BER of 1e-1.",
    "source": "2507.05531v1_Bit-Flip_Fault_Attack_Crushing_Graph_Neural_Networ.pdf",
    "length": 2095,
    "tokens": 471
  },
  {
    "text": "These expert-crafted solutions demand intimate knowledge of microarchitecture details, requiring careful parallelization of computations and memory operations, often implemented in vendor-specific languages or assembly code. While delivering exceptional performance, this manual optimization paradigm fundamentally lacks scalability and portability across different hardware architectures. To address these challenges, in addition to leveraging ex- isting software ecosystems through the aforementioned auto- mated tensor program transcompiler, we pioneer an automated approach called QiMeng-GEMM [77] based on Software Design Agent for generating high-performance libraries with matrix multiplication, i.e. GEMM, as our primary target due to its central role in LLMs [68], [78], deep learning [79], [80], and scientific computing [81]. The proposed QiMeng-GEMM is the first to automatically generate high-performance GEMM code by exploiting LLMs. Specifically, we have abstracted common GEMM optimization methods and hardware archi- tecture features, and created a set of general meta-prompts for LLMs to generate high-performance matrix multiplication op- erators. These meta-prompts enable LLMs to understand and implement optimization goals by capturing the architectural features of different platforms. We then integrate the perfor- mance feedback loop in the Software Design Agent with Tree TABLE VII PERFORMANCE COMPARISON OF GEMM AND CONVOLUTION OPERATIONS ACROSS DIFFERENT HARDWARE PLATFORMS, MEASURED IN GFLOPS (K1, A76) AND TFLOPS (A100). THE A100 GPU UTILIZES TENSOR CORES, WITH SPEEDUP RATIOS (IN PARENTHESES) FOR QIMENG-TENSOROP CALCULATED AGAINST OPENBLAS (K1, A76) AND CUBLAS CUDNN (A100).",
    "source": "2506.05007v1_QiMeng_Fully_Automated_Hardware_and_Software_Desig.pdf",
    "length": 1706,
    "tokens": 398
  },
  {
    "text": "4. Implement a CUDA kernel for signal processing with {size}-point {signal_transform }. Optimize for {optimization}. 5. Implement a CUDA kernel for image filtering using {filter_type} filter of size { filter_size}x{filter_size}. Optimize for {optimization}. Optimization Algorithms 1. Implement a CUDA kernel for simulated annealing with {size} states. Optimize for {optimization}. 2. Generate a CUDA kernel for genetic algorithm with population size {size}. Optimize for {optimization}. 3. Write a CUDA implementation for {optimization_algorithm} with {size} variables. Focus on {optimization}. 4. Write a CUDA implementation for gradient descent optimization with {size} parameters. Focus on {optimization}. 5. Create a CUDA implementation for particle swarm optimization with {size} particles in {dimension}D space. Focus on {optimization}. 18 Cryptography and Security 1. Generate a CUDA kernel for homomorphic encryption operations. Optimize for { optimization}. 2. Write a CUDA implementation for secure hashing using {hash_algorithm}. Focus on { optimization}. 3. Generate a CUDA kernel for {crypto_algorithm} encryption decryption. Optimize for {optimization}. 4. Create a CUDA implementation for blockchain mining with difficulty {size}. Focus on {optimization}. 5. Implement a CUDA kernel for password cracking using {cracking_method}. Optimize for {optimization}. Data Structures 1. Create a CUDA implementation for priority queue with {size} elements. Focus on { optimization}. 2. Create a CUDA implementation for {data_structure} with {size} elements. Focus on {optimization}. 3. Implement a CUDA kernel for operations on a B-tree with {size} nodes. Optimize for {optimization}. 4. Generate a CUDA kernel for skip list operations with {size} elements. Optimize for {optimization}. 5. Write a CUDA implementation for hash table with {size} buckets using { collision_strategy}. Focus on {optimization}.",
    "source": "2505.16968v3_CASS_Nvidia_to_AMD_Transpilation_with_Data_Models_.pdf",
    "length": 1913,
    "tokens": 449
  },
  {
    "text": "7. Conclusion The growing computational demands of modern ML models have highlighted the need for custom ML hardware. How- ever, we find that the hardware design cycle remains heavily dependent on engineers manually ensuring design inter- pretability through extensive documentation and commu- nication, creating a significant bottleneck. Leveraging ML to address this challenge presents a promising opportunity. In this paper, we examine existing research on this topic and identify the computational and learning challenges that remain unresolved. We then discuss how these challenges open new research avenues that future research in this area can address. Critically, progress in adapting LLMs for RTL- to-NL tasks and enhancing hardware design interpretability will not only streamline the hardware development process but could also contribute to broader advancements in ma- chine learning techniques with applications across multiple research domains. References Ieee standard for ip-xact, standard structure for packaging, integrating, and reusing ip within tool flows. IEEE Std 1685-2022 (Revision of IEEE Std 1685-2014), pp. 1 750, 2023. doi: 10.1109 IEEESTD.2023.10054520. Agostinelli, V., Hong, S., and Chen, L. Leapformer: Enabling linear transformers for autoregressive and si- multaneous tasks via learned proportions. In Forty- first International Conference on Machine Learning, 2024. URL id XhH1OKLANY. Ahmad, W., Chakraborty, S., Ray, B., and Chang, K.-W. A transformer-based approach for source code summa- rization. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. (eds. ), Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pp. 4998 5007, Online, July 2020. Association for Compu- tational Linguistics. doi: 10.18653 v1 2020.acl-main. 449. URL acl-main.449 . Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. GQA: Training generalized multi-query transformer models from multi-head check- points.",
    "source": "2504.08852v1_ML_For_Hardware_Design_Interpretability_Challenges.pdf",
    "length": 2004,
    "tokens": 482
  },
  {
    "text": "Li, Generative Modeling for Small-Data Object Detection, in 2019 IEEE CVF International Conference on Computer Vision (ICCV), 2019, pp. 6072 6080. [4] P. Gibson, J. Cano, E. J. Crowley, A. Storkey, and M. O Boyle, DLAS: A Conceptual Model for Across-Stack Deep Learning Acceleration, in ACM Transactions on Architecture and Code Optimization, 2024. [5] S. Mittal, A survey of FPGA-based accelerators for convolutional neural networks, vol. 32, no. 4, pp. 1109 1139. [Online]. Available: [6] X. Zhang, S. Das, O. Neopane, and K. Kreutz-Delgado, A Design Methodology for Efficient Implementation of Deconvolutional Neural Networks on an FPGA, in arXiv:1705.02583, 2017. [7] Y. Yu, T. Zhao, M. Wang, K. Wang, and L. He, Uni-OPU: An FPGA- Based Uniform Accelerator for Convolutional and Transposed Convo- lutional Networks, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, pp. 1545 1556, 2020. [8] J.-W. Chang, K.-W. Kang, and S.-J. Kang, An Energy-Efficient FPGA- Based Deconvolutional Neural Networks Accelerator for Single Image Super-Resolution, IEEE Transactions on Circuits and Systems for Video Technology, pp. 281 295, 2020. [9] J. Yan, S. Yin, F. Tu, L. Liu, and S. Wei, GNA: Reconfigurable and Efficient Architecture for Generative Network Acceleration, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, pp. 2519 2529, 2018. [10] Z. Ma, T. Dai, X. Wei, and G. Luo, An Intermediate-Centric Dataflow for Transposed Convolution Acceleration on FPGA, ACM Transactions on Embedded Computing Systems, 2022. [11] D. Xu, K. Tu, Y. Wang, C. Liu, B.",
    "source": "2507.07683v1_Accelerating_Transposed_Convolutions_on_FPGA-based.pdf",
    "length": 1591,
    "tokens": 481
  },
  {
    "text": "Edge computing platforms, such as mobile and embedded devices, face challenges due to limited processing power, storage, and energy efficiency, making it difficult to deploy these resource-intensive models, especially in real-time applications. Optimizing only from the hardware or software perspective is not enough to address these challenges. Algorithm-hardware co-design integrates both aspects, tailoring algorithms to lever- age hardware features while adapting hardware architectures to meet algorithmic needs. This approach maximizes perfor- mance, resource utilization, and energy efficiency by optimiz- ing the synergy between software and hardware. A. CNN Acceleration Toolflows Toolflows represent a system-level approach to automating the mapping of CNNs onto FPGAs. These toolflows streamline the design process, ensuring rapid deployment and high energy efficiency. As application needs and hardware capabilities evolve, CNN toolflows have adapted to drive continuous innovation in both algorithms and hardware. They encompass a wide range of technologies, from deep learning frame- works to hardware acceleration, automated optimization, and explainability analysis. These tools enable users to generate customized CNN hardware implementations without requiring deep hardware expertise, making FPGA integration more ac- cessible within the deep learning ecosystem. Table V presents various CNN-to-FPGA toolflows that leverage the algorithm- hardware co-design framework to generate optimized FPGA accelerators tailored to specific CNN-FPGA pairs. fpgaConvNet [91] utilized specialized hardware blocks to efficiently map irregular data flows like Inception, Residual, and Dense blocks. It explores the design space based on the Synchronous Data Flow (SDF) model, accounting for platform resource constraints. Several optimizations have been proposed for fpgaConvNet, including a latency-driven design [92] and a 14 Throughput (Gop s) Power (W) Fig. 3. An illustrative comparison of the hardware performance of existing CNN accelerators in terms of power consumption, latency, and throughput. The size of the bubbles corresponds to latency, with larger bubbles indicating higher latency and smaller ones indicating lower latency. The horizontal axis represents power consumption, increasing from left to right, while the vertical axis represents throughput, increasing from bottom to top. heuristic method for pruning the architecture search space. Ve- nieris et al.",
    "source": "2505.13461v1_FPGA-based_Acceleration_for_Convolutional_Neural_N.pdf",
    "length": 2480,
    "tokens": 472
  },
  {
    "text": "[24] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, Codegen: An open large language model for code with multi-turn program synthesis, arXiv preprint arXiv:2203.13474, 2022. [25] OpenAI et al., Gpt-4 technical report, 2024. [26] C. Papon and Y. Xiao, Spinalhdl. [Online]. Available: https: github.com SpinalHDL SpinalHDL [27] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, Gorilla: Large language model connected with massive apis, arXiv preprint arXiv:2305.15334, 2023. [28] Z. Pei, H.-L. Zhen, M. Yuan, Y. Huang, and B. Yu, Betterv: controlled verilog generation with discriminative guidance, in Proceedings of the 41st International Conference on Machine Learning, ser. ICML 24. JMLR.org, 2024. [29] N. Pinckney, C. Batten, M. Liu, H. Ren, and B. Khailany, Revisiting verilogeval: A year of improvements in large-language models for hardware code generation, 2025. [Online]. Available: [30] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, Zero: Memory optimizations toward training trillion parameter models, 2020. [Online]. Available: [31] S. Thakur, B. Ahmad, Z. Fan, H. Pearce, B. Tan, R. Karri, B. Dolan- Gavitt, and S. Garg, Benchmarking large language models for auto- mated verilog rtl code generation, in 2023 Design, Automation Test in Europe Conference Exhibition (DATE), 2023, pp. 1 6. [32] S. Williams and M. Baxter, Icarus verilog: open-source verilog more than a year later, Linux J., vol. 2002, no. 99, p. 3, Jul. 2002.",
    "source": "2506.04544v2_hdl2v_A_Code_Translation_Dataset_for_Enhanced_LLM_.pdf",
    "length": 1480,
    "tokens": 488
  },
  {
    "text": "Cluster channels in Ik (e.g. K-Means on MSE features) into Kk clusters 6: Step 4. Cluster-Level ILP: decide how many channels in each cluster get each bitwidth, subject to Bk 7: Step 5. Intra-Cluster ILP: within each cluster, assign specific channels to bitwidths 8: end for 9: Step 6. Combine final bitwidths into b1, . . . , bN 10: Output: (b1, . . . , bN), total SSE, actual bits used 6.1 Preprocessing for the Pipeline (Step 1-3) To preprocess channels for the hierarchical ILP pipeline, we first compute each channel s MSE under 1-, 2-, and 4-bit quantization. Next, we split channels by parameter count, which in LLMs typically yields two distinct sizes (e.g., 4096 and 11008 for LLaMA-2-7B). Within each group, we then apply three-dimensional K-Means clustering7 (based on the three computed MSE values), forming 128 3Separately defined as NormalFloats lack a 1-bit representation 4See the initial values in Appendix F 5In our implementation, we set number of iterations to 2 6We define SSE (Summed Square Error) as the sum of the squared differences between the original values and their quantized counterparts, i.e., P i xi bxi 2. 7We use 300 as the maximum number of iterations 8 clusters per group in our implementation. 6.2 Cluster-Level ILP (Step 4) Formed clusters first go through the following cluster-level ILP to be assigned budgets of 1-bit, 2-bit, and 4-bit channels. Consider C clusters, each with Sc channels c 1, . . . , C . Let P {1, 2, 4} be the available bit-precisions8.",
    "source": "2502.08141v1_LowRA_Accurate_and_Efficient_LoRA_Fine-Tuning_of_L.pdf",
    "length": 1497,
    "tokens": 404
  },
  {
    "text": "For RISC-V and ARM, a prediction error Etop1 of less than 5 is achievable. If the goal is to find the best sample, it is sufficient to re-execute the top 2 -3 of the predictions later on a real architecture. TABLE IV: Prediction results for ARM-based CPU ID LinReg DNN Bayes XGBoost Etop1( ) Qlow( ) Qhigh( ) Rtop1( ) Etop1( ) Qlow( ) Qhigh( ) Rtop1( ) Etop1( ) Qlow( ) Qhigh( ) Rtop1( ) Etop1( ) Qlow( ) Qhigh( ) Rtop1( ) 0 9.9 3.4 2.9 3.5 0.2 2.8 2.3 1.5 0.0 2.6 2.0 1.0 2.7 3.0 2.2 1.5 1 6.4 3.6 3.2 3.0 4.6 3.2 2.6 2.0 4.6 3.4 2.4 2.0 4.3 3.2 2.6 2.5 2 7.7 3.6 2.3 5.0 0.7 3.3 2.4 1.5 4.3 3.2 2.3 2.5 0.3 3.4 2.2 1.5 3 7.7 2.8 2.6 2.5 1.1 2.8 2.3 1.5 0.0 2.8 2.0 1.0 4.0 3.2 2.2 2.0 4 2.2 3.5 2.5 4.0 0.2 3.1 2.6 1.5 0.0 2.6 2.2 1.0 1.0 3.0 2.3 2.0 TABLE V: Prediction results for RISC-V-based CPU ID LinReg DNN Bayes XGBoost Etop1( ) Qlow( ) Qhigh( ) Rtop1( ) Etop1( ) Qlow( ) Qhigh( ) Rtop1( ) Etop1( ) Qlow( ) Qhigh( ) Rtop1( ) Etop1( ) Qlow( ) Qhigh( ) Rtop1( ) 0 10.9 4.0 3.9 4.0 2.2 4.0 4.0 2.0 0.0 3.8 4.2 1.0 0.0 3.4 4.1 1.0 1 0.0 4.0 4.3 1.0 0.6 3.7 4.5 1.5 2.5 3.4 4.4 2.0 4.6 3.5 4.5 2.5 2 0.0 3.4 3.7 1.0 0.0 3.2 3.8 1.0 0.0 2.8 3.4 1.0 0.0 3.0 3.8 1.0 3 0.0 3.6 3.8 1.0 0.0 3.2 3.9 1.0 2.0 2.8 3.7 1.5 4.4 3.0 3.6 2.0 4 11.0 4.0 4.2 3.0 3.6 3.6 4.2 1.5 10.7 3.2 4.0 2.0 8.2 3.8 4.0 3.0 V. CONCLUSION AND FUTURE WORK In this work, we introduced an interface for executing au- totuning workloads on simulators and explored the feasibility of using instruction-accurate simulators for autotuning of ML workloads.",
    "source": "2505.13357v1_Introducing_Instruction-Accurate_Simulators_for_Pe.pdf",
    "length": 1526,
    "tokens": 781
  },
  {
    "text": "Spargeattn: Accurate sparse attention accelerating any model inference. In International Conference on Machine Learning (ICML), 2025. [49] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. [50] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740 755. Springer, 2014. [51] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. In Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition, pages 22139 22149, 2024. [52] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In Proceedings of the IEEE CVF International Conference on Computer Vision, pages 20144 20154, 2023. [53] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017. [54] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016.",
    "source": "2505.11594v1_SageAttention3_Microscaling_FP4_Attention_for_Infe.pdf",
    "length": 1759,
    "tokens": 478
  },
  {
    "text": "Each X-PE and Y-PE directly broadcast the intermediate results to the corresponding rasterization PE within the same column or row, repeating 16 times. This broadcast overhead is small enough to implement and meets timing closure requirements. X-PE Y-PE x-term y-term x2-term y2-term ex 2 MUL 2 ADD 1 EXP Axis-oriented Rasterization PE Compute Axes â‘  Broadcast â‘¡ Broadcast â‘¡ Combine â‘¢ Axis-oriented Rasterization PE Î± Blending X-PE X-PE X-PE Y-PE Y-PE Y-PE (iii) Y-PE Line (ii) X-PE Line Î¼i x, -1 2ai, ci Î¼i y, -1 2bi (i) Rasterization Array ... ... One GS per Cycle ci(x-Î¼i x) x-term x2-term -1 2ai(x-Î¼i x)2 y-term (y-Î¼i y) y2-term -1 2bi(y-Î¼i y)2 oi x data y data GS Feature Y-PE y Î¼i y -1 2bi y-term y2-term y Î¼i y -1 2bi y-term y2-term Y-PE y Î¼i y -1 2bi y-term y2-term x Î¼i x ci -1 2ai X-PE x-term x2-term Compute Axes â‘  T Notations Figure 4: Hardware and computation flow of axis-oriented rasterization array. Computation flow and PE structure. The rasterization array renders each Gaussian and computes its contribution to a 16 16 tile of pixels continuously in each cycle. The Gaussian parameters are provided as input to the X-PE line and Y-PE line, which perform the axes computation, and the outputs from these PE lines are then fed to the rasterization PE array. To eliminate redundant processing of the Gaussian conic matrix parameters (e.g., computing the factor 1 2), we directly store the parameter set { 1 2ğ‘ğ‘–, 1 2ğ‘ğ‘–,ğ‘ğ‘–}, where ğ‘– denotes the index of each Gaussian.",
    "source": "2506.07069v1_Accelerating_3D_Gaussian_Splatting_with_Neural_Sor.pdf",
    "length": 1482,
    "tokens": 477
  },
  {
    "text": "However, the increasing size of these models intro- duces significant computational, memory, and communication challenges, restricting their deployment to a wide range of devices, especially resource-constrained devices. For many edge applications, such resource demands are impractical, making the deployment of full-scale transformers on edge devices particularly difficult, especially in scenarios requiring real-time inference. To address these challenges, model compression techniques like quantization [5] and pruning [6] have been proposed. Quantization reduces computational requirements by lower- ing numerical bitwidths, though it can lead to performance degradation. Despite these advances, quantized models often remain inefficient on edge devices due to the lack of effective acceleration on existing hardware. Equal contribution Binary transformers, an extreme form of quantized trans- formers, represent all weights and activations using binary val- ues like t 1, 1u or t0, 1u. These models significantly reduce computational complexity, model size, and communication bandwidth by replacing expensive floating-point operations with efficient bitwise logical operations [7], [8], [9], [10]. This approach enables faster execution with lower power consumption, making binary transformers a promising solution for energy efficient edge deployment. Despite these advantages, binary transformer models face significant challenges in implementation on existing hardware. The primary issue is the lack of optimized hardware units for binary matrix multiplication, as most processors and acceler- ators are designed for integer and floating-point computation, which are inefficient for binary operations. Furthermore, previ- ously proposed binary transformer models are often not well- suited for hardware optimization. For instance, BinaryBERT [11] and BiBERT [12] either do not fully binarize the model or retain some ternary representations. Similarly, BiT [13] employs both t 1, 1u and t0, 1u binarization schemes (though not for the same operation) and uses the original softmax function, which imposes a significant performance burden on hardware acceleration design, as will be discussed in a later section. To overcome these limitations, we propose COBRA, a hardware software co-designed Binary Transformer Acceler- ator optimized for edge FPGAs. COBRA introduces several innovations, including the Shifted Polarized Softmax (SPS) for hardware-efficient attention, a true 1-bit binary multiplication method for 1, 0, and 1 matrices, and integer packing to maximize bandwidth.",
    "source": "2504.16269v2_COBRA_Algorithm-Architecture_Co-optimized_Binary_T.pdf",
    "length": 2590,
    "tokens": 499
  },
  {
    "text": "For example, if the input dimension is 512 and 8 heads are used, the output dimension of each head is 64, which is restored to 512 dimensions after splicing. This design ensures the flexibility of multi-view modeling and reduces the computational complexity through dimensional decomposition. While the multi-head attention mechanism inherently lacks positional information and thus typically requires explicit positional encoding, the use of BiGRU mentioned below in our hybrid model addresses this limitation by naturally encoding sequential dependencies. BiGRUs capture temporal context through sequential state transitions, providing an implicit positional representation beneficial for modeling SSD degradation patterns. B. Bidirectional Gated Loop Cell Bidirectional gated recurrent unit (BiGRU) is an improved model based on recurrent neural network (RNN), which can capture long-range dependencies in sequence data more efficiently by combining bi-directional information flow and gating mechanism [10]. Its core design idea lies in utilizing both forward and backward information of sequences and combining the gating unit to dynamically regulate the information flow, to improve the model's ability to interpret the context. The network structure of BiGRU is shown in Fig. 2. Fig. 2. The network structure of BiGRU. Structurally, BiGRU consists of two independent GRU networks: one processes the input sequences in chronological order (forward GRU), and the other processes the input sequences in reverse order (reverse GRU).The GRU unit itself realizes the dynamic control of the information through two key structures, namely, the update gate and the reset gate [11]. The update gate determines how much of the historical memory is retained at the current moment, while the reset gate controls how much of the historical state is rewritten by the current input. This design allows GRU to avoid the gradient vanishing problem of traditional RNNs while being more concise than the LSTM structure. In the bi-directional structure, the GRUs in each of the two directions process the sequence information independently, and ultimately splice or weighted fuse the forward and backward hidden states to form a comprehensive representation containing complete contextual information.",
    "source": "2506.14830v1_Optimization_of_bi-directional_gated_loop_cell_bas.pdf",
    "length": 2287,
    "tokens": 421
  },
  {
    "text": "Assembling such datasets is challenging due to the competitive nature of the industry and the smaller number of chip designers com- pared to software engineers, leading to a lack of open-source RTL designs to use for dataset construction. As shown in Table 4, the total number of Verilog and SystemVerilog files on GitHub is approximately 93 times smaller than the num- ber of Java files. Additionally, the time required to manually label data and the expertise needed for proper labeling fur- ther prevent the use of data annotation platforms (Wang, 5 ML For Hardware Design Interpretability: Challenges and Opportunities 2024), which could otherwise expedite the creation of these datasets. While some progress has been made in automating RTL dataset creation by using publically available RTL designs from GitHub (Thakur et al., 2023), many of the sources that provide reliably structured or labeled data, such as HDLBits (HDLBits, 2024), mainly feature basic problems that do not reflect a broader set of real-world designs. As such, recent works on NL-to-RTL (Liu et al., 2024c) and RTL-to-NL (Liu et al., 2025) tasks tend to heavily rely on synthetic data to compensate for the lack of high-quality, labeled examples. 4.2. Evaluating RTL-to-NL Accuracy While NL-to-RTL tasks can be evaluated by running LLM- generated code against human-written test suites to verify functional correctness, evaluating RTL-to-NL tasks is less straightforward. Natural language descriptions or specifi- cations of hardware must convey critical implementation details, where even minor omissions or inaccuracies can significantly distort meaning. Even when data is available, research on RTL-to-NL tasks still lacks robust approaches for accurately evaluating the quality of natural language documentation. Existing reference-based measures such as BLEU or ROUGE, which target lexical similarity, could give misleadingly high scores to critically different descrip- tions simply due to lexical overlap. Similarly, embedding similarity can compare if an LLM-generated description is semantically close (i.e., uses many semantically related words, like clock, reset, etc. ), but cannot penalize omis- sions or vague descriptions that could lead to serious im- plementation mismatches.",
    "source": "2504.08852v1_ML_For_Hardware_Design_Interpretability_Challenges.pdf",
    "length": 2269,
    "tokens": 487
  },
  {
    "text": "Prefill and Decode Latency Measurement: We use time to first token (TTFT) and time between tokens (TBT) to measure the prefill and decode latency, respectively. TTFT measures the time from when a prompt is submitted to the LLM until the first generated token is produced. It reflects the initial processing delay to infer the context of a given prompt by the LLM. TBT measures the latency of generating the N th token after the LLM has produced N 1 tokens post the prefill stage (Zhang et al., 2024). MEADOW Operation Modes: During the prefill and decode stage, we execute the TPHS dataflow for the Q SM(QKT)xV layers and GEMM is used for the re- maining K, V, Proj and MLP layers. Weight Packing is applied in both stages. Note, during Decode, there is a marginal latency speedup with TPHS compared to GEMM operation for Q SM(QKT)xV since the input token size is 1. As we will see later, the decode stage latency gains are primarily stemming from weight packing. 6.2 Prefill and Decode Latency Improvements Prefill: Fig. 6a and Fig. 6b compares the TTFT achieved by MEADOW and GEMM-based OPT-125M and OPT-1.3B LLM models for varying DRAM bandwidths. At DRAM bandwidth of 12 Gbps, MEADOW achieves 1.5 -1.7 and 1.5-1.6 for OPT-125M and OPT-1.3B LLMs, respectively across different number of prefill tokens. At a low DRAM bandwidth of 1 Gbps, MEADOW achieves 1.57-2.5 and 1.55-2 lower TTFT compared to GEMM implementations for OPT-125M and OPT-1.3B LLM models, respectively. Decode: Fig. 7a and Fig. 7b compare the TBT achieved by MEADOW and GEMM-based approaches on the OPT- 125M and OPT-1.3B LLM models, across varying DRAM bandwidths.",
    "source": "2503.11663v1_MEADOW_Memory-efficient_Dataflow_and_Data_Packing_.pdf",
    "length": 1635,
    "tokens": 453
  },
  {
    "text": "x f32 , out: mref ? x ? x f32 ){ 3 Access: Iterate over segments in a batch 4 slc.for(stream s_b from 0 to num_batches){ 5 stream s_beg slc.mem_str(offs[s_b]); 6 stream s_end slc.mem_str(offs[s_b 1]); 7 Access: Iterate over embeddings in a segment 8 slc.for(stream s_ptr from s_beg to s_end){ 9 stream s_idx slc.mem_str(idxs[s_ptr]); 10 stream vec vlen x f32 buf slcv.buf_str(); Buffer stream 11 Access: Iterate over embedding vector elements 12 slcv.for vlen ((stream s_e, stream msk) from 0 to emb_len){ 13 stream s_val slcv.mem_str vlen (vals[s_idx,s_e], msk); 14 slc.push(buf, s_val); } Push into the buffer 15 Execute: Reduce embedding vectors 16 slcv.callback{ Callback moved at the end of inner loop 17 index b slc.to_val(s_b); 18 vec vlen x f32 buf_vec slc.to_val(buf); Get buffer 19 for(index e 0; e emb_len; e ){ Iterate buffer 20 vec vlen x f32 val buf_vec[e]; Get buffered element 21 vec vlen x f32 acc vload vlen (out[b,e]); 22 vstore vlen (acc val, out[b,e], acc); }}}}}} (c) Bufferized code. 1 void sls(idxs: mref ? x index , offs: mref ? x index , 2 vals: mref ? x f32 , out: mref ? x ?",
    "source": "2504.09870v1_Ember_A_Compiler_for_Efficient_Embedding_Operation.pdf",
    "length": 1102,
    "tokens": 412
  },
  {
    "text": "An ideal system would be at the point (1,1). It can be observed from these results that the proposed ResiSchedule strategy can make good use of the Piezo source when it does exceed the minimal activation thresholds, though the very low duty cycle yields very low throughput. 0.84 0.86 0.88 0.9 0.92 0.94 0.96 0.98 0 0.10.20.30.40.50.60.70.80.9 1 Power Utilization Power efficiency Piezo-LeNet Piezo-FR Piezo-HG Piezo-PV WiFi-h-LeNet WiFi-h-FR WiFi-h-HG WiFi-h-PV WiFi-o-LeNet WiFi-o-FR WiFi-o-HG Thermal-LeNet Thermal-FR Thermal-HG Fig. 10. ResiSchedule power efï¬ciency analysis D. Transition efï¬ciency Table VI-D shows the ratio of inferences using smooth- transitioned partial results and total inference count number. These results indicate that the smooth transition strategy TransitionKeep enables a signiï¬cant fraction of the inferences for all workloads on Piezo. However, a very small fraction is observed with the other, stronger power sources. For Piezo, saving the intermediate results of one incomplete inference is meaningful. However, one power cycle of the other power sources can usually process thousands or hundreds of inferences. TABLE V THE RATIO OF ADDITIONAL INFERENCES ENABLED BY THE SMOOTH TRANSITION STRATEGY VS. TOTAL INFERENCES Piezo WiFi-h WiFi-o Thermal TV-RF LeNet 0.978632 0.000574 0.000782 0.000096 0.000068 FR 0.927445 0.000538 0.000594 0.000067 0.000059 HG 0.862620 0.000319 0.000416 0.000062 0.000049 PV 0.980769 0.002529 0.003181 0.000335 0.000266 E. Power predictor With an accurate power predictor [45], [36], we can make more smooth transitions among different power levels.",
    "source": "ResiRCA.pdf",
    "length": 1613,
    "tokens": 473
  },
  {
    "text": "The carbon-optimized configuration has a 22 lower total carbon footprint but over 3 times longer latency per inference. Note that since the total energy consumption is proportional to latency (i.e. energy power delay), the total latency can not be made arbitrarily long when using a smaller area hardware configuration. This is because excessively long latencies would not only fail to meet realistic performance targets but also start to significantly impact operational carbon costs again. These trade-offs highlight the need for a careful balance between carbon footprint and latency requirements when designing AI systems. Takeaway 2: It is important to tailor the hardware to the specific characteristics of the model architecture. For example, TinyCLIP-61M 32, despite having more parameters, can achieve comparable or even lower latency than TinyCLIP-39M 16 when paired with tailored hardware configuration, also while achieving lower carbon footprint. This is due to differences in parameters, embedding dimensions, and patch sizes, which significantly impact model execution patterns and resulting hardware configurations. These findings emphasize the interdependence of model architecture and hardware design, highlighting the need for co-optimization to enhance resource utilization, performance, and environmental sustainability. 5.2 Joint Model and Hardware Architecture Search Using Different Metrics In this section, we use CATransformers to employ a joint model and the hardware architecture search with carbon footprint as a central design metric, alongside traditional metrics like accuracy and latency. We compare across each optimization modes under a 20 TOPS compute constraint, reflecting a setup comparable to publicly available edge accelerators HAILO (2025); Nvidia (2025). Takeaway 3: Optimizing for total carbon footprint yields model and hardware architectures with the lowest overall carbon impact, but at the cost of increased latency. In contrast, optimizing for energy consumption strikes a balance between latency and carbon footprint. Figure 6 presents the Pareto frontiers for latency-only, carbon-only, energy-only, and latency carbon. Each data point in the figure represents a model and hardware architecture configuration, with accuracy, carbon footprint, and latency represented on the y-axis, x-axis, and color map, respectively. Each experiment is repeated three times for consistency, and accuracy is estimated using the MS COCO dataset. When latency is not an optimization target, a maximum latency constraint of 50ms is enforced Å½Ã¡dnÃ­k et al.",
    "source": "2505.01386v2_Carbon_Aware_Transformers_Through_Joint_Model-Hard.pdf",
    "length": 2587,
    "tokens": 477
  },
  {
    "text": "[22] M. Liu, Y.-D. Tsai, W. Zhou, and H. Ren, Craftrtl: High-quality synthetic data generation for verilog code models with correct-by- construction non-textual representations and targeted code repair, arXiv preprint arXiv:2409.12993, 2024. [23] Y. Zhao, D. Huang, C. Li, P. Jin, Z. Nan, T. Ma, L. Qi, Y. Pan, Z. Zhang, R. Zhang et al., Codev: Empowering llms for verilog generation through multi-level summarization, arXiv preprint arXiv:2407.10424, 2024. [24] S. Thakur, B. Ahmad, H. Pearce, B. Tan, B. Dolan-Gavitt, R. Karri, and S. Garg, Verigen: A large language model for verilog code generation, ACM Transactions on Design Automation of Electronic Systems, vol. 29, no. 3, pp. 1 31, 2024. [25] Z. Pei, H.-L. Zhen, M. Yuan, Y. Huang, and B. Yu, Betterv: Con- trolled verilog generation with discriminative guidance, arXiv preprint arXiv:2402.03375, 2024. [26] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., Language mod- els are few-shot learners, Advances in neural information processing systems, vol. 33, pp. 1877 1901, 2020. [27] Y. Song, G. Wang, S. Li, and B. Y. Lin, The good, the bad, and the greedy: Evaluation of llms should not ignore non-determinism, arXiv preprint arXiv:2407.10457, 2024. [28] M. Renze, The effect of sampling temperature on problem solving in large language models, in Findings of the Association for Computa- tional Linguistics: EMNLP 2024, 2024, pp. 7346 7356.",
    "source": "2507.02226v1_DecoRTL_A_Run-time_Decoding_Framework_for_RTL_Code.pdf",
    "length": 1476,
    "tokens": 499
  },
  {
    "text": "After a sensor detects an activity, it anticipates the next activity to be the current classiï¬ed activity, looks up for the best sensor, and signals to activate it for the upcoming inference. However, this leads to another potential issue - what if the current inference is running on the best sensor, and the sensor does not have enough energy to run the next inference? In this case, the current sensor chooses the next best sensor for the job and signals it. The other sensor receives this as an external signal and activates itself to classify the activity. To incorporate the ER-r, we induce delays between sending the external signal and starting the inference on the same sensor. This delay depends of the extended round- robin policy. Combination of ER-r and AAS, results in more than 70 accuracy for most of the activities (Fig. 4). 0 20 40 60 80 100 Walking Climbing Cycling Running Jogging Jumping Accuracy RR3 RR3 with AAS RR6 RR6 with AAS RR9 RR9 with AAS RR12 RR12 with AAS Fig. 4: Accuracy results for AAS combined with ER-r. Even though AAS provides signiï¬cantly better results com- pared to standard round-robin, it is still unable to incorporate ensemble learning. The major challenge is the inability to run inferences in all the sensors simultaneously because of the harvested energy budget. Therefore, we need to ï¬nd the classiï¬cation result for all the sensors without activating them. Extending our assumption from AAS, we hypothesize that the most recent classiï¬cation result of a sensor must be a good representation of what its inference would be for the current activity. Hence, by memorizing or recalling the most recent classiï¬cation result, we can get the inference result of a sensor even without activating it. Even though the sensors are running in the round-robin fashion, the non-participating sensors can still impact the classiï¬cation result by virtue of recalling their most recent classiï¬cation. Combining the Recall with AAS (which we term as AASR - Activity Aware Scheduling with Recall) opens possibilities for getting a more accurate classiï¬cation. To minimize the communication overhead, and to ensure participation of all sensors, we build the recall strategy into the host device.",
    "source": "Origin.pdf",
    "length": 2226,
    "tokens": 494
  },
  {
    "text": "We also provide a breakdown of memory consumption, showing both the overhead of microscaling for the FP4 format as well as the overhead of the FGMP metadata (the per-block bit to distinguish FP4 and FP8 blocks). 5.4.2 Energy Analysis. Figure 9 shows the energy efficiency of our FGMP datapaths for different percentages of FP8 blocks in both 10 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 Normalized Energy Consumption 5.075 5.100 5.125 5.150 5.175 5.200 5.225 5.250 5.275 Perplexity (Wikitext-103) FGMP NVFP4 FP8 1 Error FGMP (70 NVFP4) Figure 10: Perplexity on Wikitext-103 versus normalized en- ergy consumption using FGMP with sensitivity-weighted clipping with different percentages of blocks retained in FP8, using the Llama-2-7B model. weights and activations. When evaluated only with stimulus re- stricted to a single data format (the labeled boxes in the figure), the NVFP4 datapath consumes 33 less energy relative to the FP8 datapath, while the FP4 8 and FP8 4 (Weight Activation) datapaths consume 16 and 17 less energy than the FP8 baseline, respec- tively. The additional overheads of muxing between the different dot-product units at fine granularity imposes a small tax\" to en- able FGMP, such that mostly FP8\" data costs slightly more energy than 100 FP8 data. However, this overhead is small compared to the energy savings of performing lower-precision arithmetic when most blocks are quantized to FP4. We also measured the energy consumption of the mixed-precision activation quantization unit performing on-the-fly activation quan- tization under random stimulus. The energy consumption of per- forming quantization on a single block is 25.7 pJ. However, this operation need only be performed after reduction, so it is amor- tized over the dot product dimension of the input tensors, which is at least 4096 for the layers of the Llama-2-7B network.",
    "source": "2504.14152v1_FGMP_Fine-Grained_Mixed-Precision_Weight_and_Activ.pdf",
    "length": 1870,
    "tokens": 470
  },
  {
    "text": "DIMM-PIM aligns perfectly with the characteristics of decoding MHA, as it enables memory bandwidth and capacity to scale pro- portionally with the number of DIMMs. Besides, FC operations are executed on the GPU side, with the batch size of these batchable parts maximized to fully utilize the GPU s computational capacity. 2.5 Challenges of DIMM-PIM Integration Integrating DIMM-PIM in GPU systems for LLM inference can be challenging for the following reasons: Hardware challenges. As shown in Fig. 3-a, DIMM leverages multi- ple co-operated DRAM chips as a rank to form the burst data on the memory bus, with each chip contributing a portion of the total data 3 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen Table 1: Related works fall short in the scenario with long contexts and large batch sizes. CPU offloading introduces a bottleneck in terms of computational efficiency and communication overhead with long contexts. PIM can effectively provide bandwidth expansion, but may be limited by its memory scalability, inappropriate kernel mapping, or inefficient cross-device scheduling. Low latency means no sacrifice in TBT compared with the GPU-only baseline in our scenario. Method Work Year Platform Capacity Scalability Bandwidth Scalability Low Latency Comm-comp Overlap Bubble Reduction Software optimization FlexGen [78] 2023 GPU CPU - CachedAttention [25] 2024 GPU CPU - NEO [40] 2025 GPU CPU Hardware-software co-optimization NeuPIMs [33] 2024 GPU HBM-PIM AttAcc [72] 2024 GPU HBM-PIM IANUS [76] 2024 GPU GDDR-PIM Hermes [70] 2025 GPU DIMM-PIM PAPI [32] 2025 GPU HBM-PIM L3 - GPU DIMM-PIM width. In this case, a KV element s different bits can be distributed to different chips after offloading (e.g., FP16 and 8 chips), which completely restricts PIM computation performed in each chip.",
    "source": "2504.17584v1_L3_DIMM-PIM_Integrated_Architecture_and_Coordinati.pdf",
    "length": 1915,
    "tokens": 465
  },
  {
    "text": "These PPs are then accumulated using 3-2 compressor to optimize computational efficiency. In summary, our core contributions are as follows: 1) We propose a finer-grained TPE notation and introduce new methods and ideas for designing and optimizing PE microarchitecture in specific applications. Unit Bit Area(um2) Delay(ns) TOP(uW) MAC 20 179.30 1.56 27.1 24 192.65 1.67 29.2 28 206.01 1.84 31.4 32 238.51 1.97 36.3 4-2 Compressor Tree 14 55.92 0.31 8.5 Full Adder 14 51.32 0.34 7.7 Accmulator 20 57.32 0.80 8.6 24 62.43 0.90 9.4 28 82.78 0.99 12.3 32 95.13 1.13 14.3 TABLE I: The main component decomposition in INT8 MAC (tested on SMIC 28nm with a 2ns clock constraint). 2) Compared to Pragmatic [5] and Laconic [38], we pro- vide a more systematic explanation of the fundamen- tal reasons for bit-sparse acceleration and design a more efficient PE micro-architecture suitable for bit- serial processing, characterized by low area and high frequency. Additionally, we discuss the comparison of other encoding methods for bit-sparse acceleration of the multiplicand. 3) Based on the new notation and transformations, we propose four optimization methods and we implement our design in RTL using the 28nm process. Applying our methods to four classic TPE architectures (include systolic array [20], 3D-Cube [27], multiplier-adder tree [48], and 2D-Matrix [30]), we achieved area efficiency improvements of 1.27 , 1.28 , 1.56 , and 1.44 , and 1.04 , 1.56 , 1.49 , and 1.20 for energy efficiency respectively. When applied to a bit-slice architecture, we achieved a 12.10 improvement in energy efficiency and 2.85 in area efficiency compared to Laconic [38]. II. BACKGROUND AND MOTIVATION A. High Width Accumulator Represents the Most Challenge For AI DSA, the performance of the TPE is key to ensuring DNN throughput.",
    "source": "2503.06342v1_Exploring_the_Performance_Improvement_of_Tensor_Pr.pdf",
    "length": 1817,
    "tokens": 486
  },
  {
    "text": "We also use macro F1-score as the primary eval- uation metric, as done by approaches in the literature, to mitigate the risk of inflated accuracy metrics that could result from the inherent class imbalance in the dataset. Our results show that the generated Q-ConvLSTM IP through our FINN extensions demonstrates superior prediction performance over several widely used financial prediction models. Specifically, it outperforms: SVM [18] by 42 , MLP [18] by 29 , TABLE III: Performance comparison of DeepLOB, ConvLSTM and Q-ConvLSTM model for the three different labels in the FI-2010 dataset. Class Metric DeepLOB ( ) ConvLSTM ( ) Q-ConvLSTM ( ) Downward-0 Precision 76.50 75.20 76.66 Recall 76.68 77.26 78.44 F1-Score 76.59 76.22 77.54 Stationary-1 Precision 82.92 81.66 83.25 Recall 77.95 71.23 73.50 F1-Score 80.36 76.09 78.07 Upward-2 Precision 74.69 72.36 73.53 Recall 78.77 79.06 79.93 F1-Score 76.68 75.56 76.60 TABLE IV: Comparison of the proposed Q-ConvLSTM model against the proposed state-of-the-art HFT models in the re- search literature.",
    "source": "2506.20810v1_FINN-GL_Generalized_Mixed-Precision_Extensions_for.pdf",
    "length": 1052,
    "tokens": 302
  },
  {
    "text": "(2023); Zhao and Guo (2023); Lacoste et al. (2019); Wu et al. (2022); Zhao et al. (2024); Li et al. (2024). Recent developments of frameworks like ACT Gupta et al. (2022), IMEC.netzero IMEC (2025), and LLMCarbon Faiz et al. (2024) enable the modeling of embodied carbon in ML systems, but a comprehensive solution for accurately quantifying total carbon footprint both operational and embodied is still lacking. Moreover, joint co-optimization techniques to minimize ML systems carbon footprint during custom hardware design remain unaddressed, representing a critical gap in sustainable AI. 3 Framework Overview In this section, we introduce CATransformers, a carbon-aware architecture search framework for sustainability- driven co-optimization of ML models and hardware architectures. As shown in Figure 2, CATransformers takes three inputs: (1) a base ML model, (2) a hardware architecture template, and (3) optimization metrics and constraints, which define the hardware and software search space. The framework consists of three core components: a multi-objective optimizer and two evaluation modules an ML model evaluator and a hardware estimator. Below, we describe each component in detail. Layer n Layer 2 Head Head Head Head Layer n-1 Embedding FFN 2 FFN 1 Pruned blocks: Text Vision Encoder Layer 1 Figure 3 Overview of the dimensions pruned for the encoder. Each layer within a transformer is pruned to the same dimensions, and the Text and Vision encoders separately. 3.1 CATransformers Inputs Base Model: The base model is a large, pre-trained model that serves as the foundation for pruned models generated by the framework. It determines the overall architecture, shape, and functionality of the optimized models. This work focuses on CLIP-based architectures, but the framework can extend to other models. Pruning is performed along multiple dimensions, including the number of layers, feedforward network size, attention heads, and embedding dimension (as illustrated in Figure 3). Hardware Architecture Template: The template (Figure 4) defines the accelerator s components and search parameters, based on prior academic and industry designs Jouppi et al. (2017); Adnan et al. (2024); Zhang et al. (2022); Wang et al. (2024).",
    "source": "2505.01386v2_Carbon_Aware_Transformers_Through_Joint_Model-Hard.pdf",
    "length": 2245,
    "tokens": 498
  },
  {
    "text": "5 Conclusion This paper presents VeriReason, a comprehensive framework integrating supervised fine-tuning with GRPO-based reinforcement learning for Verilog RTL generation. By combining explicit reasoning with testbench-driven feedback, our approach addresses key challenges in LLM-based hardware design: data scarcity, weak language-code alignment, lack of self-checking behavior, and insufficient logical reasoning. While VeriReason achieves state-of-the-art performance across model scales, the approach incurs significant computational overhead during both training (requiring numerous testbench evaluations per iteration) and inference (where reasoning steps increase generation time by 2.5-3 ). Despite these limitations, the consistent improvements across different architectures demonstrate our method s robustness and transferability. Our reward model s integration of structural correctness and functional validation encourages models to develop intrinsic self-checking capabili- ties a crucial advancement for autonomous hardware design. By open-sourcing our models and datasets, we aim to accelerate progress in LLM-based hardware design, transform digital circuit development practices, and inspire future work on computational efficiency. 9 References [1] Jason Blocklove, Siddharth Garg, Ramesh Karri, and Hammond Pearce. Chip-chat: Challenges and opportunities in conversational hardware design. In 5th ACM IEEE Workshop on Machine Learning for CAD, MLCAD. IEEE, 2023. [2] Kaiyan Chang, Ying Wang, Haimeng Ren, Mengdi Wang, Shengwen Liang, Yinhe Han, Huawei Li, and Xiaowei Li. Chipgpt: How far are we from natural language hardware design. CoRR, abs 2305.14019, 2023. [3] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv, 2107.03374, July 2021. [4] Yonggan Fu, Yongan Zhang, Zhongzhi Yu, Sixu Li, Zhifan Ye, Chaojian Li, Cheng Wan, and Yingyan Celine Lin. GPT4AIGChip: Towards Next-Generation AI Accelerator Design Automation via Large Language Models. arXiv, 2309.10730, September 2023.",
    "source": "2505.11849v1_VeriReason_Reinforcement_Learning_with_Testbench_F.pdf",
    "length": 2175,
    "tokens": 496
  },
  {
    "text": "And the entire dataset is utilized for training. To fully leverage the potential of this dataset, we employ a curriculum learning strategy, enabling the model to incrementally build knowl- edge by starting with simpler cases and advancing to more complex ones. The curriculum learning strategy involves transitioning from more granular to less granular annota- tions across hierarchical levels, which can be conceptualized as a tree structure with the following components (as shown in Figure 9): 1. Hierarchical Levels (First Layer) The training process transitions sequentially across the three hierarchical levels line, block, and module. Each level is fully trained before moving to the next, ensuring a solid foundation at simpler levels before addressing more complex tasks. 2. Granularity of Descriptions (Second Layer) Within each hierarchical level, the annotations transition from detailed descriptions to high- level descriptions. This progression ensures that the model learns finer details first and then builds an understanding of higher-level abstractions. 20 Published as a conference paper at ICLR 2025 Line Level Block Level Module Level Detailed Descriptions Medium-Detailed Descriptions High-Level Descriptions Detailed Specifications High-Level Functional Descriptions GPT- Annotated Human- Annotated Training Process GPT- Annotated Human- Annotated GPT- Annotated Human- Annotated GPT- Annotated Human- Annotated GPT- Annotated Human- Annotated 1 4 5 3 2 6 9 7 8 10 11 12 13 16 14 15 17 18 Training Data Figure 9: The adopted curriculum learning strategy visualized as a tree structure. Specifically, the terminals of the tree, enclosed by blue dotted boxes, represent specific training datasets. Our curriculum learning strategy follows a pre-order traversal of this tree structure. 3. Annotation Source Transition (Third Layer) At each level and granularity, training starts with GPT-annotated data and is followed by human-annotated data. This sequence leverages large-scale machine-generated annotations first and refines the model with high-quality, human-curated data. 4. Instruction Blending Each terminal node in this tree represents a specific training dataset, which blends tasks for Verilog understanding and Verilog generation. This enables the model to perform well across diverse tasks. The training process mirrors a pre-order traversal of this tree structure: 1. Starting at the root, training begins with the line level. 2.",
    "source": "2502.15832v1_DeepRTL_Bridging_Verilog_Understanding_and_Generat.pdf",
    "length": 2462,
    "tokens": 497
  },
  {
    "text": "They report a sparse-over-dense speedup of up to 2 for an Nvidia A100 GPU using 16bit floating-point models. We compare our FlexiSAGA architecture together with the proposed DNN pruning method to the CONV operator sparse-over-dense speedup of the one- sided SCNN and SparTen accelerators presented in [8] as they provide detailed per DNN operator results. Additionally, we compare our whole DNN sparse- over-dense speedup to DeepSparse and TensorRT, run on an Intel Xeon CPU, Nvidia Orin ARM CPU and GPU respectively. 3 Sparse Matrix Formats An important factor for processing sparse GEMM operations is in which format sparse matrices are stored. A sparse matrix format should have a good compres- sion ratio to minimize the memory footprint and allow for efficient decompres- sion such that it does not slow down the processing of a GEMM operation. In the following section, different sparse matrix formats are briefly introduced and compared to each other. Additionally, we present the compressed sparse block (CSB) format, which is tailored towards the sparse GEMM processing of Flexi- SAGA. Fig. 1(a) shows a comparison of the different sparse matrix formats for a 128 512 matrix with varying sparsities and uniformly distributed zeros. The compressed sparse row (CSR) format stores the non-zero elements of a matrix in an array, along with two additional arrays that store the column in- dices and the row pointers. The row pointers array indicates the starting index of each row in the non-zero elements array. Similarly to CSR is the compressed 1 (accessed March 13, 2025). 4 M. M. MÃ¼ller and K. LÃ¼beck et al.",
    "source": "2506.01566v1_FlexiSAGA_A_Flexible_Systolic_Array_GEMM_Accelerat.pdf",
    "length": 1616,
    "tokens": 370
  },
  {
    "text": "arXiv:2412.19437. URL [22] DeepSeek-V3-0324 Release, news250325, [Accessed 28-04-2025]. [23] A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv, C. Zheng, D. Liu, F. Zhou, F. Huang, F. Hu, H. Ge, H. Wei, H. Lin, J. Tang, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Zhou, J. Lin, K. Dang, K. Bao, K. Yang, L. Yu, L. Deng, M. Li, M. Xue, M. Li, P. Zhang, P. Wang, Q. Zhu, R. Men, R. Gao, S. Liu, S. Luo, T. Li, T. Tang, W. Yin, X. Ren, X. Wang, X. Zhang, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Zhang, Y. Wan, Y. Liu, Z. Wang, Z. Cui, Z. Zhang, Z. Zhou, Z. Qiu, Qwen3 technical report (2025). arXiv:2505.09388. URL [24] N. C. of Technology Innovation for EDA, 2024 China Postgrad- uate IC Innovation Competition: EDA Elite Challenge Q10 Guide, 905c4088385441fea110889b1fdeb30d.pdf (2024). [25] F. Salvaire, Pyspice, accessed: 2025-02-12. URL [26] A. Said, M. Shabbir, B. Broll, W. Abbas, P. V olgyesi, X. Kout- soukos, Circuit design completion using graph neural networks, Neu- ral Computing and Applications 35 (16) (2023) 12145 12157. doi: 10.1007 s00521-023-08346-x.",
    "source": "2504.10240v2_GNN-ACLP_Graph_Neural_Networks_based_Analog_Circui.pdf",
    "length": 1109,
    "tokens": 470
  },
  {
    "text": "GenEDA bridges this gap by aligning the encoder s structural understanding of netlists with the decoder s generative strengths. This encoder-decoder alignment enables generating high-level functionality directly from low-level netlist inputs, which is an unprecedentedly challenging task due to the irreversible nature of logic synthesis. Specifically, GenEDA reasons functionalities of given netlists in a wide spectrum of granularities, with outputs including: (1) general function description, (2) circuit implementation details, and (3) fine- grained exact RTL code. These GenEDA-supported new generative 1In netlist functionality reasoning tasks, the high-level specification and RTL code as ground-truth are unknown to the model. Models are only provided with the low-level netlist as inputs. arXiv:2504.09485v1 [cs.LG] 13 Apr 2025 Input Task Arch. Method Format Modality Pred. Gen. Description Direction DeepGate [18], etc. AIG Graph function pred. Reverse CircuitFusion [8] RTL quality pred. Forward MGVGA [9] AIG func. quality pred. R F Encoder NetTAG [19] Netlist Graph Text func. quality pred. R F RTLCoder [20], etc. Spec RTL gen. HDLDebugger [21], etc. RTL RTL debug AssertLLM [15], etc Spec Text (Image) assertion gen. Forward Decoder DeepRTL [22] RTL Spec Text RTL understand. gen. R F Enc-Dec GenEDA Netlist Graph Text Function gen. Reverse Task direction is forward if they follow the VLSI design flow (e.g., predicting quality at the early stage, generating RTL from spec), and reverse if they go against it (e.g., predicting or generating function from netlist). Reverse tasks are challenging due to the design flow s irreversible nature. This work [22] leverages T5, an encoder-decoder LLM. However, it targets only generative tasks, so we categorize it as a circuit decoder. TABLE I: Comparison of GenEDA with representative categories of circuit foundation models. Existing circuit encoders mainly leverage graph structure for prediction tasks, while circuit decoders focus on semantic text for generation tasks.",
    "source": "2504.09485v1_GenEDA_Unleashing_Generative_Reasoning_on_Netlist_.pdf",
    "length": 2034,
    "tokens": 483
  },
  {
    "text": "3 summarizes the computational steps that are per- formed by the ExpMul operator to calculate ex V and Alg. 4 using (3), (5), and (9) illustrates how the new hardware operator is integrated in FlashAttention-2 kernel. Algorithm 3 ExpMul(x, V) Input: x 0 and a vector V of N elements Output: A vector Out where Out[i] exV [i] 1: for i 1 : N do 2: SV , EV , MV extract(V [i]) 3: Ë†x Fixed(Clip(x, 15, 0)) 4: Ë†L Ë†x Ë†x 1 Ë†x 4 5: Out[i] Float(SV , EV Ë†L, MV ) 6: end for Algorithm 4 FlashAttention2 with Fused ExpMul Operators 1: for each query q do 2: for i 1 : N do 3: si dot( q, ki) 4: mi max(mi 1, si) 5: o i ExpMul(mi 1 mi, o i 1) ExpMul(si mi, v i ) 6: end for 7: [â„“N oN] o N 8: attn( q, K, V ) oN â„“N 9: end for The proposed ExpMul operators not only remove expensive floating-point operations like exponent function evaluation and multiplication but perform this step with low-cost quantization and without any dequantization step. Traditional quantization approaches [22] would utilize additional quantization logic to transfer operation to and from the integer domain, thus paying the extra hardware cost of this transformation. Also, even if operating in the integer domain, the computation would still involve costly multiplication and exponential operations. The proposed approach removes both overheads and simplifies significantly the computation in FlashAttention-2. The parallel hardware organization of FlashAttention-2 ker- nel employing the proposed ExpMul operators is shown in Fig. 2. The organization of the new hardware unit is the same as the original FlashAttention-2 accelerator shown in Fig. 1. However, we can highlight one key difference.",
    "source": "2505.14314v2_Low-Cost_FlashAttention_with_Fused_Exponential_and.pdf",
    "length": 1661,
    "tokens": 456
  },
  {
    "text": "FASTLIBRA-WOL in- creases the TTFT and TPOT in all the test cases, with an average increase of 1.13X and 1.11X, respectively. With the FASTLIBRA-WOL, the peak supported throughput is also decreased by 13.1 on average. Compared to FASTLIBRA- WOS, FASTLIBRA-WOL s serving performance is increased as it considers part of our cost model (Equation 5), but still has a gap to the FASTLIBRA. The insufficient LoRA loading in some dynamic scenarios can lead to a large number of LoRA cold-starts. As each query inference can only start once the required LoRA is matched in HBM, it can leads to the increase of TTFT. 6.9 The Impacts of a Large Number of LoRAs In this subsection, we investigate the effectiveness of FASTLI- BRA when thousands of LoRAs exist, although real-world scenarios always only have tens of LoRAs [3, 51]. we use the Llama-7B under the chatbot scenario as an example. The LoRA number is 1000 or 2000, and we set three types of 11 0 100 200 300 400 TTFT(ms) vLLM S-LoRA FastLibra 1000-Distinct 1000-Uniform 1000-Skewed-0.1 1000-Skewed-0.3 2000-Distinct 2000-Uniform 2000-Skewed-0.1 2000-Skewed-0.3 0 20 40 60 80 100 TPOT(ms) Figure 16: The TTFT and TPOT of FASTLIBRA and baselines with different LoRA numbers and distributions. The x-axis represents the combination of LoRA number and distribution. LoRA distributions: 1) Uniform, where queries have an equal usage probability for each LoRA. 2) Distinct, where queries are handled by polling to use a LoRA. 3) Skewed-x, where we construct queries using different LoRAs based on Gaussian distribution and set different standard deviations x. Figure 16 shows the TTFT and TPOT of FASTLIBRA, vLLM, and S-LoRA, respectively. We can observe that FASTLIBRA has both the lower TTFT and TPOT in all the test cases, with an average decrease of 55.4 and 16.2 , respectively.",
    "source": "2505.03756v1_Improving_the_Serving_Performance_of_Multi-LoRA_La.pdf",
    "length": 1828,
    "tokens": 501
  },
  {
    "text": "2) Comparision with classical TPE architecture: As de- picted in Table VII, we implement the OPT1 on conventional architectures such as TPU (systolic array), Ascend (3D-Cube), Trapezoid (multiplier-adder tree), and FlexFlow (2D-Matrix). For FlexFlow (2D-Matrix), OPT2 is employed. Subsequently, we compare the performance enhancements before and after applying these optimizations, using them as benchmarks. Based on our previous analysis of the area efficiency per PE, we observe an increase in area efficiency across all four Figure 13: The normalized speedup and energy consumption ratio of TPEs composed of OPT4E and parallel MAC. Best Case Worst Case General Case Best Case Worst Case General Case (A) (B) Figure 14: (A) Throughput for different PEs. 1 Parallel MAC (246um2) 3 OPT4C-PE (81.27um2) 1 OPT4E- PE (311um2). (B) Energy consumed per multiplication- accumulation operation. microarchitectures, by a factor of 1.27, 1.28, 1.58, 1.34 and 1.44, respectively. Energy efficiency was increased by 1.04, 1.56, 1.49, 1.11 and 1.20 times, respectively. Moreover, for the OPT2 particularly in FlexFlow (2D-Matrix), there was a slight improvement over OPT1 which aligns with our previous analysis of PE area efficiency. The reason for this improvement is that the 2D-Matrix architecture broadcasts inputs across its rows and columns, allowing a single input DFFs to be shared among PE rows and columns, leading to a dilution of the area of OPT2 s input registers, demonstrating the advantage of having lower bit-widths within PEs. 3) Comparison with the bit-slice architecture: Choosing Laconic as the comparison baseline for bit-slice architecture (Bitlet, Sibia, BitWave, OPT3, OPT4C, and OPT4E in Table VII) reveals a common trait: these methods typically improve energy efficiency significantly but generally lack in area effi- ciency. Despite their compact size, they aren t as computation- ally efficient, making it challenging to significantly increase the computational power per unit area. In terms of computa- tional efficiency, the bit-slice technique can be improved in two main ways.",
    "source": "2503.06342v1_Exploring_the_Performance_Improvement_of_Tensor_Pr.pdf",
    "length": 2099,
    "tokens": 500
  },
  {
    "text": "InTech, Apr. 2018. [23] L. Danial et al., A Pipelined Memristive Neural Network Analog-to-Digital Converter, in IEEE International Symposium on Circuits and Systems, Oct. 2020, pp. 1 5. [24] , Delta-Sigma Modulation Neurons for High-Precision Training of Memristive Synapses in Deep Neural Networks, in IEEE International Symposium on Circuits and Systems, May 2019, pp. 1 5. 15 [25] J. Wagner et al., Man or machine design automation of delta-sigma modulators, in 2018 IEEE International Symposium on Circuits and Systems (ISCAS), 2018, pp. 1 5. [26] B. Jacob et al., Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference, in IEEE CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 2704 2713. [27] A. Verdant et al., High-linearity sigma-delta converter, Patent WOEP2016 053687, 2016. [28] W. Guicquero et al., Incremental Delta Sigma Modulation with Dynamic Weighted Integration, in IEEE International Midwest Symposium on Circuits and Systems, 2018, pp. 344 347. [29] B. Wang et al., A 550 ÂµW 20 kHz BW 100.8 DB SNDR linear-exponential multi-bit incremental converter with 256-cycles in 65 nm CMOS, in IEEE Symp. VLSI Circuits, 2018, pp. 207 208.",
    "source": "2506.16903v1_RCNet_Î”Î£_IADCs_as_Recurrent_AutoEncoders.pdf",
    "length": 1206,
    "tokens": 332
  },
  {
    "text": "[58] Z. Bai, P. Dangi, H. Li, and T. Mitra, Swat: Scalable and efficient window attention-based transformers acceleration on fpgas, ArXiv, vol. abs 2405.17025, 2024. [59] C. Raffel, N. M. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, J. Mach. Learn. Res., vol. 21, pp. 140:1 140:67, 2019. [60] J. Dass, S. Wu, H. Shi, C. Li, Z. Ye, Z. Wang, and Y. Lin, Vitality: Unifying low-rank and sparse approximation for vision transformer acceleration with a linear taylor attention, 2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA), pp. 415 428, 2022. [61] H. Shi, H. Shao, W. Mao, and Z. Wang, Trio-vit: Post-training quan- tization and acceleration for softmax-free efficient vision transformer, IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 72, pp. 1296 1307, 2024. 13 [62] T. G. A. Zeng et al., Chatglm: A family of large language models from glm-130b to glm-4 all tools, ArXiv, vol. abs 2406.12793, 2024.",
    "source": "2505.03745v1_AccLLM_Accelerating_Long-Context_LLM_Inference_Via.pdf",
    "length": 1073,
    "tokens": 351
  },
  {
    "text": "In International Conference on Machine Learning, pages 21813 21824. PMLR, 2023. 1 [26] Xiaoyu Liu, Xin Ding, Lei Yu, Yuanyuan Xi, Wei Li, Zhi- jun Tu, Jie Hu, Hanting Chen, Baoqun Yin, and Zhiwei Xiong. Pq-sam: Post-training quantization for segment any- thing model. In European Conference on Computer Vision, pages 420 437. Springer, 2024. 2 [27] Yijiang Liu, Huanrui Yang, Zhen Dong, Kurt Keutzer, Li Du, and Shanghang Zhang. Noisyquant: Noisy bias-enhanced post-training activation quantization for vision transformers. In Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition, pages 20321 20330, 2023. 2 [28] Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao. Post-training quantization for vision trans- former. Advances in Neural Information Processing Systems, 34:28092 28103, 2021. 1 [29] Chengtao Lv, Hong Chen, Jinyang Guo, Yifu Ding, and Xi- anglong Liu. Ptq4sam: Post-training quantization for seg- ment anything. In Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition, pages 15941 15951, 2024. 2, 3, 6, 7, 5 [30] Alaa Maalouf, Ninad Jadhav, Krishna Murthy Jatavallab- hula, Makram Chahine, Daniel M Vogt, Robert J Wood, An- tonio Torralba, and Daniela Rus. Follow anything: Open- set detection, tracking, and following in real-time. IEEE Robotics and Automation Letters, 9(4):3283 3290, 2024. 1 [31] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Chris- tos Louizos, and Tijmen Blankevoort. Up or down? adap- tive rounding for post-training quantization. In International Conference on Machine Learning, pages 7197 7206. PMLR, 2020.",
    "source": "2503.03088v2_AHCPTQ_Accurate_and_Hardware-Compatible_Post-Train.pdf",
    "length": 1621,
    "tokens": 479
  },
  {
    "text": "12, pp. 2220 2228, 2021. [12] V. Jain, S. Giraldo, J. D. Roose, L. Mei, B. Boons, and M. Verhelst, Tinyvers: A tiny versatile system-on-chip with state-retentive emram for ml inference at the extreme edge, JSSC, vol. 58, no. 8, 2023. [13] P. P. Bernardo, C. Gerum, A. Frischknecht, K. L ubeck, and O. Bringmann, Ultratrail: A configurable ultralow-power tc-resnet ai accelerator for efficient keyword spotting, IEEE Trans. on Computer- Aided Design of Integrated Circuits and Systems, vol. 39, no. 11, pp. 4240 4251, 2020. [14] K. Kim, C. Gao, R. Grac a, I. Kiselev, H.-J. Yoo, T. Delbruck, and S.-C. Liu, A 23-uw keyword spotting ic with ring-oscillator-based time-domain feature extraction, IEEE J. of Solid-State Circuits, vol. 57, no. 11, pp. 3298 3311, 2022. [15] G. Karunaratne, M. Hersche, J. Langeneager, G. Cherubini, M. L. Gallo, U. Egger, K. Brew, S. Choi, I. Ok, C. Silvestre, N. Li, N. Saulnier, V. Chan, I. Ahsan, V. Narayanan, L. Benini, A. Sebastian, and A. Rahimi, In-memory realization of in-situ few-shot continual learning with a dynamically evolving explicit memory, in ESSCIRC 2022- IEEE 48th European Solid State Circuits Conf. (ESSCIRC), 2022, pp. 105 108. [16] J. Snell, K. Swersky, and R. Zemel, Prototypical networks for few- shot learning, in Advances in Neural Information Processing Systems, vol. 30, 2017. [17] B. M. Lake, R. Salakhutdinov, and J.",
    "source": "2505.24852v2_Chameleon_A_MatMul-Free_Temporal_Convolutional_Net.pdf",
    "length": 1378,
    "tokens": 485
  },
  {
    "text": "[10] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [11] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735 1780, 1997. [12] Haiyang Huang, Newsha Ardalani, Anna Sun, Liu Ke, Hsien-Hsin S Lee, Anjali Sridhar, Shruti Bhosale, Carole-Jean Wu, and Benjamin Lee. Towards moe deployment: Mitigating inefficiencies in mixture-of-expert (moe) inference. arXiv preprint arXiv:2303.06182, 2023. [13] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79 87, 1991. [14] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. [15] Michael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2):181 214, 1994. [16] Diederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [17] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with condi- tional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.",
    "source": "2506.07366v1_MoE-GPS_Guidlines_for_Prediction_Strategy_for_Dyna.pdf",
    "length": 1483,
    "tokens": 468
  },
  {
    "text": "In Figure 6(d), the loop tiling technique integrated with both duplication and pipelining is used to schedule all layers on the I4-L1 I4- L2 I1- L2 I3-L1 I3- L2 I3-L1 I3-L1 I3-L1 I1-L1 I1- L2 Ä‚ Power Cycles Power Harvested power (a) Naive 1 Ä‚ Power Cycles Power Harvested power (b) Naive 2 Ä‚ I1- L2 -T1 I1 -L2 -T2 Power Cycles Power Harvested power (c) Sequential Ä‚ I1-L1- T1 I1-L1- T1 I1-L1- T1 I1-L1- T1 I1-L1- T2 I1-L1- T2 I1-L1- T2 I1-L1- T2 I2- L2 -T1 I2 -L2 -T2 I2-L1- T1 I2-L1- T1 I2-L1- T1 I2-L1- T1 I2-L1- T2 I2-L1- T2 I2-L1- T2 I2-L1- T2 I1-L1 I1-L1 Ä‚ I1- L2 I1- L2 Ä‚ Power Cycles Harvested power Ä‚ I1-L1- T1 I1-L1- T1 I1-L1- T2 I1-L1- T2 Ä‚ Power I1- L2 -T1 I1 -L2 -T2 Power Cycles Harvested power Ä‚ I1-L1- T1 I1-L1- T1 I1-L1- T1 I1-L1- T1 I1-L1- T2 I1-L1- T2 I1-L1- T2 I1-L1- T2 Ä‚ Power I1-L1- T3 I1-L1- T3 I1- L2 -T1 I2-L1- T1 I2-L1- T1 I2-L1- T2 I2-L1- T2 I1- L2 -T2 I1- L2 -T3 I2-L1- T3 I2-L1- T3 I1- L2 -T4 I3-L1- T1 I3-L1- T1 I3-L1- T1 I3-L1- T1 I3-L1- T2 I3-L1- T2 I3-L1- T2 I3-L1- T2 I2- L2 -T1 I2 -L2 -T2 I4-L1- T1 I4-L1- T1 I4-L1- T1 I4-L1- T1 I4-L1- T2 I4-L1- T2 I4-L1- T2 I4-L1- T2 I3- L2 -T1 I3 -L2 -T2 I5-L1- T1 I5-L1- T1 I5-L1- T1 I5-L1- T1 I5-L1- T2 I5-L1- T2 I5-L1- T2 I5-L1- T2 I4- L2 -T1 I4 -L2 -T2 I2- L2 -T1 I2 -L2 -T2 I2-L1- T1 I2-L1- T1 I2-L1- T1 I2-L1- T1 I2-L1- T2 I2-L1- T2 I2-L1- T2 I2-L1- T2 I3-L1- T1 I3-L1- T1 I3-L1- T1 I3-L1- T1 I3-L1- T2 I3-L1- T2 I3-L1- T2 I3-L1- T2 I2- L2 -T1 I2 -L2 -T2 I4-L1- T1 I4-L1- T1 I4-L1- T1 I4-L1- T1 I4-L1- T2 I4-L1- T2 I4-L1- T2 I4-L1- T2 I3- L2 -T1 I3 -L2 -T2 I5-L1- T1 I5-L1- T1 I5-L1- T1 I5-L1- T1 I5-L1- T2 I5-L1- T2 I5-L1- T2 I5-L1- T2 I4- L2 -T1 I4 -L2 -T2 (d) Pipelining (e) ResiSchedule I1-L1 I1-L1 I1-L1 I2-L1 I2- L2 I2-L1 I2-L1 I2-L1 I1-L1 PC-i PC-i 1 PC-i PC-i 1 PC-i PC-i 1 PC-i PC-i 1 PC-i PC-i 1 I5-L1 I5- L2 I5-L1 I5-L1 I5-L1 I3-L1 I3- L2 I3-L1 I3-L1 I3-L1 I4-L1 I4-L1 I4-L1 Fig. 6. Five layer scheduling schemes: (a) Naive execution Simple architecture; (b) Naive execution ResiRCA architecture; (c) Se- quential resilient execution ResiRCA architecture; (d) Pipelining resilient execution ResiRCA architecture; and (e) Hybrid resilient execution ResiRCA architecture ResiRCA architecture; we call this execution style Pipelining.",
    "source": "ResiRCA.pdf",
    "length": 2219,
    "tokens": 1341
  },
  {
    "text": "Your answer should strictly be \" True \" or \" False \", no other content is allowed to be generated. Figure 4: Detailed prompts used in the CoT annotation process. C DISCARDING VERILOG CODE EXCEEDING 2048 TOKENS In the main submission, we state that Verilog modules and blocks exceeding 2048 tokens are ex- cluded, as 2048 is the maximum input length supported by CodeT5 . Beyond this limitation, several additional factors motivate this decision: 15 Published as a conference paper at ICLR 2025 Figure 5: The distribution of the token lengths of the generation benchmark by Chang et al. (2024a). 1. Generation Capabilities of Existing LLMs Are Limited to Small Designs Existing benchmarks for Verilog generation, including the one used in our work (Chang et al., 2024a), do not include designs exceeding 2048 tokens, with the maximum token length observed in the benchmark being 1851. As shown in Table 3 of the main submis- sion, even the state-of-the-art LLM, o1-preview, is capable of accurately generating only simple designs and struggles with more complex ones. Figure 5 illustrates the token length distribution across the benchmark, further justifying our decision to exclude Verilog mod- ules and blocks exceeding 2048 tokens. 2. Segmentation as a Common Practice Segmenting longer code into smaller chunks that fit within the predefined context window and discarding those that exceed it is a widely accepted practice in both Verilog-related research (Chang et al., 2024b; Pei et al., 2024) and studies on software programming lan- guage (Wang et al., 2023a). This approach ensures compatibility with current LLMs while maintaining the integrity and usability of the dataset. It is worth noting that the default maximum sequence length in CodeT5 is 512 tokens, and our work extends this limit to 2048 tokens to better accommodate Verilog designs. 3. Empirical Findings and Practical Challenges Our experiments reveal an important empirical observation: existing LLMs, such as GPT- 4, consistently produce accurate descriptions for shorter Verilog modules but struggle with correctness when handling longer ones. Specifically, During the annotation process, we divide the dataset into two sections: Verilog designs with fewer than 2048 tokens, and de- signs with token lengths between 2048 and 4096 tokens.",
    "source": "2502.15832v1_DeepRTL_Bridging_Verilog_Understanding_and_Generat.pdf",
    "length": 2314,
    "tokens": 491
  },
  {
    "text": "This hardware-driven adaptive scheduling signiï¬cantly impacts different data modal- ities, from large-scale to small-scale, and various magnitudes of energy income, as depicted in Fig. 10c. For scenarios with larger and predictable energy income, software-based backup and restore mechanisms can offer signiï¬cant beneï¬ts, as the energy consumed for such operations is typically a small fraction of the overall energy income. Predictive actions for saving the system state can be easily taken. However, in situations with sporadic energy income, the hardware-assisted scheduling becomes paramount. It ensures that active PEs efï¬ciently utilize available power to complete work, preventing potential losses and eliminating the need to restart tasks from the beginning. Us. as excels as a candidate for continuous learning at all scales due to the hardware s adaptability to varying data and model sizes. As data and model dimensions decrease, the hardware assistance s impact becomes more pronounced, making Us. as an excellent solution for continuous learning across diverse application sizes. Along with morphable hardware, the exemplar selection and the micro-proï¬ler play an important role for the success of Us. as. When power is highly uncertain, the morphable hardware also strongly contributes, however, as the power proï¬le becomes stable, the algorithmic contributions dominate. Fig. 10a shows the contribution of the different components of Us. as under different power proï¬les. Moreover, the algo- rithmic contributions can be extended into any classiï¬cation based application or data modality. If the learning has to be unsupervised, one needs to experiment with known clustering techniques to decide the right classiï¬cation approach. We demonstrate this by testing the exemplar selection and the Î¼ proï¬ler with different modalities of data. Our workloads included Audio [23], [35](speech classiï¬cation), 3D Point Clouds [14], [24](object classiï¬cation) and Inertial Measure- ment Unit sensor data [9], [106](fault and activity detection). Observe that, as Us. as is designed to handle dense and noisy data, it outperforms the respective state-of-the-arts (which were tuned for small, clean benchmark data).",
    "source": "Usas.pdf",
    "length": 2217,
    "tokens": 489
  },
  {
    "text": "It allows a broader exploration of the energy landscape and facilitates better mixing to avoid local energy minima solutions. In this work, we implement 100 parallel chains running at a constant temperature geo- metrically spaced from temp0 (0.01) and tempn (40). After every 15 sampling steps, adjacent pairs of chains are swapped alternatively staring with odd and even chain indexes. The update rule is given as: r exp 1 T1 1 T2 (H(sT1) H(sT2)) (3) Pswap(sT1 sT2) min(1, r) (4) 3. FPGA Setup PCIe In this work, we implement the vectorized mapping on the Xilinx Virte UltraScale VCU118 FPGA evaluation kit. The FPGA is interfaced with CPU using peripheral component intercon- nect express (PCIe) interface. In particular, we use open-source xillybus IP core for the interface with data transfer capabilities of 50MB s. The data is transferred via a memory mapped interface implemented using block memory and a designed memory controller. The digital implementation of 256 nodes and 16 color accelerators supports a network of 1024 probabilistic Ising nodes. These nodes consist of an LFSR-based pseudorandom number generator with a fixed seed, a lookup table-based sigmoidal activation, higher order multiplexer- based matrix multiplication, and a threshold that generates the output state of the neuron. These states are stored in the local memory of FPGA and then transfered to CPU via PCIe interface. 7 Acknowledgements This work is supported by MURI project from Department of Defense and Office of Naval Research. Author Contributions C.G. formulated the vectorized scheme, performed the CPU GPU benchmarks and designed the vectorized accelerator on FPGA. C.G, and S.S co-wrote the manuscript; S.S supervised the research. All authors contributed to discussions and commented on the manuscript. Competing Interests The authors declare that they have no competing financial interests. Correspondence Correspondence and requests for materials can be addressed to either C.G. or S.S. Code and Data Availability Code and Data will be made available on reasonable request by emailing C.G. or S.S. 8 Wij ğ»ğ‘  ğ‘Š!",
    "source": "2505.20250v1_Efficient_Optimization_Accelerator_Framework_for_M.pdf",
    "length": 2110,
    "tokens": 488
  },
  {
    "text": "Video Inference on Mobile Platforms The key difference between a video-based DNN application and other popular DNN inferencing applications like natural language processing (NLP) or speech-to-text is that, the former interacts with video frames which are either captured from the camera or downloaded streamed from internet and hence, has a strict latency requirement for performing inferencing within the frame deadline. As shown in the HW (Hardware) layer in Fig. 1, a typical mobile neural network video inference system has two major hardware components: (i) an SoC with a CPU GPU NPU for processing the intensive computations, and an intermediate buffer in DRAM for storing the video frames as well as the intermediate data between layers of DNNs, and (ii) a video decoder communicating with the SoC ,typically via the memory bus. Running on top of the hardware, as shown in the Application layer in Fig. 1, the neural network video inference software pipeline can be summarized as follows: Input: The raw video data is ï¬rst stored in memory (usually in the H.264 MPEG format). A hardware-based H.264 MPEG decoder decodes the compressed video bitstreams to obtain the original video frames. These frames are then buffered in a memory buffer, waiting for the next stage NN Inference. NN Inference: In this stage, the neural network (NN) pipeline takes the decoded video data to perform the inference tasks with the available compute engines (e.g., CPU, GPU, NPU, or any dedicated ASICs). Numerous prior studies (e.g., see [21] [23] and the references therein) have clearly shown that, regardless of the type of the compute hardware employed, the NN inferences are both compute and memory intensive. As a result, this stage is the main bottleneck in the NN applications. Output: Following the inference stage, the resulting Feature- Maps (FMs) are used to generate the ï¬nal tags bounding-boxes and ï¬nally report to the application (e.g., a cow has been identiï¬ed in the image with 95 conï¬dence). Fig. 1: A DNN inference pipeline on an edge device with optimizations in the application, system, and hardware levels. B.",
    "source": "PCframeSim.pdf",
    "length": 2120,
    "tokens": 482
  },
  {
    "text": ", (14) 9 where Aj,k is the element at jth row and kth column of the final attention output. As shown in Fig. 11 (a), using a single row of Q as an example, the computations in Eq. (14) mainly consists of five stages. In Stage 1, QKT is computed for several rows of K. The results are subsequently processed by the NPE in Stage 2 and 3 to obtain exp(S ) and P exp(S ), respectively. Further, the exp(S ) will be multiplied by V in Stage 4 and accumulated to obtain P exp(S )V in Stage 5. These five stages are repeated until all rows of K are processed, generating the final P exp(S )V and P exp(S ). Finally, the output is obtained by dividing P exp(S )V by P exp(S ). This rearrangement allows the multiplication of V to be executed before completing the division, thus enabling com- putation fusion and reducing data access costs. While the on- chip buffer is sufficient to store intermediate results (vector) during the decode stage, this kernel fusion technique also facilitates pipelining processing within attention computations. Therefore, we apply kernel fusion in both the prefill and decode stages to improve performance. 2) Layer Fusion for the Decode Stage: In the decode stage, the input and output activations are small vectors rather than large matrices, allowing them to be entirely stored within the on-chip buffer of the FPGA. To minimize off-chip memory access, we fuse computations of all layers in this stage by directly using the output of the current layer as the input to the subsequent layer, as illustrated in Fig. 11 (b). 3) Reusing K V for the Prefill Stage: As illustrated in Fig. 11 (c), the Î›-shaped attention patterns between adjacent rows exhibit a one-token shift overlap, offering an opportunity to reuse K and V data during the prefill stage. As such, we vertically split the attention into several tiles based on the size of RCE in our accelerator and process them sequentially. Within each tile, multiple attention rows are computed in parallel while maintaining shared access to the KV data. VI. EXPERIMENTS A.",
    "source": "2505.03745v1_AccLLM_Accelerating_Long-Context_LLM_Inference_Via.pdf",
    "length": 2049,
    "tokens": 465
  },
  {
    "text": "Therefore, rather than depending on a training dataset, our approach utilizes the generation outputs produced during deployment, while serving user requests, for learning multi-token generation. This method offers two key benefits. First, by leveraging user-generated content as labels, the approach can be seen as an unsupervised learning technique, eliminating the need for extra datasets. Second, the learning process is performed during the model deployment time, avoiding a separate training process to learn multi-token generation. In this paper, we consider parallel decoding ap- proaches as they are more training-efficient compared to other speculative decoding methods, making it suitable for online learning. The training objective is formulated as follows: arg min Ï• Ex D KL Pa(yt 1:t k y1:t, x; Ï•), Po(yt 1:t k y1:t, x; Î¸) where Ï• are the acceleration parameters, and D is the deployment data distribution. The KL-divergence measures the difference between the Pa distribution for acceleration and target Po distributions. yt 1:t k represents the predicted token sequence, and y1:t the previously generated tokens by target model with parameter Î¸. VI. EXPERIMENTS A. Evaluation Setup Models and Datasets Our proposed framework is applica- ble to a wide range of machine learning methods. However, w o Finetune FL Finetuned Metric(Ratio) FL Finetuned Metric(Acc) 0 20 40 60 80 100 accuracy ( ) 52.15 0.38 71.77 65.75 70.22 64.09 Syntax Accuracy Semantic Accuracy (a) HLS MachineEval. w o Finetune FL Finetuned Metric(Ratio) FL Finetuned Metric(Acc) 0 20 40 60 80 100 accuracy ( ) 12.0 2.0 68.0 64.0 64.0 60.0 Syntax Accuracy Semantic Accuracy (b) HLS HumanEval. w o Finetune FL Finetuned Metric(Ratio) FL Finetuned Metric(Acc) 0 10 20 30 accuracy ( ) 18.0 24.0 26.0 (c) Qiskit Benchmark. Fig. 4: Evaluation of federated learning on both classical and quantum hardware benchmarks. due to their growing popularity and practical relevance, we focus on LLMs in our experiments.",
    "source": "2506.00002v1_Advancing_AI-assisted_Hardware_Design_with_Hierarc.pdf",
    "length": 1985,
    "tokens": 492
  },
  {
    "text": "w o. sc. CORE Cloud Platform Resnet18 7.28 6.80 6.68 5.26 4.62 7.44 6.79 6.76 6.14 5.66 Resnet50 7.29 7.30 7.31 6.47 5.47 7.40 6.92 7.34 6.97 6.18 Mbnet-V2 7.01 6.79 6.20 5.80 4.33 6.89 6.75 6.30 6.54 5.14 BERT 7.31 6.85 6.74 5.95 5.60 7.08 6.74 6.94 6.52 5.99 NCF 3.50 3.48 3.66 3.47 2.17 4.65 3.41 4.53 4.16 4.06 DLRM 2.57 2.60 2.83 3.73 2.03 4.09 3.72 4.06 4.08 4.05 VGG16 7.85 7.43 7.51 5.84 5.05 7.92 7.65 7.52 7.12 6.08 Edge Platform Resnet18 7.20 7.06 7.03 5.33 5.31 7.39 6.79 7.04 6.50 5.59 Resnet50 7.37 7.12 - 6.74 5.47 7.25 6.92 - 7.02 6.26 Mbnet-V2 7.07 6.74 - 6.49 4.97 7.22 6.73 - 6.35 5.16 BERT 7.18 6.76 - 6.37 5.83 7.35 6.70 - 6.66 6.02 NCF 3.86 3.32 3.89 3.65 2.78 4.64 3.65 4.56 4.34 4.06 DLRM 2.68 2.58 3.68 3.88 3.37 4.13 3.12 4.38 4.46 4.05 VGG16 7.58 7.71 7.87 5.84 5.13 7.97 7.66 7.87 6.93 6.14 simulation-based evaluation uses the MAESTRO cost model [25] for performance and Synopsys DC, Cadence Innovus for area estimation (via RTL synthesis with the Nangate 15nm library) [19]. We set a fixed sampling budget of 40,000 for all algorithms to maintain consistency.",
    "source": "2506.03474v1_CORE_Constraint-Aware_One-Step_Reinforcement_Learn.pdf",
    "length": 1089,
    "tokens": 523
  },
  {
    "text": "[108] A. Hosny, S. Hashemi, M. Shalan, and S. Reda, DRiLLS: Deep reinforcement learning for logic synthesis, in Proceedings of the Asia and South Pacific Design Automation Conference, 2020, pp. 581 586. [109] G. Pasandi, M. Peterson, M. Herrera, S. Nazarian, and M. Pedram, Deep-PowerX: A deep learning-based framework for low-power ap- proximate logic synthesis, in Proceedings of the ACM IEEE Interna- tional Symposium on Low Power Electronics and Design, 2020, pp. 73 78. [110] A. Mirhoseini, A. Goldie, M. Yazgan, J. Jiang, E. Songhori, S. Wang, Y.-J. Lee, E. Johnson, O. Pathak, S. Bae et al., Chip placement with deep reinforcement learning, arXiv preprint arXiv:2004.10746, 2020. [111] Y.-C. Lu, J. Lee, A. Agnesina, K. Samadi, and S. K. Lim, GAN- CTS: A generative adversarial framework for clock tree prediction and optimization, in Proceedings of the IEEE ACM International Conference on Computer-Aided Design, 2019, pp. 1 8. [112] S. Nagaria and S. Deb, Designing of an optimization technique for the prediction of CTS outcomes using neural network, in Proceedings of the IEEE International Symposium on Smart Electronic Systems. IEEE, 2020, pp. 312 315. [113] Y. Kwon, J. Jung, I. Han, and Y. Shin, Transient clock power esti- mation of pre-CTS netlist, in Proceedings of the IEEE International Symposium on Circuits and Systems, 2018, pp. 1 4. [114] Y. He and F. S. Bao, Circuit routing using monte carlo tree search and deep neural networks, arXiv preprint arXiv:2006.13607, 2020.",
    "source": "2506.05007v1_QiMeng_Fully_Automated_Hardware_and_Software_Desig.pdf",
    "length": 1494,
    "tokens": 442
  },
  {
    "text": "Advantage: (1) Component type recognition: Because the matrix in- dexes are inherently ordered by the input vertex sequence, PM eliminates the need to assign identifiers to distinguish 3 LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog Topology Generation LM input: Duty cycle options 0.1 0.3 0.5 0.7 0.9 sep voltage conversion ratio 0.85973 sep efficiency 0.93273 sep Vertex order: VIN VOUT GND Sa Sa C L sep LM output: Duty cycle: unselect unselect select unselect unselect sep Connections: VIN no_edge no_edge no_edge edge_1 no_edge no_edge no_edge VOUT no_edge no_edge no_edge no_edge no_edge edge1 edge_1 GND no_edge no_edge no_edge no_edge edge_1 edge_1 no_edge Sa edge_1 no_edge no_edge no_edge edge_2 edge_2 no_edge Sa no_edge no_edge edge_1 edge_2 no_edge edge_1 edge_2 C no_edge edge_1 edge_2 no_edge edge_2 no_edge edge_1 L no_edge edge_1 no_edge no_edge edge_2 edge_1 sep LM input: Duty cycle options 0.1 0.3 0.5 0.7 0.9, voltage conversion ratio 0.85973, efficiency 0.93273, vertex order: VIN VOUT GND Sa0 Sa1 C0 L0 LM output: Connections: ( VIN , Sa0 ), ( VOUT , C0 , L0 ), ( GND Sa1 , C0 ), ( Sa0 , Sa1 , L0 ). The duty cycle is set to 0.3. Canonical formulation (CF) Pure-text adjacency-matrix-based formulation (PM) Explore a tabular formulation Input numbers as float to transformer Duty cycle options 0.1 0.3 0.5 0.7 0.9 sep voltage conversion ratio 0.85973 sep efficiency 0.93273 sep Embedding (E) E Linear layer (L) L L Float-input adjacency-matrix-based formulation (FM) E E LaMAGIC LaMAGIC2 LM input: 0.1 0.3 0.5 0.7 0.9 0.85973 0.93273 sep VIN VOUT GND Sa Sa C L sep LM output: duty_0.3 sep VIN no_edge no_edge no_edge edge_1 no_edge no_edge no_edge VOUT no_edge no_edge no_edge no_edge no_edge edge1 edge_1 GND no_edge no_edge no_edge no_edge edge_1 edge_1 no_edge Sa edge_1 no_edge no_edge no_edge edge_2 edge_2 no_edge Sa no_edge no_edge edge_1 edge_2 no_edge edge_1 edge_2 C no_edge edge_1 edge_2 no_edge edge_2 no_edge edge_1 L no_edge edge_1 no_edge no_edge edge_2 edge_1 sep Succinct float-input adjacency-matrix-based formulation (SFM) LM input: 0.1 0.3 0.5 0.7 0.9 0.85973 0.93273 sep VIN VOUT GND Sa 0 Sa 1 C 2 L 3 sep LM output: duty_0.1 sep VIN Sa 0 , VOUT C 2 L 3 , GND Sa 1 C 2 , Sa 0 Sa 1 L 3 sep Succinct float-input canonical formulation with identifier (SFCI) Shrink redundant words in natural language Figure 3.",
    "source": "2506.10235v1_LaMAGIC2_Advanced_Circuit_Formulations_for_Languag.pdf",
    "length": 2377,
    "tokens": 814
  },
  {
    "text": "The crossbar networks play a pivotal role in the system s dataflow management, including pipelining and ordering for dynamic dataflow, enabling dequantization-free computation for multi-precision values in the RMPU as well as top-k sorting and quantization in the VVPU. Also, the RMPU Engine serves as the core module for computational power. These results justify the observed area and power consump- tion of LightNobel. When compared to GPUs, LightNobel requires only 21.94 of area and 19.37 of power compared to A100, and 21.63 of area and 22.60 of power compared to H100. It achieves up to 37.29 , 43.35 higher power efficiency than A100 and H100 with the chunk option and up to 5.39 , 5.21 without it. These results are particularly significant, as the LightNobel accelerator is implemented in a 28nm process, whereas A100 and H100 use more advanced 7nm and 4nm processes, underscoring LightNobel s superior area and power efficiency. Moreover, since LightNobel supports significantly longer sequence lengths compared to GPUs, it is expected to have even better efficiency for longer sequence lengths. 9 Related Works and Discussion 9.1 Previous Works on PPM There have been efforts to optimize PPM, but they have failed to address critical memory-related challenges. Fastfold [14] and Scale- fold [70] tackle communication overhead issues between multiple GPUs during PPM training by employing scheduling and paral- lelism techniques. While these methods improve training scalabil- ity, the benefits are limited in the inference phase. MEFold [32] and PTQ4Protein [51] introduce quantization to PPM. MEFold applies weight-only quantization, leaving memory-related challenges aris- ing from activation unresolved. It supports a maximum sequence length of 2,828 with a peak memory requirement of 78.7 GB in an 80 GB memory environment. LightNobel achieves the same with just 12.1 GB of memory, improving scalability by 6.05 . PTQ4Protein quantizes both weights and activations but conducts experiments only on proteins with a maximum sequence length of 700, reducing peak memory to 11.6 GB. For the same sequence length, Light- Nobel achieves a peak memory usage of 7.1 GB, indicating 1.63 better scalability.",
    "source": "2505.05893v1_LightNobel_Improving_Sequence_Length_Limitation_in.pdf",
    "length": 2213,
    "tokens": 496
  },
  {
    "text": "Structured sparsity typically results in lower sparsity levels because important weights often co-exist within the same structure as prunable weights, preventing their independent removal. In contrast, unstructured sparsity achieves greater sparsity due to its flexibility, making it a better approach for model compression in scenarios where lower data transfer is desired and where hardware can handle its computational challenges. SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs Linear 90.4 Others 9.6 8 Cores Linear 85.8 Others 14.2 16 Cores Linear 81.0 Others 19.0 32 Cores Figure 3. Inference Breakdown: Linear layers dominate the la- tency during LLM inference. Results profiled from Llama 3 8B running on Intel(R) Xeon(R) Gold 6430L CPU with 512 context length. 2.3 GEMM Mapping During both inference stages, the most compute-intensive operations in LLMs are mapped to matrix multiplication, also known as General Matrix Multiply (GEMM). These operations dominate the computational workload in both the core linear layers and attention mechanisms. Figure 3 shows that linear layers dominate the latency, especially at small contexts. Linear layers in LLMs can be represented as matrix mul- tiplications, as illustrated in Figure 2. In the example, the weight matrix (W) contains the weights, where each column corresponds to the weights of a single neuron. Each neuron uses its unique set of weights to process inputs. The input matrix (IN) contains the input values, with each row repre- senting an individual input. Each input undergoes identical processing, being multiplied by the same set of weights. The result of this computation is the output matrix (OUT). Each column in OUT corresponds to a neuron, with its rows containing the computed outputs for each input. The num- ber of rows is the same as the number of inputs as an output row is computed for each of the input rows. 2.4 Advanced ISA Extensions Our kernel is designed to utilize two specialized instruc- tion sets: AVX (Advanced Vector Extensions) and AMX (Advanced Matrix Extensions). AVX is a set of SIMD (Single Instruction, Multiple Data) instructions extending the x86 architecture, enabling parallel operations on data vectors. It uses special AVX registers to store arguments and outputs for these operations.",
    "source": "2502.12444v1_SparAMX_Accelerating_Compressed_LLMs_Token_Generat.pdf",
    "length": 2319,
    "tokens": 489
  },
  {
    "text": "2) 6:3 Compressor-Based Popcount instead of CPU- Optimized Popcount: The design of the Popcount unit is a critical consideration for the RBMM Engine, as it is replicated h times in each RBMM PE. The CPU-optimized popcount, as introduced in [25], leads to additional 12K LUTs and 1536 DSPs due to the inclusion of multipliers. 3) Pipeline Execution of RBMM Engine instead of Serial Execution: As outlined in Section III-B4, the components within the RBMM Engine are fully pipelined, and the engine s initiation interval (II) is one clock cycle. If pipeline scheduling is disabled, the RBMM Engine s execution after each call is serialized, resulting in only 20 of the throughput achieved by the pipelined version. V. CONCLUSION In this paper, we propose COBRA, a co-designed Binary Transformer Accelerator optimized for edge FPGAs. Our approach incorporates a hardware-friendly Shifted Polarized Softmax (SPS) design, a Real 1-bit Binary Matrix Multi- plication Engine (RBMM), and an adaptive computational flow, enabling us to fully harness the potential of binary transformer acceleration. COBRA achieves a 3.5 improve- ment in throughput and a 2.2 gain in energy efficiency with negligible accuracy degradation compared to state-of-the-art binary transformer accelerators. These on-board evaluation results highlight the performance of our design, enabling highly efficient edge inference of transformer models. REFERENCES [1] J. D. M.-W. C. Kenton and L. K. Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, in Proceedings of naacL-HLT, vol. 1. Minneapolis, Minnesota, 2019, p. 2. [2] D. Alexey, An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv: 2010.11929, 2020. [3] H. Kaeley, Y. Qiao, and N. Bagherzadeh, Support for stock trend prediction using transformers and sentiment analysis, arXiv preprint arXiv:2305.14368, 2023.",
    "source": "2504.16269v2_COBRA_Algorithm-Architecture_Co-optimized_Binary_T.pdf",
    "length": 1918,
    "tokens": 475
  },
  {
    "text": "Evaluation Metrics: We evaluate the performance of the GBFA attack using three metrics: post-attack test accuracy (PAC), the number of flipped bits (nbit), and the Attack Success Rate (ASR). Post-attack test accuracy refers to the percentage of test data correctly classified by the GNN model after the attack. nbit represents the number of bits flipped by TABLE V EVALUATION OF GBFA PERFORMANCE ON GNN MODELS USING CORA AND PUBMED DATASETS AT BER 1E-1 GNN Model Dataset Layer 1 Layer 2 Layer 3 PAC ASR nbit PAC ASR nbit PAC ASR nbit GCN Cora 68 23 917 24 70 204 21 73 22 GCN PubMed 41 63 6400 41 53 819 40 42 19 GAT Cora 37 57 9190 40 58 428 22 76 46 GAT PubMed 48 46 12800 47 48 6 63 27 7 TABLE VI EVALUATION OF GBFA ATTACK ON GRAPHSAGE MODEL AT THE LOWEST BER AND AT BER OF 1E-1 GNN Model- Dataset Layer PAC (Min BER) Min BER nbit (Min BER) PAC (1e-1) ASR (1e-1) nbit (1e-1) GraphSAGE-Cora 1 76 1e 3 22 35 62 22928 2 66 1e 2 1 13 88 22 GraphSAGE-PubMed 1 75 1e 3 32 42 56 640 2 71 1e 2 1 40 58 38 TABLE VII EVALUATION OF GBFA ATTACK ON GIN MODEL AT THE LOWEST BER AND AT BER OF 1E-1 GNN Model- Dataset Layer PAC (Min BER) Min BER nbit (Min BER) PAC (1e-1) ASR (1e-1) nbit (1e-1) GIN-Cora 1 58 1e 2 917 12 85 9171 2 55 1e 2 40 13 83 409 3 33 1e 2 40 13 85 409 4 52 1e 2 40 15 83 409 5 60 1e 1 40 60 5 409 GIN-PubMed 1 60 1e 2 320 57 22 3200 2 59 1e 2 40 56 22 409 3 60 1e 2 40 56 19 409 4 60 1e 2 40 58 12 409 5 60 1e 2 40 58 8 409 Fig.",
    "source": "2507.05531v1_Bit-Flip_Fault_Attack_Crushing_Graph_Neural_Networ.pdf",
    "length": 1438,
    "tokens": 508
  },
  {
    "text": "Privacy-preserving Federated Learn- ing for Internet of Medical Things under Edge Computing. IEEE journal of biomedical and health informatics, 27(2):854 865, 2022. [10] Cheng Wang, Zenghui Yuan, Pan Zhou, Zichuan Xu, Ruixuan Li, and Dapeng Oliver Wu. The security and privacy of mobile-edge computing: An artificial intelligence perspective. IEEE Internet of Things Journal, 10(24):22008 22032, 2023. [11] Jinhyuk Kim and Shiho Kim. Hardware accelerators in embedded systems. In Artificial Intelligence and Hardware Accelerators, pages 167 181. Springer, 2023. [12] Ji Lin, Wei-Ming Chen, Yujun Lin, Chuang Gan, Song Han, et al. MCUNet: Tiny deep learning on iot devices. Advances in neural infor- mation processing systems, 33:11711 11722, 2020. [13] Swapnil Sayan Saha, Sandeep Singh Sandha, and Mani Srivastava. Machine learning for microcontroller-class hardware: A review. IEEE Sensors Journal, 22(22):21362 21390, 2022. [14] Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, and Song Han. On-device training under 256kb memory. Advances in Neural Information Processing Systems, 35:22941 22954, 2022. [15] Young D Kwon, Rui Li, Stylianos I Venieris, Jagmohan Chauhan, Nicholas D Lane, and Cecilia Mascolo. TinyTrain: resource-aware task-adaptive sparse training of DNNs at the data-scarce edge. arXiv preprint arXiv:2307.09988, 2023. [16] Yushan Huang, Ranya Aloufi, Xavier Cadet, Yuchen Zhao, Payam Bar- naghi, and Hamed Haddadi. Low-Energy On-Device Personalization for MCUs. In 2024 IEEE ACM Symposium on Edge Computing (SEC), pages 45 58. IEEE, 2024. [17] Erez Manor and Shlomo Greenberg. Custom Hardware Inference Accelerator for Tensorflow Lite for Microcontrollers.",
    "source": "2503.22567v2_Benchmarking_Ultra-Low-Power_Î¼NPUs.pdf",
    "length": 1691,
    "tokens": 488
  },
  {
    "text": "To balance search time and optimal performance, few-shot NAS frameworks such as MetaNAS leverage meta-learning to optimize architectures efficiently [46]. Addition- ally, On-NAS [47] addresses memory constraints in embedded devices by employing an expectation-based operation selection method and an edge-pair search strategy. This approach reduces memory consumption while maintaining high performance, making it a promising solution for onboard deep learning deployment. In conclusion, NAS plays a crucial role in compressing and optimizing deep learning models for onboard AI, enabling automated adaptation to onboard hardware con- straints. By leveraging hardware-aware and few-shot NAS methods, onboard systems can achieve efficient, high-performance compressed model design while minimizing memory usage and computational overhead. 3 Efficient Inference To achieve efficient inference on resource-constrained edge devices, the combination of optimization strategies plays vital role. In onboard setup with low processing capability, computation offloading enables efficient inference through computationally intensive tasks delegated to external resources, reducing the burden onboard. While offloading is limited by network constraints or privacy concerns, model partitioning 9 Task-01 Task-01 Task-02 Task-N Offload Model Selection Task-01 Model-01 Model-02 Model-N Model Local Models Edge Nodes Edge Server Time Constraint Energy Constraint Inference Tasks Fig. 3: General architecture of edge-based computational offloading. offers a viable alternative by distributing computations between the device and exter- nal systems to enhance inference for execution. Moreover, in onboard execution, where responsiveness and minimal energy consumption are required, early exit strategies fur- ther optimize inference by allowing models to terminate processing once a confident prediction is reached. This section highlights these techniques in detail by discussing their benefits and trade-offs in achieving efficient inference for optimized onboard processing. 3.1 Distributed Computation for Onboard AI: Computation Offloading Computation offloading enables resource-constrained devices to execute deep learn- ing tasks efficiently by delegating computationally intensive operations to nearby edge servers. This approach reduces energy consumption and improves inference speed, which is critical for real-time applications such as autonomous navigation and surveillance [74 79]. While network latency and security risks remain concerns, offloading enhances scalability and optimizes resource utilization across distributed systems [80, 81]. To improve inference efficiency, DNNs can leverage offloading strategies that selec- tively assign tasks between local devices and edge servers. Zhou et al.",
    "source": "2505.08793v1_Onboard_Optimization_and_Learning_A_Survey.pdf",
    "length": 2801,
    "tokens": 499
  },
  {
    "text": "Du, X. Hu, Y. Hao, G. Xu, Y. Wen, L. Li, Q. Guo, and Y. Chen, Qimeng-cpu-v2: Automated superscalar processor design by learning data dependencies, arXiv preprint arXiv:2505.03195, 2025. [34] S. Cheng, C. Li, Z. Du, R. Zhang, X. Hu, X. Li, G. Xu, Y. Wen, and Q. Guo, Revisiting automatic pipelining: Gate-level forwarding and speculation, in Proceedings of the 61st ACM IEEE Design Automation Conference, 2024, pp. 1 6. [35] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim et al., Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023. [36] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin et al., Code llama: Open foundation models for code, arXiv preprint arXiv:2308.12950, 2023. [37] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. Li et al., Deepseek-coder: When the large language model meets programming the rise of code intelligence, arXiv preprint arXiv:2401.14196, 2024. [38] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang et al., Qwen technical report, arXiv preprint arXiv:2309.16609, 2023.",
    "source": "2506.05007v1_QiMeng_Fully_Automated_Hardware_and_Software_Desig.pdf",
    "length": 1193,
    "tokens": 469
  },
  {
    "text": "To ensure the qual- ity of the generated annotations, we have conducted human evaluations, as detailed in Appendix A. To further expand the training dataset and im- prove model performance, we augment our dataset with open-source Verilog datasets from RTL- Coder (Liu et al., 2024), MG-Verilog (Zhang et al., 2024), and DeepCircuitX (Li et al., 2025). These datasets provide additional RTL designs with di- verse structures and functionalities, while also in- corporating different annotation strategies. The di- versity in annotations improves the model s adapt- ability to varying description styles, enhancing its robustness across various RTL-related tasks. 3.1.2 RTL Code Understanding RTL code understanding focuses on summarizing the functionality of existing Verilog code, enhanc- ing collaboration and comprehension among hard- ware designers. The dataset for this task is derived from the RTL code generation dataset, with Verilog code as input and corresponding natural language description as output. In the absence of a standard- ized benchmark for this task, we build upon the benchmark introduced in DeepRTL, which origi- nally comprises 100 Verilog designs. To improve evaluation reliability and ensure broader coverage, we extend this benchmark to include 500 high- quality Verilog modules with diverse functional- ities. Each module is annotated by professional hardware designers with a concise summary of its functionality along with a detailed description of the specific operations involved in its execution. This extended benchmark establishes a more ro- bust and comprehensive foundation for evaluating RTL code understanding capabilities. 3.2 Embedding-Based Tasks 3.2.1 Natural Language Code Search Natural language code search refers to the process of querying a large codebase using natural language to find relevant code snippets. It involves embed- ding both the user query and each code snippet into vectors, then calculating their similarity. The snip- pet with the highest similarity score is considered the best match for the user s requirements. This task is particularly crucial for hardware design, as it enables code reuse, improves efficiency, and ac- celerates the transition from user specifications to RTL code. For this task, we reuse the dataset and benchmark from the RTL code understanding task.",
    "source": "2506.15697v1_DeepRTL2_A_Versatile_Model_for_RTL-Related_Tasks.pdf",
    "length": 2341,
    "tokens": 469
  },
  {
    "text": "Available: http: arxiv.org abs 2201.11903 [71] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, Large language models are zero-shot reasoners. [Online]. Available: [72] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, HuggingGPT: Solving AI tasks with ChatGPT and its friends in hugging face. [Online]. Available: [73] B. Y. Lin, Y. Fu, K. Yang, F. Brahman, S. Huang, C. Bhagavatula, P. Ammanabrolu, Y. Choi, and X. Ren, SwiftSage: A generative agent with fast and slow thinking for complex interactive tasks. [Online]. Available: [74] S. S. Raman, V. Cohen, E. Rosen, I. Idrees, D. Paulius, and S. Tellex, Planning with large language models via corrective re-prompting. [Online]. Available: [75] B. Xu, Z. Peng, B. Lei, S. Mukherjee, Y. Liu, and D. Xu, Rewoo: Decoupling reasoning from observations for efficient augmented language models, 2023. [Online]. Available: [76] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou, Self-consistency improves chain of thought reasoning in language models. [Online]. Available: [77] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan, Tree of thoughts: Deliberate problem solving with large language models. [Online]. Available: [78] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. [Online].",
    "source": "2504.09775v3_Understanding_and_Optimizing_Multi-Stage_AI_Infere.pdf",
    "length": 1401,
    "tokens": 460
  },
  {
    "text": "To facilitate the development of code generation This paper focues on C-based HLS. static void compute_mult_16bit( hls::stream uint16_t inStream1, hls::stream uint16_t inStream2, hls::stream uint16_t outStream, int vSize) { execute: for (int i 0; i vSize; i ) { pragma HLS LOOP_TRIPCOUNT min size max size outStream (inStream1.read() inStream2.read()); } } HLS-based design module multi_16bit( input clk, input rst_n, input start, input [15:0] ain, input [15:0] bin, output [31:0] yout, output done ); reg [15:0] areg; reg [15:0] breg; reg [31:0] yout_r; reg done_r; reg [4:0] i; always (posedge clk or negedge rst_n) if (!rst_n) i 5'd0; else if (start i 5'd17) i i 1'b1; else if (!start) i 5'd0; always (posedge clk or negedge rst_n) if (!rst_n) done_r 1'b0; else if (i 5'd16) done_r 1'b1; else if (i 5'd17) done_r 1'b0; assign done done_r; always (posedge clk or negedge rst_n) begin if (!rst_n) begin areg 16'h0000; breg 16'h0000; yout_r 32'h00000000; end else if (start) begin if (i 5'd0) begin areg ain; breg bin; end else if (i 5'd0 i 5'd17) begin if (areg[i-1]) yout_r yout_r ({16'h0000, breg} (i-1)); end end end assign yout yout_r; endmodule Verilog-based design 0 20 40 60 80 100 HLS Verilog normalized token usage Token Comparison Figure 2: HLS-based and Verilog-based programs. with language models, various datasets, approaches, and pre-trained models have been introduced over the past decade.",
    "source": "2502.13921v2_Exploring_Code_Language_Models_for_Automated_HLS-b.pdf",
    "length": 1407,
    "tokens": 516
  },
  {
    "text": "Instead, we used Brevitas solely to extract weights and quantisation parameters, constructing a custom QONNX graph from the corresponding QCDQ nodes. QONNX project extends ONNX framework by introducing custom quantisation operators (Quant, BipolarQuant, Trunc) to support arbitrary- precision uniform quantisation [26]. The conversion from QCDQ is achieved by applying the ConvertQCDQtoQONNX transformation to the graph. This transformation converts the QCDQ operators into Quant nodes, as shown below: QuantizeLinear Clip DequantizeLinear Quant (7) The QONNX LSTM graph is subsequently converted to the FINN-ONNX representation for further processing through the FINN flow. This conversion is performed using the ConvertQONNXtoFINNONNX transformation, which re- places QONNX operators and activations with the matmul, multithreshold operators, as shown below: Tanh Quant Multithreshold Mul Add (8) Finally, the LSTM computation graph also includes sig- moid and tanh activations, which are not directly sup- ported in the above transformation. However, since they are both monotonically increasing, we model these using threshold operators. We developed functions that will generate thresholds for tanh and sigmoid activations utilised in the ConvertQONNXtoFINNONNX tranformation to achieve the the result shown in eq 8. Subsequently the graph is functionally verified and passed to downstream layers for further stream- lining. At this stage, the primary goal is to minimise floating- point operations in the computation graph (the Mul and Add nodes introduced in the previous step) and eliminate operators by fusing absorbing them wherever possible to reduce the resource usage of the design. We try to achieve this by applying pre-existing transformations to the graph wherever possible and developing transformations (new or adaptations of existing FINN transformations) for unique compute patterns that are not fully absorbed (streamlined). Table I describes all the transformations used in the quantised LSTM graph streamlining process. The transformations we developed re- arrange the computations in the graph to position the floating- point operators close to the next multi-thresholding operator, allowing it to be absorbed by it. Figure 5 illustrates examples where our transformations allow the floating point operations to be absorbed into the nearest multi-thresholding operator.",
    "source": "2506.20810v1_FINN-GL_Generalized_Mixed-Precision_Extensions_for.pdf",
    "length": 2394,
    "tokens": 498
  },
  {
    "text": "III. SIMILARITY IN VIDEO APPLICATIONS Due to the limited computing resources and the strict power energy budget constraints [2], enabling high-quality fast inference on mobile devices is very challenging for DNN applications (e.g., object detection for videos). In fact, the inference of VGG-16 [31], which is a popular DNN model, takes 240 ms to execute on an embedded Adreno 640 [32] GPU, which is far from real-time. Moreover, most of the ex- isting solutions targeting such applications demand specialized hardware and or compiler support, and it is not straightforward for a developer to deploy them without multi-domain expertise across the optimization, compilers and hardware spectrum. However, in the speciï¬c context of video applications, the temporal continuity nature of the video data presents itself as an opportunity that is yet to be fully exploited. As a sample work, Euphrates [9] employs MVs to detect small changes between frames and skips unnecessary inferences. However, the number of skipped inferences there is static. We believe that, by looking deeper into the frame similarities and identify- ing reuse opportunities at a ï¬ner level , we can signiï¬cantly reduce the number of inferences, thereby reduce the burden on the hardware. Further, if we can dynamically exploit this opportunistic similarity (i.e., the inference is invoked based on runtime contents), the solution can encompass most vision applications without affecting the current hardware stack. 0 20 40 60 4x4 8x8 16x16 32x32 1080x1920 Tile Size 0.1 Identical Tiles (a) Similarity study at different tile size across adjacent frames for a video in VIRAT dataset [33]. 0 5 10 15 0 2 4 6 8 10 12 14 16 Pixel-level Difference 95 of the pixels differ within 3 (b) Distribution of pixel-level difference across two adjacent frames. Fig. 2: Similarity study. As discussed in Sec. II, most existing optimizations focus on accelerating the inference processing computation only and not fully understand the underlying characteristics of the input data, and thus some input-speciï¬c optimization opportunities could be easily missed. For example, for the mobile video inference application considered in this paper, the input data are continuous video frames, and can potentially contain rich similarity across different frames. To explore these opportunities, Fig.",
    "source": "PCframeSim.pdf",
    "length": 2345,
    "tokens": 491
  },
  {
    "text": "1, p. 17512, 2023. [Online]. Available: [10] M. A. Cervera, S. R. Soekadar, J. Ushiba, J. D. R. MillÃ¡n, M. Liu, N. Birbaumer, and G. Garipelli, Brain-computer interfaces for post- stroke motor rehabilitation: a meta-analysis, Annals of Clinical and Translational Neurology, vol. 5, no. 5, pp. 651 663, 2018. [11] M. SebastiÃ¡n-Romagosa, W. Cho, R. Ortner, N. Murovec, T. Von Oertzen, K. Kamada, B. Z. Allison, and C. Guger, Brain Computer Interface Treatment for Motor Rehabilitation of Upper Extremity of Stroke Patients A Feasibility Study, Frontiers in Neuroscience, vol. 14, 2020. [Online]. Available: frontiersin.org journals neuroscience articles 10.3389 fnins.2020.591435 [12] R. Mane, T. Chouhan, and C. Guan, BCI for stroke rehabilitation: motor and beyond, Journal of Neural Engineering, vol. 17, no. 4, p. 041001, 2020. [13] P. D. E. Baniqued, E. C. Stanyer, M. Awais, A. Alazmani, A. E. Jackson, M. A. Mon-Williams, F. Mushtaq, and R. J. Holt, Brain computer interface robotics for hand rehabilitation after stroke: a systematic review, Journal of NeuroEngineering and Rehabilitation, vol. 18, no. 1, p. 15, 2021. [Online]. Available: [14] H.-J. Hwang, V. Y. Ferreria, D. Ulrich, T. Kilic, X. Chatziliadis, B. Blankertz, and M. Treder, A Gaze Independent Brain-Computer Interface Based on Visual Stimulation through Closed Eyelids, Scientific Reports, vol. 5, no. 1, p. 15890, 2015. [Online].",
    "source": "2504.06996v1_Neural_Signal_Compression_using_RAMAN_tinyML_Accel.pdf",
    "length": 1403,
    "tokens": 461
  },
  {
    "text": "We then uti- lize open-source ABC (Brayton and Mishchenko, 2010) tool to extract delay and area metrics, where delay metric is reported by the static timing analy- sis, and area metric reflects the total logic footprint, which directly impacts manufacturing cost. This process provides a dataset that captures essential performance characteristics of RTL designs, facili- tating learning-based performance estimation. For a comprehensive summary of all dataset statistics, please refer to the Appendix D. 4 Methodology 4.1 Model Training We choose Llama-3.1 (Dubey et al., 2024) and DeepSeek-Coder (Guo et al., 2024) as the base models for training. Specifically, we fine-tune meta-llama Llama-3.1-8B-Instruct2 and deepseek- ai deepseek-coder-6.7b-instruct3. Our training con- sists of two stages. In the first stage, we follow the curriculum learning strategy adopted by Deep- RTL (Liu et al., 2025) and train the base model solely on RTL code generation and understanding data. In the second stage, we incorporate embed- ding data into the training set and train the model on both RTL code generation understanding and embedding tasks, utilizing the training framework of GRIT (Muennighoff et al., 2025).",
    "source": "2506.15697v1_DeepRTL2_A_Versatile_Model_for_RTL-Related_Tasks.pdf",
    "length": 1206,
    "tokens": 288
  },
  {
    "text": "For the reordering, the controller maintains a table that specifies the required channel order for each layer and timestep. It forwards these channels in the required order to MX converter. The MX converter then groups 4-bit multiplier 8-bit multiplier 4b W 4b A Oï¬€-chip Memory MX Converter Reordering Controller Systolic arrays Systolic arrays Systolic arrays Systolic arrays W Buï¬€er A Buï¬€er O Buï¬€er PE PE PE PE Precision-flexible PE Fig. 9. MixDiT accelerator architecture, built around a systolic array with an MX converter and reordering controller for precision-flexible MX processing. TABLE II HARDWARE CONFIGURATIONS Systolic array size 16x16 PEs Memory bandwidth 936GB s of systolic arrays 1024 On-chip memory 28MB Frequency 500 MHz Peak perf. (MX9) 262 TOPS and converts the reordered matrix into MX format before storing it in off-chip memory. The MX converter is composed of combinational logic, resulting in negligible latency. The accelerator components operate in a pipelined manner, and the latencies of reordering controller and MX converter are effectively hidden by the significantly larger latency of the systolic array. MX systolic array. Our mixed precision technique handles MX6 MX9 (for linear layers), MX9 MX9 (for attention layers), and MX6 MX6 operations. The processing elements in the systolic array should support these three types of multiplication. We adopted precision-flexible processing ele- ment for MX operations proposed in DaCapo [2]. This PE has four 4-bit multipliers that can do the dot product of the 4-bit mantissa. Therefore, when the group size is 16, MX6 MX6, which needs 4-bit mantissa multiplication, takes 4 cycles per group dot product. MX6 MX9 and MX9 MX9 need 8-bit mantissa multiplication. The outputs of the four 4- bit multipliers mentioned above become one 8-bit multiplier output. Therefore, it takes 16 cycles per group dot product. IV. METHODOLOGY Models and datasets. For evaluation, we use DiT-XL [3], Pixart-Î£ [4], and Stable Diffusion 3 (SD3) [1]. We denote the generated image resolution by appending {256, 512, 1024} to the model name.",
    "source": "2504.08398v1_MixDiT_Accelerating_Image_Diffusion_Transformer_In.pdf",
    "length": 2100,
    "tokens": 502
  },
  {
    "text": "This modification significantly improves performance on the TinyNLP benchmark but, when not paired with other architectural modules, results in a slight decrease in performance with respect to the base BERT(2MB) model on the GLUE benchmark, as can be appreciated in Fig. 2. 13 BERT NE EA We combine the Nano Embedder with Efficient Attention to create the BERT NE EA model, leveraging a broader vocabulary together with reduced weights and acti- vations overhead. This combination leads to a performance gain on both TinyNLP and GLUE tasks, where BERT NE EA achieves respectively an accuracy of 87.51 and a score of 61.20, i.e. re- spectively over 3 and 9 points with respect to the original BERT(2MB). These results highlight the advantage of combining embedding efficiency with an optimized attention mechanism in ultra-compact models. EmbBERT By finally integrating a parallel path with a Conv-1D followed by an Fully Connected layer we obtain the EmbBERT architecture. In this way, we add with a minimal memory overhead a modified feed forward block to the main attention path of BERT NE EA, i.e. a further simple token-mixing convolutional and fully connected layer. This addition provides a marginal improvement on the TinyNLP benchmark, but achieves significant success on the more complex tasks contained in the GLUE benchmark. Specifically, it improves performance by over 2 points in the GLUE score compared to the BERT NE EA model. EmbBERT-Q Finally, the EmbBERT model is quantized using the 8-bit post-training quantization procedure outlined in Section 3.1.3, resulting in the proposed EmbBERT-Q model. In the TinyNLP benchmark, EmbBERT-Q achieves an average accuracy of 88.17 , marking an improvement of nearly 1 percentage point over its unquantized counterpart. This increase can be attributed to the regularization effect that quantization may induce under certain conditions, combined with the relative simplicity of the tasks in this benchmark. On the GLUE benchmark, which evaluates broader natural language understanding tasks, EmbBERT-Q demonstrates exceptional robustness to quantization, achieving an overall GLUE score of 62.81. This represents a minimal performance drop of 0.7 percentage points compared to the unquantized EmbBERT version.",
    "source": "2502.10001v1_EmbBERT-Q_Breaking_Memory_Barriers_in_Embedded_NLP.pdf",
    "length": 2266,
    "tokens": 482
  },
  {
    "text": "Carbon-efficient design optimization for computing systems. In Proceedings of the 2nd Workshop on Sustainable Computer Systems, pages 1 7, 2023. Mariam Elgamal, Doug Carmean, Elnaz Ansari, Okay Zed, Ramesh Peri, Srilatha Manne, Udit Gupta, Gu-Yeon Wei, David Brooks, Gage Hills, and Carole-Jean Wu. CORDOBA: Carbon-Efficient Optimization Framework for Computing Systems . In 2025 IEEE International Symposium on High Performance Computer Architecture (HPCA), 2025. Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi, Prateek Sharma, Fan Chen, and Lei Jiang. Llmcarbon: Modeling the end-to-end carbon footprint of large language models, 2024. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets, 2023. Soroush Ghodrati, Sean Kinzer, Hanyang Xu, Rohan Mahapatra, Byung Hoon Ahn, Dong Kai Wang, Lavanya Karthikeyan, Amir Yazdanbakhsh, Jongse Park, Nam Sung Kim, and Hadi Esmaeilzadeh. Tandem processor: Grappling with emerging operators in neural networks. In ASPLOS, 2024. Google. Helping you bring local ai to applications from prototype to production, 2020.",
    "source": "2505.01386v2_Carbon_Aware_Transformers_Through_Joint_Model-Hard.pdf",
    "length": 1592,
    "tokens": 466
  },
  {
    "text": "Unlike fully custom datapaths for all Softmax operations, our approach introduces an ISA extension to accelerate only the exponential function while optimizing the remaining opera- tions in software. This hybrid method balances efficiency and flexibility, supporting a broader range of applications at a low cost. 32 64 128 256 512 1024 2048 Sequence Length 10 1 100 101 102 103 Latency ( s) (a) Softmax Latency Baseline SW Optim SW EXP SW Optim SW EXP HW Optim Baseline SW Optim SW EXP SW Optim SW EXP HW Optim Implementation 0 2 4 6 8 10 12 Latency (Âµs) (b) Softmax Latency of Seq Len 32 MAX EXP NORM 32 64 128 256 512 1024 2048 Sequence Length 10 1 100 101 102 Energy Consumption (mJ) (c) Softmax Energy Consumption Baseline SW HW Optim 32 64 128 256 512 1024 2048 Sequence Length 1 2 4 8 16 32 GFLOPS (d) FlashAttention Performance Baseline Softmax Optim 32 64 128 256 512 1024 2048 Sequence Length 0 20 40 60 80 100 Percentage of Latency ( ) (e) FlashAttention Latency GEMM Softmax BL Others Softmax Optim 32 64 128 256 512 1024 2048 Sequence Length 10 20 40 80 160 GFLOPS W (f) FlashAttention Energy Efficiency Baseline Softmax Optim Fig. 6. Performance, latency, and energy analysis for Softmax and FlashAttention-2 kernels. TABLE IV COMPARISON OF STATE-OF-THE-ART SOFTMAX ACCELERATORS Ref Precision Accuracy Evaluated Tech Frequency Area Power Throughput Strategy [MSE] Model [nm] [GHz] [Âµm2] [mW] [GOPS] Zhu et al. [16] FX16 1.06e-10 2.28e-121 Transformer-XL 28 2.78 1.641 10081 183921 - 22.24 13.12 ,1 FX16 quant. Koca et al. [17] FX16 - BERT FPGA - - - - No fine-tuning Kim et al. [18] FX8 FX16 71.2e-12 4.77e-122 - 28 3.12 2.52 7100 249002 22.82 52.462 24.96 20 ,2 - Xia et al.",
    "source": "2504.11227v1_VEXP_A_Low-Cost_RISC-V_ISA_Extension_for_Accelerat.pdf",
    "length": 1689,
    "tokens": 477
  },
  {
    "text": "2019. Nvgaze: An Anatomically-informed Dataset for Low-latency, Near-eye Gaze Estimation. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 1 12. [27] Kyungjin Lee, Juheon Yi, Youngki Lee, Sunghyun Choi, and Young Min Kim. 2020. GROOT: A Real-time Streaming System of High-fidelity Volumetric Videos. In Proceedings of the ACM IEEE International Conference on Mobile Computing and Networking (MobiCom). 1 14. [28] T. Lee and T. Hollerer. 2008. Hybrid Feature Tracking and User Interaction for Markerless Augmented Reality. In 2008 IEEE Virtual Reality Conference. 145 152. [29] Magic Leap. 2020. Magic Leap 1 is a Wearable Computer for Enterprise Produc- tivity. \"\". [30] Xiaoxu Meng, Ruofei Du, and Amitabh Varshney. 2020. Eye-dominance-guided Foveated Rendering. IEEE Transactions on Visualization and Computer Graphics (2020), 1972 1980. [31] Microsoft. 2020. HoloLens 2 Tech Specs. \" us p holoLens-2 91pnzzznzwcp ?activetab pivot:techspecstab\". [32] Microsoft Research Blog. 2020. Second Version of HoloLens HPU will Incorporate AI Coprocessor for Implementing DNNs. \" us research blog second-version-hololens-hpu-will-incorporate-ai- coprocessor-implementing-dnns \". [33] Naoya Muramatsu, Chun Wei Ooi, Yuta Itoh, and Yoichi Ochiai. 2017. Deep- Holo: Recognizing 3D Objects Using a Binary-Weighted Computer-Generated Hologram. In SIGGRAPH Asia 2017 Posters. [34] NASA. 2020. NASA at Home Virtual Tours and Apps. \" nasa-at-home-virtual-tours-and-augmented-reality\". [35] Takashi Nishitsuji, Yota Yamamoto, Takashige Sugie, Takanori Akamatsu, Ryuji Hirayama, Hirotaka Nakayama, Takashi Kakue, Tomoyoshi Shimobaba, and Tomoyoshi Ito. 2018.",
    "source": "HoloAR.pdf",
    "length": 1672,
    "tokens": 497
  },
  {
    "text": "(A: activations, W: weights, Pi: partial- sums, swi: scaling factor for weights, spi: scaling factors for partial-sums) they apply learnable scale factors to both. Additionally, these approaches left room for improving training efficiency. Our ap- proach addresses these limitations by aligning the quantization granularities of weights and partial-sums to column-wise level. Table I compares recent works and our method across three key factors: the finest quantization granularity, the ability to train from scratch, and the use of learnable scale factors. III. PROPOSED METHOD In this section, we introduce our column-wise quantization approach for both weights and partial-sums, evaluating its significance in relation to dequantization overhead and training efficiency. Additionally, we present our CIM-oriented convolu- tion framework that seamlessly reflects the hardware architec- ture, addressing the inefficiencies associated with implementing column-wise quantization. A. Column-wise Weight and Partial-sum Quantization As illustrated in Fig. 3, in typical CIM-based architectures, the convolution operation begins with the independent quantiza- tion of weights, W, and activations, A. The quantized weights are mapped and tiled into arrays, which are then used in the MAC operation on each array, followed by the quantization of the resulting partial-sums. Expanding on this workflow, our method improves it by em- ploying column-wise quantization for both weights and partial- sums, as shown in Fig. 3. For instance, a column of weights in an array is quantized and multiplied by the corresponding quantized activation, generating the partial-sum as follows, Pi Wi swi Aqi, (1) where z rounds z to the nearest integer. After the MAC operation, the resulting partial-sums are quantized: Wi swi Aqi 1 spi Pi 1 spi . (2) As demonstrated in the equations, the weight and partial- sum quantization processes are clearly distinct due to the separate rounding functions, enabling independent optimization for better flexibility and precision in the overall quantization process. As the quantizer, we employ the LSQ [10] method to train scale factors for both weights and partial-sums separately, optimizing the model for fine-grained quantization effectively.",
    "source": "2502.07842v2_Column-wise_Quantization_of_Weights_and_Partial_Su.pdf",
    "length": 2265,
    "tokens": 486
  },
  {
    "text": "The detail of modeling the total embodied carbon of a hardware device is in Appendix B. The embodied carbon emission of an LLM exe- cution over time t is calculated by amortizing the hardware s total embodied carbon Cem,total over its lifetime (LT), typically 5 to 7 years (Ostrou- chov et al., 2020). Thus, the embodied carbon for a time period t is given by: Cem t LT Cem,total (2) Total carbon is thus given by: Ctotal Eop CI t LT Cem,total (3) 3.5 Summary and Implementation FUEL provides a systematic framework for eval- uating the environmental impact of LLM serving, using FU as a comparison basis. To demonstrate its effectiveness and generalizability, we will present three case studies exploring different comparison configurations: model size ( 4), quantization ( 5), and hardware ( 6). For broadly applicable in- sights, we focus on two widely used model families, Qwen2.5 (Qwen et al., 2025) and Llama2 (Touvron et al., 2023), and conduct experiments using the open-source LLM serving platform vLLM (Kwon 5 0 5 10 15 Qscore 1 2 3 4 Carbon Emission (gCO2eq) 1e 4 (a) Qwen 5 0 5 10 Qscore (b) Llama Qwen-7B Llama-7B Qwen-14B Llama-13B Qwen-32B Figure 2: Carbon emission per FU for different model sizes across Qscores at QPS 1 req s. 0 20 Qscore 0.00 0.02 0.04 0.06 0.08 Density 7B (mean 11.1, std 6.6) 14B (mean 14.4, std 5.3) 32B (mean 15.6, std 5.5) 0 20 Qscore 7B (mean 6.8, std 5.9) 13B (mean 8.7, std 5.3) (a) Qwen (b) Llama Figure 3: Qscore distribution of outputs across different model sizes on the NewsQA dataset. et al., 2023). We use a carbon intensity of 518 gCO2eq, the 12-month average of our server s re- gion, to calculate operational carbon emissions.",
    "source": "2502.11256v2_Unveiling_Environmental_Impacts_of_Large_Language_.pdf",
    "length": 1680,
    "tokens": 494
  },
  {
    "text": "Table 11: HILTs supporting the CASCADE Arrays Values in Common Value Unit Batch size x input token length in HILT 32,768 B x L Active CASCADE array columns 8,192 columns Spare CASCADE columns for CREST 16 columns Columns per CASCADE array 8,208 columns Rows per CASCADE array 64 rows CASCADE array size 525,312 PEs CASCADE arrays in a TRIMERA 384 arrays Total CASCADE rows in a TRIMERA 24,576 rows PEs in a TRIMERA 201,719,808 PEs TRIMERA total spare columns for CREST 6,144 columns CASCADE array clock in SLD chip 12 GHz Clocks to output delay without CASCADE 24,616 clocks Clocks to output delay with CASCADE 488 clocks HILT and BID chips clock speeds 2 GHz HILT unit cell (D latch plus transmission gate) 8 Tr Full custom HILT bit cell in TSMC N2 0.012 Âµm2 HILT overhead (decoders, clock buffers) 16 Input Activations HILTs Activation HILT storage tristate latches 131,072 bits Activation HILT stage 2 tri-state latches 8,192 bits Activation HILT stage 3 tri-state latches 512 bits Activation HILT stage 4 tri-state latches 32 bits Activation HILT output bit width (1 row) 4 bits Activation HILT total tri-state latches 139,812 bits CASCADE array activation HILT bits 8,388,608 bits CASCADE activation HILT bitcells area 102,098 Âµm2 CASCADE activation HILT total area 121,545 Âµm2 TRIMERA bit-width of all activation HILTs 3,221,225,472 bits Total TRIMERA activation HILT area 47 mm2 Output sum HILTs Output sums HILT storage tristate latches 262,144 bits Output sums HILT stage 2 tri-state latches 16,384 bits Output sums HILT stage 3 tri-state latches 1,024 bits Output sums HILT stage 4 tri-state latches 64 bits Output sums HILT output bit width (1 column) 8 bits Output sums HILT total tri-state latches 279,624 bits CASCADE output sums HILT bits 2,151,677,952 bits CASCADE output sums HILT bitcells area 26,188,078 Âµm2 CASCADE output sums HILT total area 31,176,283 Âµm2 Total TRIMERA output sums HILT area 31 mm2 Output sums SIPO FIFO 8 :128 Weights are stored directly in the CASCADE arrays Total HILT for TRIMERA TRIMERA activation HILT data 384 MBytes TRIMERA output sums HILT data 257 MBytes TRIMERA total HILT data 641 MBytes Total CASCADE memory HILT area 78 mm2 HILT die area 143 mm2 CASCADE Array HILT of area 54 Time to transfer HILT memory over data fabric 16.42 Âµs 28 16 CASCADE Array HILT support in a TRIMERA The HILT die contains HILT data arrays to feed the CASCADE arrays with activations, collect calculated sums from the output, and provide the CREST comparison logic. The weights are stored directly in the CASCADE array in the SLD. Table 11 shows the support logic, HILT arrays, and FIFOs feeding the CASCADE arrays with activations and weights and collecting output sums.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2700,
    "tokens": 780
  },
  {
    "text": "Both hardware blocks were implemented in C 1 and synthesized into Verilog using Catapult HLS with a 28-nm standard-cell library. For verifying the correctness of the C code, we integrated it to llama2.c [32] and we received exactly the same replies as the original implementation for all examined queries. Both designs operate at the same pipelined latency with a clock frequency of 500 MHz. Latency depends on the size of the hidden dimension, requiring 8, 10, and 12 cy- cles for d {16, 64, 256} elements, respectively. Verilog was synthesized using the Cadence digital implementation flow, while power consumption was estimated with the PowerPro power analysis and optimization tool. The reported power consumption represents the average power measured after executing attention kernels for various Large Language Models and benchmarks from PromptBench. Figs. 4 and 5 show the area and power of the proposed FLASH-D hardware and the parallel hardware implementation of the FlashAttention2 computation kernel, for the two ex- amined reduced precision floating-point formats and different sizes of the hidden dimension. Power estimation excludes memory power and focuses solely on the average power consumption of the computation kernel. The memory power in both cases is expected to be identical, as both approaches implement the same FlashAttention algorithm using the same computation order and data flows. The difference lies solely on how the computation kernel is executed internally. As shown in Fig. 4, FLASH-D reduces the hardware area by more than 20 in all examined cases. These savings are a direct result of the restructured FlashAttention kernel.",
    "source": "2505.14201v1_FLASH-D_FlashAttention_with_Hidden_Softmax_Divisio.pdf",
    "length": 1661,
    "tokens": 328
  },
  {
    "text": "This optimization serves the purpose of hiding the latencies of the tile load operations by increasing the compute density of the innermost loop and exposing more independent work to the hardware. The K loop may also be unrolled to avoid frequent CSR writes. E. Summary of the MTE ISA MTE proposes to reuse the vector register file to store matrix tile operands. This strategy makes it possible to eliminate the need for extra architectural state to support MMA instructions. Similarly to AMX, MTE also uses a CSR for controlling the tile shapes but simplifies the configuration process by enabling only three simultaneous matrix shapes for A, B, and C, which are programmed by software using a few scalar instructions. MTE also provides a set of instructions to configure the vector programming model based on the MTE state, allowing a seamless transition from matrix to vector processing mode without the need for memory or register moves. In addition, MTE defines a geometry-agnostic programming model independent of the matrix multiplication shapes supported by the microarchitecture, while defining a standard way for the software to expose the required geometry to the hardware. IV. ARCHITECTURE SUPPORT FOR MTE INSTRUCTIONS This section describes the architecture support required by MTE. Section IV-A describes two possible microarchitecture implementations of the MTE MMA instructions. Section IV-B describes the support required by the MTE memory instruc- tions, while Section IV-C discuses additional aspects to fully support MTE. A. Supporting the tfmul and tmul Instructions When computing tfmul and tmul MMA operations, de- scribed in Section III-C3, inputs from the A and B tiles contribute to multiple C output elements. In this section, we describe the architecture support required by MTE in the context of i) a dedicated systolic array; and ii) a standard vector processor. 1) Systolic array:: A common approach to implement MMA operations is through dedicated systolic arrays consist- ing of a grid of functional units, such as those found in Intel AMX or Google TPUs [5], [38], [39], [29]. These architectures directly read from tile registers, delivering high throughput per operation.",
    "source": "2507.03522v1_A_Flexible_Instruction_Set_Architecture_for_Effici.pdf",
    "length": 2207,
    "tokens": 458
  },
  {
    "text": "2943 2952. [3] A. Mishra and other, Accelerating sparse deep neural networks, arXiv preprint arXiv:2104.08378, 2021. [4] A. Zhou et al., Learning N:M fine-grained structured sparse neural networks from scratch, in Inter. Conf. on Learning Representations (ICLR), May 2021. [5] Z.-G. Liu et al., Systolic tensor array: An efficient structured-sparse gemm accelerator for mobile CNN inference, IEEE Comp. Arch. Letters, vol. 19, no. 1, pp. 34 37, 2020. [6] G. Jeong et al., Vegeta: Vertically-integrated extensions for sparse dense gemm tile acceleration on cpus, in IEEE Inter. Symp. on High- Performance Comp. Arch. (HPCA), Feb. 2023, pp. 259 272. [7] Z.-G. Liu et al., S2TA: Exploiting structured sparsity for energy- efficient mobile CNN acceleration, in IEEE Inter. Symp. on High- Performance Comp. Arch. (HPCA), Apr. 2022, pp. 573 586. [8] C. Peltekis, V. Titopoulos, C. Nicopoulos, and G. Dimitrakopoulos, DeMM: A decoupled matrix multiplication engine supporting relaxed structured sparsity, IEEE Comput. Archit. Lett., p. 17 20, 2024. [9] R. Salay, R. Queiroz, and K. Czarnecki, An analysis of ISO 26262: Using machine learning safely in automotive software, 2017. [10] D. Sarvamangala and R. V. Kulkarni, Convolutional neural networks in medical image understanding: a survey, Evolutionary intelligence, vol. 15, no. 1, pp. 1 22, 2022. [11] L. Ma and S. Tian, A hybrid cnn-lstm model for aircraft 4d trajectory prediction, IEEE access, vol. 8, pp. 134 668 134 680, 2020. [12] K.-H. Huang and J. A. Abraham, Algorithm-based fault tolerance for matrix operations, IEEE Trans. on Computers, vol. C-33, no. 6, pp. 518 528, 1984.",
    "source": "2504.18628v1_Periodic_Online_Testing_for_Sparse_Systolic_Tensor.pdf",
    "length": 1632,
    "tokens": 497
  },
  {
    "text": "Therefore, in order to achieve optimal progress, we need to select the best activation solution offered by both the computation modes. Given a power supply level, we can derive the optimal tile size and actual duplication granularity to form the activation solution m, n, aG for sequential or pipelined computation modes, respectively. Then, a global activation strategy can pick up the best one of these two and generate a hybrid solution for the concerned power level. Throughput model Achieving the maximal computation progress under the harvested energy has two implications. The ï¬rst one is that we expect more energy can be used for program progress. The other is more subtle in that we expect the power can be consumed quickly in order to receive more energy from outside. In this regard, the metric throughput measured by computations (convolutional MACs) per second is a useful proxy for ResiRCA in energy-harvesting scenarios. We use the number of convolutional MAC operations to represent the computations. For the sequential computation mode, the throughput for Layer Lk can be expressed as below: Thrsequ Lk (m n)Lk aGLk LatLk (6) The average throughput with a LC-convolution CNN infer- ence can be expressed as shown below. Thrsequ ave PLk LC Lk 1 (m n)Lk aGLk PLk C Lk 1 LatLk (7) For the pipelining computation mode, all the LC layers are executed in parallel. The throughput can be expressed as follows: Thrpipe ave PLk LC Lk 1 (m n)Lk aGLk Latpipe (8) 2) Activation strategy formulation : The activation strategy for the sequential mode can be described as shown below.",
    "source": "ResiRCA.pdf",
    "length": 1587,
    "tokens": 364
  },
  {
    "text": "Gamma: Automating the hw mapping of dnn models on accelerators via genetic algorithm. In Proceedings of the 39th International Conference on Computer-Aided Design, pages 1 9, 2020. [18] Size Zheng, Yun Liang, Shuo Wang, Renze Chen, and Kaiwen Sheng. Flextensor: An automatic schedule exploration and optimization framework for tensor computation on heterogeneous system. In Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, pages 859 873, 2020. 10 [19] Sheng-Chun Kao, Michael Pellauer, Angshuman Parashar, and Tushar Krishna. Digamma: Domain-aware genetic algorithm for hw-mapping co-optimization for dnn accelerators. In DATE, 2022. [20] Qingcheng Xiao, Size Zheng, Bingzhe Wu, Pengcheng Xu, Xuehai Qian, and Yun Liang. Hasco: To- wards agile hardware and software co-design for tensor computation. In 2021 ACM IEEE 48th Annual International Symposium on Computer Architecture (ISCA), pages 1055 1068. IEEE, 2021. [21] Bahador Rashidi, Chao Gao, Shan Lu, Zhisheng Wang, Chunhua Zhou, Di Niu, and Fengyu Sun. Unico: Unified hardware software co-optimization for robust neural network acceleration. In Proceedings of the 56th Annual IEEE ACM International Symposium on Microarchitecture, pages 77 90, 2023. [22] Stijn Eyerman, Lieven Eeckhout, Tejas Karkhanis, and James E Smith. A mechanistic performance model for superscalar out-of-order processors. ACM Transactions on Computer Systems (TOCS), 27(2):1 37, 2009. [23] Chloe Ching-Yun Hsu, Celestine Mendler-DÃ¼nner, and Moritz Hardt. Revisiting design choices in proximal policy optimization. arXiv preprint arXiv:2009.10897, 2020.",
    "source": "2506.03474v1_CORE_Constraint-Aware_One-Step_Reinforcement_Learn.pdf",
    "length": 1664,
    "tokens": 443
  },
  {
    "text": "An arrow 1 indicates the fixed position of the weight tile along matrix columns, M, where each tile is reused M m times. Arrow 2 shows the input tile moving k units down the input matrix, while the stationary weight tile remains fixed. As the input tile traverses from east to west, the weight tile remains in place and is reused for subsequent inputs, shifting vertically by k. The output tile generation, marked by 2 , follows this sequence and repeats K k times, indicated by 3 . c) Output Stationary: The OS strategies depicted in Fig. 1(d) and (e) showcase another commonly employed group of techniques. These approaches retain partial sums in internal memory until the computation of output results is completed, thereby optimizing inner products, which are fundamental to matrix-matrix multiplication. This method leverages the spatial locality of processing element arrays to reduce external data retrievals for partial sums. The primary distinction between the row-oriented OS and col-oriented OS schemes lies in the sequence of generating the weight matrix. In the row-oriented OS, outputs are generated row by row, while the column- oriented strategy produces results column by column. Fig. 1(d) shows arrows with black circle indicators where the input and weight tiles slide vertically and horizontally, respectively. Upon generating an output tile marked by 1 , indicated by arrows in 2 , the input tile returns to the initial position in the input matrix, and the weight tile shifts right by k. After repeating the data flow K k times, as shown by arrows marked as 3 , the input tile is moved downward by m, and the weight tile is moved to the starting weight matrix location. On the other hand, the column-oriented strategy initially redirects the weight tile, as shown by arrows in 2 , and later shifts the input tile, indicated by 3 . d) Problems of the fixed stationary scheme: Table II shows the external memory access values for different sta- tionary schemes. These fixed stationary schemes have two problems: higher external memory access and concurrent read write demands. For the first problem, in this table, m, n, and k represent tile dimensions and the available hardware computation resources, typically much smaller than M, N, and K. For instance, with an IS configuration, the EMA Fig. 2. Matrix Mapping for Matrix-Matrix Multiplication with Proposed Stationary Schemes of the input matrix is noticeably less compared to others, highlighting the overall EMA reduction.",
    "source": "2503.19640v1_An_Efficient_Data_Reuse_with_Tile-Based_Adaptive_S.pdf",
    "length": 2499,
    "tokens": 500
  },
  {
    "text": "Specifically, we test three static temperature settings: T 0.5, T 0.7, and T 0.9, representing conservative, balanced, and exploratory decoding behaviors, respectively. For each temperature, we generate RTL code from a shared set of design instructions and evaluate the functionality of the output based on a pass fail criterion. As shown in Table IV, in fixed-temperature decoding, the model applies a uniform degree of randomness throughout generation, regardless of token context. Lower temperatures (e.g., T 0.5) bias the model toward high-confidence predictions, yielding deterministic but often repetitive and rigid code. In contrast, higher temperatures (e.g., T 0.9) promote diversity and exploration, but increase the likelihood of producing structurally invalid or semantically inconsistent outputs. Note that from a modeling perspective, adaptive temperature control better aligns with the internal behavior of transformer-based LLMs. During generation, different layers and attention heads specialize in capturing hierarchical and positional dependencies, especially important in structured domains like code. A fixed-temperature setting fails to leverage this internal structure, applying uniform sampling across both high-confidence and uncertain regions. C. Reduction of Hallucination and Repetition in RTL Code To evaluate the impact of our full decoding framework on RTL code quality, we analyze the outputs of both LLMs, CodeLlama 7B and QwenCoder-2.5 14B, before and after applying our combined contrastive temperature-adaptive decoding method. As shown in Table V, the baseline top- k decoding strategy exhibits a considerable number of both hallucinated3 and repetitive4 outputs. This is while applying our proposed decoding framework reduces these incidents. This highlights the complementary strengths of contrastive decoding and temperature adaptation: the former penalizes semantically redundant token choices, encouraging meaningful variation, while the latter enforces deterministic behavior in 3Hallucination in RTL code refers to the generation of syntactically valid but semantically incorrect or meaningless hardware constructs that do not correspond to the design intent, including illogical control flows. 4Repetition refers to the unintended duplication of RTL code segments, such as repeated logic, redundant declarations, or loops of identical expressions. TABLE V: Number of Hallucinated and Repetitive RTL Code Outputs before and after Applying Contrastive TA Decoding.",
    "source": "2507.02226v1_DecoRTL_A_Run-time_Decoding_Framework_for_RTL_Code.pdf",
    "length": 2507,
    "tokens": 478
  },
  {
    "text": "287 Authorized licensed use limited to: Penn State University. Downloaded on August 10,2023 at 18:50:20 UTC from IEEE Xplore. Restrictions apply. performs linear transformations on the attribute data of each voxel pair (the voxel in leveln and its siblings along x, y, and z dimensions) to obtain a low-pass component and a high- pass component. The high-pass component is quantized and entropy encoded, while the low-pass component proceeds to the next level (leveln 1) and serves as a prediction for the voxel s attribute in this upper level [14]. Note that, this step also needs to be performed sequentially across the tree layers. Takeaway: The prior octree-based works for both geometry and attribute compression suffer from performance inefï¬cien- cies, mainly because the octree construction, serialization and attribute transformations involve sequential computations. To improve the performance, next we want to explore the hidden spatio-temporal locality opportunities (missed by the prior works), and speed up both the geometry and attribute compression from both the intra- and inter-frame perspectives. 2) Optimizing the Intra-Frame Compression: Driven by the observations above, next we relax the sequential update approach that exists in the prior works, and employ the (intermediate) generated Morton Codes to reveal the hidden parallelism opportunities for compressing a PC frame (shown in Figs. 4 c and 4 d for the geometry and attribute compression, respectively). Proposed Intra-Frame Geometry Compression: As can be seen from Fig. 4 c , the modiï¬ed components in our pipeline compared to the previously-proposed geometry compression approach (depicted in Fig. 4 a ) include the following: Morton Code Generation: Given the raw PC, instead of constructing the octree point-by-point, now the ï¬rst step is to generate the Morton codes in one shot (note that this can be performed in parallel and only takes 0.5ms). This additional pre-processing step can draw an overall layout for all the points, which will further help to parallelize the octree construction. Octree Construction4: Using the Morton codes generated in the previous step, now the octree can be constructed in parallel by employing techniques similar to [31], [64].",
    "source": "PCcompress.pdf",
    "length": 2248,
    "tokens": 491
  },
  {
    "text": "I will be shifted to the left by 2i bits in MBE. TABLE IV: Components described by hardware primitives. PE PE PE PE PE PE PE PE PE NP MP M MP MT N NP NT PE Array PE Array PE Array NT (E) The GEMM loop from the PE microarchitecture perspective (D) PE microarchitecture (C) PE Array (A) GEMM loop from the PE perspective (B) GEMM loop from the PE array perspective Sum Carry Full Adder High Width Accmulator Compressor Tree 0 B -B 2B -2B 0 B -B 2B -2B 0 B -B 2B -2B B Multiplicand(A) ai-1 ai ai 1 ai-2 ai 2 Multiplier(B) Encode logic Encode logic Encode logic CPPG Mux Shift Mux Mux Shift Shift MAC Figure 4: The GEMM loop from the PE microarchitecture perspective. spective on how the BW of MACs impacts the performance of TPEs. A. BW Dimension and New Hardware Primitives In a multiplier, the calculation process can be visualized as a multiplicand expanded into multiple sub-operands, which are then multiplied in parallel with another operand (resulting in the PPs of different bit-weights), and finally reducing all PPs to obtain the result. This can be expressed as follows: C A B BW 1 X bw 0 SubAbw B. (1) It should be noted that the size of BW and the form of SubAbw are related to specific encoding methods. In this paper, we focus on the acceleration opportunities brought by the BW dimension, rather than the design of specific encoding methods. Here, we only use two examples to illustrate that Eq. (1) can broadly represent the multipliers. For an 8-bit MBE [49], SubAbw and BW are as follows: SubAbw ( 2a2bw 1 a2bw a2bw 1)22bw, BW 4, (2) where a2bw represents the 2bw-th bit of A. For an 8-bit complement bit-serial method [17], SubAbw as follows: SubAbw ( abw2bw, if bw BW 1 abw2bw, if bw BW 1 , BW 8.",
    "source": "2503.06342v1_Exploring_the_Performance_Improvement_of_Tensor_Pr.pdf",
    "length": 1714,
    "tokens": 502
  },
  {
    "text": "[226] 2023-06 Liang et al. [227] 2023-07 GPTAIGChip [134] 2023-09 SpecLLM [225] 2024-01 Table 12. Explorations in LLM-aided architecture design, covered in Section 5.8 LLM for Analog Circuit Design Method Open Link Date LADAC [229] 2023-12 AnalogCoder [230] 2024-05 FLAG [231] 2024-05 ADO-LLM [232] 2024-06 LaMAGIC [233] 2024-07 Artisan [234] 2024-11 LEDRO [235] 2024-11 AnalogXpert [236] 2024-12 AnalogGenie [237] 2025-01 Table 13. Works on LLMs for analog circuit design, covered in Section 5.9. 5.8 LLMs for Hardware Architecture Design For hardware architecture design, LLMs have been explored in two primary areas: circuit archi- tecture design [134, 226, 227] and specification document processing [225]. The comparison and timeline of these works are shown in Table 12. These works aim to leverage LLMs to enhance automation, reduce design complexity, and improve efficiency in architectural decision-making. In circuit architecture design, GPT4AIGChip [134] proposes an automated prompt-generation pipeline using in-context learning to guide LLMs in generating high-quality AI accelerator designs. This approach enables the structured decomposition of hardware design tasks, improving the consistency and efficiency of generated architectures. LCDA [226] applies LLMs to accelerate the software-hardware co-design process, particularly for compute-in-memory architectures in AI accel- erators. It addresses the cold-start problem in traditional co-design approaches by leveraging LLMs to guide design space exploration, significantly reducing the search time. QGAS [227] extends the application of LLMs to quantum computing, using GPT-4 to iteratively design variational quantum algorithm ansatz architectures and translate the architecture into quantum assembly language code. For specification document processing, SpecLLM [225] tackles the inefficiencies and error- prone nature of developing architecture specifications in architecture design. It explores the use of LLMs to automate both the generation of specifications and the review of existing documentation. To structure the problem, the authors categorize architecture specifications into three levels, covering different degrees of design abstraction.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 2221,
    "tokens": 491
  },
  {
    "text": "Bajwa, I. and Choudhary, M. A rule based system for speech language context understanding. 23, 12 2006. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners, 2020. URL https: arxiv.org abs 2005.14165. Chang, K., Chen, Z., Zhou, Y., Zhu, W., kun wang, Xu, H., Li, C., Wang, M., Liang, S., Li, H., Han, Y., and Wang, Y. Natural language is not enough: Benchmarking multi- modal generative ai for verilog generation, 2024. URL Chen, J., Li, Z., Hu, X., and Xia, X. Nlperturbator: Studying the robustness of code llms to natural language variations, 2024. URL 2406.19783. Chen, W., Ray, S., Bhadra, J., Abadir, M., and Wang, L.-C. Challenges and trends in modern soc design verification. IEEE Design and Test, 34(5):7 22, 2017a. doi: 10.1109 MDAT.2017.2735383. Chen, Y.-H., Krishna, T., Emer, J. S., and Sze, V. Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks. IEEE Journal of Solid- State Circuits, 52(1):127 138, 2017b. doi: 10.1109 JSSC. 2016.2616357.",
    "source": "2504.08852v1_ML_For_Hardware_Design_Interpretability_Challenges.pdf",
    "length": 1373,
    "tokens": 483
  },
  {
    "text": "Figure 3 illustrates how to operator models the tanh activation at three different INT quantisation levels. We demonstrate that our extensions to the FINN framework encompass all essential components, including FINN compiler transformations that map the LSTM compute graph to pre-built hardware building blocks, along with support for converting custom operators such as mapping sigmoid and tanh acti- vations to comparison based thresholding operations available through finn-hlslib. These components enable the construction of hardware-accelerated LSTM layers within the framework, allowing the seamless integration into larger models. A key advantage of quantisation using FINN is that all LSTM layer weights can be stored in the on-chip memory of the FPGA, enhancing performance and addressing limitations encountered in other implementations [13]. As a result, we chose to extend this framework to develop a generalized deployment pipeline for recurrent neural networks. D. Limit Order Books A limit order book (LOB) is a record of pending limit orders maintained by an exchange. Each limit order is an instruction to buy or sell a security at a specified price or better. When limit orders are placed, they are recorded at the exchange by updating the order book to track them. Orders are executed when the market reaches or exceeds their specified price levels [17]. LOBs represent investors sentiments and, hence, can be used to predict the change in stock prices for certain time horizons. With the availability of public LOB datasets, multiple machine-learning stock price prediction models have been proposed in the literature. These architectures range from 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 Inputs (x) 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 Activation (y) tanh(x) (Original) INT2 (4 levels) INT4 (16 levels) INT6 (64 levels) Fig. 3: The figure illustrates how the Multithresholding oper- ator approximates the tanh activation function over a defined input range for INT2, INT4 and INT6 bitwidths. traditional machine learning techniques like support vector machines (SVMs) to deep learning architectures like CNNs, LSTMs, ConvLSTMs and attention-based architectures [1], [18] [22].",
    "source": "2506.20810v1_FINN-GL_Generalized_Mixed-Precision_Extensions_for.pdf",
    "length": 2202,
    "tokens": 493
  },
  {
    "text": "2007. Internet Economics: The Use of Shapley Value for ISP Settlement. In ACM CoNEXT. [61] Charith Mendis, Alex Renda, Saman Amarasinghe, and Michael Carbin. 2019. Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation using Deep Neural Networks. In ICML. [62] Microsoft. 2024. Announcing the preview of new Azure VMs based on the Azure Cobalt 100 processor. announcing-the-preview-of-new-azure-vms-based-on-the-azure-cobalt-100- processor 4146353 [63] Jason E Miller, Harshad Kasture, George Kurian, Charles Gruenwald, Nathan Beckmann, Christopher Celio, Jonathan Eastep, and Anant Agarwal. 2010. Graphite: A Distributed Parallel Simulator for Multicores. In HPCA. [64] Christoph Molnar. 2020. Interpretable machine learning. Lulu. com. [65] Stefano Moretti, Fioravante Patrone, and Stefano Bonassi. 2007. The Class of Microarray Games and the Relevance Index for Genes. Top (2007). [66] Ramasuri Narayanam and Yadati Narahari. 2010. A Shapley Value-Based Approach to Discover Influential Nodes in Social Networks. IEEE T-ASE (2010). [67] Quan M. Nguyen and Daniel Sanchez. 2023. Phloem: Automatic Acceleration of Irregular Applications with Fine-Grain Pipeline Parallelism. In HPCA. [68] SÃ©bastien Nussbaum and James E Smith. 2001. Modeling Superscalar Processors via Statistical Simulation. In PACT. [69] Pablo Montesinos Ortego and Paul Sack. 2004. SESC: SuperESCalar simulator. In ECRTS. [70] Santosh Pandey, Lingda Li, Thomas Flynn, Adolfy Hoisie, and Hang Liu. 2022. Scalable Deep Learning-Based Microarchitecture Simulation on GPUs. In SC. [71] Santosh Pandey, Amir Yazdanbakhsh, and Hang Liu. 2024. TAO: Re-Thinking DL- based Microarchitecture Simulation.",
    "source": "2503.23076v1_Concorde_Fast_and_Accurate_CPU_Performance_Modelin.pdf",
    "length": 1680,
    "tokens": 485
  },
  {
    "text": "The evaluation includes simulation results for multiple RISC-V benchmarks supported by soft processors, such as dhrystone and coremark, as listed in Table IV. These bench- marks are designed to evaluate the processor s performance across various processing aspects. The complexity of each benchmark is measured by the machine instructions retired (minstret) [31], representing the total number of instructions executed by the processor. The results for resource utilization, power, and timing met- rics are derived from the report generated from the synthesis process. The timing metric is assessed by manually configuring the processor s clock input to a default reference frequency of 50 MHz and extracting the worst setup slack and worst hold slack values from the Vivado timing report. These two values offer insights into the timing performance of the soft processor at this default frequency and can be used to estimate its maximum operating frequency. B. Experiment for Soft Processor s Evaluation Framework This experiment primarily focuses on determining the time required to evaluate a set of soft processor configurations and comparing it with other reference evaluation frameworks. The evaluation time is defined as the duration from receiving the soft processor configuration parameters to the completion of recording all performance metrics, including execution time, resource utilization, power consumption and timing perfor- mance. Three design evaluation frameworks are considered: Direct evaluation without Acceleration: This framework represents the typical evaluation flow for design methods such as RCBO [10], which assesses each parameter configuration as a separate task and excludes design checkpoints to accelerate the synthesis process. Evaluation with a fixed checkpoint: This framework sets the checkpoint file used in logic synthesis as the one obtained from the evaluation of the default soft processor configuration and disables the configuration matcher and checkpoint dataset. The default configuration for each soft processor is provided in Table II. Evaluation with our retrieval strategy proposed in Sec- tion IV: For the proposed evaluation framework, we instruct it to complete the automated design space exploration process before the experiment. This ensures that the configuration matcher has a sufficient pool of configuration candidates to select from, thereby enhancing evaluation acceleration. To ensure a fair comparison, all three evaluation frame- works are provided with an identical set of soft processor configurations. In this experiment, ten randomly generated configurations were created for each supported soft processor, serving as the candidate experiment set.",
    "source": "2506.06817v1_ASPO_Constraint-Aware_Bayesian_Optimization_for_FP.pdf",
    "length": 2717,
    "tokens": 454
  },
  {
    "text": "[6] demonstrates the feasibility of using signed 16-bit uniform quantization of the weights and gradients for various initialization-based meta- learning methods, but reduces memory requirements only by 7 2 Activation address 9 0 Input address Time 0 2 6 13 9 3 16 15 12 11 5 7 2 4 1 x9 x7 x8 x5 x6 x3 x4 x1 x2 x0 18 14 10 8 0 x11 x12 x10 17 19 0 1 2 3 4 5 6 5 7 8 9 10 6 9 8 11 12 13 14 0 1 2 3 4 5 6 7 8 9 10 11 12 12 15 16 9 13 17 18 19 Input seq. Input mem. Activation memory Dilation Zero-valued activations Residual connection Causal convolution Layer 2 1 0 3 Kernel index 0 1 2 0 1 2 0 1 2 0 Oldest input is overwritten FIFO Same input used in different kernel positions (a) (b) - Chameleon WS (c) 90 lower 7 lower Fig. 8. (a) Four-layer TCN with 13 inputs. Indices show greedy processing order; colors indicate per-layer memory locations. Causal convolutions and residuals are annotated, while the processing and storage of zero activations resulting from dilation (white) is skipped by Chameleon. (b) FIFO-style activation memory allocation over time in Chameleon. Each layer overwrites its oldest activation during execution. Indices show processing order; bar lengths indicate activation lifetimes. (c) Memory and compute comparison between WS TCN inference and Chameleon s strategy, using 130k parameters and producing identical outputs. (a) This work [11, 19] UltraTrail [13] 32,79 kB 32,79 kB 4 kB 2.2 kB 1.3 kB No residuals, ping-pong buffer Complex control, triple buffer 0 1 0 1 2 Conv1D Conv1D Res. TCN layer with residual path 2 kB 0 2 Low overhead, single memory 1 4 smaller memory And 2 OoM longer input At 5.5 more weights kB (b) 19 Fig. 9. (a) Comparison of residual operation steps and activation memory sizes across TCN accelerators.",
    "source": "2505.24852v2_Chameleon_A_MatMul-Free_Temporal_Convolutional_Net.pdf",
    "length": 1758,
    "tokens": 470
  },
  {
    "text": "We evaluate the accuracy of the GNN forward model fÎ¸ on a test set drawn from 19 of the 20 topologies. One topology RVCO is entirely excluded from training, validation, and test splits to assess generalization to unseen architectures. Prediction quality is measured using standard re- gression metrics: coefficient of determination (R2), root mean squared error (RMSE), and mean ab- solute error (MAE), computed independently for each of the 16 performance metrics. We also report the mean relative error per metric, computed as the average across all test samples where each metric is defined. As summarized in Table 2, the model achieves high accuracy across all dimensions, with an average R2 of 0.971. 0 5 10 15 20 25 30 35 Relative Error ( ) 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 Density Mean: 3.65 Median: 1.69 Mode: 1.38 Figure 5: Distribution of relative error ( ) across the test set for the GNN forward model. Plot is trimmed at the 95th percentile. To evaluate end-to-end prediction accuracy at the sample level, we compute the mean relative error per instance, defined as the average relative error across all valid (non-masked) performance metrics for each test sample. Figure 5 shows the distribution of this quantity across the test set (trimmed at the 95th percentile to reduce the impact of outliers). The distribution is sharply concentrated, indicating that most predictions closely match their corresponding target vectors. Without percentile trimming, the overall mean relative error across the full test set is 9.14 . Table 2: Prediction accuracy of the forward GNN on all 16 circuit performance metrics.",
    "source": "2505.21923v1_FALCON_An_ML_Framework_for_Fully_Automated_Layout-.pdf",
    "length": 1636,
    "tokens": 358
  },
  {
    "text": "Xiao et al. [36] introduced DQN-KD, applying knowledge distilation with reinforcement learning to minimize memory utilization where it achieved 50.4 fewer FLOPs (flops full form floating point operations per second) than baseline with 47.5 parameter reduction. To reduce the communication overhead during onboard deployment, Itahara et al., [37] demonstrated semi-supervised federated learning with knowledge distillation that deducted 99 communication cost while maintaining similar accuracy compared to benchmark via onboard local models outputs exchange between heterogeneous devices and optimizes local model. This proposed approach adapted KD for onboard deployment by transferring global knowledge from a teacher model to client models using soft labels, allowing clients to learn generalized patterns despite non-IID data. Moreover, Qu et al. [38] demonstrated an adaptive quantized federated knowledge distillation approach for edge devices to address the bottleneck of communication cost and speed up model training while maintaining accuracy with 2 of the original model size. Luo et al. [39] proposed KeedEdge with deep neural network (DNN) with 5.86 improvement in the student model through knowledge distillation with 14 model reduction aiming to lower complexity and latency for UAV positioning. To address the challenges of heterogeneous computation, Qi et al. [40] intro- duced bidirectional knowledge distillation to facilitate efficient and scalable onboard deployment for modulation classification in IoT-edge systems in federated learning framework. The proposed knowledge distillation approach of multi-teacher knowledge distillation, where the global network was regarded as a student network that uni- fies the heterogeneous knowledge from multiple teacher networks, enabled lightweight model updates by transferring essential insights between local and global models, min- imizing communication costs and reducing memory and computational demands on devices. Overall, knowledge distillation is crucial for onboard deployment as it creates lightweight models that approximate the accuracy of larger counterparts while reduc- ing memory, computational demands, and inference latency. By transferring soft labels from a teacher model, it encodes rich inter-class relationships, enabling effi- cient deployment in resource-constrained environments. This is especially vital for real-time applications like autonomous vehicles, where fast, localized decision-making is essential. Furthermore, distillation allows flexible deployment across diverse devices by optimizing student models for specific capabilities, enhancing performance in het- erogeneous environments. It also supports continual learning by enabling incremental updates without full retraining, reducing computational overhead.",
    "source": "2505.08793v1_Onboard_Optimization_and_Learning_A_Survey.pdf",
    "length": 2811,
    "tokens": 498
  },
  {
    "text": "8, pp. 3974 3987, 2021. [7] K. Dai, Z. Xie, and S. Liu, DCP-CNN: Efficient Acceleration of CNNs With Dynamic Computing Parallelism on FPGA, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2024. [8] K. Hegde, R. Agrawal, Y. Yao, and C. W. Fletcher, Morph: Flexible acceleration for 3d cnn-based video understanding, in Proc. MICRO, 2018, pp. 933 946. [9] D. Moolchandani, A. Kumar, and S. R. Sarangi, Accelerating CNN inference on ASICs: A survey, Journal of Systems Architecture, vol. 113, p. 101887, 2021. [10] D. Gyawali, Comparative analysis of cpu and gpu profiling for deep learning models, arXiv preprint arXiv:2309.02521, 2023. [11] Q. Zhang, M. Zhang, T. Chen, Z. Sun, Y. Ma, and B. Yu, Recent ad- vances in convolutional neural network acceleration, Neurocomputing, vol. 323, pp. 37 51, 2019. [12] S. F. Beldianu and S. G. Ziavras, Asic design of shared vector accelerators for multicore processors, in 2014 IEEE 26th International Symposium on Computer Architecture and High Performance Comput- ing, 2014, pp. 182 189. [13] A. Jose, K. T. Alense, L. Gijo, and J. Jacob, FPGA Implementation of CNN Accelerator with Pruning for ADAS Applications, in 2024 IEEE 9th International Conference for Convergence in Technology (I2CT), 2024, pp. 1 6. [14] K. Guo et al., Angel-eye: A complete design flow for mapping cnn onto embedded fpga, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 37, no. 1, pp. 35 47, 2017. [15] S. Liu and W. Luk, Towards an efficient accelerator for dnn-based remote sensing image segmentation on fpgas, in Proc. FPL, 2019, pp. 187 193.",
    "source": "2505.13461v1_FPGA-based_Acceleration_for_Convolutional_Neural_N.pdf",
    "length": 1630,
    "tokens": 495
  },
  {
    "text": "[19] Y. Lin, H. Tang, S. Yang, Z. Zhang, G. Xiao, C. Gan, and S. Han, Qserve: W4a8kv4 quantization and system co-design for efficient llm serving, ArXiv, vol. abs 2405.04532, 2024. [20] G. Xiao, J. Lin, M. Seznec, J. Demouth, and S. Han, Smoothquant: Accurate and efficient post-training quantization for large language models, ArXiv, vol. abs 2211.10438, 2022. [21] J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, and S. Han, Awq: Activation-aware weight quanti- zation for on-device llm compression and acceleration, Proceedings of Machine Learning and Systems, vol. 6, pp. 87 100, 2024. [22] A. Agrawal, A. Panwar, J. Mohan, N. Kwatra, B. S. Gulavani, and R. Ramjee, Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills, ArXiv, vol. abs 2308.16369, 2023. [23] S. Zhao, D. Israel, G. V. den Broeck, and A. Grover, Prepacking: A simple method for fast prefilling and increased throughput in large language models, ArXiv, vol. abs 2404.09529, 2024. [24] S. Ge, Y. Zhang, L. Liu, M. Zhang, J. Han, and J. Gao, Model tells you what to discard: Adaptive kv cache compression for llms, ArXiv, vol. abs 2310.01801, 2023. [25] G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis, Efficient streaming language models with attention sinks, ArXiv, vol. abs 2309.17453, 2023.",
    "source": "2505.03745v1_AccLLM_Accelerating_Long-Context_LLM_Inference_Via.pdf",
    "length": 1324,
    "tokens": 465
  },
  {
    "text": "Send Importance Sampling Coreset Construction Clustering Coreset Construction AAC Last Classification Result End Yes Yes No Yes No Yes No 1a 2a 1b 2b Figure 8: Decision flow of Seeker. Strategy Sensor Energy Comm. Energy Total Energy Avg Acc ( ) D0 0.54 8.27 8.81 D1 29.23 8.27 37.5 80.03 D2 16.58 8.27 24.85 77.37 D3 1.07 15.97 17.04 78.30 D4 0.87 15.97 16.84 85.30 raw data 70.16 70.16 87.23 Table 2: Energy breakdown of different Seeker strate- gies (in ğœ‡Joules). The accuracy reported is the average case over 1000 inferences. DNN inference and communicate the results to the host; D3: Clustering based coreset construction at the sensor, and com- municate the coreset to the host; host runs DNN inference on the reconstructed data; and D4: Importance sampling based coreset construction at the sensor and communicate the coresets to the host; host recovers the original data with the pre-programmed generator, and performs inference with the recovered data. Table 2 lists the energy requirements of each of these decisions. 4.2 Efficient Hardware Accelerator Energy harvesting brings challenges in both average power levels and power variability. Performing DNN inference un- der such conditions often limits exploitation of inherent DNN parallelism within the energy budget. Therefore, many prior works use custon DNN accelerators, typically based on (non- volatile) resistive RAM (Re-RAM) based [47, 56] crossbar architecture, to perform DNN inference on EH-sensor nodes. Seeker s inference engine follows the design proposed in ResiRCA [56] and modifies it to cater towards new quanti- zation requirements: We have two different instances of the Re-RAM crossbar in our system - one for the 16bit model and one for the 12-bit model. The nonvolatile nature of the Re- RAMs helps in performing intermittent computing with the harvested energy. Moreover, techniques like loop tiling and partial sums [56] can further break down the computation to maximize forward progress with minimum granularity.",
    "source": "Seeker.pdf",
    "length": 2002,
    "tokens": 481
  },
  {
    "text": "2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. New Approx. Approaches Model Constructor Learning Network Build HDL Models C C Models ROUP Library ALWANN AxConv2D Extension Synopsys DC RTL Synthesis Layer-Level Filter-Level Kernel-Level1 Kernel-Level2 Initial Network Representation (Uniform Structure) ALWANN [7] TSMC 45-nm Final AxNNs Fig. 1. The MAx-DNN toolflow architecture, extension of ALWANN [7]. II. THE ALWANN APPROXIMATION FRAMEWORK The ALWANN framework [7] takes the following inputs: (i) the trained (frozen) network model in protobuf format, (ii) a library of approximate multipliers, and (iii) architecture constraints for the hardware accelerator (e.g., pipelined or power-gated mode, number of approximate units). It assumes accurate addition and approximate multiplication, as well as one approximation type per convolutional layer. Moreover, to improve the accuracy without re-training, the network weights are tuned updated according to the multipliers properties. The approximate networks, labeled as AxNNs, are modified versions of the initial model and satisfy the user constraints for the architecture of the accelerator. To enable ALWANN, TensorFlow is extended to support approximate quantized layers by creating a new operator, which replaces the conventional QuantizedConv2D layers with AxConv2D layers. This operator allows to specify which approximate multipliers to employ via the AxMult(str) para- meter (a C C model of the multipliers is necessary), and optionally use the weight tuning feature via AxTune(bool). The frozen model is processed by the TensorFlow transform graph tool, which inserts the AxConv2D layers, and then, the Pareto- optimal AxNNs are extracted by the NSGA-II algorithm. In this paper, we equip ALWANN with new approximation approaches regarding the distribution of the approximate mul- tiplications across the network.",
    "source": "2506.21371v1_MAx-DNN_Multi-Level_Arithmetic_Approximation_for_E.pdf",
    "length": 2209,
    "tokens": 491
  },
  {
    "text": "This would require 1,560 TeraPHY optical chiplets per ZettaLith. This illustrates how fast the TRIMERA chip-to- chip data fabric on the WSSCB is. If it is certain that ZettaLiths will not be connected together at high bandwidth, all these Ethernet connections can be eliminated from the ZettaLith design to save manufacturing cost, design time, and complexity. Any external connectivity can then be provided by the PCIe 6.0 interfaces. A first generation ZettaLith may omit the 800 GbE interfaces to reduce TTM. 18 Hybrid Bonds The SLD-HILT interface around a million hybrid bonds, as shown in Table 13. The hybrid bond pitch of 8.6 Âµm is above TSMC s projected minimum of 3.0 Âµm for the A16 node. To achieve a very even power and ground distribution over the entire SLD chip, 787,968 of the hybrid bonds are power and ground. This minimizes the differences between PEs resulting from their position on the die, reducing the IR droop and ground bounce margins required and simplifying simulation. Although backside power distribution will be available for the A16 and A16 nodes, it is not used for the SLD chip as the backside of the die has DRIE silicon heat-sink fines etched into it. 19 CPUs WSSCB CPU stacks handle high-speed computations not suitable for the main TRIMERA array, as commonly required in CPU-GPU systems for AI inference and high performance computing. This flexibility extends to future implementations, where arrays of chips on a single WSSCB design can become increasingly varied and application-specific as new compatible SLDs are developed. 19.1 CPU stacks A first generation ZettaLith only requires enough CPU processing to facilitate FP4 Transformer inference. The initial amount of CPU power only needs to be adequate , not as much as possible . The CASCADE array compute performance is so much higher than any feasible CPU performance that the CPUs are only useful for control and operations which cannot be parallelized. An optimized CPU-SRAM-BID stack can be provided later. As the JETSTREAM cooling will already be in place, CPU stacks can be designed for high power consumption and dissipation, equal to TRIMERA. In this way, substantially greater performance can be achieved for the CPU than could be achieved with conventional power supply and cooling systems.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2295,
    "tokens": 500
  },
  {
    "text": "4, MEMHD demonstrated comparable or higher accuracy compared to BasicHDC with 10240D: our 128x128 achieved higher accuracy for both MNIST and FMNIST, while 512x128 showed comparable results for ISOLET. So, Table II compares our models with IMC baselines with 10240D when using 128x128 IMC array. We evaluate three key metrics: com- putation cycles, array usage, and AM utilization. Computation cycles refer to the number of operations performed when using a single array, while array usage indicates the number of arrays required to map the entire AM structure. AM utilization repre- sents the ratio of actually mapped columns to the total columns in the IMC array. For partitioning, we consider two suitable scenarios for each dataset. Our model leverages lower dimen- sional vectors, which substantially reduces the EM s memory requirements. This reduction in dimensionality leads to fewer computation cycles and requires fewer IMC arrays, enhancing overall efficiency. Compared to the baseline, MEMHD achieves fully-utilization of AM at all times. As shown in Table II-(a), for MNIST and FMNIST datasets, our model demonstrates an 80 improvement in computational efficiency and requires 71 fewer arrays compared to the baseline. For ISOLET dataset, as presented in Table II-(b), our model achieves a 20 increase in computational efficiency and uses 17.5 fewer arrays. F. Energy Consumption and Cycles of AM Fig. 7 presents a comparative analysis of normalized AM energy consumption and cycles across all baseline models. While some baseline binary HDC models use ID-Level en- coding, all models employ MVM-based associative search for inference, enabling a fair comparison of AM performance. The figure showcases models demonstrating equivalent accuracy to MEMHD 128x128 on the FMNIST dataset, as shown in Fig. 3. The total number of arrays required to map the entire AM structure is proportional to the model s dimensionality. When the entire AM is mapped to arrays at once, models without partitioning can perform inference in a single cycle. However, for partitioned models, the number of cycles increases proportionally to the number of partitions. While partitioning strategies effectively reduce the number of required arrays, they proportionally increase the number of cycles, resulting in constant energy consumption across different partitioning schemes. MEMHD distinguishes itself by enabling associative search with just one computation in a single 128x128 array.",
    "source": "2502.07834v1_MEMHD_Memory-Efficient_Multi-Centroid_Hyperdimensi.pdf",
    "length": 2477,
    "tokens": 492
  },
  {
    "text": "Figure 14 presents the generation through- put results for Llama2-13B and Mixtral-8x7B models, evaluated us- ing two real-world LLM inference traces. We exclude Oaken-HBM and QServe for Mixtral-8x7B model, as Oaken-HBM s memory can- not accommodate the entire model and QServe lacks support for MoE layers. Tender, which employs systolic arrays, suffers under- utilization due to the padding required by varying prompt lengths within a batch. Conversation trace features short output lengths, resulting in a brief generation phase. As the bandwidth bottleneck due to the KV cache is noticeable in generation phase, a short gen- eration length reduces the advantage of Oaken s KV cache quanti- zation. Conversely, BurstGPT trace features longer output lengths, where KV cache quantization in Oaken becomes more beneficial. Mixtral-8x7B model utilizes grouped query attention to reduce its KV cache size compared to multi-head attention. Quantization baselines, including Oaken-LPDDR and Tender, show little to no performance gain over full-precision baselines. However, as batch size increases or with the BurstGPT trace with longer generation lengths, Oaken-LPDDR demonstrates greater performance gains. In summary, Oaken delivers an advantage over existing solutions in real-world scenarios for the Llama2-13B and Mixtral-8x7B models.",
    "source": "2503.18599v2_Oaken_Fast_and_Efficient_LLM_Serving_with_Online-O.pdf",
    "length": 1335,
    "tokens": 313
  },
  {
    "text": "arXiv:2506.05007v1 [cs.AR] 5 Jun 2025 1 QiMeng: Fully Automated Hardware and Software Design for Processor Chip Rui Zhang1, Yuanbo Wen1, Shuyao Cheng1, Di Huang1, Shaohui Peng2, Jiaming Guo1, Pengwei Jin1, Jiacheng Zhao1, Tianrui Ma1, Yaoyu Zhu1, Yifan Hao1, Yongwei Zhao1, Shengwen Liang1, Ying Wang1, Xing Hu1, Zidong Du1, Huimin Cui1, Ling Li2,3, Qi Guo1, Yunji Chen1,3, 1State Key Lab of Processors, Institute of Computing Technology, CAS 2Intelligent Software Research Center, Institute of Software, CAS 3University of Chinese Academy of Sciences Abstract Processor chip design technology serves as a key frontier driving breakthroughs in computer science and related fields. With the rapid advancement of information technology, conventional design paradigms face three major challenges: the physical constraints of fabrication technologies, the escalating demands for design resources, and the increasing diversity of ecosystems. Automated processor chip design has emerged as a transformative solution to address these challenges. While recent breakthroughs in Artificial Intelligence (AI), particularly Large Language Models (LLMs) techniques, have opened new possibilities for fully automated processor chip design, substantial challenges remain in establishing domain-specific LLMs for processor chip design. In this paper, we propose QiMeng, a novel system for fully automated hardware and software design of processor chips. QiMeng comprises three hierarchical layers. In the bottom-layer, we construct a domain-specific Large Processor Chip Model (LPCM) that introduces novel designs in architecture, training, and inference, to address key challenges such as knowledge representation gap, data scarcity, correctness assurance, and enormous solution space. In the middle-layer, leveraging the LPCM s knowledge representation and inference capabilities, we develop the Hardware Design Agent and the Software Design Agent to automate the design of hardware and software for processor chips. Currently, several components of QiMeng have been completed and successfully applied in various top-layer ap- plications, demonstrating significant advantages and providing a feasible solution for efficient, fully automated hardware software design of processor chips.",
    "source": "2506.05007v1_QiMeng_Fully_Automated_Hardware_and_Software_Desig.pdf",
    "length": 2271,
    "tokens": 473
  },
  {
    "text": "The detailed workflow is illustrated in Figure 2. A. Checkpoint Retrieval for Fast Design Evaluation During the processor design process, design methods often produce configurations that share several components with previously evaluated processors. This overlap allows us to store key information from the evaluation of earlier configu- rations and reuse it to expedite the evaluation of new designs. Modern synthesis tools, such as Vivado and Quartus Prime, support incremental synthesis, which leverages stored synthe- sis data in checkpoint files to streamline subsequent runs [20]. When synthesizing a new configuration that differs from a previously synthesized version, these tools detect changes and re-synthesize only the modified components, significantly reducing synthesis time. Fig. 3. Accelerated evaluation flow. To take advantage of the incremental synthesis feature for faster evaluation, two key components are required. The first is a database of checkpoints to store designs that can serve as starting points for incrementally synthesizing new configurations. The second is a matching function to identify an appropriate starting point from the database, minimizing potential synthesis time by reusing previously synthesized configurations. We propose dynamically building the checkpoint database during the optimization process. This database stores check- point files generated from previously synthesized processor configurations, allowing efficient reuse in subsequent synthesis tasks. For reference checkpoint selection, we introduce a configuration matcher that identifies the checkpoint file with the highest overlap with the processor currently under eval- uation. By minimizing the number of modified components requiring re-synthesis, this approach significantly reduces syn- thesis time. The flow of the accelerated evaluation process is illustrated in Figure 3. To implement the reference checkpoint selection, we de- velop a configuration matching function that identifies the most similar configuration in the dataset based on a weighted Euclidean distance metric. The mathematical formulation of the matching function is as follows: MatchConfig(x, w, Q) argmin q Q d 1 X i 0 wi (xi qni)2 (7) In this equation, xi represents the i-th parameter of the con- figuration x being evaluated, and qni denotes the i-th param- eter of the n-th stored configuration q within the checkpoint dataset Q. The weight vector w Rd quantifies the relative importance of changes in each parameter for configuration matching.",
    "source": "2506.06817v1_ASPO_Constraint-Aware_Bayesian_Optimization_for_FP.pdf",
    "length": 2539,
    "tokens": 465
  },
  {
    "text": "3: while t tm do 4: a1, , aE Ï€Î¸t(s0); Sample E compound actions (Sec. 3.1) 5: Î¾1, , Î¾E Decode( a1, , aE , D, {gi}K i 1); Decode via scaling graph (Sec. 3.2) 6: R(Î¾1), , R(Î¾E) Reward( U(Î¾1), , U(Î¾E) , w, {hj}M j 1); Equation (6), (10), (11) 7: Rmax, Î¾best FindBest(R(Î¾1), , R(Î¾E)); Find Î¾ with the maximum reward 8: if Rmax R then 9: break; 10: Ë†Rt Î±r 1 E PE k 1 R(Î¾k) (1 Î±r) Ë†Rt 1; Equation (7), (8) 11: L(Î¸t) ComputeObj( R(Î¾1), , R(Î¾E) , Ë†Rt, Ï€Î¸t); Equation (9), (12), (13), (14) 12: Î¸t 1 Î¸t Î· Î¸L(Î¸t); Policy update via gradient ascent 13: t t 1; 14: return Î¾best; where DKL is the forward KL divergence [23] and Î²r is a factor for the KL objective, encouraging the updated policy to stay close to the current policy. Lastly, to balance exploration and exploitation, we include an entropy regularization term that encourages the policy to maintain uncertainty in its action distributions. This regularization prevents early convergence to suboptimal, overconfident actions and encourages diverse exploration early in training. To reduce unnecessary randomness later, we multiply the entropy bonus by a decaying factor Î²e [8], which gradually shifts the policy from exploration to exploitation: Le(Î¸t) Î²e N X i 1 H(Ï€i,Î¸), (14) where H is the entropy function.",
    "source": "2506.03474v1_CORE_Constraint-Aware_One-Step_Reinforcement_Learn.pdf",
    "length": 1259,
    "tokens": 444
  },
  {
    "text": ": A survey on deep neural network pruning: Taxonomy, comparison, analysis, and recommendations. IEEE Transactions on Pattern Analysis and Machine Intelligence (2024) [14] Li, G., Ma, X., Wang, X., Yue, H., Li, J., Liu, L., Feng, X., Xue, J.: Optimiz- ing deep neural networks on intelligent edge accelerators via flexible-rate filter pruning. Journal of Systems Architecture 124, 102431 (2022) [15] Liu, D., Kong, H., Luo, X., Liu, W., Subramaniam, R.: Bringing ai to edge: From deep learning s perspective. Neurocomputing 485, 297 320 (2022) [16] Xia, M., Zhong, Z., Chen, D.: Structured pruning learns compact and accurate models. arXiv preprint arXiv:2204.00408 (2022) [17] Yu, F., Cui, L., Wang, P., Han, C., Huang, R., Huang, X.: Easiedge: A novel global deep neural networks pruning method for efficient edge computing. IEEE Internet of Things Journal 8(3), 1259 1271 (2020) [18] Ro, Y., Choi, J.Y. : Autolr: Layer-wise pruning and auto-tuning of learning rates in fine-tuning of deep networks. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, pp. 2486 2494 (2021) [19] Tanaka, H., Kunin, D., Yamins, D.L., Ganguli, S.: Pruning neural networks without any data by iteratively conserving synaptic flow. Advances in neural information processing systems 33, 6377 6389 (2020) [20] Frantar, E., Alistarh, D.: Sparsegpt: Massive language models can be accu- rately pruned in one-shot. In: International Conference on Machine Learning, pp. 10323 10337 (2023). PMLR [21] Kohama, H., Minoura, H., Hirakawa, T., Yamashita, T., Fujiyoshi, H.: Single- shot pruning for pre-trained models: Rethinking the importance of magnitude pruning.",
    "source": "2505.08793v1_Onboard_Optimization_and_Learning_A_Survey.pdf",
    "length": 1658,
    "tokens": 486
  },
  {
    "text": "[7] M. Liu et al., Chipnemo: Domain-adapted llms for chip design, arXiv preprint arXiv:2311.00176, 2023. [8] K. Chang et al., Chipgpt: How far are we from natural language hardware design, arXiv preprint arXiv:2305.14019, 2023. [9] Synopsys showcases, [10] Primis.ai, [11] R. Schuster et al., You autocomplete me: Poisoning vulnerabilities in neural code completion, in 30th USENIX Security Symposium (USENIX Security 21), 2021, pp. 1559 1575. [12] H. Aghakhani et al., Trojanpuzzle: Covertly poisoning code-suggestion models, in IEEE S P, 2024, pp. 1122 1140. [13] Z. Yu et al., Codeipprompt: intellectual property infringement assess- ment of code language models, in International conference on machine learning, 2023, pp. 40 373 40 389. [14] A. F. Noah et al., Codecloak: A method for evaluating and mitigating code leakage by llm code assistants, arXiv preprint arXiv:2404.09066, 2024. [15] L. Niu et al., {CodexLeaks}: Privacy leaks from code generation language models in {GitHub} copilot, in 32nd USENIX Security Symposium (USENIX Security 23), 2023, pp. 2133 2150. [16] A. J. Menezes et al., Handbook of applied cryptography. CRC press, 2018. [17] S. Liu et al., Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source dataset and lightweight solution, 2024. Available: [18] Z. Wang et al., Llms and the future of chip design: Unveiling security risks and building trust, in 2024 IEEE Computer Society Annual Symposium on VLSI (ISVLSI), 2024, pp. 385 390. [19] S. Thakur et al., Verigen: A large language model for verilog code generation, ACM TODAES, 2023.",
    "source": "2503.13116v4_VeriLeaky_Navigating_IP_Protection_vs_Utility_in_F.pdf",
    "length": 1588,
    "tokens": 481
  },
  {
    "text": "arXiv preprint arXiv:1909.08593, 2019. [243] Shobha Vasudevan, David Sheridan, Sanjay Patel, David Tcheng, Bill Tuohy, and Daniel Johnson. Goldmine: Automatic assertion generation using data mining and static analysis. In Design, Automation and Test in Europe Conference and Exhibition (DATE), 2010. [244] Samuele Germiniani and Graziano Pravadelli. Harm: a hint-based assertion miner. IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems (TCAD), 2022. [245] Zhuomin Chai, Yuxiang Zhao, Wei Liu, Yibo Lin, Runsheng Wang, and Ru Huang. Circuitnet: An open-source dataset for machine learning in vlsi cad applications with improved domain-specific evaluation metric and learning strategies. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), 2023. [246] Zeju Li, Changran Xu, Zhengyuan Shi, Zedong Peng, Yi Liu, Yunhao Zhou, Lingfeng Zhou, Chengyu Ma, Jianyuan Zhong, Xi Wang, et al. Deepcircuitx: A comprehensive repository-level dataset for rtl code understanding, generation, and ppa analysis. arXiv preprint arXiv:2502.18297, 2025. [247] Shang Liu, Wenji Fang, Yao Lu, Qijun Zhang, and Zhiyao Xie. Towards big data in ai for eda research: Generation of new pseudo-circuits at rtl stage. In Asia and South Pacific Design Automation Conference (ASP-DAC), 2025. [248] Shang Liu, Jing Wang, Wenji Fang, and Zhiyao Xie. Syncircuit: Automated generation of new synthetic rtl circuits can enable big data in circuits. In Design Automation Conference (DAC), 2025. [249] Samuel Coward, Theo Drane, Emiliano Morini, and George A Constantinides. Combining power and arithmetic optimization via datapath rewriting. In Symposium on Computer Arithmetic (ARITH), 2024. [250] Chen Chen, Guangyu Hu, Dongsheng Zuo, Cunxi Yu, Yuzhe Ma, and Hongce Zhang.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 1798,
    "tokens": 489
  },
  {
    "text": "IV. RESULTS A. Design Space 1) Accuracy: Figure 6 presents the accuracy (left-side plots), energy consumption (center plots), and the speedup (right-side plots) for our approach over the number of skipped blocks (x-axis) for ResNet-110 (upper plots) and ResNet-20 (bottom plots) on CIFAR-10 100. Note that the baselines are shown for comparison only and are not following the number of skipped blocks in x-axis. From the accuracy plots, we see that our approach without skipping achieves a similar accuracy to the baselines. On CIFAR-10, the best accuracy is delivered by our approach at 91.44 for ResNet-20 and by SkipNet at 92.67 for ResNet-110. On CIFAR-100, the original models give the highest accuracy at 73.59 and 67.37 for ResNets- 110 and -20, respectively. More importantly, the plot shows the adaptability opportunities enabled by our approach, by allowing full control over the model size (i.e., selecting the number of skipped blocks). For instance, for ResNet-110 CIFAR-10 (top- left plot, solid blue curve) the accuracy remains close (within 1 ) to the original one until around 20 blocks are skipped (out of the 54 possible). With 20 blocks, the accuracy drops only 0.23 but 37 of blocks are skipped. At a 10 accuracy drop, 36 blocks can be skipped in the ResNet-110 on CIFAR-10 0 9 18 27 36 45 54 0 45 90 Accuracy 0 9 18 27 36 45 54 10x 20x Speed Up 0 1 2 3 4 5 6 7 Skipped Blocks 0 45 90 Accuracy 0 1 2 3 4 5 6 7 Skipped Blocks 15J 30J 45J 60J Energy 0 1 2 3 4 5 6 7 Skipped Blocks 1x 2x 3x Speed Up CIFAR-10 CIFAR-100 Original SkipNet Ours Original SkipNet Ours 0 9 18 27 36 45 54 100J 200J 300J Energy ResNet-20 ResNet-110 Figure 6. Accuracy and Energy per infrence for original, SkipNet, and Ours on the left and center plots, respectively. Speedup over the Original baseline (right-most plots) for SkipNet and Ours. ResNet-110 on top plots, ResNet-20 on bottom plots.",
    "source": "2505.17626v1_Leveraging_Stochastic_Depth_Training_for_Adaptive_.pdf",
    "length": 1889,
    "tokens": 502
  },
  {
    "text": "A Case for (Partially)-TAgged GEometric History Length Predictors. JILP (2006). [9] Anastasios N. Angelopoulos, Rina Foygel Barber, and Stephen Bates. 2024. Theoretical Foundations of Conformal Prediction. arXiv:2411.11824 [10] Anastasios N Angelopoulos, Stephen Bates, et al. 2023. Conformal Prediction: A Gentle Introduction. Foundations and Trends in Machine Learning (2023). [11] Todd Austin, Eric Larson, and Dan Ernst. 2002. SimpleScalar: An Infrastructure for Computer System Modeling. Computer (2002). [12] Fabrice Bellard. 2005. QEMU, a Fast and Portable Dynamic Translator. In ATEC. [13] Leopoldo Bertossi, Benny Kimelfeld, Ester Livshits, and MikaÃ«l Monet. 2023. The Shapley Value in Database Management. ACM SIGMOD Record (2023). [14] Nathan Binkert, Bradford Beckmann, Gabriel Black, Steven K. Reinhardt, Ali Saidi, Arkaprava Basu, Joel Hestness, Derek R. Hower, Tushar Krishna, Somayeh Sardashti, Rathijit Sen, Korey Sewell, Muhammad Shoaib, Nilay Vaish, Mark D. Hill, and David A. Wood. 2011. The gem5 Simulator. SIGARCH Comput. Archit. News (2011). [15] Hadi Brais, Rajshekar Kalayappan, and Preeti Ranjan Panda. 2020. A Survey of Cache Simulators. Comput. Surveys (2020). [16] Derek Bruening. 2024. DynamoRIO: Tracing and Analysis Framework. [17] Derek Lane Bruening. 2004. Efficient, Transparent, and Comprehensive Runtime Code Manipulation. Ph. D. Dissertation. Massachusetts Institute of Technology. [18] Victoria CaparrÃ³s Cabezas and Markus PÃ¼schel. 2014. Extending the Roofline Model: Bottleneck Analysis with Microarchitectural Constraints. In IISWC. [19] Trevor E Carlson, Wim Heirman, and Lieven Eeckhout. 2011.",
    "source": "2503.23076v1_Concorde_Fast_and_Accurate_CPU_Performance_Modelin.pdf",
    "length": 1636,
    "tokens": 480
  },
  {
    "text": "2024. EC-NAS: Energy Consumption Aware Tabular Benchmarks for Neural Architecture Search. arXiv:2210.06015 [cs.LG] [6] Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa. 2018. MetaReg: Towards Domain Generalization using Meta- Regularization. In Advances in Neural Information Processing Systems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa- Bianchi, and R. Garnett (Eds. ), Vol. 31. Curran Associates, Inc. 647bba344396e7c8170902bcf2e15551-Paper.pdf [7] Konstantin Berestizshevsky and Guy Even. 2018. Sacrificing Accuracy for Reduced Computation: Cascaded Inference Based on Softmax Confidence. CoRR abs 1805.10982 (2018). arXiv:1805.10982 [8] Andrea Bragagnolo, Enzo Tartaglione, and Marco Grangetto. 2022. To update or not to update? Neurons at equilibrium in deep models. arXiv:2207.09455 [cs.LG] [9] Han Cai, Ji Lin, Yujun Lin, Zhijian Liu, Haotian Tang, Hanrui Wang, Ligeng Zhu, and Song Han. 2022. Enable Deep Learning on Mobile Devices: Methods, Systems, and Applications. ACM Transactions on Design Automation of Electronic Systems 27, 3 (March 2022), 1 50. doi:10.1145 3486618 [10] Han Cai, Ligeng Zhu, and Song Han. 2018. ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware. CoRR abs 1812.00332 (2018). arXiv:1812.00332 [11] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training Deep Nets with Sublinear Memory Cost. arXiv:1604.06174 [cs.LG] [12] Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, and Jingren Zhou. 2024.",
    "source": "2505.12523v1_Energy-Aware_Deep_Learning_on_Resource-Constrained.pdf",
    "length": 1494,
    "tokens": 478
  },
  {
    "text": "10.12 FP4 PE power consumption estimate The power consumption of a single PE in the CASCADE array is estimated in Table 5. In digital CMOS circuits, power consumption is dominated by dynamic switching power. This is governed by the equation P Î±CVÂ²f, where Î± represents the switching activity factor, C is the node capacitance, V is the supply voltage, and f is the operating frequency. 10.13 Sparsity Sparsity in AI transformers refers to the strategic design of network architectures that selectively activates a limited subset of parameters or connections during processing, thereby reducing computational and memory demands while maintaining or improving overall model performance. (Fuad et al., 2023) provides a survey on sparsity explorations in transformer-based accelerators. The percentage zero weights used in Table 5 is the worst case of the typical 90 -95 range of sparsity after Top-K sparsification of quantized transformers. ZettaLith hardware automatically uses the natural arbitrary sparsity of a quantized transformer or Top-K sparsified transformer to reduce power, but not to increase performance. The zero weight calculation takes the same time as any other weight. Using sparsity (e.g. by re-organizing weights and activations to create blocks of zero weights, by MoE and other higher level means of skipping large parts of a transformer calculation) can also be used to effectively increase inference speed and reduce inference power. These optimizations are implemented at the high level configuration of the transformer inference, not at the PE level, and do not affect PE design. 11 SHAPE: Simple Hybrid Array of Processing Elements SHAPE represents a novel processing architecture wherein an ultra-dense extremely regular array of PEs operating at a high clock frequencies in a logic die is synchronized, managed, and interfaced via a hybrid bonded memory and control die. While the SLD operates at 12 GHz, the HILT operates at 1.5 GHz and the Base Interface Die (BID) operates asynchronously at normal CMOS clock frequencies. The BID is used for all standard circuits including complex logic, I O, analog, and mixed signal circuits. The BID is intended to be re-usable across designs e.g. the CPU stacks should be able to use identical BIDs. The HILT die is produced using a CMOS process optimal for low leakage high density logic, mostly operating at 1.5 GHz (one eighth the SLD clock frequency).",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2424,
    "tokens": 500
  },
  {
    "text": "(a) Comparison of simulated real-time KWS power and peak TOPS W estimates for different PE array sizes. (b) Data layout for activation, bias, and weight memories to support both 4 4 and 16 16 PE array usage, allocating LSBs of weight memories to the top-left 4 4 section. In 4 4-mode, gray memories are powered off, while the remaining weight memories are virtually stacked to double the row count. 4.3 higher 2 lower v1 v1 v2 v1 16 v2 Fig. 12. Peak GOPS, real-time KWS power and accuracy comparison on 12-class Google Speech Commands (GSC, v1 and v2) between KWS accel- erators. bj 1 2k V X i 1 2(log2 sj i) 1, Wj sj. (8) The OPE is then reused to implement the division by 2k in Equation (8), via a right shift with 2 log2(k) bits. Our second technique allows efficiently supporting dif- ferent deployment scenarios, from leakage-dominated real- time KWS inference to throughput- and dynamic-power- constrained FSL operation with large DNN embedders. For in- stance, Vocell [10] has a 36 leakage contribution (excluding analog feature extraction) during real-time KWS, similar to the 30 for UltraTrail [13] and 33 for Giraldo et al. [11] with limited throughputs of 0.13, 3.8 and 0.26 GOPS, respectively. In contrast, TinyVers [12] achieves a 4-135 higher through- put of 17.6 GOPS: however, this comes at the cost of an order- of-magnitude increase in real-time KWS power consumption. Hence, to support low-leakage operation without degrading the design s throughput, we introduce a dual-mode PE array whose size is reconfigurable, which enables Chameleon to efficiently support both ÂµW-power real-time inference and high-throughput tasks with large DNN embedders. To determine the optimal PE array sizes for the two modes, we simulate real-time KWS and peak TOPS W performance in Fig. 11(a), assuming SRAMs dominate total system power. The analysis identifies array sizes of 4 and 16 as optimal.",
    "source": "2505.24852v2_Chameleon_A_MatMul-Free_Temporal_Convolutional_Net.pdf",
    "length": 1899,
    "tokens": 492
  },
  {
    "text": "By clock 33,260 the last of the 32,768 batches has been written to output sums HILT. Of course, it is not necessary to calculate all 32,768 batches of 24,576 activations 8,192 columns each time. Control circuitry should be added to allow appropriate subsets of the maximum calculation. 13.3 Parallel adder tree alternative The partial sums from each CASCADE array are added sequentially. If they were added in parallel using an adder tree, the entire computation would be complete in 32,885 clock cycles, resulting in 99.64 efficiency. However, this would complicate chip layout, with each successive pair of additions being over greater physical distances. Pattern dependent ground bounce would also be exacerbated. At 12 GHz clock frequency, such complications could lead to significant difficulties. Therefore, CASCADE uses sequential additions, at the expense of 1.12 efficiency. 13.4 Summary of CASCADE technique The CASCADE mechanism occurs across two chips in the TRIMERA stack- the SLD for computation and storage of weights, and the HILT die for storage of batches of activations and output sums. Some characteristics include: Column Oriented: Each column of the output is calculated independently, with no cross-column calculation except for CREST nearest neighbor multiplexing every 64 rows. Weight-Stationary Design: The entire weight matrix of 201,326,592 FP4 weights is preloaded into the array before computation begins, and remains unchanged during the calculation of a batch. Direct Weight Loading: Weight loading occurs asynchronously directly from HBM without requiring intermediate cache storage. Parallel Partial Sum Propagation: After multiplication with stored weights, partial sums propagate vertically down each column independently. For arrays up to 24,576 rows (activations), or batches less than 32,768 the partial sums do not need to be transferred from chip to chip, only the completed sums from the 8,192 columns. Broadcast Activation Flow: Unlike conventional horizontal activation pipeline flow, a single FP4 activation value enters simultaneously at the PEs of all 8,208 (8,192 plus 16 spares) columns. While this is a little more complex in hardware than systolically pumping the activations from left to right through the array, it is worth the extra hardware complexity to avoid the delay in activation availability, and the complexity of skewed data.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2388,
    "tokens": 481
  },
  {
    "text": "Overall, Ember compiles PyTorch and TensorFlow embedding operations into optimized DLC code (Section 4), which is then used to generate Any CPU Statement ğ‘†ğ‘‡ğ‘€ğ‘‡ Memory Reference ğ‘šğ‘Ÿğ‘’ğ‘“ CPU variable ğ‘£ğ‘ğ‘Ÿ Stream Variable ğ‘  Unsigned Integer uint For-Loop ğ¹ğ‘‚ğ‘… :: for (ğ»ğ¸ğ´ğ·) { ğ‘†ğ·ğ¸ğ¶ ğµğ‘‚ğ·ğ‘Œ} For Header ğ»ğ¸ğ´ğ· :: ğ‘  ğ‘Ÿto ğ‘Ÿstep uint Stream Declaration ğ‘†ğ·ğ¸ğ¶ :: ğ‘  ğ‘ ğ‘’ Loop Body ğµğ‘‚ğ·ğ‘Œ :: ğ¶ğ´ğ¿ğ¿ ğ¶ğ´ğ¿ğ¿? ğ¹ğ‘‚ğ‘…ğ¶ğ´ğ¿ğ¿? Callback ğ¶ğ´ğ¿ğ¿ :: callback() { ğ‘‡ğ‘‰ğ´ğ¿ ğ‘†ğ‘‡ğ‘€ğ‘‡ } Value Conversion ğ‘‡ğ‘‰ğ´ğ¿ :: ğ‘£ğ‘ğ‘Ÿ to_val(ğ‘ ) Loop Range ğ‘Ÿ :: ğ‘  ğ‘£ğ‘ğ‘Ÿ uint Stream Expression ğ‘ ğ‘’ :: ğ‘™ğ‘  ğ‘ğ‘  ğ‘ğ‘  ğ‘ğ‘  Load Stream ğ‘™ğ‘  :: load_str(ğ‘šğ‘Ÿğ‘’ğ‘“[ğ‘–ğ‘›ğ‘‘ğ‘–ğ‘ğ‘’ğ‘  ],ğ‘¢ğ‘–ğ‘›ğ‘¡) Indices ğ‘–ğ‘›ğ‘‘ğ‘–ğ‘ğ‘’ğ‘  :: ğ‘  ğ‘£ğ‘ğ‘Ÿ uint ALU Stream ğ‘ğ‘  :: alu_str(ğ‘ ğ‘œğ‘,ğ‘ 1,ğ‘ 2) Stream Ops ğ‘ ğ‘œğ‘ :: .",
    "source": "2504.09870v1_Ember_A_Compiler_for_Efficient_Embedding_Operation.pdf",
    "length": 661,
    "tokens": 555
  },
  {
    "text": "23 C.1.2 Exo Optimized Code Example void test(int8_t A[12544][256] , int8_t B[256][64] , int8_t C[12544][64]) { config_st ((64)); config_ex(WEIGHT_STATIONARY , NO_ACTIVATION , 1, false , false); config_ld ((64), 1.0f, 16, 2); config_ld ((256) , 1.0f, 16, 1); config_ld(0, 1.0f, 0, 0); uint32_t res 1 31; uint32_t a 0; uint32_t b 16 16 4 4 8 sizeof(int8_t) 16; for (int_fast32_t io 0; io 98; io ) { for (int_fast32_t i 0; i 8; i ) { mvin( 0, res ((i) (1024)) 16, (16), (16) ); mvin( 0, res ((i) (1024) 256) 16, (16), (16) ); mvin( 0, res ((i) (1024) (2) (256)) 16, (16), (16) ); mvin( 0, res ((i) (1024) (3) (256)) 16, (16), (16) ); for (int_fast32_t ko 0; ko 4; ko ) { mvin2( A[(16 i 128 io)][64 ko], a ((i) (4096) (ko) (1024)) 16, 16 (4) , (16) ); if (io 0) { if (i 0) { mvin3( B[(64 ko)][0], b ((ko) (4096)) 16, 16 (4) , (16) ); } } if (io 0) { if (i 0) { mvin3( B[(16 64 ko)][0], b ((ko) (4096) 1024) 16, 16 (4) , (16) ); } } if (io 0) { if (i 0) { mvin3( B[(32 64 ko)][0], b ((ko) (4096) (2) (1024)) 16, 16 (4) , (16) ); } } if (io 0) { if (i 0) { mvin3( B[(48 64 ko)][0], b ((ko) (4096) (3) (1024)) 16, 16 (4) , (16) ); } } preload(b ((ko) (4096)) 16, res ((i) (1024)) 16 0x40000000 , (16), (16), (16), (16)); compute_preloaded(a ((i) (4096) (ko) (1024)) 16, (( uint32_t)0), (16), (16), 16, 16); preload(b ((ko) (4096) 256) 16, res ((i) (1024) 256) 16 0x40000000 , (16), (16), (16), (16)); compute_preloaded(a ((i) (4096) (ko) (1024)) 16, (( uint32_t)0), (16), (16), 16, 16); preload(b ((ko) (4096) (2) (256)) 16, res ((i) (1024) (2) (256)) 16 0x40000000 , (16) , (16), (16), (16)); compute_preloaded(a ((i) (4096) (ko) (1024)) 16, (( uint32_t)0), (16), (16), 16, 16); preload(b ((ko) (4096) (3) (256)) 16, res ((i) (1024) (3) (256)) 16 0x40000000 , (16) , (16), (16), (16)); compute_preloaded(a ((i) (4096) (ko) (1024)) 16, (( uint32_t)0), (16), (16), 16, 16); preload(b ((ko) (4096) 1024) 16, res ((i) (1024)) 16 0x40000000 , (16), (16), (16), (16)); compute_preloaded(a ((i) (4096) (ko) (1024) 256) 16, (( uint32_t)0), (16), (16), 16, 16); preload(b ((ko) (4096) 1024 256) 16, res ((i) (1024) 256) 16 0x40000000 , (16), (16), (16), (16)); compute_preloaded(a ((i) (4096) (ko) (1024) 256) 16, (( uint32_t)0), (16), (16), 16, 16); preload(b ((ko) (4096) 1024 (2) (256)) 16, res ((i) (1024) (2) (256)) 16 0 x40000000 , (16), (16), (16), (16)); compute_preloaded(a ((i) (4096) (ko) (1024) 256) 16, (( uint32_t)0), (16), (16), 16, 16); preload(b ((ko) (4096) 1024 (3) (256)) 16, res ((i) (1024) (3) (256)) 16 0 x40000000 , (16), (16), (16), (16)); compute_preloaded(a ((i) (4096) (ko) (1024) 256) 16, (( uint32_t)0), (16), (16), 16, 16); ... Unrolling continues } mvout( C[(16 i 128 io)][0], res ((i) (1024)) 16, (16), (16) ); mvout( C[(16 i 128 io)][16], res ((i) (1024) 256) 16, (16), (16) ); mvout( C[(16 i 128 io)][32], res ((i) (1024) (2) (256)) 16, (16), (16) ); mvout( C[(16 i 128 io)][48], res ((i) (1024) (3) (256)) 16, (16), (16) ); } } fence(); } Figure 18: Example of hand-optimized matrix multiplication code from Ikarashi et al. [26], used as a baseline in Sec. 4.3.",
    "source": "2505.18574v2_Autocomp_LLM-Driven_Code_Optimization_for_Tensor_A.pdf",
    "length": 3083,
    "tokens": 1418
  },
  {
    "text": "This result shows that the sparse graph rep- resentation is more suitable for complex circuits due to its short token length. Additionally, our component-type tokens let model to better learn different component s representa- tions to boost the generalizability. In summary, these results suggest that our proposed SFCI is the best formulation to perform the topology generation. 0.02 0.04 0.06 0.08 0.10 Tolerance 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 Success rate SFCI-NDP SFCI-NCT SFCI Figure 8. Success rates of models trained with SFCI and its two variants using 3, 4, 5-component circuits: (1) SFCI-NDP and (2) SFCI-NCT, as in Figure 4. Table 5. MSEs of voltage conversion ratio and efficiency evaluated on SFCI and its two variants using 3, 4, 5-component circuits: (1) SFCI-NDP and (2) SFCI-NCT, as in Figure 4. MSE Voltage Eff SFCI-NDP 0.0033 0.0028 SFCI-NCT 0.0012 0.0022 SFCI 0.0006 0.0002 6. Ablation Study Discussion 6.1. SFCI without Duty-Cycle Prefix When shrinking formulations, the prefix containing five duty cycle numbers is a common feature across data points and might be removed, as stated at the end of Section 4.2. Thus, we evaluate how omitting this prefix affects model performance by training a variant SFCI-NDP without the duty-cycle prefix, as in Figure 4. The results in Figure 8 and Table 5 show that SFCI-NDP suffers a performance drop compared to SFCI, showing that this prefix provides an effective guidance for model s decision making. 6.2.",
    "source": "2506.10235v1_LaMAGIC2_Advanced_Circuit_Formulations_for_Languag.pdf",
    "length": 1502,
    "tokens": 428
  },
  {
    "text": "[19] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Lau- rent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. [20] Benjamin Spector and Chris Re. Accelerating llm inference with staged speculative decoding. In ICML Workshop, 2023. [21] Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. Draft verify: Lossless large language model acceleration via self- speculative decoding. In ACL, 2024. [22] Guseul Heo, Sangyeop Lee, Jaehong Cho, Hyunmin Choi, Sanghyeon Lee, Hyungkyu Ham, Gwangsun Kim, Divya Mahajan, and Jongse Park. Neupims: Npu-pim heterogeneous acceleration for batched llm inferencing. In ASPLOS, 2024. [23] Jaehyun Park, Jaewan Choi, Kwanhee Kyung, Michael Jaemin Kim, Yongsuk Kwon, Nam Sung Kim, and Jung Ho Ahn. Attacc! unleashing the power of pim for batched transformer-based generative model inference. In ASPLOS, 2024. [24] Cong Li, Zhe Zhou, Size Zheng, Jiaxi Zhang, Yun Liang, and Guangyu Sun. Specpim: Accelerating speculative inference on pim-enabled system via architecture-dataflow co-exploration. In ASPLOS, 2024. [25] Minseok Seo, Xuan Truong Nguyen, Seok Joong Hwang, Yongkee Kwon, Guhyun Kim, Chanwook Park, Ilkon Kim, Jaehan Park, Jeongbin Kim, Woojae Shin, et al. Ianus: Integrated accelerator based on npu-pim unified memory system. In ASPLOS, 2024. [26] Xiurui Pan, Endian Li, Qiao Li, Shengwen Liang, Yizhou Shan, Ke Zhou, Yingwei Luo, Xiaolin Wang, and Jie Zhang. Instinfer: In-storage attention offloading for cost-effective long-context llm inference.",
    "source": "2502.15470v2_PAPI_Exploiting_Dynamic_Parallelism_in_Large_Langu.pdf",
    "length": 1630,
    "tokens": 486
  },
  {
    "text": "tional capabilities to produce diverse, large, and unseen analog circuit topologies. The advancement holds both profound engineering and scientific significance, demonstrating that generative AI can not only meet human expertise but also unlock the possibilities beyond human capability. The key contributions in this paper are: (1) We propose a generative engine, AnalogGenie, built on a GPT model to generate diverse analog circuits by predicting the next device pin to connect in the circuit; (2) We introduce a sequence-based, pin-level graph representation that efficiently and ex- pressively captures large analog circuit topologies; (3) We develop a comprehensive dataset of ana- log circuit topologies to advance research in analog electronic design automation using generative AI and introduce an augmentation scheme to enhance data diversity. (4) Experiment results show that AnalogGenie is capable of automatically generating far more, large-scale, valid, unseen, and high-performance topologies compared to existing graph generation and foundation model work. 2 PRELIMINARIES AND RELATED WORKS 2.1 DESIGN PROCESSES OF ANALOG CIRCUITS The design process of analog circuits begins with creating the circuit topology, which involves de- termining the device types (i.e., NMOS PMOS transistor, capacitor, resistor, inductor, etc.) and the number of devices, and defining how they are interconnected. Following this, designers perform device sizing, i.e., optimizing the physical dimensions of devices to achieve desired performance. Finally, the physical layout (i.e., mask design) is developed to prepare for manufacturing. Note that a physical design is the representation of an IC in terms of planar geometric shapes corresponding to the different stacked physical layers (e.g., metal, oxide, or semiconductor) during the fabrication process. Of all these stages, topology design demands the most creative effort, as it needs to be conceptualized from scratch by human designers. While significant progress has been made in au- tomating device sizing (Wang et al., 2020; Cao et al., 2022; Gao et al., 2023; Cao et al., 2024) and layout design (Kunal et al., 2019; Xu et al., 2019), the topology generation remains a challenging problem due to its abstract and complex nature. This work aims to address this thorny issue.",
    "source": "2503.00205v1_AnalogGenie_A_Generative_Engine_for_Automatic_Disc.pdf",
    "length": 2331,
    "tokens": 481
  },
  {
    "text": "RQ1 shows that the surveyed applications do not cover the full spectrum of RS problems. While edge AI does not resonate well with unstressed applications that can be processed on the ground, like long-term climate monitoring, other onboard-relevant use cases remain unexplored, such as data compression and disaster response. Indeed, the NewSpace era enables real-time alerts for numerous emergency situations, such as wildfires. Similarly, local authorities greatly benefit from post-disaster damage assessments, e.g., after floods [81]. Exploring lightweight implementations of SOTA methods on FPGA could significantly advance both research and industry adoption. Concurrently, DL-based data compression pipelines have gained relevance over the past decade, including for EO data [38]. With SmallSats imaging payloads generating data at 640 Mbps [119], efficient onboard compression methods present a valuable research opportunity. Overlooked Prevalent DL Architectures. Although RQ2 highlights the vast diversity of encountered models, the surveyed literature overlooks several widespread model families. Indeed, all DL-based studies rely on CNN or GNN architectures, with no examples of transformers or recurrent networks. This gap may partly reflect the recent emergence of Vision Transformers (ViTs), which have not yet seen mature FPGA implementations. However, it may also highlight the relative implementation simplicity of graph flows and convolution pipelines on FPGA platforms. Nevertheless, ViTs have significantly advanced DL in RS research [115] and, despite their complexity, their superior performance makes them a promising target for edge deployment [105]. Similarly, no surveyed work explores RNNs, despite their natural fit for temporal tasks recurrent in RS, such as monitoring change across time or along flight paths. Investigating the 21 22 Manuscript submitted to ACM FPGA-Enabled Machine Learning Applications in Earth Observation: A Systematic Review 27 application of RNNs onboard flying platforms represents a valuable research direction. At the same time, the absence of support for transformer and recurrent architectures in FPGA toolchains (e.g., FINN, Vitis AI) poses a barrier to broader adoption. To unlock the potential of modern DL architectures in edge environments, automated frameworks must evolve to support these emerging layers, including their quantized and pruned variants. Beyond FPGAs: Coarse-Grained Reconfigurable Arrays (CGRA).",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 2478,
    "tokens": 481
  },
  {
    "text": "While these approaches demonstrate promising results in compressing LLMs while preserving performance, the resid- ual computational and memory demands remain impractical for deployment on resource-constrained edge devices. This highlights the need for more aggressive model compression methods that combine orthogonal techniques, such as quanti- zation and pruning, to produce even more compact LLMs. C. LLM Accelerators The remarkable performance of LLMs has driven efforts [9], [10], [16], [45] to deploy them in edge scenarios. One approach to achieve this goal involves integrating multiple edge devices into a unified system to enhance computational capacity and enable fast LLM acceleration. For instance, DFX [45] combines multiple FPGAs into a single large-scale ac- celerator, enabling low-latency, high-throughput inference for GPT-2 [46]. Another approach is to compress LLMs first and then design specialized accelerators tailored for the compact models. For example, LlamaF [9] uses quantization to com- press both activations and weights of TinyLlama [47] into 8-bit formats and accelerates the resulting quantized MV multiplica- tions with a fully pipelined accelerator. Moreover, FlightLLM [16] integrates quantization and pruning to compress LLMs and develops a dedicated accelerator with two key innovations: (1) a configurable sparse DSP chain optimized for diverse sparsity patterns to enhance computational efficiency, and V K V QKT SV K Q Q QKT SV Prefill Decode WA WB X X Prefill Decode WQ K V X X WO WQ K V WO KCache VCache OMHA OMHA OFFN OFFN MHA FFN WQ WK WV Softmax LayerNorm Act MHA FFN LayerNorm WO WA WB Embedding Layer0 Layer1 ...... LayerN Layer1 LM Head a The structure of LLMs b The prefill and decode stages of LLM inference Fig. 3. (a) The structure of LLMs. (b) Illustrating the key computations during the prefill and decode stages of LLM inference. (2) an always-on-chip decode scheme that reduces memory bandwidth requirements through low-precision quantization.",
    "source": "2505.03745v1_AccLLM_Accelerating_Long-Context_LLM_Inference_Via.pdf",
    "length": 2002,
    "tokens": 466
  },
  {
    "text": "Type Model Model size Open source VerilogEvalv2-SR ( ) VerilogEvalv2-CC ( ) RTLLM v2 ( ) Base LLMs GPT-4o - 64.1 73.7 76.2 57.6 66.1 69.0 56.5 70.3 75.2 DeepSeek-R1 [44] 671B 77.5 84.7 87.4 79.1 85.1 87.1 64.7 75.8 79.7 DeepSeek-V3 [1] 671B 62.4 71.7 75.0 68.7 76.3 78.2 59.1 71.5 73.3 QWQ-32B [45] 32B 64.2 77.3 80.1 64.0 77.8 80.9 52.9 68.0 71.2 DeepSeek-R1-Distill-Qwen-32B [44] 32B 43.9 63.3 69.2 53.8 69.8 73.8 42.4 62.1 67.0 DeepSeek-R1-Distill-Qwen-7B [44] 7B 0.6 2.2 3.5 2.0 7.0 11.3 0.0 0.0 0.0 Qwen2.5-Coder-32B-Instruct [39] 32B 47.5 60.7 64.7 46.6 59.0 62.8 47.8 63.9 67.8 Qwen2.5-Coder-7B-Instruct [39] 7B 31.3 49.3 54.6 30.5 46.8 52.0 36.1 52.4 57.6 CodeV-R1-Distill 7B 65.2 75.2 77.5 65.5 75.6 78.2 57.2 71.9 77.1 CodeV-R1 CodeV-R1 7B 68.8 78.2 81.1 69.9 78.2 80.9 68.0 78.2 81.7 WSR: Specification-to-RTL. CC: Code Completion. TABLE IV RESULTS OF AUTOMATED OS CONFIGURATION OAPTIMIZATION BY AUTOOS COMPARED WITH EXISTING METHODS.",
    "source": "2506.05007v1_QiMeng_Fully_Automated_Hardware_and_Software_Desig.pdf",
    "length": 945,
    "tokens": 497
  },
  {
    "text": "Given that the memristors can not be precisely set to the desired conductance states, we obtain large deviations in the computa- tions. Table 1 shows the results for the different basic operations. The last row shows the increased variation when trying to set the cell on the positive bitline to a state in between the low and high conductance, e.g. for achieving a weight of 0.5. Thus, for this work we stick to binary programming of the states. In order to model weights with higher precision, we can stack multiple crossbars, where each crossbar represents a bit level. That means for a 4-bit resolution, a first crossbar would model the weight states -4, 0, 4, the second -2, 0, 2 and the last -1, 0, 1. Thus, with a stack of 3 crossbars we can model 15 different weight levels from -7 to 7. In the physical hardware the input and output levels can not be freely chosen. In this work, we assume a resolution of 8 bit for the digital-to-analog converter (DAC) providing the input voltage and the analog-to-digital converter (ADC) reading the output currents. As memristors only support a specific phys- ical value range, any inputs and parameters have to be scaled with their respective quantization scales. Further explanation will be given in Section 4. In order to fit larger matrices into the crossbars, we tile the weights across multiple crossbars of e.g. 128 128 memristor pairs, as this is a size that can be assumed to be the realistic maximum for current hardware [24]. All DAC, ADC and tiling parameters are freely configurable in our framework. The memristor cell conductance state functions and additional noises are not configurable, but strictly based on the Synaptogen simulation parameters of real hardware. 3. Automatic Speech Recognition Our ASR system consists of a Conformer encoder [25] with a Connectionist-Temporal-Classification (CTC) [26] output loss layer. We chose the Conformer as encoder architecture as it is widely used in current research literature [27].",
    "source": "2505.24721v1_Running_Conventional_Automatic_Speech_Recognition_.pdf",
    "length": 1991,
    "tokens": 448
  },
  {
    "text": "Huang, O. Bastani, D. Jayaraman, Y. Zhu, L. Fan, and A. Anandkumar. Eureka: Human-level reward design via coding large language models, 2024. URL 12 [38] M. U. Nasir, S. Earle, J. Togelius, S. James, and C. Cleghorn. Llmatic: Neural architecture search via large language models and quality diversity optimization. In Proceedings of the Genetic and Evolutionary Computation Conference, GECCO 24, page 1110 1118. ACM, July 2024. doi: 10.1145 3638529.3654017. URL [39] N. Nayak, T. O. Odemuyiwa, S. Ugare, C. Fletcher, M. Pellauer, and J. Emer. Teaal: A declar- ative framework for modeling sparse tensor accelerators. In Proceedings of the 56th Annual IEEE ACM International Symposium on Microarchitecture, MICRO 23, page 1255 1270, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400703294. doi: 10.1145 3613424.3623791. URL [40] K. Nguyen, S. Schoedel, A. Alavilli, B. Plancher, and Z. Manchester. Tinympc: Model- predictive control on resource-constrained microcontrollers. In IEEE International Conference on Robotics and Automation (ICRA), 2024. [41] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474, 2022. [42] NVIDIA. Nvdla, 2018. URL [43] NVIDIA. About cuda, 2024. URL [44] OpenAI. Introducing chatgpt, 2022. URL [45] OpenXLA. Xla developer guide, 2024. URL [46] A. Orhon, A. Wadhwa, Y. Kim, F. Rossi, and V. Jagadeesh.",
    "source": "2505.18574v2_Autocomp_LLM-Driven_Code_Optimization_for_Tensor_A.pdf",
    "length": 1499,
    "tokens": 490
  },
  {
    "text": "Experimental results in a commercial 16-nm technology demonstrate that the proposed technique improves the area and power consumption by 37 and 68 , respectively, while maintaining the same peak performance. In the proposed architecture, the maximum amount of ex- ploitable weight sparsity is defined by an integer architectural parameter the maximum virtual growth. While the standard systolic array size is determined by the number of rows and columns, the proposed virtually upscaled systolic array (VUSA) is characterized by the number of rows, columns, and virtual columns. This approach preserves the generic and flexible nature of systolic arrays while expanding the design space to better exploit sparsity. To the best of the MOCAST 2025 Paper Preprint 2025 IEEE arXiv:2506.01166v1 [cs.AR] 1 Jun 2025 4 5 6 7 8 9 1 2 3 ğ¼31 ğ¼21 ğ¼11 ğ¼32 ğ¼22 ğ¼12 ğ¼33 ğ¼23 ğ¼13 ğ‘¤1 ğ‘¤2 ğ‘¤3 ğ‘¤4 ğ‘¤5 ğ‘¤6 ğ‘¤7 ğ‘¤8 ğ‘¤9 ğ¼11 ğ¼12 ğ¼13 ğ¼21 ğ¼22 ğ¼23 ğ¼31 ğ¼32 ğ¼33 . ğ‘¤1 ğ‘¤4 ğ‘¤7 ğ‘¤2 ğ‘¤5 ğ‘¤8 ğ‘¤3 ğ‘¤6 ğ‘¤9 (1) (2) Fig. 1. Systolic array with weight-stationary data flow. authors knowledge, this is the first systolic array architecture that introduces a new dimensionality to effectively leverage unstructured weight sparsity in a weight-stationary systolic array without compromising scalability or programmability. The rest of the paper is structured as follows. Section II outlines the background. Subsequently, in Section III, we outline the architecture of the proposed VUSA. A quantitative, theoretical analysis of the expected gain of the proposed technique is presented in Section IV. Section V includes experimental results for a commercial 16-nm technology and real-life DNN applications.",
    "source": "2506.01166v1_VUSA_Virtually_Upscaled_Systolic_Array_Architectur.pdf",
    "length": 1647,
    "tokens": 497
  },
  {
    "text": "For simplicity, we denote the input matrix for the second stage as just ğ‘€. In this stage, ğ‘€is again normalized into Ë†ğ‘€, which has no row or column containing all even numbers except for zeros. The algorithm then converts the input matrix to the CSD representation ğ‘€ğ‘’ğ‘¥ğ‘ğ‘Ÿ ( 1, 0, 1)ğ‘‘ğ‘–ğ‘›,ğ‘‘ğ‘œğ‘¢ğ‘¡,ğµ, where ğµis the span of the powers of the CSD digits (i.e., the difference between the minimal and maximal bit-shifts associated with the CSD digits plus one). The CSE algorithm starts with the ğ‘€ğ‘’ğ‘¥ğ‘ğ‘Ÿmatrix, and a list of implemented values ğ¿ğ‘–ğ‘šğ‘ğ‘™initialized with the elements of the input vector: ğ¿ğ‘–ğ‘šğ‘ğ‘™ [ğ‘£1, ğ‘£2, . . . , ğ‘£ğ‘‘ğ‘–ğ‘›]. These two objects defines the state of the algorithm, and the algorithm updates the state iteratively until exhausting all common subexpressions. For each update step, the algorithm selects a two-term subexpression and implements it. A two-term subexpression is defined as an operation with the general form ğ‘ ğ‘ ğ‘ , characterized as a four-tuple: the two inputs ğ‘and ğ‘, the sign, and the bit-shift ğ‘ for the second operand2. 2The order of ğ‘and ğ‘is fixed by their location in the matrix in practice for the uniqueness of the four-tuple. Manuscript submitted to ACM 8 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu Fig. 2. An example of the graph constructed from the constant matrix ğ‘€in the first stage of the da4ml algorithm without a delay constraint. The graph is constructed with Prim s algorithm, and the MST is shown in colored edges. The root vertex ğ‘£0 is on the bottom left.",
    "source": "2507.04535v1_da4ml_Distributed_Arithmetic_for_Real-time_Neural_.pdf",
    "length": 1521,
    "tokens": 499
  },
  {
    "text": "i Neural Signal Compression using RAMAN tinyML Accelerator for BCI Applications Adithya Krishna, Sohan Debnath, AndrÃ© van Schaik, Mahesh Mehendale and Chetan Singh Thakur Abstract High-quality, multi-channel neural recording is in- dispensable for neuroscience research and clinical applications. Large-scale brain recordings often produce vast amounts of data that must be wirelessly transmitted for subsequent offline analysis and decoding, especially in brain-computer interfaces (BCIs) utilizing high-density intracortical recordings with hundreds or thousands of electrodes. However, transmitting raw neural data presents significant challenges due to limited communi- cation bandwidth and resultant excessive heating. To address this challenge, we propose a neural signal compression scheme utilizing Convolutional Autoencoders (CAEs), which achieves a compression ratio of up to 150 for compressing local field potentials (LFPs). The CAE encoder section is implemented on RAMAN, an energy-efficient tinyML accelerator designed for edge computing, and subsequently deployed on an Efinix Ti60 FPGA with 37.3k LUTs and 8.6k register utilization. RAMAN leverages sparsity in activation and weights through zero skip- ping, gating, and weight compression techniques. Additionally, we employ hardware-software co-optimization by pruning CAE encoder model parameters using a hardware-aware balanced stochastic pruning strategy, resolving workload imbalance issues and eliminating indexing overhead to reduce parameter storage requirements by up to 32.4 . Using the proposed compact depthwise separable convolutional autoencoder (DS-CAE) model, the compressed neural data from RAMAN is reconstructed offline with superior signal-to-noise and distortion ratios (SNDR) of 22.6 dB and 27.4 dB, along with R2 scores of 0.81 and 0.94, respectively, evaluated on two monkey neural recordings. Keywords Convolutional neural networks (CNNs), deep learn- ing, hardware acceleration, sparse processing, machine learning, Convolutional Autoencoders (CAEs), tinyML, edge computing, stochastic processing. I. INTRODUCTION In recent years, the Brain-Computer Interface (BCI) has garnered significant attention for facilitating direct commu- nication between the human brain and external devices [1] [4].",
    "source": "2504.06996v1_Neural_Signal_Compression_using_RAMAN_tinyML_Accel.pdf",
    "length": 2288,
    "tokens": 490
  },
  {
    "text": ". , G}. 1 Assign each token t to any GPU g that hosts its expert: d(t) min{g (f(t), g) P}; 2 Lg {t d(t) g} for all g {1, . . . , G}; 3 while maxg Lg ming Lg 1 do 4 gh arg maxg Lg; gc arg ming Lg; 5 l Lgh Lgc 2 m ; 6 e arg maxe E(gh) {t d(t) gh f(t) e} ; 7 if (e , gc) P and copies(e ) Cmax and params(e ) Mgc then 8 copy weights of e on gc; P P {(e , gc)}; 9 Reassign the first tokens {t d(t) gh f(t) e } to gc by setting d(t) gc; 10 Update Lgh and Lgc; 11 return P, d; We model the expert activation distribution in each layer of the MoE model using a multinomial distribution and estimate its parameters via Maximum Likelihood Estimation (MLE). The multinomial distribution is commonly used to model counts of outcomes across discrete classes, in our case the selection of experts within a layer. MLE selects the distribution that the observed data has the maximum likelihood to fit into. Table 1: Impact of skewness on expert distribution estimation and system performance. Higher skew- ness leads to higher error rate, indicating reduced estimation accuracy and degraded performance. Dataset Skewness Error rate ( ) MMLU 1.39 1.80 Alpaca Eval 1.40 0.98 SST2 1.99 16.00 Formally, let pl i denote the probability of se- lecting expert i in layer l. Assuming that each token s expert selection is an independent and identically distributed (i.i.d.) sample from this multinomial distribution, the MLE of pl i is given by: Ë†pl i nl i N , (1) where N is the total number of tokens observed at layer l, and nl i is the number of tokens that activated expert i. When the training data come as batches, the estimation becomes a moving average. We note that expert selection is primarily governed by local token-level features and routing mechanisms.",
    "source": "2506.07366v1_MoE-GPS_Guidlines_for_Prediction_Strategy_for_Dyna.pdf",
    "length": 1744,
    "tokens": 489
  },
  {
    "text": "[10] V. Costan and S. Devadas, Intel SGX explained. Cryptology ePrint Archive, Paper 2016 086, 2016. [11] P. P. Ray, A review on tinyml: State-of-the-art and prospects, Journal of King Saud Univer- sity - Computer and Information Sciences, vol. 34, no. 4, pp. 1595 1623, 2022. [12] D. Li, X. Chen, M. Becchi, and Z. Zong, Evaluating the energy efficiency of deep convo- lutional neural networks on cpus and gpus, in 2016 IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking (Social- Com), Sustainable Computing and Communications (SustainCom) (BDCloud-SocialCom- SustainCom), pp. 477 484, 2016. [13] Y. Abadade, A. Temouden, H. Bamoumen, N. Benamar, Y. Chtouki, and A. S. Hafid, A comprehensive survey on tinyml, IEEE Access, vol. 11, pp. 96892 96922, 2023. [14] E. BUBER and B. DIRI, Performance analysis and cpu vs gpu comparison for deep learn- ing, in 2018 6th International Conference on Control Engineering Information Technol- ogy (CEIT), pp. 1 6, 2018. [15] C. R. Banbury, V. J. Reddi, M. Lam, W. Fu, A. Fazel, J. Holleman, X. Huang, R. Hurtado, D. Kanter, A. Lokhmotov, D. Patterson, D. Pau, J. sun Seo, J. Sieracki, U. Thakker, M. Ver- helst, and P. Yadav, Benchmarking tinyml systems: Challenges and direction, 2021.",
    "source": "2506.01827v1_Memory_Access_Characterization_of_Large_Language_M.pdf",
    "length": 1280,
    "tokens": 419
  },
  {
    "text": "Driven by the above discussion and the potential optimiza- tion opportunities presented by EA and AE, we propose D ej a View, an energy-efï¬cient design for 360 video streaming on VRs. As shown in Fig. 4, D ej a View leverages compute lo- cality to bypass computations and provides signiï¬cant energy savings, with the following two-step optimization strategy: a For each frame, if the head orientation remains the same, we take advantage of the EA opportunity. b If exploiting the EA opportunity is not possible, we take advantage of the AE opportunity, by performing computation for only one eye (and construct the result for the other eye). C. InterFrame-IntraEye (EA) Computation Optimization We plan to leverage the EA opportunity when the user s head orientation does not change. Intuitively, as mentioned earlier in Sec. III (Fig. 3), a Transformation and b Projec- tion Computation remain unchanged. To understand all the contributing factors which affect computations, we further investigate the important inputs of the VR headset. This can help us identify and isolate proper memoization candidates for carefully tweaking our design decisions to maximize the reuse beneï¬ts. Further, we also study the overheads introduced by our design modiï¬cations to perform a fair comparison with the state-of-the-art. What (features) to Memoize? As discussed earlier, at any moment during VR video processing, the execution pipeline is not only impacted by the head orientation, but also by other features such as video frame rate, video types semantics information, pixel values, user interactions. To better under- stand which of these are the best candidates (features, using machine learning parlance) for memoization and whether they are sufï¬cient or not, we next discuss input parameters and their impact on the computation: Head orientation: Any changes in this affect the matrix T2 as discussed in Tab. I, thus changing the transformation matrix T and eventually leading to re-computation of the most compute-intensive projection matrix P. Thus, it is a critical feature in projection computation executions. Pixel values: The pixel contents values (denoted as F in Fig. 3) matter only during data transfer (from the input 360 frame to the framebuffer) in the projection mapping stage, after the coordinate mappings (P in Fig.",
    "source": "DejaView.pdf",
    "length": 2329,
    "tokens": 499
  },
  {
    "text": "Splitwiser: Efficient LM inference with constrained resources Asad Aali Stanford University Adney Cardoza UT Austin Melissa Capo UT Austin Abstract Efficient inference of LLMs remains a crucial chal- lenge, with two main phases: a compute-intensive prompt compu- tation and a memory-intensive token generation. Despite existing batching and scheduling techniques, token generation phases fail to fully utilize compute resources, especially when compared to prompt computation phases. To address these challenges, we propose Splitwiser, a methodology that splits the two phases of an LLM inference request onto the same GPU, thereby reducing overhead and improving memory access and cache utilization. By eliminating the need to transfer data across devices, Splitwiser aims to minimize network-related overheads. In this report, we describe the basic structure of our proposed pipeline while shar- ing preliminary results and analysis. We implement our proposed multiprocessing design on two widely-used and independent LLM architectures: Huggingface and vLLM. We open-source our code for the respective implementations: 1) Huggingface: github.com asad-aali splitwiser 2) vLLM: github.com adney11 vllm-sysml I. INTRODUCTION Generative Large Language Models (LLMs) have become essential in computing, offering vast capabilities in natural language processing. However, their widespread adoption has led to challenges, particularly in inference efficiency. Existing LLMs, often running on expensive GPUs, face issues with overhead and resource utilization. Prior work, specifically Splitwise [5], addresses this issue by proposing to split the inference phase into two individual processes that can be run on separate GPUs. This approach allows operators to make use of the vast array of heterogeneous GPUs available in their cluster simultaneously, increasing the total hardware utilization while reducing the time to serve a batch of inference requests. The problem this work is aimed at solving is towards making split inference more accessible to operators that don t have immediate access to a vast number of GPUs, and would still like to enjoy the benefits of split inference. To that end, Splitwiser aims to address this challenge by optimizing the processing of split phase inference of LLMs on a single GPU. II. BACKGROUND RELATED WORKS A. Large Language Models Modern Large Language Models (LLMs) are mainly based on transformer architectures, which use attention mechanisms and multi-layer perceptron layers for input and output gen- eration.",
    "source": "2505.03763v1_Splitwiser_Efficient_LM_inference_with_constrained.pdf",
    "length": 2551,
    "tokens": 502
  },
  {
    "text": ", aN(t)). The immediate reward for sensor si is defined as: Ri(t) Î³ Ai(t), if ai(t) P and inference is correct, Î´, if ai(t) P and inference is incorrect, Î·, if ai(t) NP. Here, Î³ 0 scales the reward for correct participation, Î´ 0 penalizes incorrect inference, and Î· 0 penalizes non-participation, with Î· Î´ ensuring that remaining idle 8 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 is more penalizing than at least attempting participation. The cost incorporates energy consumption and future op- portunities. Let ei(t) be the energy expenditure for sensor si if it participates at time t, accounting for capture, infer- ence, and communication costs. Introduce a discount factor Î² [0, 1), and let Vi(t 1) represent the expected future utility of sensor si given its current decisions and predicted energy availability. The cost is: Ci(t) ei(t) Î²Vi(t 1). The overall utility is: Ui(t) Ri(t) Ci(t). We assume that Ai(t) is non-decreasing in the quality of sensor si s data (e.g., higher SNR yields higher Ai(t)). We also assume that energy resources, accuracy gains, and reward penalty parameters are finite and bounded, and that sensors have consistent estimation mechanisms for Ai(t) and Ë†Ei(t 1). Potential Function Construction To prove convergence, we define a potential function that reflects the collective utility of the sensor network: Î¦(a(t)) N X i 1 Ui(ai(t), a i(t)). Since Ui(t) Ri(t) Ci(t), we have: Î¦(a(t)) N X i 1 [Ri(t) Ci(t)].",
    "source": "ParticipationGamesICML25.pdf",
    "length": 1618,
    "tokens": 486
  },
  {
    "text": "1 9. [23] M. Milakov and N. Gimelshein, Online normalizer calculation for softmax, arXiv preprint arXiv:1805.02867, 2018. [24] K. Alexandridis and G. Dimitrakopoulos, Online alignment and addi- tion in multiterm floating-point adders, IEEE Trans. on VLSI Systems, vol. 33, no. 4, pp. 1182 1186, 2025. [25] N. A. Koca et al., Hardware-efficient softmax approximation for self- attention networks, in Intern. Symp. on Circuits and Systems (ISCAS), 2023, p. 1 5. [26] W. Wang et al., SOLE: hardware-software co-design of softmax and layernorm for efficient transformer inference, in IEEE ACM Intern. Conference on Computer Aided Design (ICCAD), 2023, p. 1 9. [27] M. E. Sadeghi et al., Peano-vit: Power-efficient approximations of non- linearities in vision transformers, in ACM IEEE Intern. Symposium on Low Power Electronics and Design (ISLPED), 2024, pp. 1 6. [28] J. Koenig et al., A hardware accelerator for computing an exact dot product, in IEEE Symp. on Comp. Arith. (ARITH), 2017, pp. 114 121. [29] D. Kalamkar et al., A study of bfloat16 for deep learning training, arXiv preprint arXiv:1905.12322, 2019. [30] K. Zhu et al., Promptbench: A unified library for evaluation of large language models, arXiv preprint arXiv:2312.07910, 2023. [31] S. Kim et al., I-bert: Integer-only bert quantization, in International conference on machine learning. PMLR, 2021, pp. 5506 5518.",
    "source": "2505.14314v2_Low-Cost_FlashAttention_with_Fused_Exponential_and.pdf",
    "length": 1378,
    "tokens": 427
  },
  {
    "text": "The proposed methodology comprises five key steps: estimating the neuron importance, determining layer importance, XAI-guided neuron skipping and approximation, estimating energy consumption, and analyzing the accuracy-energy tradeoffs. Algorithm 1 de- lineates the steps involved in generating XAI-guided AxDNNs. Algorithm 1: XAI-Gen: XAI-guided AxDNN genera- tion Inputs : Number of layers in trained model Î¸: L; Number of neurons in layers: Yl; Set of approximate multipliers: m; Step size: Î´; Initial conductance threshold: tc; Skipping threshold: tp; Quality constraint: Qc Energy constraint: Ec; Quantization scale: q Error magnitude of multipliers: Mg; Dataset: D Outputs: AxDNN model: Î¸ ; El; Ql 1: R {m0, m1, ....ml} Initialize a list of accurate multipliers for each layer 2: for each l in L do 3: for each y in Yl do 4: IG(x) R 1 Î³ 0 F(x Î³(x x )) y y xi dÎ³ Integrated gradients for each neuron 5: Cy,l(x) P i (xi x i) IG(x) Neuron conductance for each neuron 6: end for 7: zl P jCy,l(x) Aggregated neuron conductance for each layer 8: end for 9: Zvall AscendingSort(zl, L) Sort zl from least to most important layer 10: while (traversing layers with Zvall) do 11: (ml , R ) SelectMult(Zvall, tc, Mg, m) Select the approximate multiplier ml , and update the list R for layer l 12: np (Cy,l(x), tp, Î¸) Indices of least important neurons 13: Î¸p (np, Î¸q) Skipping the least important neurons 14: Î¸q Quantize(q, Î¸p) Quantize models 15: Î¸ Generate(R , Î¸q) Generate AxDNN models 16: (El, Ql) Evaluate(R , S, D, Î¸ ) Evaluate energy efficiency and quality using the analytical model S and model Î¸ 17: if (El Ec) (Ql Qc) then 18: break 19: end if 20: Commit tc and m l in layer l 21: Reduce tc by Î´ value 22: end while 23: return (Î¸ , El, Ql) The inputs to the XAI-Gen algorithm are the total number of layers L and the total number of neurons Yl in a layer l of a pre-trained model Î¸, a set of approximate multipliers m, step size Î´, an initial layer conductance threshold tc, skipping threshold tp, quantization scale q and dataset D. XAI-Gen also takes energy constraint Ec, quality constraint Qc, and error magnitude of approximate multipliers Mg (mean average error) as input.",
    "source": "2503.16583v1_Explainable_AI-Guided_Efficient_Approximate_DNN_Ge.pdf",
    "length": 2182,
    "tokens": 643
  },
  {
    "text": "2.1.1 Statistical Learning Algorithms Statistical learning methods, including linear regression, support vector machines [4], and tree- based models such as random forests and gradient boosting [5], have been widely used for analog design tasks. The statistical learning methods have proven to be effective when computational resources are limited and available data is sparse. The interpretability pro- vided by statistical learning models, including the analysis of the importance of features and the visualization of decision processes such as the structure of a tree, provide benefit when analyzing the relationships amongst design parameters [5, 6]. , Zhengfeng Wu, Nnaemeka Achebe, Ziyi Chen, Vaibhav V. Rao, Pratik Shrestha, and Ioannis Savidis Figure 1: An overview of applying machine learning for the synthesis and physical design of an analog and RF circuit. 2.1.2 Neural Network-Based Approaches Over the past ten years, neural network-based approaches have gained traction in analog design automation, beginning with the versatile multi-layer percep- tron (MLP) [7]. More recently, graph neural networks (GNNs) [8 11] have shown significant promise in learning circuit connectivity and topological features. For graph representations to model analog circuit behaviors at the device level, vertices often represent active and passive devices, while edges represent nets [8, 9]. However, some models treat both devices and nets as nodes when intercon- nect features are emphasized for better prediction or optimization performance [10, 11]. Generative adversarial networks (GAN) have also been applied to guide the routing of an analog IC [12]. Transfer learning permits the reuse of pre-trained model layers on new tasks [13], during either standalone model training [14] or execution of optimization [15]. Transfer learning allows for the gen- eralization of models across circuit design objectives including for different technology nodes and different circuit topologies [14 16], which reduces data requirements and improves model performance. 2.2 Optimization Two classes of optimization algorithms include classic methods and surrogate-assisted. A discussion of each follows. 2.2.1 Classic Optimization Algorithms Algorithms based on gra- dient descent are computationally efficient and are often used for problems with differentiable cost functions [17]. However, the al- gorithms are limited by potential convergence to local minima, especially in non-convex design spaces.",
    "source": "2506.00007v1_Emerging_ML-AI_Techniques_for_Analog_and_RF_EDA.pdf",
    "length": 2492,
    "tokens": 502
  },
  {
    "text": "We also conduct experiments on proteins that GPUs cannot process without the chunk option to evaluate the performance of LightNobel on proteins with long sequence lengths. Figure 14(d) shows LightNobel achieves 2.34-3.30 , 1.94-2.97 lower latency with the chunk option Baseline PPM (w o Chunk) Baseline PPM (w o Chunk) Baseline PPM (w Chunk) Baseline PPM (w Chunk) LightNobel LightNobel Baseline PPM (w o Chunk) Baseline PPM (w Chunk) LightNobel Baseline PPM (w o Chunk) Baseline PPM (w Chunk) LightNobel (a) (b) Peak Mem Requirement (GB, Log Scale) CASP16 CAMEO 1 2 3 4 Dataset 4,957.94 CASP14 CASP15 11.52 7.79 6.14 11.52 7.79 6.14 308.80 36.67 11.19 308.80 36.67 11.19 597.35 54.36 14.28 597.35 54.36 14.28 208.87 41.29 208.87 41.29 Peak Mem Requirement (GB, Log Scale) CASP16 CAMEO 1 2 3 4 Dataset 4,957.94 CASP14 CASP15 11.52 7.79 6.14 308.80 36.67 11.19 597.35 54.36 14.28 208.87 41.29 1 2 3 4 Sequence Length (K) 10 1 2 3 4 5 6 7 8 9 9,945 3,360 1,660 80GB Peak Mem Requirement (GB, Log Scale) 15,121GB 1 2 3 4 Sequence Length (K) 10 1 2 3 4 5 6 7 8 9 9,945 3,360 1,660 80GB Peak Mem Requirement (GB, Log Scale) 15,121GB Baseline PPM (w o Chunk) Baseline PPM (w Chunk) LightNobel (a) (b) Peak Mem Requirement (GB, Log Scale) CASP16 CAMEO 1 2 3 4 Dataset 4,957.94 CASP14 CASP15 11.52 7.79 6.14 308.80 36.67 11.19 597.35 54.36 14.28 208.87 41.29 1 2 3 4 Sequence Length (K) 10 1 2 3 4 5 6 7 8 9 9,945 3,360 1,660 80GB Peak Mem Requirement (GB, Log Scale) 15,121GB Figure 15: Peak memory requirement of PPM across (a) datasets and (b) various sequence lengths. (a) (b) Normalized Computation Cost (10K) 2 4 8 14 6 10 12 10 1 2 3 4 5 6 7 8 9 LightNobel LightNobel Baseline PPM Baseline PPM LightNobel Baseline PPM LightNobel Baseline PPM 128 72 Sequence Length (K) 43.48 Decreased Normalized Computation Cost (10K) 2 4 8 14 6 10 12 10 1 2 3 4 5 6 7 8 9 LightNobel Baseline PPM 128 72 Sequence Length (K) 43.48 Decreased 10 Normalized Memory Footprint (100K) 5 10 15 20 25 1 2 3 4 5 6 7 8 9 1,966 508 LightNobel LightNobel Baseline PPM Baseline PPM LightNobel Baseline PPM Sequence Length (K) 74.10 Decreased 10 Normalized Memory Footprint (100K) 5 10 15 20 25 1 2 3 4 5 6 7 8 9 1,966 508 LightNobel Baseline PPM Sequence Length (K) 74.10 Decreased (a) (b) Normalized Computation Cost (10K) 2 4 8 14 6 10 12 10 1 2 3 4 5 6 7 8 9 LightNobel Baseline PPM 128 72 Sequence Length (K) 43.48 Decreased 10 Normalized Memory Footprint (100K) 5 10 15 20 25 1 2 3 4 5 6 7 8 9 1,966 508 LightNobel Baseline PPM Sequence Length (K) 74.10 Decreased Figure 16: (a) Computational cost of PPM and (b) memory footprint of PPM across various sequence lengths.",
    "source": "2505.05893v1_LightNobel_Improving_Sequence_Length_Limitation_in.pdf",
    "length": 2643,
    "tokens": 926
  },
  {
    "text": "To facilitate the deployment of LLMs on memory-constrained edge devices, weight quantization emerges as one of the most effective approaches, as it can substan- tially reduce the memory overhead of large language models. However, weight outliers significantly impact model accuracy, making it challenging to quantize the model to ultra-low bits while maintaining model accuracy. Thus, effectively protecting weight outliers is crucial [24], [25]. B. Related Works Previous works [5], [8] [11] on weight quantization, in- cluding single-precision and mixed-precision methods, aim to minimize memory overhead without affecting the accuracy of LLMs. Several single-precision quantization methods such as GPTQ [5] and AWQ [8] quantize the weight matrix in single- precision. Among them, GPTQ introduces an innovative one- shot weight quantization technique that leverages approximate second-order information. AWQ protects the salient weights by observing the distribution of activation values. Regrettably, these methods suffer from a significant precision loss when quantizing to ultra-low bits, due to outlier issues. In addition, some mixed-precision quantization methods are proposed to maintain model accuracy. For instance, OWQ [9] and LLM- MQ [10] store weight outliers in FP16 Format separately from normal values. PB-LLM [11] binarizes a portion of the model s weights while preserving some outliers. However, these methods still suffer from accuracy loss due to inter-group data variability. Additionally, methods like Olive [12] and Tender [13] employ hardware-software co-design for outlier-aware quantization. They quantize both activations and weights to 4 bits and focus on outliers in activations rather than weights. C. Observation and Key Idea Through extensive exploration of existing implementations, we have three crucial observations that indicate significant op- timization potential for hardware-software co-design of mixed- precision quantization, as shown in Figure 3. Observation I: Mixed-precision quantization methods through coarse-grained groups struggle to strike a balance between model accuracy and memory overhead. Existing mixed-precision quantization algorithms operate at the group level, with each group typically consisting of more than 128 weight values, and the distribution of outliers within the group is random.",
    "source": "2504.19746v1_FineQ_Software-Hardware_Co-Design_for_Low-Bit_Fine.pdf",
    "length": 2353,
    "tokens": 489
  },
  {
    "text": "This is generally a challenging method of scaling, as obtaining more data with good quality is not always available and also results in increased total training time of the model. The third scaling impact we found was the benefit of increasing the number of beams and doing a beam search. Beam search is a heuristic search algorithm which allows the model to explore multiple token Prog ID Edit Dist Example P37 1 Incorrect immediate value causes wrong division factor and early loop termination Ground truth: asr r2, r2, 2 Predicted: asr r2, r2, 1 P127 1 Array index offset error causes wrong element compar- ison Ground truth: sub r3, r3, 2 Predicted: sub r3, r3, 1 P63 12 Register overwrite corrupts loop counter before multi- plication Ground truth: mov r0, r2; ldr r1, [r3, r1, lsl 2]; mul r0, r0, r1 Predicted: ldr r0, [r3, r1, lsl 2]; mul r0, r0, r1 P153 17 Incorrect instruction sequence fails to compute absolute value Ground truth: sub r2, r2, r3; cmp r2, 0; rsblt r2, r2, 0 Predicted: sub r1, r2, r3; eor r2, r1, r2; sub r2, r2, r1 P47 19 Mismatched memory access offsets cause incorrect data retrieval Ground truth: str r1, [fp, -404]; ldr r2, [fp, -404] Predicted: str r1, [fp, -404]; ldr r2, [r3, -20] Table 6: Armv5 Syntactically similar generations can still produce critical semantic errors. paths in parallel during an inference. Intuitively, beam search allows the model to explore alternative options for next token generation, settling on the most likely token. Beam searching presents an obvious trade-off between computational resources utilization for an inference and prediction accuracy. Combined with a large context window, this is a very powerful technique which we found to be more pronounced when a model was not already near perfect accuracy: in Figure 5, we show an increase going up to 99.39 with the use of beam search for assembly transpilation.",
    "source": "2506.14606v1_Guaranteed_Guess_A_Language_Modeling_Approach_for_.pdf",
    "length": 1881,
    "tokens": 489
  },
  {
    "text": "The result- ing analysis uncovers both expected performance trends as well as surprising disparities between hardware specifica- tions and actual performance, including ğœ‡NPUs exhibiting unexpected scaling behaviors with increasing model com- plexity. We hope our findings provide valuable insights to both developers and hardware architects. 2 BACKGROUND MOTIVATION 2.1 Resource-Constrained Neural Computing The shift from cloud-based to on-device neural computing has numerous advantages for real-time data processing, es- pecially with increasing concerns regarding data privacy and security [8]. Unlike cloud-based solutions, local inference mitigates security risks by processing sensitive data locally, which is particularly advantageous in domains such as med- ical diagnostics and surveillance [9, 10]. Additionally, local processing reduces end-to-end latency alongside operating costs for model vendors. However, traditional NN acceler- ators, such as GPUs and TPUs, are ill-suited to resource- constrained environments given their power consumption and large form factors [11, 12]. MCUs are compact, low-power computing platforms, often reliant on a single CPU and shared memory bus [13]. While MCUs are commonly adopted for resource-constrained IoT applications [14 16], they generally lack the computational resources for efficient NN inference. Specifically, the compu- tational capability of typical MCUs is often limited to a few million MAC operations per second, far below the tens of billions MACs s required for real-time NN inference. Their 1Upon publication SRAM Global Buï¬€er PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE Figure 1: typical ğœ‡NPU hardware architecture absence of dedicated hardware acceleration results in large latency overheads and elevated power consumption during NN processing. Limited SRAM and flash memory also often poses challenges for efficiently managing the large weight matrices required for NN models. Given the various shortcomings of traditional MCUs, microcontroller-scale ğœ‡NPUs are emerging as a response. These specialized NN accelerators offer dedicated neural pro- cessing hardware, providing higher throughput for NN work- loads, meeting the stringent requirements of real-time NN inference [17 19] while maintaining low-power operation. Collectively, ğœ‡NPUs position themselves as a key solution for efficient, real-time NN processing in resource-constrained environments.",
    "source": "2503.22567v2_Benchmarking_Ultra-Low-Power_Î¼NPUs.pdf",
    "length": 2432,
    "tokens": 502
  },
  {
    "text": "arXiv:2506.00424v2 [cs.LG] 15 Jun 2025 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning Chamika Sudusinghe 1 Gerasimos Gerogiannis 1 Damitha Lenadora 1 Charles Block 1 Josep Torrellas 1 Charith Mendis 1 Abstract Sparse tensor programs are essential in deep learn- ing and graph analytics, driving the need for op- timized processing. To meet this demand, spe- cialized hardware accelerators are being devel- oped. Optimizing these programs for acceler- ators is challenging for two reasons: program performance is highly sensitive to variations in sparse inputs, and early-stage accelerators rely on expensive simulators. Therefore, ML-based cost models used for optimizing such programs on general-purpose hardware are often ineffec- tive for early-stage accelerators, as they require large datasets for proper training. To this end, we introduce COGNATE, a novel framework that leverages inexpensive data samples from general- purpose hardware (e.g., CPUs) to train cost mod- els, followed by few-shot fine-tuning on emerging hardware. COGNATE exploits the homogeneity of input features across hardware platforms while effectively mitigating heterogeneity, enabling cost model training with just 5 of the data samples needed by accelerator-specific models to achieve comparable performance. We conduct extensive experiments to demonstrate that COGNATE out- performs existing techniques, achieving average speedups of 1.47 (up to 5.46 ) for SpMM and 1.39 (up to 4.22 ) for SDDMM. 1. Introduction Sparse tensor programs have gained increased significance with the recent advancements in sparse deep learning and graph analytics (Beltagy et al., 2020; Ye Ji, 2021; Child et al., 2019; Dao et al., 2021) workloads. As a result, many hand-crafted performance optimization techniques have been suggested to improve the performance of sparse 1University of Illinois Urbana-Champaign, USA. Correspon- dence to: Chamika Sudusinghe Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s).",
    "source": "2506.00424v2_COGNATE_Acceleration_of_Sparse_Tensor_Programs_on_.pdf",
    "length": 2106,
    "tokens": 471
  },
  {
    "text": "We further compare the normalized throughput with the A100 GPU and FlightLLM when processing different input prefill and output decode token sizes. As shown in Fig. 13, we observe that: (1) Benefiting from both algorithmic and hardware optimizations, our method outperforms the A100 GPU and FlightLLM (U280 and VHK158) across various input and output token lengths, achieving a 1.77 3.64 11 1 1.39 1.91 1.05 1.25 0.00 0.50 1.00 1.50 2.00 2.50 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 AccLLM AccLLM w o Sparisity AccLLM w o W2 AccLLM w o KV4 AccLLM w o Flexiable DSPPacking Strategy Normalized Latency Hardware Utilization Q K V O Attention FFN Noramlized Latency Fig. 14. The ablation studies on the end-to-end latency and hardware utilization of AccLLM evaluated on Llama-2-7B [4] containing Q K V O, Attention, and FFN in decode stage. 0 0.5 1 1.5 2 2.5 3 3.5 4 3k 4k 5k 6k 7k KV Cache Size (GB) Sequence Length FP16 FP16 Î»-Shaped Attention KV4 KV4 Î»-Shaped Attention Fig. 15. The ablation studies on KV cache size of AccLLM. improvement in geometric mean throughput. (2) Particularly, when the input and output token sizes are 128 and 512, our approach demonstrates much better performance. This per- formance gain stems from our accelerator s reconfigurability, which efficiently accommodates both prefill and decode stages. Hardware Ablation Studies: We further conduct abla- tion studies on latency reduction and hardware utilization of different methods used in AccLLM. As shown in Fig. 14, we observe the following: (1) Although 2:4 semi-structured pruning provides limited improvements in hardware utiliza- tion, it effectively eliminates redundant parameters, thereby enhancing throughput and achieving 1.39 speedup.",
    "source": "2505.03745v1_AccLLM_Accelerating_Long-Context_LLM_Inference_Via.pdf",
    "length": 1725,
    "tokens": 469
  },
  {
    "text": "Baseline PT takes the Full Power DNN and uses techniques proposed by (Yang et al., 2018) and (Yang et al., 2017) to prune, quantize, and compress the model. Baseline iNAS PT designs the network from the ground up while combining the work of iNAS (Mendis et al., 2021) and EAP (Yang et al., 2018, 2017). 7 We also compare our approach with recent state-of-the-art methods specifically designed for in- termittent systems, namely Stateful (Yen et al., 2022), ePerceptive (Montanari et al., 2020), and DynBal (Yen et al., 2023). These methods introduce various techniques such as embedding state information into the DNN, multi-resolution inference, multi-exit architectures, and runtime reconfig- urability to handle intermittency in energy-harvesting devices. We have faithfully re-implemented these methods as per the descriptions and adjusted them for a fair comparison under our setup. Results: Table 1 shows the accuracy of our approach against the baselines and the recent state-of-the- art methods using the TI MSP board powered by piezoelectric energy harvesting. The inferences meeting the SLO requirements are the only ones considered for accuracy; i.e., a correct classification violating the latency SLO is considered as incorrect . Datasets Full Power AP PT iNAS PT Stateful ePerceptive DynBal NExUME FMNIST 98.70 71.90 79.72 83.68 85.40 86.25 87.50 88.90 CIFAR10 89.81 55.05 62.00 66.98 68.50 70.20 71.75 76.29 MHEALTH 89.62 59.76 65.40 71.56 73.80 74.95 76.10 80.75 PAMAP 87.30 57.38 65.77 70.33 72.20 73.35 74.50 75.16 AudioMNIST 88.20 67.29 73.16 75.41 76.80 77.95 78.60 80.01 Table 1: Accuracy comparison on TI MSP board using piezoelectric energy harvesting. As observed in Table 1, NExUME consistently outperforms the state-of-the-art methods across all datasets.",
    "source": "NexUME.pdf",
    "length": 1781,
    "tokens": 486
  },
  {
    "text": "[84] binarize a 4 layers CNN, and Pitonak et al. [95] compare 2,3 and 4-bit quantization using FINN. The Vitis AI compiler [5] supports the Flexible design approach, using a Deep Learning Processor Unit (DPU) flashed to the FPGA able to run multiple different AI workloads. The peak Operations Per Cycle (OPC) of the DPU can be customized and matched to the algorithm [5]. Sabogal and George [104] test multiple networks with different DPU configurations, demonstrating that some networks have utilization as low as 14.2 (ESPNet) and up to 85.6 (U-Net) using the B1024 (1024 OPC) architecture. Vitis AI can leverage multiple DPU cores to increase performance as demonstrated by Nguyen et al. [87] and Yu et al. [136]. On the downside, Vitis AI requires PTQ or QAT, as it only supports the i8 datatype [5]. Other automated frameworks include MATLAB (2023a) [31] and the Xilinx System Generator (XSG) [42], but both works do not go into detail concerning their implementation. Bahl et al. [8] use VGT, a VHDL compiler generating circuits for abstract CNNs similar to FINN; it has, however, no continuing support. Frameworks that are not classified do not provide enough information on their implementation. Design Pattern. We split each workload into two distinct design patterns. Flexible (F) designs are bitstreams that can support multiple different kernels and network topologies without requiring reconfiguration. Supporting different networks and kernels usually requires weights to be loaded from off-chip memory. Zhang et al. [137] use a Flexible design 14See Section 6.3 for a discussion about the confusion between [GOP], [GOPs], and [GOP s]. Manuscript submitted to ACM FPGA-Enabled Machine Learning Applications in Earth Observation: A Systematic Review 19 Table 3. FPGA Optimization Taxonomy Table Implementation Choices Design Metrics Peformance Metrics Impl. Fam. P Ref. FPGA Model Name Prec.",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 1905,
    "tokens": 456
  },
  {
    "text": "Groupwise Normalization Each output channel may still exhibit significant internal variability even with per-output-channel quantization. To address this, groupwise normalization is often used to allow each group of elements share a separate scale. We follow QLoRA s design of using 64-element normalization scaled by the absmax (i.e., maximum absolute value) in each group [9]. Data-Free Post-Training Quantization Unlike quantization-aware training (QAT) [11, 51, 17, 39], our approach adds no overhead to fine-tuning. By automatically searching for quantization mappings and thresholds, it frees users from manual tuning [39, 54], saving both development and computation resources. Moreover, contrary to some methods that vary compression ratios over time [39, 51], LowRA maintains a consistent compression ratio, ensuring persistent memory savings during fine-tuning. Per-Output-Channel Thresholds and Mappings Figure 3 visualizes the roles of thresholds and mappings in the process of quantization. Thresholds refer to the boundary points ( bin edges ) that partition the continuous domain of normalized parameters into discrete intervals and thus specific bitstring encodings. Mappings, on the other hand, specify the representative values assigned to each encoded bitstring and thus the intervals. As discussed in 2.3, fine-grained designs of quantization mappings and thresholds could lead to significantly more accurate approximation and reconstruction of parameters. LowRA allows each output channel to adopt a different combination of mappings and thresholds for more precise fine-grained quantization. Data-Free One-Shot Post-Training Quantization Most quantization-aware training (QAT) techniques achieve higher task performance by incurring additional training overhead and learning task-specific quantization parameters [11, 51, 17, 39]. Similarly, many post-training quantization 5 -1.00 1.00 Thresholds Mappings Original Quantized -0.92 -0.31 0.46 0.78 -0.76 -0.12 0.63 Figure 3: Roles of mappings and thresholds in quantization. Circles represent thresholds whereas crosses represent mappings. Colored Triangles represent the process of converting a range of original unquantized real values - partitioned by thresholds - to the mapped values corresponding to each quantization level. methods require a calibration set for quantization scheme learning [24, 16].",
    "source": "2502.08141v1_LowRA_Accurate_and_Efficient_LoRA_Fine-Tuning_of_L.pdf",
    "length": 2379,
    "tokens": 495
  },
  {
    "text": "represents the embedding similarity metric. The best results are highlighted in bold, and the second-best results are underscored. 2002), ROUGE (Lin, 2004), and METEOR (Baner- jee and Lavie, 2005) which primarily assess lexi- cal similarity, as well as the embedding similar- ity and GPT score metrics introduced in Deep- RTL (Liu et al., 2025), which focus on semantic similarity. This combination of evaluation met- rics provides a comprehensive assessment of the model s ability to understand RTL code, capturing both surface-level and deeper, semantic-level un- derstanding. For further details on how to compute these metrics, please refer to the Appendix I. For natural language code search, we utilize the benchmark introduced in Section 3.2.1. To assess the model s ability to retrieve relevant code from a large codebase based on a user s query, we fol- low the bitext mining setting from MTEB (Muen- nighoff et al., 2022). In our evaluation process, the inputs consist of two sets: the first set contains functional descriptions, while the second set con- sists of Verilog code snippets. For each description in the first set, the best matching code snippet in the second set is identified using cosine similarity. We report F1 score, precision, and recall for each model, with F1 serving as the primary evaluation metric for natural language code search. For functionality equivalence checking, we uti- lize the benchmark introduced in Section 3.2.2. To evaluate the models ability to check functional equivalence, we follow the pair classification set- ting from MTEB (Muennighoff et al., 2022). In this evaluation, the inputs consist of several pairs of RTL codes. For each pair, the model assigns a binary label: 1 for \"functionally equivalent\" and 0 for \"functionally inequivalent\". The binary label is determined by calculating the cosine similarity of their embeddings and comparing the similarity score to a predefined threshold. For each model, we first identify the optimal accuracy threshold and compute the accuracy score. We then determine the best F1 threshold and report the F1, precision, and recall scores. Finally, we calculate the average pre- cision score based on the similarity scores of the code pairs and their corresponding ground-truth labels.",
    "source": "2506.15697v1_DeepRTL2_A_Versatile_Model_for_RTL-Related_Tasks.pdf",
    "length": 2279,
    "tokens": 496
  },
  {
    "text": "Section 2 begins by reviewing key LLM trends and presenting system-level challenges inherent in conventional datacenter infrastructure. Section 3 describes the vision of Huawei CloudMatrix and details the design of CloudMatrix384. We then introduce the serving system architecture and optimization techniques employed in CloudMatrix-Infer in Section 4. A detailed performance evaluation is presented in Section 5. Finally, Section 6 outlines our future research directions before Section 7 concludes the paper. 2 LLM Trends and Their Challenges for Datacenter Infrastructure In this section, we first discuss recent trends in large language model (LLM) design that are shaping the landscape of AI computing ( 2.1). We then present the corresponding system-level challenges these trends impose on conventional datacenter infrastructure ( 2.2). 2.1 LLM Trends The rapid evolution of LLMs has been marked by three prominent trends: the ever-increasing model parameter counts, the adoption of sparsity through Mixture-of-Experts (MoE) architectures, and the extension of context windows. These developments aim to enhance model performance while addressing computational efficiency and scalability. Ever-Larger Parameter Counts. Empirical scaling laws suggest that increasing the number of parameters in LLMs leads to improved model performance across various tasks [33]. Recent developments exemplify this trend: Meta s Llama 4 Behemoth boasts nearly 2 trillion parameters, while its counterpart, Llama 4 Maverick, comprises 400 billion parameters [39]. DeepSeek-V3, developed by DeepSeek-AI, contains 671 billion parameters [15]. Google s PaLM model includes 540 billion parameters [8], and xAI s Grok-1 features 314 billion parameters [54]. These models underscore the industry s ongoing pursuit of scaling LLMs to enhance capabilities in reasoning, multilingual understanding, and code generation. Sparsity through MoE. To manage the escalating costs of training and inference, modern LLMs increasingly adopt sparsely-activated MoE architectures, which decouple total model capacity from per-token computational requirements. Notable implementations include Mixtral 8 7B, which comprises 46.7 billion total parameters but activates only 12.9 billion per token by routing each token to 2 of 8 experts per layer, achieving performance comparable to GPT-3.5 while maintaining computational efficiency [32].",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2403,
    "tokens": 469
  },
  {
    "text": "To further delineate the models capabilities, we track the proportion of cases that pass out of 5 generated samples and compute the average as the success rate. For syntax correctness, this success rate measures the proportion of code samples that successfully compile and, for functional accuracy, the fraction that passes unit tests. 5 EXPERIMENTAL RESULTS 5.1 BASELINE MODELS For the baseline models, we select OpenAI s GPT-4-turbo (GPT-4) and GPT-3.5-turbo (GPT-3.5), as well as the o1-preview model, OpenAI s latest reasoning model designed to address complex problems across diverse domains, including programming. These models are chosen for their status as the most advanced general-purpose LLMs currently available, with demonstrated excellence in Verilog generation (Chang et al., 2024b; Thakur et al., 2024; Liu et al., 2024). For a comparison with models specifically fine-tuned on Verilog, please refer to Appendix J. 5.2 VERILOG UNDERSTANDING As shown in Table 2, DeepRTL consistently outperforms GPT-4 across all evaluation metrics. Tradi- tional metrics like BLEU and ROUGE offer inconsistent assessments due to their inability to capture semantic similarity accurately: while DeepRTL-16b excels in BLEU-4 and ROUGE-L, DeepRTL- 220m leads in ROUGE-1 and ROUGE-2. In contrast, embedding similarity and GPT score provide a more accurate assessment of the models capabilities in understanding Verilog code. Compared to CodeT5 , the performance of DeepRTL-direct, which is trained directly without curriculum learn- ing, highlights the effectiveness of our dataset. And the subsequent improvements when employ- ing the curriculum learning strategy underscore its benefits. Additionally, the poor performance of LLaMA2-70B-Chat underscores the unreliability of the MG-Verilog annotations Zhang et al. (2024). To further validate our model s performance, we have conducted human evaluations, which show that DeepRTL-220m, GPT-4, and o1-preview achieve accuracies of 78 , 72 , and 67 , respec- tively. These results align closely with the embedding similarity and GPT score metrics, further affirming the effectiveness of these evaluation methods.",
    "source": "2502.15832v1_DeepRTL_Bridging_Verilog_Understanding_and_Generat.pdf",
    "length": 2156,
    "tokens": 502
  },
  {
    "text": "Fol- lowing are several important considerations of implementing a placeable and routable INTAR design with high frequency. Distribute Independent Tasks Across Dies. For multi-die FPGAs [34], [35], cross-die wires are limited and often negatively affect the timing. Thus, when encountering a design with resource utilization higher than the single-die capacity, cross-die wiring latency can be the bottleneck of low clock frequency. A simple heuristic is to place independent tasks (e.g., multi-head attentions) into different dies instead of exe- cuting with all resources to avoid cross-die communications. Since there is no data movement between these tasks, we also do not add FIFOs between these tasks, which further reduces the cross-die wiring. Matching On-chip Memory Parallel Access Dimension. To serve multiple MAC units in each PEA, scratchpad memory is partitioned by access patterns. In INTAR, the PEA in the CCs may require varying patterns between tasks. For example, Figure 6 shows two dependent GEMMs: the first computes 2048 MACs simultaneously (4 8 64), and the second computes 1024 MACs (4 4 64). Since the PEA is reused across tasks, we aim to equalize the throughputs of the two matrix multiplies for efficiency, necessitating further partitioning of the second GEMM. This causes high-fanout nets at the write ports, increases cycle time, and reduces frequency. We address this by tiling operands so that output tiles have equal dimensions (e.g., 16 16), aligning parallel read write dimensions even after matrix transpose. Fig. 6. Example of a design with high-fanout nets on write ports of on-chip memory banks due to unaligned parallel memory access dimensions. To evaluate INTAR, we will show its broadness in DNN accelerations (Section V) and its advantages when deploying more complex DNNs on different FPGAs (Section VI). V. EVALUATION 1: MULTI-TASK KERNELS IN HDV DNNS A.",
    "source": "2502.08807v2_InTAR_Inter-Task_Auto-Reconfigurable_Accelerator_D.pdf",
    "length": 1901,
    "tokens": 437
  },
  {
    "text": "2021. Mixed Dimension Embeddings with Application to Memory-Efficient Recommendation Systems. In 2021 IEEE International Symposium on Informa- tion Theory (ISIT) (Melbourne, Australia). IEEE Press, 2786 2791. doi:10.1109 ISIT45174.2021.9517710 [17] Google. 2021. MLIR Sparsifier. [18] U. Gupta, C. Wu, X. Wang, M. Naumov, B. Reagen, D. Brooks, B. Cottel, K. Hazel- wood, M. Hempstead, B. Jia, H. S. Lee, A. Malevich, D. Mudigere, M. Smelyanskiy, L. Xiong, and X. Zhang. 2020. The Architectural Implications of Facebook s DNN- Based Personalized Recommendation. In 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE Computer Society, Los Alamitos, CA, USA, 488 501. doi:10.1109 HPCA47549.2020.00047 [19] Tae Jun Ham, Juan L. AragÃ³n, and Margaret Martonosi. 2015. DeSC: decoupled supply-compute communication management for heterogeneous architectures. In Proceedings of the 48th International Symposium on Microarchitecture (Waikiki, Hawaii) (MICRO-48). Association for Computing Machinery, New York, NY, USA, 191 203. doi:10.1145 2830772.2830800 [20] William L. Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems (Long Beach, California, USA) (NIPS 17). Curran Associates Inc., Red Hook, NY, USA, 1025 1035. [21] Olivia Hsu, Maxwell Strange, Ritvik Sharma, Jaeyeon Won, Kunle Olukotun, Joel S. Emer, Mark A. Horowitz, and Fredrik KjÃ¸lstad. 2023. The Sparse Abstract Machine.",
    "source": "2504.09870v1_Ember_A_Compiler_for_Efficient_Embedding_Operation.pdf",
    "length": 1550,
    "tokens": 459
  },
  {
    "text": ". . , vN attention is defined as si dot( q, ki) fi esi P j esj Attn( q, K, V) X i fi vi The attention score si represents the similarity between a given query and the ith key vector computed via a dot product. To identify the most relevant tokens, the softmax function is applied to all scores corresponding to the same query vector. Softmax first exponentiates each score and then divides it by the sum of all exponentials. The final output is obtained by multiplying the attention scores of a query by all value vectors, where the contribution of each value to the output is determined by its corresponding attention score normalized via softmax. Exponentiating scores can cause infinite values that would affect the final result. To prevent this, safe softmax subtracts the maximum score from all scores, thus avoiding overflow while keeping its core properties fi esi max P j esj max. B. Baseline FlashAttention Algorithm Algorithmically, attention calculation faces one major bot- tleneck. The softmax operation should be applied across the entire sequence length. This requirement reduces the extent to which attention can be parallelized or limits the application of attention to shorter sequence lengths [22]. To overcome this limitation, FlashAttention [16], driven by the online calculation of softmax function [23], reorganized computations involved in attention kernel and enabled arbitrary tiling that reduced also memory traffic. The forward pass of baseline FlashAttentionm, which is the focus of this work, is shown in Alg. 1 using vector-oriented operations. An equivalent block-based definition can be found in [16]. At each iteration, the dot product of the query vector and a key vector yields a similarity score, denoted as si. Subsequently, mi holds the current maximum similarity score, while â„“i incrementally accumulates the sum of the expo- nentials of each si minus the present maximum score. The multiplication by emi 1 mi in the calculation of â„“i adjusts the prior maximum value used whenever the current maximum mi differs from the previous maximum mi 1.",
    "source": "2505.14201v1_FLASH-D_FlashAttention_with_Hidden_Softmax_Divisio.pdf",
    "length": 2083,
    "tokens": 434
  },
  {
    "text": "Given that H RM I is full-column rank, then the expansion of X RI D to X HX RM D, where M I, increases the dimensionality of the nullspace of the input activations such that nullity( XT ) nullity(XT ) Proof. Recall that our expansion matrix H is M I where M I. For input activations X RI D, it follows that the expanded input activations X HX RM D. By the rank-nullity theorem, it follows that I rank(XT ) nullity(XT ), M rank( XT ) nullity( XT ) 2 By construction, H is full-column rank. It then follows that rank( X) rank( HX) rank(X), and therefore nullity( XT ) nullity(XT ) M I 0 Thus, there is a strictly positive increase in the dimensionality of the nullspace, concluding the proof. As our goal is to ï¬nd a quantization mapping Q such that XT W XT Q(W ) or, equivalently, that w Q(w) nullspace(XT ) (for every column w in W ), one could expect increasing the nullspace of the input feature space to help reduce the quantization error. In other words, though rotation does not alter the intrinsic dimensionality of the data, the higher redundancy in the projected space could be exploited by a carefully designed quantizer. The placement of these expanded Hadamard matrices is crucial to avoid an excessive increase in computational com- plexity without substantial beneï¬t in model quality. For this reason, we solely focus on the down projection layer (i.e., the last layer in the feed-forward block). Our reasoning is two-fold: (1) as shown in [4], this is one of the most sensitive layers to quantize, and (2) when following the placement of Hadamard matrices in [2], the down projection layer is naturally left with an online Hadamard matrix multiplication, which allows for ï¬ne-grained control of the expansion process. Expanding other layers, like the QKV layers, would require either inserting more online Hadamard matrices or expanding all the layers connected by the same rotation matrix.",
    "source": "2503.17513v1_Improving_Quantization_with_Post-Training_Model_Ex.pdf",
    "length": 1904,
    "tokens": 462
  },
  {
    "text": "Cocotb: Coroutine-based cosimulation testbench for vhdl and systemverilog, 2025. URL Version 1.9.2. Open-source under the BSD License. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL 02, page 311 318, USA, 2002. Association for Computational Linguistics. doi: 10.3115 1073083.1073135. URL 1073135. Stephen Williams. Icarus verilog, 2025. URL Open-source Verilog simulation and synthesis tool. Claire Wolf and the YosysHQ contributors. Yosys open synthesis suite, 2025. URL https: github.com YosysHQ yosys. Version 0.17. Open-source under the ISC License. 10 Wilson Snyder and Verilator Contributors. Verilator: Open-source systemverilog simulator, 2025. URL Version 5.031. Licensed under LGPL- 3.0-only or Artistic-2.0. Cadence Design Systems, Inc. Xcelium logic simulator, 2025. URL https: www.cadence.com en_US home tools system-design-and-verification simulation-and-testbench-verification xcelium-simulator.html. Commercial logic simulation tool supporting SystemVerilog, VHDL, SystemC, and UVM. Anthropic. Claude 3.7 sonnet, 2025. URL claude-3-7-sonnet. Hybrid reasoning language model with standard and extended thinking modes. OpenAI. Gpt-4.1, 2025a. URL Released April 14, 2025. Enhanced performance in coding, instruction following, and long-context comprehension with a 1 million token context window. Knowledge cutoff: June 2024. OpenAI. Openai o1, 2024. URL Released December 5, 2024. Reasoning-focused large language model designed for complex tasks in science, mathematics, and coding. OpenAI. Openai o4-mini, 2025b. URL introducing-o3-and-o4-mini . Released April 16, 2025. Enhanced reasoning capa- bilities with support for text and image inputs.",
    "source": "2506.14074v1_Comprehensive_Verilog_Design_Problems_A_Next-Gener.pdf",
    "length": 1831,
    "tokens": 492
  },
  {
    "text": "Onboard Optimization and Learning: A Survey Monirul Islam Pavel1, Siyi Hu1 , Mahardhika Pratama1, Ryszard Kowalczyk1 1 STEM, University of South Australia, Mawson Lakes, 5095, South Australia, Australia. Corresponding author(s). E-mail(s): Contributing authors: monirul Abstract Onboard learning is a transformative approach in edge AI, enabling real-time data processing, decision-making, and adaptive model training directly on resource- constrained devices without relying on centralized servers. This paradigm is crucial for applications demanding low latency, enhanced privacy, and energy effi- ciency. However, onboard learning faces challenges such as limited computational resources, high inference costs, and security vulnerabilities. This survey explores a comprehensive range of methodologies that address these challenges, focusing on techniques that optimize model efficiency, accelerate inference, and support collaborative learning across distributed devices. Approaches for reducing model complexity, improving inference speed, and ensuring privacy-preserving compu- tation are examined alongside emerging strategies that enhance scalability and adaptability in dynamic environments. By bridging advancements in hardware- software co-design, model compression, and decentralized learning, this survey provides insights into the current state of onboard learning to enable robust, efficient, and secure AI deployment at the edge. Keywords: onboard, on-device, learning, training, inference, optimization, Edge AI 1 Introduction The increasing adoption of edge artificial intelligence (AI) has driven a paradigm shift toward onboard optimization and learning, enabling devices to process data, make 1 arXiv:2505.08793v1 [cs.LG] 7 May 2025 decisions, and update models locally without relying on centralized servers [1]. Tradi- tional cloud-based AI architectures face significant limitations, including high latency, bandwidth constraints, and security risks associated with data transmission [2, 3]. These issues are particularly critical for applications such as autonomous vehicles, industrial IoT, and smart infrastructure, where real-time decision-making and respon- siveness are essential. Onboard learning 1 addresses these challenges by performing computations directly on resource-constrained devices, ensuring faster, more efficient, and privacy-preserving AI inference and adaptation [4 6]. Despite its advantages, onboard learning introduces several challenges.",
    "source": "2505.08793v1_Onboard_Optimization_and_Learning_A_Survey.pdf",
    "length": 2487,
    "tokens": 475
  },
  {
    "text": "arXiv:2405.04434. URL [20] DeepSeek-V2.5: A New Open-Source Model Combining General and Coding Capabilities, news0905, [Accessed 12-02-2025]. [21] DeepSeek-AI, A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Guo, D. Yang, D. Chen, D. Ji, E. Li, F. Lin, F. Dai, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Bao, H. Xu, H. Wang, H. Zhang, H. Ding, H. Xin, H. Gao, H. Li, H. Qu, J. L. Cai, J. Liang, J. Guo, J. Ni, J. Li, J. Wang, J. Chen, J. Chen, J. Yuan, J. Qiu, J. Li, J. Song, K. Dong, K. Hu, K. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Xu, L. Xia, L. Zhao, L. Wang, L. Zhang, M. Li, M. Wang, M. Zhang, M. Zhang, M. Tang, M. Li, N. Tian, P. Huang, P. Wang, P. Zhang, Q. Wang, Q. Zhu, Q. Chen, Q.",
    "source": "2504.10240v2_GNN-ACLP_Graph_Neural_Networks_based_Analog_Circui.pdf",
    "length": 757,
    "tokens": 354
  },
  {
    "text": "What is the optimal balance between chunked, continuous, and disaggregated batching for hybrid RAG-decode pipelines? Through various case studies, we demonstrate HERMES s various capabilities. To this end, we make the following contributions: A Heterogeneous Multi-Stage Simulation Framework: HERMES is the first simulator to model distributed LLM serv- ing pipelines with heterogeneous clients, multi-stage workflows, and multi-level memory hierarchies, bridging the gap between idealized models and real-world deployments. Batching strategy for Frontier Multi-Stage LLM Infer- ence Ã“: With support for LLM stages like KV cache retrieval, Retrieval Augmented Generation and Reasoning throughput compute time scaling, HERMES is able to provide insights into optimal batching strategy of different pipelines.Our key observations are: i) Disaggregated batching is able provide highest throughput energy in most cases. ii) Chunked batching provides high throughput and is able to sustain higher request injection rate but requires relaxed TTFT SLOs. iii) Continuous batching is optimal for TTFT is most cases, but is unable to sustain high injection rates. Table III summarizes the optimal batching strategy for different usecase, serving system size and varying optimization metrics. Actionable Design Insights : Through case studies, we quantify critical trade-offs, such as i) interplay between RAG embedding model and retrieval components placements, ii) memory hierarchy design for remote KV cache retrieval on end-to-end pipeline efficiency. By enabling precise exploration of these dimensions, HER- MES provides a critical tool for architects navigating the complex interplay between evolving LLM workflows and future hardware platforms. This paper is organized as follows. Section II introduces key concepts related to LLM inference pipeline stages and scheduling strategies. Section III presents the design of HERMES , our modular simulation framework. We describe its architecture, hardware abstraction layers, ML-assisted runtime prediction, and integra- tion with external simulators. In Section IV, we demonstrate how HERMES enables detailed exploration of modern software paradigms such as Retrieval-Augmented Generation (RAG) and scaling inference compute time with reasoning trees. Section V highlights the practical utility of HERMES through case studies. First, in Section V-A, we use HERMES to unravel 2 the complex design space of batching strategies across different LLM pipelines.",
    "source": "2504.09775v3_Understanding_and_Optimizing_Multi-Stage_AI_Infere.pdf",
    "length": 2499,
    "tokens": 500
  },
  {
    "text": "On the GLUE benchmark, which evaluates broader natural language understanding tasks, EmbBERT-Q demonstrates exceptional robustness to quantization, achieving an overall GLUE score of 62.81. This represents a minimal performance drop of 0.7 percentage points compared to the unquantized EmbBERT version. In Appendix A we show that the BERT NE EA model, instead, suffers significant performance drops of up to 15 percentage points on GLUE due to post-training 8-bit quantization. The quantization process gives substantial memory savings, reducing the total memory required to store and execute EmbBERT-Q from the around 2 MB of the unquantized EmbBERT to just 781 kB, considering both weights and activations (a 2.4 reduction in memory demand). While quantization often introduces trade-offs in accuracy, the robustness of the EmbBERT architecture highlights its suitability for deployment in constrained environments where such memory optimization techniques are critical. Through the comprehensive ablation study performed in this section, we have examined the contri- butions of key architectural components, including the Nano Embedder and Efficient Encoder. Main- taining the total memory usage below the 2 MB budget throughout the study, we have demonstrated that the inclusion of these architectural components and of 8-bit quantization in EmbBERT-Q leads to an Average Accuracy improvement of 4.24 percentage points on the TinyNLP benchmark and a 10.71 point increase on the GLUE benchmark score, with respect to the original BERT(2MB) model. With its carefully designed architecture and 8-bit quantization, EmbBERT-Q pushes the frontier of ultra-compact language models, delivering state-of-the-art performance in environments with stringent memory and computational constraints. 7 Conclusions In this work, we presented EmbBERT-Q, a novel language model specifically designed for tiny devices and extreme resource constraints. EmbBERT-Q achieves Accuracies and Scores comparable to the ones of models with up to 25 its memory footprint, and stands out as the highest-performing solution within the 2 MB memory budget explored in this paper. By leveraging an innovative architectural design specifically tailored for extremely memory- and computationally-constrained environments, EmbBERT-Q effectively balances parameter efficiency and competitive accuracy.",
    "source": "2502.10001v1_EmbBERT-Q_Breaking_Memory_Barriers_in_Embedded_NLP.pdf",
    "length": 2366,
    "tokens": 472
  },
  {
    "text": "We illustrate the details of depthmap hologram processing in Fig. 4a and Algo. 1 as two ma- jor steps (more details on the depthmap hologram algorithm can be found elsewhere [4, 18, 55, 63]). As shown in Fig. 4a, the depthmap input is first sliced into several planes (M depth planes in this case). With these depth planes, the first step, Forward Propagation (de- noted â¶in Fig. 4a), is to overlay the ith plane on the propagation result of the previous 1st to (i 1)th planes, and then propagate to the next (i 1)th plane. Note from Line 3 to Line 5 in Algo. 1 that, such forward propagation is massively parallel at the depth plane level (across planes) as well as at the pixel level (within one plane). Each depth plane processes the forward-propagation from the hologram plane independently, and each pixel on a particular depth plane goes through the exact processing sequence (HP2DP in Line 5; more details can be found in [4, 18]). This makes hard- ware parallelization and pipelining easier on a block tensor type of architecture such as GPUs. Note, however, that, this step also requires sequential barriers within each plane (Line 6 synchro- nizes the threads in a warp block for one depth plane) and across planes (Line 7 synchronizes the results from all the depth planes, before moving forward to the second step). Hence, as we will show later in this section, such barriers sliced into the massive parallel execution can cause load imbalance and instruction stalls, which slow down the entire execution and impact performance. The sec- ond step, Backward-Propagate (denoted â·in Fig. 4a), accumulates the results of each depth plane, backpropagates it to the hologram plane via the DP2HP procedure (in Line 11), and generates the final hologram for this depthmap input. Like the first step, this step also involves synchronizations between planes (in Line 12), which can again impact parallelization and slow down the entire execution. Intuitively, the execution performance is mainly determined by the number of depth planes (the outer for-loop in the algorithm) as well as the number of pixels in each depth plane (the inner for-loop in the algorithm).",
    "source": "HoloAR.pdf",
    "length": 2167,
    "tokens": 493
  },
  {
    "text": "We update the batch normalization (BN) weights for the quantization subnet candidate before evaluating PIM-based NN accuracy in MNSIM. A lower mu- tation probability is assigned to the quantization map and a higher mutation probability to the PIM configuration parameters so that the top quantization candidates are cross-checked with different PIM configurations. Once the best subnet candidate is identified, we fine-tune the model for one last time to achieve optimal PIM accuracy. 4 Results and Discussions 4.1 Experimental Setup The proposed CrossNAS framework is compatible with any PIM simulator that provides accuracy, energy, and delay metrics. For our experiments, we use MNSIM 2.0 [34] as the baseline simulator. The PIM accelerator is configured with a 64 64 tile arrangement, with each tile containing 2 2 PE arrays, and uses 1-bit memristor devices as the basic crossbar elements. We evaluate the performance of CrossNAS against previous exploration methods, including NACIM [14], UAE [30], NAS4RRAM [31], and Gibbon [28], on the CIFAR- 10 and CIFAR-100 [18] datasets. CIFAR-10 is selected because it serves as a common benchmark used in all previous related studies, allowing consistent comparisons. All experiments are conducted on a single NVIDIA GeForce RTX 2080 Ti GPU paired with an Intel Core i9-9820X CPU at 3.30GHz. 4.2 Training and Search Details During the neural network architecture search phase, we train the supernet using stochastic gradient descent (SGD) optimizer [23] for 1000 epochs, starting with a learning rate of 0.1 and a batch size of 128. A learning rate scheduler is applied, reducing the learning rate by a factor of 5 every 250 epochs. The evolutionary algorithm runs for 10 cycles, evaluating 50 new candidates per cycle, consisting of 25 mutation candidates and 25 crossover candidates. In the mixed-precision quantization and PIM configuration search phase, training begins with a floating-point (FP) model to achieve high accuracy. An SGD optimizer is used with an initial learning rate of 0.1, training for 200 epochs, with a step learning rate sched- uler reducing the rate by a factor of 5 at epochs 60, 120, and 160.",
    "source": "2505.22868v1_CrossNAS_A_Cross-Layer_Neural_Architecture_Search_.pdf",
    "length": 2168,
    "tokens": 485
  },
  {
    "text": "New York, NY, USA: Association for Computing Machinery, 2025, p. 1002 1007. [Online]. Available: 3697624 [10] S. Liu et al., Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source dataset and lightweight solution, in 2024 IEEE International Workshop on LLM-Aided Design. IEEE, 2024. [11] N. Mashnoor et al., Llm-ift: Llm-powered information flow tracking for secure hardware, 2025. [Online]. Available: 2504.07015 [12] M. Akyash et al., Self-hwdebug: Automation of llm self-instructing for hardware security verification, in 2024 IEEE Computer Society Annual Symposium on VLSI (ISVLSI), 2024, pp. 391 396. [13] Y. Fu et al., Gpt4aigchip: Towards next-generation ai accelerator design automation via large language models, in 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD). IEEE, 2023, pp. 1 9. [14] M. Akyash et al., Evolutionary large language models for hardware security: A comparative survey, in Great Lakes Symposium on VLSI (GLSVLSI), 2024, pp. 1 6. [15] S. Thakur et al., Verigen: A large language model for verilog code generation, 2023. [Online]. Available: [16] Z. Pei et al., Betterv: Controlled verilog generation with discriminative guidance, 2024. [Online]. Available: [17] M. Gao et al., Autovcoder: A systematic framework for automated verilog code generation using llms, arXiv preprint arXiv:2407.18333, 2024. [18] M. Liu et al., Craftrtl: High-quality synthetic data generation for verilog code models with correct-by-construction non-textual representations and targeted code repair, 2024. [Online]. Available: [19] B. Perozzi et al., Let your graph do the talking: Encoding structured data for llms, 2024. [Online].",
    "source": "2505.13479v1_RTL_Graph-enhanced_LLM_for_RTL_Code_Generation.pdf",
    "length": 1683,
    "tokens": 475
  },
  {
    "text": "2 Early Quantization: In the original MoE computation flow, as shown in Figure 10a, BF16 token data is transmitted during token dispatch, resulting in high communication volume. To mitigate this, we introduce early quantization by performing INT8 quantization before sending token data within FusedDispatch. Specifically, instead of sending BF16 data, we transmit INT8- quantized data together with its scaling factor. This reduces the communication payload during the data exchange phase. Given a token data with 7,168 dimensions, the INT8 representation requires 7 KB per token. The scaling factor occupies 4 bytes (INT32), but for alignment, we allocate 512 B. As a result, the transfer message size for each token is 7.5 KB. This optimization substantially reduces communication overhead in the most bandwidth-intensive stage. 3 Static Execution via Shared-Memory Pre-allocation: To avoid dynamic memory alloca- tion and its associated CPU-NPU synchronization overhead, we statically pre-allocate shared-memory buffers in each NPU rank for data arriving from every other rank in the MoE layer. The required buffer size is: 22 UB Switch UB Switch UB Switch NPU NPU 910 Die AIV Memory UBuffer SDMA 910 Die Memory SDMA AIV UBuffer Fig. 11. SDMA-based vs. AIV-direct communication across NPUs. The red and blue lines indicate data transmission paths using SDMA and AIV-direct, respectively. buffer_size rank_num max_tokens msg_size, (1) where max_tokens local_batch min(topK, experts_per_die) (2) max_tokens is the worst-case number of tokens an NPU may send to a single peer, and msg_size is the per-token message length (7.5 KB after INT8 quantization for token dispatch and 14 KB for token combine). With this space pre-allocated, both FusedDispatch and FusedCombine directly write data into the target NPU memory buffer via AIV-direct communication, avoiding an intermediate local copy and the subsequent remote read, thus reducing memory traffic and synchronization latency.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 1979,
    "tokens": 450
  },
  {
    "text": "[9] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [10] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Sum- marizing source code using a neural attention model. In 54th Annual Meeting of the Association for Computational Linguistics 2016. Association for Computational Linguistics, 2073 2083. [11] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2018. Map- ping language to code in programmatic context. arXiv preprint arXiv:1808.09588 (2018). [12] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems 33 (2020), 9459 9474. [13] Jia Li, Ge Li, Yongmin Li, and Zhi Jin. 2023. Structured chain-of-thought prompt- ing for code generation. arXiv preprint arXiv:2305.06599 (2023). [14] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. 2023. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161 (2023). [15] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, RÃ©mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022.",
    "source": "2502.13921v2_Exploring_Code_Language_Models_for_Automated_HLS-b.pdf",
    "length": 1525,
    "tokens": 492
  },
  {
    "text": "Even under these modest assumptions, the projected demand rivals the daily electricity consumption of Seattle and its surrounding area (24.8 GWh) [43]. As AI agents become 12 Accuracy ( ) Latency (seconds) Energy (Wh query) Power 71.4 Million Queries day (Watt) Power 13.7 Billion Queries day (Watt) 8B ShareGPT 4.23 (1 ) 0.32 (1 ) 1.0 M 182.7 M Reflexion 38 649.34 (153.7 ) 41.53 (130.9 ) 123.6 M 23.7 G LATS 80 380.90 (90.1 ) 22.76 (71.7 ) 67.7 M 13.0 G 70B ShareGPT 6.40 (1 ) 2.55 (1 ) 7.6 M 1.5 G Reflexion 67 720.00 (112.6 ) 348.41 (136.5 ) 1.0 G 198.9 G LATS 82 305.67 (47.8 ) 158.48 (62.1 ) 471.5 M 90.5 G TABLE III: Energy and power demands of handling an AI agent service request on HotpotQA. We report accuracy, latency, GPU energy consumption, and datacenter-wide power demand under current and future traffic scenarios (71.4 Million Queries day and 13.7 Billion Queries day) for two agentic workflows (Reflexion and LATS) using Llama-3.1-Instruct 8B and 70B models. ShareGPT serves as the baseline for conventional single-turn LLM inference. Numbers in parentheses denote the multiplicative increase relative to ShareGPT. Reflexion and LATS design points were selected based on the highest-accuracy configurations in Figure 22. The datacenter-wide power is computed by P (Wh query) (Queries day) (24 hours)). increasingly embedded in everyday applications, their query volume could approach, or exceed, that of traditional search engines. For instance, Google Search processes over 13.7 billion queries per day [12], roughly 192 the 71.4 million agentic queries assumed above. If this growth in user base and usage persists, AI-infrastructure demand could rise dra- matically, potentially exceeding sustainable limits and under- scoring the significant challenges posed by test-time scaling. Datacenter-wide power demands.",
    "source": "2506.04301v1_The_Cost_of_Dynamic_Reasoning_Demystifying_AI_Agen.pdf",
    "length": 1834,
    "tokens": 497
  },
  {
    "text": "By constraining the quantization group size to 1x16 (instead of per-tensor or per-channel), our method effectively contains outlier effects within each block while improving FP4 quantization accuracy. To overcome (C2), we propose a two-level quantization method for P to fully utilize the presentative range of the FP8 scaling factor, enhancing the quantization accuracy of P. Specifically, this approach first normalizes each token s range to [0, 448 6] through per-token quantization, then applies FP4 microscaling quantization for enhanced precision. To address (C3), we identify the most accuracy-sensitive matrix multiplication among the five in backpropagation and maintain its accuracy in FP16. Result. Our FP4 attention, named SageAttention3, could achieve 1038 TOPS on RTX5090, which is a 5 speedup than FlashAttention. Furthermore, we demonstrate that 8-bit trainable attention, named SageBwd, could achieve lossless performance when fine-tuning base models for instruction- following tasks, but is not suitable for pretraining tasks. Contribution. Our work makes the following key contributions: (1) We design the first FP4 attention to accelerate inference, achieving 1000 TOPS on RTX5090. (2) We propose the first trainable low-bit attention, enabling accelerated training with lossless fine-tuning performance, while revealing key insights for low-bit attention in training.",
    "source": "2505.11594v1_SageAttention3_Microscaling_FP4_Attention_for_Infe.pdf",
    "length": 1388,
    "tokens": 300
  },
  {
    "text": "Various ML models can be trained to provide early predictions or These authors contributed equally to this work. Corresponding author arXiv:2504.03711v1 [cs.AR] 28 Mar 2025 2 Wenji Fang, Jing Wang, Yao Lu, Shang Liu, Yuchao Wu, Yuzhe Ma, and Zhiyao Xie (a) Type I: Task-Specific AI for EDA Paradigm Feature Extraction ML Model Design Single-Stage Circuit Data Single EDA Task Label Collection ML Model Training (c) Type II: General Decoder-Based Circuit Foundation Model Paradigm LLM Decoder (General) Phase 1: Pre-Train Auto- Regressive Phase 2: Application Task-Specific Circuit Data Generate Prompt RAG SFT . Textual Unlabeled Data (b) Type II: General Encoder-Based Circuit Foundation Model Paradigm Phase 2: Application General Circuit Embedding Embeddings of similar circuits will be closer Circuit Encoder (General) Phase 1: Pre-Train Self-Supervise (no label) Fine-Tune Lightweight ML Model (Task-Specific) Graph Text Unlabeled Circuit Data Pre-trained LLM Predictive EDA Tasks Function Reasoning Verification Quality Timing Area RTL Code Verification Flow control Generative EDA Tasks Main Research Focus in Circuit Foundation Models Fig. 1. Different paradigms of AI for EDA techniques. (a) Type I: Supervised Predictive AI Techniques for EDA. This type of work has been extensively studied. (b) (c) Type II: Foundation AI Techniques for EDA (i.e., Circuit Foundation Models). This type of work includes two paradigms, named encoder-based and decoder-based circuit models. Both paradigms develop the foundation AI model through two stages: self-supervised pre-training and fine-tuning. Our survey will focus on the emerging type II methods. optimizations for circuits, bypassing time-consuming downstream design and simulation steps. Learning from prior design solutions, ML models can perform circuit quality evaluations at early design stages and thus guide early design optimizations.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 1897,
    "tokens": 439
  },
  {
    "text": "[12] H. Chen, J. Zhang, Y. Du, S. Xiang, Z. Yue, N. Zhang, Y. Cai, and Z. Zhang, Understanding the potential of fpga-based spatial accel- eration for large language model inference, ACM Transactions on Reconfigurable Technology and Systems, 2024. [13] J. Zhuang, Z. Yang, S. Ji, H. Huang, A. K. Jones, J. Hu, Y. Shi, and P. Zhou, Ssr: Spatial sequential hybrid architecture for latency throughput tradeoff in transformer acceleration, in Proceedings of the 2024 ACM SIGDA International Symposium on Field Programmable Gate Arrays, 2024, pp. 55 66. [14] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., Language models are unsupervised multitask learners, OpenAI blog, vol. 1, no. 8, p. 9, 2019. [15] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770 778. [16] S. Xie, R. Girshick, P. Doll ar, Z. Tu, and K. He, Aggregated residual transformations for deep neural networks, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 1492 1500. [17] J. Cai, Y. Wei, Z. Wu, S. Peng, and K. Ma, Inter-layer scheduling space definition and exploration for tiled accelerators, in Proceedings of the 50th Annual International Symposium on Computer Architecture, 2023, pp. 1 17. [18] J. Cong, H. Huang, C. Ma, B. Xiao, and P. Zhou, A fully pipelined and dynamically composable architecture of cgra, in 2014 IEEE 22nd Annual International Symposium on Field-Programmable Custom Com- puting Machines. IEEE, 2014, pp. 9 16. [19] E. Baek, D. Kwon, and J. Kim, A multi-neural network acceleration architecture, in 2020 ACM IEEE 47th Annual International Symposium on Computer Architecture (ISCA).",
    "source": "2502.08807v2_InTAR_Inter-Task_Auto-Reconfigurable_Accelerator_D.pdf",
    "length": 1766,
    "tokens": 498
  },
  {
    "text": "[Online]. Available: LLMCompiler [18] S. Kim, S. Moon, R. Tabrizi, N. Lee, M. W. Mahoney, K. Keutzer, and A. Gholami, An LLM Compiler for Parallel Function Calling, in Proceedings of the International Conference on Machine Learning (ICML), 2024. [19] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica, Efficient Memory Management for Large Language Model Serving with PagedAttention, in Proceedings of the ACM Symposium on Operating System Principles (SOSP), 2023. [20] J. Landry, Landry Announces Meta Selects North Louisiana as Site of 10 Billion Artificial Intelligence Optimized Data Center, 2024. [Online]. Available: [21] Y. Leviathan, M. Kalman, and Y. Matias, Fast inference from transform- ers via speculative decoding, in Proceedings of the 40th International Conference on Machine Learning, 2023. [22] G. Li, H. A. A. K. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem, CAMEL: Communicative agents for mind exploration of large lan- guage model society, in Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2023. [23] M. Luo, X. Shi, C. Cai, T. Zhang, J. Wong, Y. Wang, C. Wang, Y. Huang, Z. Chen, J. E. Gonzalez, and I. Stoica, Autellix: An efficient serving engine for llm agents as general programs, 2025. [24] Meta, Llama-3.1-70B-Instruct, 2025. [Online]. Available: https: huggingface.co meta-llama Llama-3.1-70B-Instruct [25] Meta, Llama-3.1-8B-Instruct, 2025. [Online]. Available: https: huggingface.co meta-llama Llama-3.1-8B-Instruct [26] MLPerf, MLPerf Inference: Datacenter. [Online].",
    "source": "2506.04301v1_The_Cost_of_Dynamic_Reasoning_Demystifying_AI_Agen.pdf",
    "length": 1596,
    "tokens": 489
  },
  {
    "text": "Speciï¬cally, we analyze four scenarios, 241 2020 ACM IEEE 47th Annual International Symposium on Computer Architecture (ISCA) 978-1-7281-4661-4 20 31.00 2020 IEEE DOI 10.1109 ISCA45697.2020.00030 namely, InterFrame-IntraEye (EA), IntraFrame-InterEye (AE), IntraFrame-IntraEye (AA), and InterFrame-InterEye (EE), that are critical in capturing the head movement and eye correlation for projection computation. Out of these four scenarios, we observe that EA computation for head orientation can be exploited for temporal reuse memoization since there is little difference between two previous head orientations, and AE computation for exploiting the correlation between both the eyes by spatial reuse correlating the coordinate relationship between both eyes. Based on this observation, we develop computation optimization mechanisms for facilitating temporal reuse memoization and spatial reuse that can be integrated with a VR projection computation pipeline to signiï¬cantly reduce energy consumption of the device. The proposed ar- chitecture is named D ej a View, a play on the word D ej a vu, as it uses previous or already seen views. To the best of our knowledge, this is the ï¬rst work that leverages head orientation and correlation between eyes to do efï¬cient memoization and in turn result in compute reduction in the VR video streaming domain. The major contributions of the paper can be summarized as follows: From an open-source 360 VR video dataset [3], we identify both temporal reuse and spatial locality that exists in user behavior. We formally analyze the potential input invari- ability in the projection computation during 360 video streaming, which manifests in the head movement locality (temporal reuse) and the stationarity relationship between two eyes (spatial reuse). Such invariances are leveraged as reuse opportunities to reduce the compute-heavy projection computation. We design two complementary schemes to capture both temporal and spatial reuse opportunities. We propose a memoization scheme, called EA, to capture recent head orientation data for temporal reuse, and for the spatial reuse, we design the AE scheme, which leverages the stationary relationship between two eyes to efï¬ciently reduce the amount of projection computation. We implement both our schemes as a software enhancement to the existing compute pipeline in NVIDIA GPUs.",
    "source": "DejaView.pdf",
    "length": 2375,
    "tokens": 490
  },
  {
    "text": "Model Params (M) Top1 Accuracy Latency (ms) Pf Squeezenet 4,253,864 70.10 43.45 10 MobileNEt V2 4,253,864 68.20 41.5 10 Inception V4 23,851,784 76.74 74 6 Resnet50 95,154,159 79.20 98.22 5 ResNet18 44,964,665 76.26 35 6 DenseNet-201 20,242,984 79.80 152.21 2 DenseNet-121 8,062,504 78.72 102.35 3 Xxception 22,910,480 77.80 119.2 4 NasNet 5,326,716 77.90 120 3 InceptionResnetV2 2,510,000 80.30 251.96 1 Table 8: Pretrained models for CIFAR-100 using Imagenet. F Spot Instance Price Variation We proï¬le the spot price of 4 types of C5 EC2 VMs over a 2-week period in August 2020. The price variation is shown in Fig18. 0 100 200 300 Time 0.1 0.2 0.3 Price ( ) xlarge 2xlarge 4xlarge 8xlarge Figure 18: Spot instance price variation (time is in hours). USENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation 1057",
    "source": "cocktail.pdf",
    "length": 844,
    "tokens": 297
  },
  {
    "text": "Random also acts as a lowest common denominator to circumvent the time and complexity of replicating ML methods proposed by other authors. Other sections of this review highlight the lack of openly available information, data sets and designs. In the absence of being able to replicate work, random-based methods are a means to compare performance between different applications of ML. However, caution is needed because performance vs random does not measure how well a technique generalises. The comparative studies demonstrate that different ML methods perform differently for the same application. Therefore, a method that performs well against random in one application may not perform well in another. This makes the insight gained from research that compares ML methods valuable. 12 CHALLENGES AND OPPORTUNITIES The surveyed material presents a rich and varied set of machine learning techniques and applications for verifying electronic designs. The number of publications on this topic has increased and showcases successes for EDA practitioners to use or build upon. However, trends were seen that hinder progress: A lack of standard benchmarks, withholding code and data, and obfuscating work undertaken with private companies make it difficult to replicate results and measure progress. Techniques are evaluated on simple designs without comparisons to other well-established and 30 40 Figure 11. A count of the type of metrics used to assess machine learning for microelectronic design verification. Metrics of the same type are not double-counted within the same piece of material. If a single piece of research material employs more than one metric of the same type, it only increases the count of that metric type by one. Measures relating to task performance were used most frequently. Figure 12. A count of the baselines seen in the literature for assessing the performance of a machine learning application for microprocessor verification. effective methods other than random. Research rarely explores whether a technique will generalise beyond the application tested or scale to real-world systems. It is rare to see work justify the choice of machine learning technique and how it is applied. Research is confined to a tool or ML type, and it is rare to see an exploration of alternative methods. If comparisons between techniques are made, these tend to be within the same family of techniques. The criteria for assessing the success of a technique are confined to a single metric and do not capture the criteria for real-world adoption. Research treats verification as a one-shot problem, whereas in industry it is a rolling process throughout development. These trends create problems of generalisation, replication and assessment.",
    "source": "2503.11687v1_Review_of_Machine_Learning_for_Micro-Electronic_De.pdf",
    "length": 2755,
    "tokens": 495
  },
  {
    "text": "[45] proposed dynamic and time-sensitive test construction. [46] proposed private benchmarking, a solution where test datasets are private and models are evaluated without revealing the test data to the model. Similarly, [47] proposed a comprehensive and contamination-free evaluation of LLMs for coding, which collects new problems over time from contests and other sources. [48] studied data contamination of popular code generation benchmarks and quantified their overlap with pre- training corpus through surface- and semantic-level matching. III. EVALUATION A. Experiment Setup Our investigation has two phases: (1) evaluating contamina- tion across multiple foundation models using benchmarks to establish baseline Verilog code contamination levels and (2) deliberately contaminating a clean model to assess TED s [43] mitigation efficacy. All experiments used an Nvidia A100 GPU (80GB) with CUDA 12.2. This section details the methodology. Models: We use widely recognized baseline models with different sizes and sources. We pick CodeGen2.5, Minitron 4b, Mistral 7b, phi-4 mini, LLaMA-{1,2,3.1}, GPT-{2,3.5,4o}, Deepseek-Coder, and CodeQwen 1.5 for evaluation [49], [50]. This selection encompasses models ranging from earlier itera- tions to state-of-the-art LLM families, balancing commercial and open-source offerings. Fine-tuning Setup: To establish a fair setting for contam- ination assessment and to enable controllable mitigation, we simulate data contamination for LLaMA-3.1-8B. We specifi- cally chose this model due to its moderate contamination rate (Section III-C). We fine-tuned two separate instances on distinct training datasets: one using the 55M RTLCoder [21] dataset within its native training framework and another using the 78M filtered Verigen [14], [51] via the Alpaca [52] library. After experimenting with various hyperparameter configurations, we determined that epoch 3 and learning rate (lr) 1e 5 with the Adam optimizer produced the highest contamination rate, creating optimal conditions for observing contamination effects. For inference, we used temperature (temp) 0.8, top-p 0.95, and maximum context length 2048.",
    "source": "2503.13572v3_VeriContaminated_Assessing_LLM-Driven_Verilog_Codi.pdf",
    "length": 2155,
    "tokens": 488
  },
  {
    "text": "According to the circuit synthesis report listed in Table I, it s observed that NumPPs 4 3 2 1 0 MBE [12] 81 (31.6 ) 108 (42.2 ) 54 (21.1 ) 12 (4.7 ) 1 (0.4 ) EN-T [45] 72 (28.1 ) 108 (42.2 ) 60 (23.4 ) 15 (5.9 ) 1 (0.4 ) NumPPs {8,7} {6,5} 4 {3,2} {1,0} bit-serial 9 (3.5 ) 84 (32.8 ) 70 (27.3 ) 84 (32.8 ) 9 (3.5 ) TABLE II: The number of partial products (NumPPs) under different encoding within the range of INT8 ( 128 127). with the bit-width of the accumulator increases in MAC, the predominant factors constraining the performance progres- sively transition to the area and delay of the accumulator. For instance, in a 32-bit accumulation, the logical area occupied by the full adders and accumulator accounts for 61.4 , and the logic delay is as high as 74.6 , severely restricting the frequency of the MACs. B. Fine-grained Description of the TPE Microarchitecture The RTL-based description of the TPE microarchitecture is overly detailed for designers to understand the acceleration mechanism at the algorithm level, while the hardware block diagram representation is too abstract for the underlying imple- mentation level. Using a notation between the hardware block diagram and RTL can help designers understand the accelera- tion mechanism at both levels. However, existing design space representations [24], [32], [35], [47], [50] focus on architecture with MAC as the basic unit and don t explicitly represent the reduction logic (adder-trees) brought by spatial unrolling and the reduction logic in PEs constitutes a significant portion of the PE area and serves as a critical factor that affects timing. These limitations make it difficult to capture acceleration opportunities at the PE microarchitecture level. Therefore, there is a need to develop a more comprehensive notation for TPE to capture data flow and operational specifics in detail, enabling further exploration of hardware architecture optimization methods under specific application conditions.",
    "source": "2503.06342v1_Exploring_the_Performance_Improvement_of_Tensor_Pr.pdf",
    "length": 1977,
    "tokens": 480
  },
  {
    "text": "Widely used in sequence modeling (Transformer [5], ViT [36], GAT [37]) 256 token sequences. The hidden dimension is 1024. Min data size: 0.5 MB, Max data size: 1.0 MB, On-chip memory constraint: 0.625 MB Increase (Q, K) De- crease (Q, K A) Increase (V ) Decrease (A, V V ) FFN Layer (FFN) Three layers of linear projections for feature extractions. Input is a single vector. Elementary block in many CV and LLM applications (ResNet [15], YoloV3 [38], Transformer) Input size final output size 256. First layer output size 1024, second layer output size 2048. Min data size: 0.5 KB, Max data size: 2 KB, On-chip mem- ory constraint: 1 KB Increase Increase Decrease Multi-layer CNN (M- CNN) Four layers of convolutions with upsampling and pooling layers Used in many CV applications (ResNet, VGG [39]) Input size 224, kernel size 3, 2 2 upsampling and pooling size. Min data size: 98 KB, Max data size: 1.57 MB, On-chip mem- ory constraint: 125 KB. Increase Increase Decrease Decrease Variational Autoencoder (VAE) Contains an encoder in convolutions for the latent space distribution and a decoder in transpose convolutions. Used for image compression and the generator model in GAN (VAE-GAN [40]) Input size 28, kernel sizes are 4 and 8, 2 channels and 2 filters Min data size: 1.3 KB, Max data size: 3.1 KB, On-chip memory constraint: 1.5 KB Decrease (encoder 1) Decrease (encoder 2) Increase (decoder 1) In- crease (decoder 2) Gating Network (GN) Two parallel linear projections with element-wise product and a sequential linear projection.",
    "source": "2502.08807v2_InTAR_Inter-Task_Auto-Reconfigurable_Accelerator_D.pdf",
    "length": 1542,
    "tokens": 419
  },
  {
    "text": "XXX-X-XXXX-XXXX-X XX XX.00 20XX IEEE Optimization of bi-directional gated loop cell based on multi-head attention mechanism for SSD health state classification model Zhizhao Wen Department of Computer Science Rice University Houston, United States Ruoxin Zhang Department of Computer Science Rice University Houston, United States Chao Wang Department of Computer Science Rice University Houston, United States Abstract Aiming at the critical role of SSD health state prediction in data reliability assurance, this study proposes a bidirectional gated recurrent unit (BiGRU-MHA) hybrid model incorporating a multi-head attention mechanism, which effectively enhances the accuracy and stability of storage device health classification prediction by innovatively integrating temporal feature extraction and key information focusing capabilities. The model utilizes the bidirectional timing modeling advantage of BiGRU network to capture the forward and backward dependencies of SSD degradation features, and at the same time introduces the multi-head attention mechanism to dynamically assign feature weights to enhance the identification of sensitive indicators of health status. The experimental results show that the proposed model achieves 92.70 and 92.44 classification accuracy on the training set and test set, respectively, with a difference of only 0.26 , demonstrating excellent model generalization performance. Further analyzed by the subject work characteristic curve (ROC), the area under the curve (AUC) on the test set reaches 0.94, which confirms that the model has a highly robust binary classification discriminative ability. This study not only provides a new technical path for SSD health prediction but also breaks through the bottleneck of the traditional model in terms of the performance difference between the training-testing set with a generalization error of only 0.26 , which is of great practical value for the preventive maintenance of industrial-grade storage systems. The result can significantly reduce the probability of data loss by warning potential failure risks in advance, while optimizing the maintenance cost, providing verifiable intelligent decision support for building a highly reliable computer storage system, which is widely applicable to the health management of cloud computing data centers and edge storage devices. Keywords-Solid-state disk health state prediction, multi-head attention mechanism, bi-directional gated loop cell. I. INTRODUCTION With the acceleration of the digitization process, the demand for data storage is growing exponentially, and the reliability of storage media has become a core issue in ensuring data security [1].",
    "source": "2506.14830v1_Optimization_of_bi-directional_gated_loop_cell_bas.pdf",
    "length": 2694,
    "tokens": 484
  },
  {
    "text": "10. Images generated by FP16 model and MixDiT. TABLE III IMAGE QUALITY EVALUATION Model (p1, p2) Precision (W A) Method FID IS CLIP score DiT-XL-256 (0, 0) FP (16 16) FP16 17.32 231.20 - INT (6 6) Q-DiT 29.27 169.49 - MX (6 6) MX6 67.27 59.99 - MX (6 6) MixDiT 15.39 232.85 - DiT-XL-512 (1, 0) FP (16 16) FP16 20.55 216.83 - INT (6 6) Q-DiT 18.04 209.77 - MX (6 6) MX6 108.70 22.88 - MX (6 6) MixDiT 20.15 217.77 - SD3-1024 (5, 20) FP (16 16) FP16 74.07 - 29.56 MX (6 6) MX6 199.78 - 22.74 MX (6 6) MixDiT 72.48 - 29.34 Pixart-Î£-1024 (1, 20) FP (16 16) FP16 69.96 - 31.34 INT (6 6) ViDiT-Q 84.74 - 31.93 MX (6 6) MX6 71.66 - 31.31 MX (6 6) MixDiT 69.29 - 31.50 Implementation. We implement the MX quantized model with PyTorch 2.0, Huggingface Diffusers library, and triton language [8]. We develop our accelerator on top of the open- source DaCapo accelerator simulator [2]. Table II shows the hardware configuration for the simulator. We set the group size to 16 and the subgroup size to 2 for MX, and employ a batch size of 1 throughout the experiments. V. EVALUATION Image quality. Figure 10 visualizes the output image quality of the FP16 model and MixDiT, demonstrating that MixDiT s output image quality matches that of FP16. Table III reports quantitative results for image quality. As Q-DiT and ViDiT- Q are designed to use high activation bitwidths (e.g., INT8 8 or INT4 8), they suffer from poor image quality when using INT6 6 precision.",
    "source": "2504.08398v1_MixDiT_Accelerating_Image_Diffusion_Transformer_In.pdf",
    "length": 1448,
    "tokens": 493
  },
  {
    "text": "Using data from monkeys K and L, we trained CAE models on a combined dataset with 80 of the recordings from each monkey and evaluated their performance on individual test sets. Table IV compares models trained separately on monkeys K and L with those trained on the x TABLE III: Comparison between stochastic pruning and standard magnitude-based pruning for 8-bit quantization. Model Sparsity ( ) Monkey K Monkey L Stochastic Pruning Magnitude-based Pruning Stochastic Pruning Magnitude-based Pruning SNDR (dB) R2 Score Size (kB) SNDR (dB) R2 Score Size (kB) SNDR (dB) R2 Score Size (kB) SNDR (dB) R2 Score Size (kB) MobileNetV1- CAE(1.00x) 25 25.23 2.53 0.90 0.08 2456.384 25.20 2.50 0.89 0.08 3633.728 28.22 2.37 0.95 0.18 2456.384 28.19 2.37 0.94 0.17 3633.728 50 25.22 2.54 0.89 0.08 1671.488 25.20 2.51 0.89 0.08 2456.384 28.24 2.38 0.95 0.17 1671.488 28.18 2.40 0.94 0.18 2456.384 75 25.05 2.48 0.89 0.09 886.592 25.14 2.50 0.89 0.08 1279.04 28.27 2.35 0.95 0.17 886.592 28.18 2.41 0.94 0.18 1279.04 MobileNetV1- CAE(0.25x) 25 24.33 2.21 0.87 0.10 173.36 23.94 2.02 0.86 0.14 246.32 28.63 2.34 0.95 0.15 173.36 28.48 2.28 0.95 0.17 246.32 50 24.18 2.21 0.87 0.13 124.72 23.91 2.03 0.86 0.14 173.36 28.48 2.29 0.95 0.14 124.72 28.55 2.29 0.95 0.16 173.36 75 24.37 2.19 0.87 0.10 76.08 24.03 2.05 0.86 0.14 100.4 28.49 2.28 0.95 0.15 76.08 28.57 2.31 0.95 0.15 100.4 DS-CAE1 25 22.78 2.38 0.82 0.16 10.8 22.75 2.35 0.81 0.14 14.256 27.55 2.42 0.94 0.15 10.8 27.78 2.51 0.94 0.16 14.256 50 22.70 2.21 0.81 0.19 8.496 22.69 2.30 0.81 0.14 10.8 27.55 2.52 0.94 0.15 8.496 27.78 2.53 0.94 0.16 10.8 75 22.61 2.21 0.81 0.13 6.192 22.38 2.18 0.80 0.14 7.344 27.43 2.41 0.94 0.13 6.192 27.61 2.50 0.94 0.15 7.344 The parameter size of the encoder part.",
    "source": "2504.06996v1_Neural_Signal_Compression_using_RAMAN_tinyML_Accel.pdf",
    "length": 1749,
    "tokens": 755
  },
  {
    "text": "[22] introduce FPGA-targeted transfer learning and a dedicated multithreaded parallel HLS design space explorer. Zou et al. [23] propose FlexWalker, a multi-objective HLS design space exploration framework that employs upper confidence bound (UCB)-coordinated heterogeneous regression models for design quality prediction across parameter configurations, augmented with probabilistic sampling and elastic Pareto frontiers to mitigate regression modeling inaccuracies. In summary, SOTA prediction methodologies in HLS depend primarily on GNNs, leveraging their inference capabilities to achieve high-precision predictions on unseen datasets. Dominant GNN architectures that include graph convolutional networks (GCNs) [24], graph attention networks (GATs) [25], and graph isomorphism networks (GINs) [2] are rooted in message-passing mechanisms. However, inherent limitations of MPNNs, such as long-range dependency failures and over-smoothing phenomena, critically degrade prediction accuracy, partially explaining the persistent performance gaps in HLS QoR estimation. The recent GNN architecture CoGNNs [1] demonstrates adaptive capability with dynamic message-passing mechanism, which is particularly beneficial for HLS prediction tasks. In this work, we adopt CoGNNs for predicting post-HLS metrics. Meta-heuristics have been the predominant approaches and have achieved significant progress in solving HLS DSE over the past decades, and they continue to undergo diverse and flourishing development. The main problems of the traditional meta-heuristic algorithms lie in the difficulty of the designed random operator to escape the local optimum and the numerous difficulties in parameter tuning. Inspired by the works proposed in [26, 27], we propose a LLM-driven metaheuristic framework for HLS DSE, which takes advantage of LLMs powerful information comprehension capabilities to interpret the functional roles of various pragmas and their impacts on the final routing outcomes. 3 Preliminaries 3.1 Problem Formulation HLS DSE can be formulated as a MOOP, where the primary objective is to identify a set of Pareto-optimal configurations that simultaneously minimize latency and resource utilization. Given a behavioral description and optional synthesis directives [pragma1, pragma2, ..., pragman], the design space DS can be formulated as the Cartesian product of the combination of pragmas.",
    "source": "2504.19649v2_Intelligent4DSE_Optimizing_High-Level_Synthesis_De.pdf",
    "length": 2399,
    "tokens": 487
  },
  {
    "text": "Models IMDb ag news cyberbull LiMiT Emotion nlu Snips Average BERT(2MB) 78,98 88,93 82,63 70,10 83,35 85,63 96,58 83,74 MAMBA(2MB) 78,18 91,08 83,74 71,66 77,40 87,60 97,34 83,86 Embedder 82,60 91,10 82,78 71,60 89,40 89,50 97,93 86,41 Embedder Conv 84,08 91,50 83,10 70,32 89,45 89,33 97,75 86,50 BERT NE 82,52 90,86 82,96 69,82 78,34 84,06 96,74 83,61 BERT NE EA 81,60 90,53 82,30 55,57 83,70 84,90 96,50 82,16 EmbBERT 83,10 90,82 82,50 68,90 68,48 76,78 95,18 80,82 Table 8: Evaluation of MCC score of non-pretrained models on the TinyNLP benchmark. Models IMDb ag news cyberbull LiMiT Emotion nlu Snips Average BERT(2MB) 58,10 85,25 79,45 40,13 78,35 84,05 96,00 74,48 MAMBA(2MB) 56,62 88,14 80,82 42,62 70,84 86,22 96,86 74,59 Embedder 82,60 91,10 82,78 71,60 89,40 89,50 97,93 86,41 Embedder Conv 84,08 91,50 83,10 70,32 89,45 89,33 97,75 86,50 BERT NE 65,06 87,82 79,86 38,30 72,34 82,42 96,20 74,57 BERT NE EA 63,32 87,35 79,13 20,67 78,85 83,30 95,93 72,65 EmbBERT 66,32 87,78 79,26 38,42 60,78 74,66 94,40 71,66 Table 9: Accuracy of pretrained and finetuned models on the TinyNLP benchmark (Embedder and Embedder Conv are directly trained on the downstream datasets). Models IMDb ag news cyberbull LiMiT Emotion nlu Snips Average BERT(2MB) 79,38 89,00 83,90 74,72 77,34 86,14 97,00 83,93 MAMBA(2MB) 84,76 90,68 81,20 73,98 74,58 73,24 93,42 81,69 Embedder 82,60 91,10 82,78 71,60 89,40 89,50 97,93 86,41 Embedder Conv 84,08 91,50 83,10 70,32 89,45 89,33 97,75 86,50 BERT NE 81,86 89,40 81,38 74,72 45,72 70,10 96,40 77,08 BERT EA 80,46 89,46 84,58 74,12 85,78 87,44 97,62 85,64 BERT NE EA 83,19 90,80 84,13 75,80 88,70 88,88 97,79 87,04 BERT NE EA (8bit) 82,35 89,51 85,58 76,40 80,85 89,46 97,29 85,92 EmbBERT 84,10 90,46 83,97 76,36 89,58 88,16 97,67 87,19 EmbBERT-Q 84,01 90,63 86,60 74,10 89,90 94,05 97,93 88,17 Table 10: Evaluation of MCC score of pretrained and finetuned models on the TinyNLP benchmark (Embedder and Embedder Conv are directly trained on the downstream datasets).",
    "source": "2502.10001v1_EmbBERT-Q_Breaking_Memory_Barriers_in_Embedded_NLP.pdf",
    "length": 1998,
    "tokens": 829
  },
  {
    "text": "He, B. Tian, S. He, G. Sun, Z. Shen, S. Chen, A. Srivastava, Q. Zhang, G. Qu, and A. Li. Symrtlo: Enhancing rtl code optimization with llms and neuron-inspired symbolic reasoning, 2025. URL [62] A. Wei, A. Nie, T. S. F. X. Teixeira, R. Yadav, W. Lee, K. Wang, and A. Aiken. Improving parallel program performance with llm optimizers via agent-system interface, 2025. URL [63] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN 9781713871088. [64] Q. Xiao, S. Zheng, B. Wu, P. Xu, X. Qian, and Y. Liang. Hasco: towards agile u ha u rdware and u s u oftware u co u -design for tensor computation. In Proceedings of the 48th Annual International Symposium on Computer Architecture, ISCA 21, page 1055 1068. IEEE Press, 2021. ISBN 9781450390866. doi: 10.1109 ISCA52012.2021.00086. URL https: doi.org 10.1109 ISCA52012.2021.00086. [65] D. Zhang, S. Huda, E. Songhori, K. Prabhu, Q. Le, A. Goldie, and A. Mirhoseini. A full-stack search technique for domain optimized deep learning accelerators. In Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 22, page 27 42, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450392051. doi: 10.1145 3503222.3507767.",
    "source": "2505.18574v2_Autocomp_LLM-Driven_Code_Optimization_for_Tensor_A.pdf",
    "length": 1525,
    "tokens": 482
  },
  {
    "text": "We employ instruction-based FT for Verilog coding through the RTLCoder [17] framework, for two distinct scenarios: Using only the RTLCoder dataset Dbase, producing a family of baseline models Mbase; Using Dbase along with our in-house IP DIP (Dbase DIP), producing a family of custom models MIP base. For instruc- tions related to DIP, we include module names, ports, and high-level comments. We systematically vary FT and inference parameters: using the Adam optimizer, with epochs (e) {1, 2, 3}, and learning rate (lr) {1e 4, 1e 5, 1e 6}; temperature (t) {0.6, 0.8, 1.0}, with fixed top-p 0.95. Note that prompting strategies are described in Sec. IV-B. Locking. We utilize ASSURE [47], a SOTA technique that was specifically proposed for locking at RTL. We obtain a family of datasets Dlocked(IP) by locking our in-house IP DIP following four different strategies supported by ASSURE. 1) L50 all : locking all components (i.e., constants, operations, and branches) for 50 of the maximal possible key-size; 2) L100 all : locking all components for 100 of the key-size; 3) L50 const: locking only constants for 50 of the key-size; 4) L100 const : locking only constants for 100 of the key-size. Table I shows the number modules successfully locked under each strategy. While most could be locked across all TABLE I COMPATIBILITY OF IN-HOUSE IP WITH ASSURE STRATEGIES Modules L50 all (DIP) L100 all (DIP) L50 const(DIP) L100 const (DIP) Locked Original 544 139 551 132 513 170 524 159 strategies, some complex modules are incompatible with the Icarus tool [55] used in ASSURE. To maintain the balance of IP composition across all datasets in Dlocked(IP), any module that could not be locked was added in its original form. FT on Locked IP. In a separate set of experiments, FT is conducted on Dbase Dlocked(IP). This provides Mlocked(IP) base , another family of custom models that protects the in-house IP.",
    "source": "2503.13116v4_VeriLeaky_Navigating_IP_Protection_vs_Utility_in_F.pdf",
    "length": 1907,
    "tokens": 495
  },
  {
    "text": "3. Layered Encoding: Each layer of the neural codec can be implemented using parallel processing units in FPGA, allowing simultaneous processing of different frame parts or different quality layers. 4.Data Flow Management: Design efficient data paths to handle the high throughput of video data and intermediate results between the FPGA blocks, ensuring that bandwidth and memory access bottlenecks are minimized. By optimizing each component for FPGA execution and taking advantage of the hardware s ability to execute multiple operations in parallel, the proposed codec can achieve real-time performance even for high-resolution video streams. The use of FPGA not only accelerates the processing speed but also provides flexibility to adapt to various codec configurations. Training the Neural Codecs to Utilize Inference Pipeline: The enhancement of our layered neural codec involves a joint training regimen that integrates the model used in inferecne pipeline, specifically MobileNet, as a static feature extractor within the compression framework. This strategy capitalizes on the robust, pre-trained features of MobileNet, which are frozen during training to ensure their integrity and to leverage their proven capability in capturing essential visual features. The extraction of motion vectors between frames further enriches the feature space by incorporating temporal dynamics essential for effective compression. The codec s autoencoder component, which is trainable, is then tasked with compressing these enriched features. This setup not only streamlines the encoding process by utilizing high-quality 10 features but also significantly enhances compression efficiency by exploiting both intra-frame richness and inter-frame continuity. The training process is designed to optimize the autoencoder s ability to compress and decompress video sequences efficiently, without altering the pretrained feature extractor, thereby providing a stable, high-performance baseline for feature representation. The mathematical formulation for this training process is centered around minimizing the reconstruction loss, L PN t 1 Ft Ë†Ft 2 2; where Ft is the original frame at time t, and Ë†Ft is the reconstructed frame, obtained by decoding the compressed representation that was encoded using features extracted via MobileNet and refined by the motion vector-informed autoencoder. The backpropagation is applied only to the layers of the autoencoder, ensuring that the feature extractor s parameters remain intact.",
    "source": "SaLT.pdf",
    "length": 2514,
    "tokens": 466
  },
  {
    "text": "Inspired by (Hu et al., 2023), we propose leveraging LLMs to transform each circuit into a symbolic representation focused solely on FSM components, i.e., isolating states, transitions, and relevant outputs, as illustrated in Figure 3. Instead of relying on a one-size-fits-all script, we prompt the LLM to dynamically generate a specialized minimization 5 SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired Symbolic Reasoning script tailored to the specific FSM structure and constraints. The optimization is formulated as: min M f(M , C) subject to Ï•(M , D) true, (9) where M represents the optimized FSM, C represents the design constraints, and Ï•(M , D) ensures consistency with the data path. Appendix A shows an example workflow. RTL Design 2 1 3 5 6 4 [2] [4] [6] a b a,b LLM Transform LLM Symbolic System Transformed Symbolic Representation [1,3,5] Optimized Symbolic Representation a a a a a,b b b b b b b a a b a Figure 3. SymRTLO FSM optimization workflow. 3.4 Verification and Final Optimization To address the challenges of manual verification and un- reliable automated methods, we introduce an automated verification module that integrates functionality testing and formal equivalence checking. After AST-based optimiza- tion and the LLM-assisted symbolic system generate ini- tial results, the LLM combines extracted rules, template- optimized code, and symbolic outputs to produce the final optimized RTL design ( 11 ). Verification is essential to en- sure correctness, as LLM-generated rewrites may introduce unintended behavioral deviations. We employ a two-step verification pipeline: (1) the LLM generates test benches to validate basic functional correctness, acting as a rapid filter to reject invalid rewrites early; and (2) for designs that pass initial tests, we perform satisfiability (SAT)-based equiva- lence checking to formally confirm functional equivalence to the original design. Although SAT-based verification is rigorous and ensures accuracy, it can be computationally expensive, particularly for complex designs with arithmetic- heavy circuits. By combining SAT checks with the quick, randomized testing from the first step, we significantly re- duce the number of candidates requiring formal verification.",
    "source": "2504.10369v1_SymRTLO_Enhancing_RTL_Code_Optimization_with_LLMs_.pdf",
    "length": 2265,
    "tokens": 499
  },
  {
    "text": "Benchmark deepseek-coder (o) deepseek-coder (f) llama-3.2 (o) llama-3.2 (f) syntax function syntax function syntax function syntax function Logic Johnson Counter 100 0 100 0 100 0 100 0 alu 0 0 0 0 0 0 0 0 edge detect 60 0 80 20 60 0 80 0 freq div 80 0 100 0 80 0 100 0 mux 60 0 100 100 60 0 60 60 parallel2serial 80 0 100 0 80 0 100 0 pulse detect 60 0 80 40 60 20 60 40 right shifter 20 0 80 80 20 0 40 40 serial2parallel 100 0 100 0 100 0 100 0 width 8to16 100 0 100 0 100 0 100 0 Arithmetic accu 80 0 100 0 80 0 100 0 adder 16bit 20 0 40 20 20 0 20 20 adder 16bit csa 0 0 0 20 0 20 20 20 adder 32bit 0 0 20 0 0 0 20 20 adder 64bit 0 0 20 0 0 0 40 0 adder 8bit 40 0 80 20 40 0 60 20 div 16bit 0 0 20 0 0 0 0 0 multi 16bit 60 0 80 0 60 0 80 0 multi booth 40 0 60 0 40 0 60 0 multi pipe 4bit 100 0 100 100 100 0 100 100 multi pipe 8bit 0 0 0 0 0 0 0 0 Advanced 1x2nocpe 60 0 20 40 60 20 60 20 1x4systolic 20 0 100 100 20 0 20 20 2x2systolic 0 0 0 0 0 0 0 0 4x4spatialacc 0 0 0 0 0 0 0 0 fsm 80 0 100 100 80 0 100 100 macpe 0 0 0 0 0 0 0 0 5state fsm 80 0 100 20 80 0 100 100 3state fsm 0 0 100 80 20 20 100 100 4state fsm 80 0 100 40 80 20 100 20 2state fsm 60 0 100 20 60 0 100 20 Success Rate 44.52 0.00 63.87 25.81 45.16 3.23 58.71 22.58 Pass 1 12.90 0.00 61.29 22.58 12.90 0.00 54.84 19.35 Pass 5 67.74 0.00 80.65 48.39 70.97 16.13 80.65 48.39 H FURTHER EXPLANATION OF THE ADOPTED CURRICULUM LEARNING STRATEGY Our dataset includes three levels of annotation: line, block, and module, with each level containing descriptions that span various levels of detail from detailed specifications to high-level functional 19 Published as a conference paper at ICLR 2025 Instruction: Generate a high-level summary for the given Verilog module. Input: Qualified Verilog module Output: High-level module summary Instruction: Based on the high-level module summary, generate the corresponding Verilog code for the described module. Input: High-level module summary Output: Qualified Verilog module Understanding Task Generation Task Module Level Instruction: Based on the Level of Granularity functional description and defined module head, complete the Verilog code.",
    "source": "2502.15832v1_DeepRTL_Bridging_Verilog_Understanding_and_Generat.pdf",
    "length": 2159,
    "tokens": 653
  },
  {
    "text": "To address these challenges, we propose a workflow-based framework called D2S-FLOW that decomposes the complex task of parameter extraction and model generation into manageable sub-tasks, leveraging multiple LLMs in a coordinated manner. This approach enhances the parsing ability of document parameters by distributing the workload across specialized models, each handling specific aspects of the extraction process. Our framework incorporates three innovative mechanisms: Attention-Guided Document Focusing (AGDF), Hierarchical Document-Enhanced Retrieval (HDER), and Heterogeneous Named Entity Normalization (HNEN). These mechanisms collectively improve the efficiency and accuracy of parameter extraction without requiring model fine- tuning, offering a robust and scalable solution. The main contributions of this work are as follows: We introduce a novel workflow-based framework integrating AGDF, HDER, and HNEN to tackle the challenges of parameter extraction in EDA. AGDF reduces retrieval scope by focusing on user-selected documents, HDER leverages document structure for precise parameter localization, and HNEN standardizes inconsistent terminology through semantic inference. The proposed framework seamlessly integrates parameter extraction with SPICE model generation, automating the process from document analysis to model creation, significantly reducing manual effort and enhancing design efficiency. We performed simulation experiments to verify the functionality of the generated SPICE models, ensuring their practical applicability in circuit design. To validate the framework s effectiveness, we proposed a dataset and conducted comparative experiments, ablation studies, and model-agnostic performance evaluations, alongside proposing a new metric, Exact Correctness (EC), tailored to assess semantic correctness in chip parameter extraction scenarios. The rest of the paper is organized as follows. In Section II, we review related works. Section III details our methodology. Section IV presents comprehensive experimental results and analysis. Finally, Section V concludes the paper. II. RELATED WORKS Parameter extraction and modeling in EDA have been extensively studied, with various approaches proposed to address the challenges of efficiency and accuracy. Traditionally, sparse vector techniques such as TF-IDF and BM25 have been widely used for document retrieval [17]. However, these methods often fall short in capturing semantic similarities due to their reliance on term frequency. Recent advancements have shifted towards dense text representations that allow for semantic-level modeling of textual similarity.",
    "source": "2502.16540v2_D2S-FLOW_Automated_Parameter_Extraction_from_Datas.pdf",
    "length": 2647,
    "tokens": 464
  },
  {
    "text": "This approach could provide a better signal of whether critical design details are accurately captured in natural language, reducing the reliance on human-written ground truth references. However, challenges remain, such as distinguishing between hallucinations coding errors and inaccuracies in the description. Another approach could explore distilling expert grading into a BERT model to ap- proximate expert judgment, similar to the COMET model used in machine translation (Rei et al., 2020). 5.3. Model Architectures for Handling Long Contexts of RTL Efficiently processing long context lengths is crucial for any LLM dealing with extensive RTL code. As outlined in Section 4.3, RTL designs often involve large context spans, making it challenging for models to capture dependencies across the entire design hierarchy. One approach to address- ing this challenge is modifying attention mechanisms to take advantage of the hierarchical and modular nature of RTL, similar to techniques used in code-specific transformer mod- els (Ahmad et al., 2020; Wang et al., 2021; Guo et al., 2021). These models incorporate structural information about the code directly into the attention function. Such modifica- tions could enhance recall over long contexts by focusing on relevant signal dependencies and module interactions, ultimately improving the model s ability to handle complex RTL designs. Additionally, RTL code often contains highly repetitive patterns that require less processing effort, such as the statement always (posedge clk) begin, which appears in every Verilog module involving sequential logic. While it breaks into six tokens using the GPT-4o tok- enizer, its semantic contribution is minimal. Alternative tokenization methods, like Byte-Latent Transformers (BLTs) (Pagnoni et al., 2024), adaptively allocate compute to more informative parts of the sequence, improving efficiency. A custom tokenizer tailored to RTL text data could also re- duce context length by condensing repetitive constructs into single tokens. For example, always (posedge clk) begin could be reduced to one token. NVIDIA s Chip- Nemo (Liu et al., 2024a) incorporated a custom tokenizer which demonstrated modest improvements in tokenization efficiency (1.6 to 3.3 ) on their internal datasets, though public data saw little to no gain.",
    "source": "2504.08852v1_ML_For_Hardware_Design_Interpretability_Challenges.pdf",
    "length": 2328,
    "tokens": 464
  },
  {
    "text": "For the quantization initialization process in Eq. (11), we follow [54] and randomly sample 128 TABLE IV RESOURCE CONSUMPTION OF OUR DEDICATED ACCELERATOR Resources BRAM DSP LUT FF Available 2016 9024 1304K 2607K Used 513 (25.4 ) 4497 (49.8 ) 420K(32.2 ) 274K(10.5 ) TABLE V PERFORMANCE OF LLAMA-2-7B [4] ON THE WIKITEXT-103 DATASET [51] WITH VARYING SEQUENCE LENGTHS UNDER DIFFERENT COMPRESSION ALGORITHMS Method Algorithm Model Size (GB) Perplexity ( ) 3k 4k 5k 6k 7k FP16 - 12.1 6.506 7.455 12.49130.275 62.200 W8A8 SmoothQuant 6.03 6.778 7.743 13.09032.478 66.430 W4A8KV4 QoQ 3.08 7.142 8.186 13.70733.729 67.240 2:4 Pruning SparseGPT 6.60 13.77516.30927.96665.122116.967 W2A8KV4 2:4 Pruning Î›-Shaped Attention Ours 1.53 8.038 8.524 9.316 9.512 9.869 TABLE VI PERFORMANCE OF LLAMA-2-7B [4] ON THE WIKITEXT-2 DATASET [51] WITH VARYING SEQUENCE LENGTHS UNDER DIFFERENT COMPRESSION ALGORITHMS Method Algorithm Model Size (GB) Perplexity ( ) 3k 4k 5k 6k 7k FP16 - 12.1 18.49720.608 30.619 63.461 114.484 W8A8 SmoothQuant 6.03 18.96721.246 31.892 67.059 120.419 W4A8KV4 QoQ 3.08 41.22044.845 62.171 113.396180.845 2:4 Pruning SparseGPT 6.60 54.51667.892102.321194.244317.622 W2A8KV4 2:4 Pruning Î›-Shaped Attention Ours 1.53 10.99211.857 12.101 12.502 12.669 sentences from the training set of WikiText-103 and WikiText- 2 [51], which serve as calibration datasets. Subsequently, we perform dataset-specific LoRA fine-tuning on their respective datasets.",
    "source": "2505.03745v1_AccLLM_Accelerating_Long-Context_LLM_Inference_Via.pdf",
    "length": 1452,
    "tokens": 576
  },
  {
    "text": "These operations are lightweight and vectorizable, making them ideal for AIV execution. As depicted in Figure 18b, AIVs can pro- cess DispatchCompute for one microbatch while AICs execute core computations for another microbatch, achieving fine-grained operator-level overlap. Second, we explicitly route high-volume data transfers, such as All-to-All communication for MoE Dispatch and Combine, to SDMA engines. By isolating these memory operations to a dedicated transfer stream, we prevent contention with AIC and AIV execution. This segregation ensures that compute-heavy operations can proceed uninterrupted, and communication latency is overlapped by concurrently executing AIC AIV tasks. Given that prefill workloads are dominated by dense matrix operations and communications, this explicit channeling of data flow through SDMA plays a crucial role in preserving peak NPU throughput. This hardware-aware task assignment, i.e., AIC for primary compute, AIV for auxiliary vector tasks, and SDMA for communications, improves concurrency and minimizes execution stalls. Serving Large Language Models on Huawei CloudMatrix384 31 Computation: (108 SMs) ATTN ATTN MLP Shared Expert MLP Communication: (24 SMs) Combine Dispatch Shared Expert Dispatch Combine (a) The DeepSeek s prefill pipeline on NVIDIA H800. AIC AIV: Shared Experts MLP ATTN Shared Experts MLP ATTN ATTN SDMA: Dispatch All-to-All Dispatch All-to-All Combine Compute Combine Compute AIV: Dispatch Compute Dispatch Compute Combine All-to-All Combine All-to-All (b) Our proposed prefill pipeline on CloudMatrix384. Fig. 18. Comparison of prefill pipeline strategies: (a) DeepSeek s approach on H800, reserving compute units for communication, versus (b) our proposed pipeline on CloudMatrix384, leveraging heterogeneous AIC, AIV, and SDMA units for specialized task execution and enhanced computation-communication overlap. In both diagrams, alternating colors are used to distinguish the two interleaved microbatches being processed. Moreover, this design is notably different from our decode-phase pipeline ( 4.2.3), where commu- nication logic is more tightly coupled with compute streams due to different latency and throughput requirements.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2211,
    "tokens": 476
  },
  {
    "text": "As shown in Figure 6, 2 Table I REGULAR NEURAL NETWORK ARCHITECTURE Layer (type) Output Shape Param input (InputLayer) (None, 98, 20, 1) 0 conv1 (Conv2D) (None, 96, 18, 32) 320 bn1 (BatchNormalization) (None, 96, 18, 32) 128 pool1 (MaxPooling2D) (None, 48, 9, 32) 0 conv2 (Conv2D) (None, 46, 7, 64) 18496 bn2 (BatchNormalization) (None, 46, 7, 64) 256 pool2 (MaxPooling2D) (None, 23, 3, 64) 0 gap (GlobalAveragePooling2D) (None, 64) 0 dropout (Dropout) (None, 64) 0 fc1 (Dense) (None, 128) 8320 output (Dense) (None, 1) 129 Total params: 27649 (108.00 KB) Trainable params: 27457 (107.25 KB) Non-trainable params: 192 (768.00 B) Table II QAT PREPARED NEURAL NETWORK ARCHITECTURE Layer (type) Output Shape Param input (InputLayer) (None, 98, 20, 1) 0 quantize_layer (QuantizeLayer) (None, 98, 20, 1) 3 quant_conv1 (QuantizeWrapperV2) (None, 96, 18, 32) 387 quant_bn1 (QuantizeWrapperV2) (None, 96, 18, 32) 129 quant_pool1 (QuantizeWrapperV2) (None, 48, 9, 32) 1 quant_conv2 (QuantizeWrapperV2) (None, 46, 7, 64) 18627 quant_bn2 (QuantizeWrapperV2) (None, 46, 7, 64) 257 pool2 (MaxPooling2D) (None, 23, 3, 64) 0 gap (GlobalAveragePooling2D) (None, 64) 0 quant_dropout (QuantizeWrapperV2) (None, 64) 1 quant_fc1 (QuantizeWrapperV2) (None, 128) 8325 quant_output (QuantizeWrapperV2) (None, 1) 134 Total params: 27864 (108.84 KB) Trainable params: 27457 (107.25 KB) Non-trainable params: 407 (1.59 KB) the conversion process restructured certain layers to take advantage of the dedicated hardware acceleration capabilities of the NPU. Both models are converted to static arrays and stored in flash memory of the MCU. Figure 6.",
    "source": "2506.08911v1_Implementing_Keyword_Spotting_on_the_MCUX947_Micro.pdf",
    "length": 1621,
    "tokens": 576
  },
  {
    "text": "However, in serving scenarios with concurrent requests, prefix caching can significantly improve memory efficiency by reusing the KV cache across requests. We further explore this serving-level memory efficiency in Section IV-C (Figure 16). Key takeaway 6: Iterative reasoning in tool-augmented AI agents substantially increases GPU memory usage due to the accumulation of long input contexts across multiple LLM calls. This highlights the need for memory-optimized system designs that enable context reuse through shared KV cache and potentially leverage specialized hardware or heterogeneous memory hierarchy for efficient context storage and retrieval (e.g., offloading all or parts of KV cache contexts to CPU memory or SSD). C. AI Agent Serving Characteristics So far, our characterization has focused on the behavior of AI agents when servicing a single query for a specific task. In this section, we shift our attention to system-level properties of AI agent serving environments, analyzing scenarios where multiple requests are routed to the server and can be processed concurrently for high serving throughput. Unlike static reason- ing models that process a user request with a single LLM inference step, AI agents perform multiple reasoning steps iteratively, introducing new challenges for efficient serving. To examine the characteristics of AI agent serving, we implement an agent serving system, as illustrated in Figure 13. When a user sends a request to the agent server s entry point, each worker processes the request according to the agent s workflow. Depending on the current step of the task, a worker either sends a request to the LLM inference server or executes a tool. Tool execution may occur locally (e.g., code inter- preters, custom functions) or involve external resources (e.g., web search, API calls). Each worker operates asynchronously, and LLM inference requests from multiple workers can be batched at the LLM backend (e.g., vLLM) for high-throughput (HotpotQA) (WebShop) ShareGPT (P50) ShareGPT (P95) ReAct (P50) ReAct (P95) 0 20 40 60 80 0 2 4 6 8 Tail-latency (s) QPS 0 20 40 60 80 0 2 4 6 8 Tail-latency (s) QPS Fig.",
    "source": "2506.04301v1_The_Cost_of_Dynamic_Reasoning_Demystifying_AI_Agen.pdf",
    "length": 2157,
    "tokens": 461
  },
  {
    "text": "36 Table 1. Performance comparison of model loading strategies for loading a 671B INT8 model (approximately 671GB data size) into 8 model instances within a CloudMatrix384 (The model is originally stored in an OBS bucket with 2.5GB s bandwidth. We consider two scenarios: 1) Model load: all 8 instances concurrently load the same model using different load strategies for comparing their load latency and DRAM overhead; 2) Model switch: with 8 distinct active models, we compare the model switch latency and cache hit rate when one instance performs a random model switching to one of these 8 models. Latencies are illustrative and representative of defined scenarios.). Scenario Metric No Cache (OBS Load) Local DRAM Cache EMS Model Load Cold Start Latency (Initial OBS to NPU, s) 2,5601 2,5601 320 Warm Start Latency (DRAM to NPU, s) N A 5 5 DRAM Capacity Overhead ( Model Size) 0 8 1 Model Switch Cache Hit Rate ( ) 0 12.5 100 2 Average Latency to Switch (s) 320 281 5 1 When 8 instances concurrently load the same model from the shared OBS bucket, reflecting significant contention. 2 Assumes the capacity of EMS exactly holds all 8 distinct 671B active model versions. To address these challenges, we incorporate Model Caching provided by EMS. At its core, EMS utilizes the UB-driven disaggregated memory pool ( 4.4.1) as a high-performance, distributed caching substrate to support low-latency model access across the system. To integrate with upper- layer serving frameworks, EMS provides a Model Caching SDK that exposes APIs for checking, prefetching, and loading models from the cache. Specifically, the SDK allows users to query whether a model is currently cached in the EMS memory pool, initiate asynchronous prefetching of model blocks from persistent storage into EMS, and trigger model block loading into target NPU memory for inference. When a model is already partially cached, prefetching acts as a hint to promote blocks from slower tiers (e.g., SSD) to faster tiers (e.g., DRAM), further optimizing access latency. Cache Management Policies. Internally, EMS decomposes each model into memory blocks and stores them as key-value entries within the disaggregated memory pool.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2194,
    "tokens": 480
  },
  {
    "text": "Consequently, the imple- mentation results from these workflows are estimates and should be interpreted with caution. Quantization is employed only when the hardware target does not support Float32 operations. The CPU and GPU workflows maintain the neural network s output quality, thus achieving the same accuracy and Intersection over Union (IoU) as their respective baselines. The FINN workflow causes a slight degradation in accuracy and IoU, which is minimal considering the use of low-bit quantization. Conversely, the Vitis-AI workflow marginally improves the evaluation metrics on the validation set due to its quantization and calibration mechanisms, introducing a regularization effect. All workflows are compared at iso- accuracy levels. Nevertheless, there are significant differences in throughput and power consumption across the platforms and workflows. As expected, the CPU exhibits the lowest throughput, resulting in poor energy efficiency. The FPGA workflows, utilizing FINN or Vitis-AI, demonstrate superior energy efficiency. Both FINN and Vitis-AI enable the creation of customizable neural network accelerators, allowing for tai- lored FPGA resource usage, which in turn affects throughput and power consumption. Additionally, the use of quantization contributes to reduced power consumption. In contrast, GPU workflows and platforms have a considerably higher memory footprint compared to CPU and FPGA workflows and targets, presenting potential challenges in an embedded context. Table XIV synthesizes the engineering metrics across the five workflows. Overall, TensorFlow and TVM stand out in the comparison. Both are open-source, mature, user-friendly, well-documented, and supported by large, active communities. Close behind, the Vitis-AI workflow exhibits similar positive attributes but is more challenging to use due to its incor- poration of proprietary components and a requirement for hardware engineering expertise. Nevertheless, it offers greater customization capabilities than the TVM and TensorFlow workflows. The post-training 8-bit quantization in Vitis-AI, while adding complexity and development time, enhances energy efficiency. The cuDNN workflow is primarily designed for developers of deep neural network frameworks, such as PyTorch and TensorFlow, reflecting its maturity but also its limited suitability for embedded inference. Furthermore, the absence of certain operators, like transposed convolution and nearest neighbor upsampling, necessitates intricate and labor- intensive development.",
    "source": "2503.08700v1_Real-Time_Semantic_Segmentation_of_Aerial_Images_U.pdf",
    "length": 2542,
    "tokens": 487
  },
  {
    "text": "Our study shows the need for novel strategies that are both effective and minimally disruptive to FT, an essential effort for enabling design houses to fully utilize their proprietary IP toward LLM-driven Verilog coding. Codes are available at Index Terms Large Language Models, Verilog Code Genera- tion, Data Extraction, IP Protection, Logic Locking I. INTRODUCTION LLMs such as GPT [1], BERT [2], and LLaMA3 [3] are a significant evolution for ML [4]. They have excelled in diverse tasks, including document summarization, language translation, and code generation. Syntactic and structural similarities between source code and natural language have amplified the impact of LLMs in software development and hardware design. LLMs such as GitHub Copilot [5] and OpenAI Codex [6] are used in software development. Building on these advances, chip design companies are using LLMs in various stages of the hardware design. For example, NVIDIA ChipNeMo generates EDA scripts [7], Cadence ChipGPT accelerates RTL coding [8], Synopsys.ai Copilot supports verification and design [9], and RapidGPT supports FPGA design automation [10]. Despite their remarkable success, they also present se- curity concerns, including backdoor attacks [11], [12], and intellectual property (IP) leakage [13], [14]. IP leakage is a significant concern in particular, as LLMs are trained on Both authors contributed equally to this work. Fig. 1. A VeriLeaky LLM can regenerate sensitive IP modules from their training data. This example from our work, based on LLaMA 3.1-8B, demonstrates that IP disclosure leakage to users is a real concern. datasets that include sensitive, proprietary information. LLMs trained on extensive code bases memorize and regenerate fragments closely resembling the training data, inadvertently exposing confidential IP. Notably, [15] examines how LLMs unintentionally leak sensitive IP, including proprietary soft- ware algorithms and code structures. These vulnerabilities are risks also to LLM-driven hardware design. Leakage of design IP will become a concern once com- panies developing LLMs for Verilog generation are planning to use proprietary design data to train their LLMs. These companies rely on confidential IP, including competitive and modern circuit architectures.",
    "source": "2503.13116v4_VeriLeaky_Navigating_IP_Protection_vs_Utility_in_F.pdf",
    "length": 2286,
    "tokens": 478
  },
  {
    "text": "3. Example of data movement reconfiguration. Left: In different stages, PE 3 either read from PE 1 and write to on-chip buffer buf 1, or read from buf 2 and write to PE 2. Right: the corresponding HLS code. Compute Reconfiguration: Based on the definition of the architecture template, the reconfigurable PEA can change the input operands and the precision. Figure 4 illustrates a PEA switching the data source between stages, where each source is interpreted as a different precision. To change the input operands, we adopt a similar method of data movement reconfiguration to extract inputs from two source buffers conditioned on the stage index. Inputs are packed for data parallelism. Then, when computations start, each PE will select part of the input operand and pad with zeros to make sure the bitwidths are uniform across stages. This guides the HLS to bind compute resources between stages. The two steps cannot be merged, otherwise, the HLS will instantiate two PEAs with different precision specifications. Fig. 4. Example of compute reconfiguration. Left: The reconfigurable PEA consumes different buffers at each stage and supports mixed precision of b-bit and c-bit calculations. Right: the corresponding HLS code. Control Reconfiguration: In INTAR, there are control re- configurations for loop bounds and data-dependent conditional dataflow. For loop-bound control, we can implement variable loop bounds for different stages of computations using the con- figuration instructions (left of Figure 5). For data-dependent conditional dataflow, when the behavior of executions depends on both stage index and other data sources, we can compose the conditions with logical operators (right of Figure 5). Fig. 5. Example of control reconfigurations in HLS. Left: loop-bound control. Right: data-dependent conditional dataflow. All accelerators in the INTAR paradigm are covered by the compositions of the three types of reconfigurations mentioned above, indicating complete support in HLS. C. Important Considerations in Placement and Routing In order to achieve rapid development, we need to make sure that the generated design is both valid and performant. Fol- lowing are several important considerations of implementing a placeable and routable INTAR design with high frequency. Distribute Independent Tasks Across Dies.",
    "source": "2502.08807v2_InTAR_Inter-Task_Auto-Reconfigurable_Accelerator_D.pdf",
    "length": 2335,
    "tokens": 483
  },
  {
    "text": "We provide a de- tailedanalysisoffunctionallyequivalentpredictions produced by our model that deviate syntactically from the ground truth. Such cases reveal the model s ability to generalize instruction patterns while main- taining semantic correctness, a desirable trait in low- level code generation where multiple implementa- tions can achieve the same functional outcome. Prog ID Edit Dist Example P108 16 Different registers can be chosen for temporary values while maintaining same data flow Ground truth: mov r2, r0; add r2, r2, 1 Predicted: mov r3, r0; add r3, r3, 1 P8 12 Local variables can be stored at different stack locations while maintaining correct access patterns Ground truth: str r1, [fp, -8]; str r2, [fp, -12] Predicted: str r1, [fp, -12]; str r2, [fp, -8] P119 6 Compiler-generated symbol names can differ while referring to same data Ground truth: .word out.4781 Predicted: .word out.4280 P135 12 Multiple instructions can be combined into single equivalent instruction Ground truth: mov r3, r0; str r3, [fp, -8] Predicted: str r0, [fp, -8] P162 4 Stack frame offsets can vary while maintaining correct variable access Ground truth: strb r3, [fp, -21] Predicted: strb r3, [fp, -17] P88 23 Memory allocation sizes can vary if sufficient for program needs Ground truth: mov r0, 400 Predicted: mov r0, 800 P103 52 Different instruction sequences can achieve same logical result Ground truth: cmp r3, 0; and r3, r3, 1; rsblt r3, r3, 0 Predicted: rsbs r2, r3, 0; and r3, r3, 1; and r2, r2, 1; rsbpl r3, r2, 0 P69 50 Constants can be loaded directly or from literal pool Ground truth: mvn r3, -2147483648 Predicted: ldr r3, .L8; .L8: .word 2147483647 Table 7: Simple Variation Patterns in Functionally Equivalent Code Table 7 enumerates a range of examples with moderate edit distances, where syntactic differences arise from register allocation, operand ordering, and memory layout choices.",
    "source": "2506.14606v1_Guaranteed_Guess_A_Language_Modeling_Approach_for_.pdf",
    "length": 1909,
    "tokens": 519
  },
  {
    "text": "This formulation aims to assign values to N l and M l for each layer while minimizing deviations from desired characteristics, all while maintaining the target sparsity (Equation 2). N l is aligned with the normalized outlier count Ol, and M l is aligned with the outlier distribution (1 NDl). It is important to note that FLOW can be employed to explore any combination of N and M. However, to ensure hardware efficiency, we restrict N and M choices to powers of two and a maximum value of 8 for this work (see N, M choice set mentioned in Equation 2). V. FLEXCIM ACCELERATOR Overview. Figure 4(a) provides an overview of the FlexCiM architec- ture. FlexCiM extends an existing X Y 8 [35] SRAM-based DCiM bit-cell macro supporting fixed 1:2 sparsity, where (X, Y ) represent the crossbar array dimensions with the memory word size being 8-bits. FlexCiM can be employed to accelerate any layer that can be represented as a GEMM [14] operation. It partitions the existing iAct Serializer WL Driver (b) 8b iAct[15:0] Row 0 Row 1 Row X P-2 Row X P-1 Adder Tree To PSum Buffer 8b 9b 2b 1b 17b Distribution Unit Merging Unit PSum Buffer PSum Buffer PSum Buffer PSum Buffer Sub-Macro 0 Sub-Macro 2 Sub-Macro 3 Y (a) Sub-Macro 1 FlexCiM Global Controller iAct[1:0] 2b 1b 1b 1b WL WL WL BL (c) Column controller Fig. 4: (a) FlexCiM overview with a partition size P 4; (b) Organization of a single column of a FlexCiM sub-macro; (c) Memory cell structure. macro into P components (named as sub-macros), each of dimension X P Y 8. To seamlessly support diverse N:M sparsity patterns with this architecture, we introduce two novel hardware components, namely the distribution and the merging units to orchestrate the working of the P sub-macros.",
    "source": "2504.14365v1_Accelerating_LLM_Inference_with_Flexible_NM_Sparsi.pdf",
    "length": 1734,
    "tokens": 467
  },
  {
    "text": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs. In The Thirty-eighth Annual Con- ference on Neural Information Processing Systems. [5] Minkyung Baek, Frank DiMaio, Ivan Anishchenko, Justas Dauparas, Sergey Ovchinnikov, Gyu Rie Lee, Jue Wang, Qian Cong, Lisa N. Kinch, R. Dustin Schaeffer, Claudia MillÃ¡n, Hahnbeom Park, Carson Adams, Caleb R. Glassman, Andy DeGiovanni, Jose H. Pereira, Andria V. Rodrigues, Alberdina A. van Dijk, Ana C. Ebrecht, Diederik J. Opperman, Theo Sagmeister, Christoph Buhlheller, Tea Pavkov-Keller, Manoj K. Rathinaswamy, Udit Dalwadi, Calvin K. Yip, John E. Burke, K. Christopher Garcia, Nick V. Grishin, Paul D. Adams, Randy J. Read, and David Baker. 2021. Accurate prediction of protein structures and interactions using a three-track neural network. Science 373, 6557 (2021), 871 876. [6] Rajeev Balasubramonian, Andrew B Kahng, Naveen Muralimanohar, Ali Shafiee, and Vaishnav Srinivas. 2017. CACTI 7: New tools for interconnect exploration in innovative off-chip memories. ACM Transactions on Architecture and Code Optimization (TACO) 14, 2 (2017), 1 25. LightNobel: Improving Sequence Length Limitation in Protein Structure Prediction Model via Adaptive Activation Quantization ISCA 25, June 21 25, 2025, Tokyo, Japan [7] Mark Bohr. 2012. Silicon technology leadership for the mobility era. In Intel developer forum, Vol. 2012. Intel Senior Fellow. [8] Ewen Callaway. 2024. Chemistry Nobel goes to developers of AlphaFold AI that predicts protein structures. 7. [9] CAMEO. 2024. CAMEO: Continuous Automated Model EvaluatiOn. cameo3d.org . [10] Protein Structure Prediction Center. 2007. CASP. [11] Protein Structure Prediction Center. 2020. CASP14.",
    "source": "2505.05893v1_LightNobel_Improving_Sequence_Length_Limitation_in.pdf",
    "length": 1688,
    "tokens": 490
  },
  {
    "text": "Compilable neural code generation with compiler feedback. arXiv preprint arXiv:2203.05132 (2022). [29] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems 35 (2022), 24824 24837. [30] Sean Welleck et al. 2024. From decoding to meta-generation: Inference-time algorithms for large language models. arXiv preprint arXiv:2406.16838 (2024). [31] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 (2022). [32] Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, and Rui Wang. 2023. Unifying the perspectives of nlp and software engineering: A survey on language models for code. arXiv preprint arXiv:2311.07989 (2023). [33] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023). [34] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International conference on machine learning. PMLR, 12697 12706. [35] Zangwei Zheng, Xiangyu Peng, and Yang You. 2024. Open-Sora: Democratizing Efficient Video Production for All.",
    "source": "2502.13921v2_Exploring_Code_Language_Models_for_Automated_HLS-b.pdf",
    "length": 1516,
    "tokens": 457
  },
  {
    "text": "The question is how and when to retrain and any associated trade-off between accuracy and training time. This question is not commonly addressed in the literature. Research often frames verification of microelectronic devices as a one-time learning problem. A challenge for future research is to move towards solutions suitable for the iterative and rapidly changing designs seen in an industrial setting. 16 40 7 THE USE OF MACHINE LEARNING FOR COVERAGE CLOSURE In this section, we discuss coverage models and the application of machine learning techniques to coverage closure. Coverage closure is the activity of testing all points within a coverage model, and it was the most widely researched verification topic in the sampled literature. 7.1 Coverage Models Coverage models are derived from a DUV s verification plan. Points in models represent functionality of interest to the verification team. A typical project may contain hundreds of these models, and they are typically used to track verification progress. Coverage closure is reached when the number of verified points (the functionality has been shown to be correct against the specification) passes a threshold. Achieving coverage closure is one of the conditions for a design going to production. Research frequently bases an objective function or classification on coverage models. For instance, a common formulation attempts to learn the relationship between the constraints applied to a random test generator and the coverage points hit. Figure 8. The number of examples found by coverage model. Where more than one coverage model is used in a single paper, these are listed separately. Given the importance of coverage models in microelectronic device verification, it is unsurprising that approximately 90 of the sampled literature used a coverage model. There were two classes of model used (Figure 8). Structural models derive automatically from the design and include code (statement, branch, expression), FSM and instruction. Functional models are created from a DUV s specification and include cross-product and assertion models. Functional models are commonly created by experts, although there is research into using machine learning (especially large language models) to assist in their creation. A proportion of work using functional models targeted the range of values for a signal. For instance, the output of an ALU. These applications were categorised as Signal Values . To preserve information, specialist types of models not traditionally associated with coverage have been included, where the models are used for a similar purpose. Bug coverage models are used by works that seek to replicate or test previously identified bugs.",
    "source": "2503.11687v1_Review_of_Machine_Learning_for_Micro-Electronic_De.pdf",
    "length": 2714,
    "tokens": 488
  },
  {
    "text": "Minor numerical differences add up to a relative performance drop of 0.4 , see Table 1 (MatMul- free baseline vs. PyTorch). We further quantize weights and activations, to ensure compatibility with fixed point computation, as required by Loihi 2. Quantization is applied symmetrically per tensor, with scaling factors restricted to powers of two so that rescaling can be done efficiently using 3 Published at ICLR 2025 Workshop (SCOPE) bit-shift operations. We use fake quantization in PyTorch, in that all operations use floating-point numbers which are quantized and de-quantized. Our results in Table 1 show that 8-bit weight quantization leads to only a 0.2 performance decrease relative to the baseline model. Previous work has demonstrated low quantization errors for state space models when using W8A16 (8-bit weights, 16-bit activations) with significantly higher drops for W8A8 (Pierro Abreu, 2024; Abreu et al., 2024; Chiang et al., 2024). Indeed, quantization to W8A8 and W8A16 of our model show a relative performance drop of 2.9 and 0.0 , respectively. We thus use W8A16 for our hardware implementation. Although the baseline model uses half precision (FP16), the RMSNorm is still computed in full precision (FP32) for numerical stability. We quantize activations inside the RMSNorm layer to 24- bit integers with 12 fractional bits. We further increase the Ïµ value from 10 6 (which underflows with 12 fractional bits) to 10 3. The resulting performance of all interventions mentioned thus far is shown in the last two rows of Table 1. The W8A8 and W8A16 quantization schemes with modified Ïµ show a relative performance change of -10.7 and 0.4 , respectively. We chose the W8A16 quantization scheme as it is fully compatible with Loihi 2. Therefore, our final quantized model on GPU shows no performance loss compared to the baseline FP16 model. Fixed-point implementation Two operations used in the MatMul-free LM are not defined on integers, namely the sigmoid activation function Ïƒ and the inverse-square-root in the RMSNorm.",
    "source": "2503.18002v2_Neuromorphic_Principles_for_Efficient_Large_Langua.pdf",
    "length": 2041,
    "tokens": 481
  },
  {
    "text": "Note that ExecTime (f) is estimated by aver- aging the execution times of the function obtained through 159 SoCC 21, November 1 4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al. 0 200 400 600 Time (ms) Exec Time(ms) Stage-wise SLO(ms) (a) Social Network. 0 Time (ms) Exec Time (ms) Stage-wise SLO (ms) 600 450 300 150 (b) Media Service. 0 Time (ms) Exec Time (ms) Stage-wise SLO (ms) 400 300 200 100 (c) Hotel Reservation. Figure 7: Slack for various Functions in each Application. offline profiling and StageSLO (f) is allotted in proportion to it. The batch size represents the number of requests that can be served by a function without violating the allotted stage-wise SLO. 4.3 Reactive Scaler (RS) Though the introduction of Request Batching 5 allows Kraken to reduce the containers provisioned, load mispredic- tions and probability miscalculations can still occur, leading to resource mismanagement, which could potentially affect the SLO compliance. To deal with this, Kraken also employs the RS 7 to scale containers up or down in response to re- quest overloading at containers (due to under-provisioning) and container over-provisioning, respectively. In case of inadequate container provisioning, the Overload Detector 7a in the RS 7 detects the number of allocated con- tainers for each DAG stage and calculates the estimated wait times of their queued requests (Algorithm 2 b ). If it detects requests whose wait times exceed the cost of spawning a new container (the cold start of the function), overloading is said to have occurred at the stage. In such a scenario, Kraken batches these requests ( _ğ‘‘ğ‘’ğ‘™ğ‘ğ‘¦ğ‘’ğ‘‘_ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘ in Algorithm 2) onto a newly-spawned container(s) (Algorithm 2 c ). This is because requests that have to wait longer than the cold start would be served faster at a newly created container than by waiting at an overloaded container.",
    "source": "kraken.pdf",
    "length": 1875,
    "tokens": 487
  },
  {
    "text": "Instead of backpropagation and gradient descent, other approaches, e.g., Extreme Learning Machines (ELMs) [6] and zeroth-order optimization (ZOO) [7], have been applied to on-device learning. However, ELM is only applicable to tiny-scale models consisting of one hidden layer [8], while ZOO suffers from a slow convergence and long training time [9]. This work presents InstantFT, an ultra-fast fine-tuning approach based on LoRA. As shown in Fig. 1, InstantFT runs significantly faster than the baseline LoRA approaches, without com- promising accuracy and robustness against data distribution shifts. 2 Baselines This section describes the baseline fine-tuning strategies. Table 1 presents a quantitative comparison between our InstantFT and baselines when applied to a LeNet-5-like model (Fig. 4) for two image classification datasets. We consider a simple L-layer network composed of fully-connected (FC) and convolution (Conv) layers, each having a weight and bias Wi, bi i 1,...,L. We denote the forward activation of the i-th layer by xi. The FC layer computes xi Wixi 1 bi, where xi 1, xi, bi Rd, Wi Rd d. The simplest approach is to fine-tune the entire network (FT-All) or only biases b1, . . . , bL (FT-Bias [10]). As illustrated in Fig. 2 (top right), another possible approach is to fine-tune parameters of the last layer (WL, bL) while keeping the rest frozen (FT-Last). While FT-Last requires significantly (854 1133x) less computational cost for backpropagation than FT-All, it only updates 1.4 of the pa- rameters and has a limited fine-tuning capability (Fig. 1). FT-Bias updates even fewer parameters than FT-Last, but it only reduces the backward FLOPs by 2.4 3.1x compared to FT-All, as it still requires gradients of forward activations dx.",
    "source": "2506.06505v1_InstantFT_An_FPGA-Based_Runtime_Subsecond_Fine-tun.pdf",
    "length": 1762,
    "tokens": 447
  },
  {
    "text": "[91] Feihui Li, Chrysostomos Nicopoulos, Thomas Richardson, Yuan Xie, Vijaykrishnan Narayanan, and Mahmut Kandemir. Design and management of 3d chip multiprocessors using network-in-memory. ACM SIGARCH Computer Architecture News, 34(2):130 141, 2006. [92] Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. The dawn after the dark: An empirical study on factuality hallucination in large language models. arXiv preprint arXiv:2401.03205, 2024. [93] Lei Li, Yankai Lin, Shuhuai Ren, Peng Li, Jie Zhou, and Xu Sun. Dynamic knowledge distillation for pre-trained language models. arXiv preprint arXiv:2109.11295, 2021. [94] Pengfei Li, Jianyi Yang, Mohammad A. Islam, and Shaolei Ren. Making ai less thirsty : Uncovering and addressing the secret water footprint of ai models. arXiv preprint arXiv:2304.03271, April 2023. Accessed: 2023-10-20. [95] Shiyao Li, Xuefei Ning, Ke Hong, Tengxuan Liu, Luning Wang, Xiuhong Li, Kai Zhong, Guohao Dai, Huazhong Yang, and Yu Wang. Llm-mq: Mixed-precision quantization for efficient llm deployment. In The Efficient Natural Language and Speech Processing Workshop with NeurIPS, volume 9, 2023. [96] Sean Lie. Cerebras architecture deep dive: First look inside the hardware software co-design for deep learning. IEEE Micro, 43(3):18 30, 2023. [97] Zi Lin, Diana Jin, and Saurabh Singh. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security (CCS), pages 465 482. ACM, 2022. [98] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of network pruning.",
    "source": "NSF_LLM_Medium_Proposal.pdf",
    "length": 1660,
    "tokens": 496
  },
  {
    "text": "As shown in Table I, the instruction formats use rd (destination register) and rs1 (source register) as 5-bit fields to address the FPU s 32 64 register file, with the most significant bit of the entire instruction distinguishing between scalar and packed-SIMD operations. The Snitch core s decoder and FPU subsystem are updated to support these instructions and seamlessly activate the ExpOpGroup in the FPU. C. Optimized Softmax Kernel To speed up the execution of the Softmax function on the enhanced Snitch cluster, we develop optimized software routines that exploit the underlying ISA of the Snitch cores, TABLE I SNITCH RISC-V ENCODINGS FOR FEXP AND VFEXP Format Encoding (32-bit) FEXP rd, rs1 001111100000{rs1}000{rd}1010011 VFEXP rd, rs1 101111100000{rs1}000{rd}1010011 NORM Loop for N: flh ft1, 0(a2) fdiv.h ft2, ft1, sum fsh ft2, 0(a2) addi a2, a2, 2 addi a3, a3, -1 bnez a3, loop for(i 0;i N;i ){ y[i] sum; } for(i 0;i N;i ){ y[i] exp(x[i]-max) sum y[i]; } for(i 0;i N;i ){ if(x[i] max_val) max x[i];} MAX Loop for N: flh ft1,0(a2) fmax.h max,ft1,max addi a2,a2,2 addi a3,a3,-1 bnez a3, loop EXP Loop for N 8: ssr ft1 read double ssr ft2 write double frep N 8,8 vfsub.h ft3,ft1,max vfsub.h ft4,ft1,max vfexp.h ft3,ft3 vfexp.h ft4,ft4 vfsgnj.h ft2, ft3 vfsgnj.h ft2, ft4 vfadd.h ft3, ft3 vfadd.h ft4, ft4 EXP Loop for N: Initialization flh ft0, 0(a0) fsub.h ft1, ft0, ft5 ... Exp approximation srli a2, ft1, 20 andi a2, a2, 2047 bgeu a2,1067,overflow fmul.d ft2, const1, ft1 fadd.d ft2, ft2, const2 fmul.d ft2, ft2, const3 fcvt.h.d ft1, ft2 Update y[i], sum Solve overflow ... MAX Loop for N 16: ssr ft0 read double frep N 16,4 vfmax.h ft3,ft3,ft0 vfmax.h ft4,ft4,ft0 vfmax.h ft5,ft5,ft0 vfmax.h ft6,ft6,ft0 NORM Loop for N 16: fdiv.h (1 sum),1,sum ssr ft0 read double ssr ft1 write double frep N 16,4 vfmul.h ft1,(1 sum),ft0 vfmul.h ft1,(1 sum),ft0 vfmul.h ft1,(1 sum),ft0 vfmul.h ft1,(1 sum),ft0 Baseline C code Baseline Assembly Optim Assembly Fig.",
    "source": "2504.11227v1_VEXP_A_Low-Cost_RISC-V_ISA_Extension_for_Accelerat.pdf",
    "length": 1962,
    "tokens": 793
  },
  {
    "text": "Furthermore, as our experiments in this section indicate, DFR_DPRR can achieve comparable or higher accuracy in comparison to the cutting-edge neural network-based methods as well as analog implementations. Finally, to allow future quantitative comparison of design tradeoffs among analog, digital, and other design styles, the implementation result of the proposed DFR_DPRR generated using Xilinx Vivado 2021.1 is summarized in Table 9. We confirmed that the proposed DFR_DPRR achieves exactly the same classification accuracy obtained in the Python simulations performed in Section 5.2. 6 CONCLUSION In this paper, we proposed a computationally efficient reservoir representation, namely DPRR, based on the dot product between features in the reservoir of shifted time. The proposed DPRR contributes to the implementation of accurate and compact hardware for reservoir-based classification. We also proposed a fully digital DFR using DPRR that is suitable for solving multivariate time-series classification tasks, which have been difficult to achieve high accuracy using reservoir computing methods. Through the FPGA implementation of the proposed DFR using DPRR, the proposed method was evaluated on 12 different multivariate time-series classification tasks. The DFR using DPRR outperformed conventional machine learning methods in terms of accuracy, despite its hardware resources in terms of power-delay product and memory usage being orders of magnitude smaller. The proposed model is therefore the most suitable among all investigated methods for solving multivariate time-series classification tasks via hardware. REFERENCES [1] Miquel L Alomar, Miguel C Soriano, Miguel Escalona-MorÃ¡n, Vincent Canals, Ingo Fischer, Claudio R Mirasso, and Jose L RossellÃ³. 2015. Digital implementation of a single dynamical node reservoir computer. IEEE Transactions on Circuits and Systems II: Express Briefs 62, 10 (2015), 977 981. [2] Lennert Appeltant, Miguel Cornelles Soriano, Guy Van der Sande, Jan Danckaert, Serge Massar, Joni Dambre, Benjamin Schrauwen, Claudio R Mirasso, and Ingo Fischer. 2011. Information processing using a single dynamical node as complex system. Nature Communications 2, 1 (2011), 1 6. [3] Lennert Appeltant, Guy Van der Sande, Jan Danckaert, and Ingo Fischer. 2014.",
    "source": "2504.11981v1_Hardware-Friendly_Delayed-Feedback_Reservoir_for_M.pdf",
    "length": 2293,
    "tokens": 493
  },
  {
    "text": "Seed GSM8K DROP SageBwd BF16 SageBwd BF16 42 0.5133 0.5125 0.7316 0.7364 233 0.5027 0.5042 0.7269 0.7295 1234 0.4973 0.4973 0.7329 0.7342 5678 0.5201 0.5208 0.7340 0.7332 1 0.5049 0.5057 0.7278 0.7404 Avg 0.5077 0.5081 0.7307 0.7348 Std 0.0090 0.0089 0.0032 0.0040 Table 6: Comparison of SageBwd and BF16 performance on MMLU and HellaSwag across different seeds on Qwen2.5 (1.5B). Seed MMLU HellaSwag SageBwd BF16 SageBwd BF16 42 0.5814 0.5873 0.9089 0.9065 233 0.5746 0.5785 0.9082 0.9049 1234 0.5805 0.5836 0.9025 0.9047 5678 0.5736 0.5693 0.9112 0.9053 1 0.5830 0.5823 0.9058 0.9075 Avg 0.5786 0.5802 0.9073 0.9058 Std 0.0043 0.0069 0.0033 0.0012 Table 7: Comparison of SageBwd and BF16 performance on GSM8K and DROP across different seeds on Qwen2.5 (3B). Seed GSM8K DROP SageBwd BF16 SageBwd BF16 42 0.5982 0.6232 0.7800 0.7812 233 0.5997 0.5974 0.7786 0.7812 1234 0.6156 0.6103 0.7786 0.7824 5678 0.6065 0.6012 0.7816 0.7853 1 0.6171 0.6073 0.7813 0.7832 Avg 0.6074 0.6079 0.7800 0.7827 Std 0.0001 0.0001 0.0000 0.0000 Table 8: Comparison of SageBwd and BF16 performance on MMLU and HellaSwag across different seeds on Qwen2.5 (3B).",
    "source": "2505.11594v1_SageAttention3_Microscaling_FP4_Attention_for_Infe.pdf",
    "length": 1138,
    "tokens": 520
  },
  {
    "text": "Note that, the Morton codes have already been generated as intermediate results during the geometry compression, thus, can be (re)used to identify the spatio-locality for the attribute compression without any extra cost. Speciï¬cally, as shown in Fig. 6, in order to capture the spatial locality in attributes (e.g., points with similar Morton codes tend to have similar colors), our proposed pipeline ï¬rst sorts the points using the Morton codes and then partition group them into multiple segments. The points within one segment are geometrically close to each other, and hence their attributes are also likely to be similar. Thanks to this, for each segment, we just need to store one medium value as Base and several residual values as Deltas (which are mostly small, due to similarity). And ï¬nally, we quantize these deltas for achieving higher compression ratio. In this example, two vectors (as there are two segments) store the ï¬nal data, including Mid 51,Delta [0,0] for the ï¬rst one segment, and Mid 54,Delta [0] for the second. 2) What are the Pros and Cons? : As indicated in Fig. 6, compared to RAHT, our proposal reuses the intermediate Morton codes, which have been computed during the geome- try compression, to precisely identify the points with similar attributes from a set of irregular points. This is expected to be much faster than RAHT, and in fact, our experimental results show 49 speedup (53ms vs 2.6s). However, as also shown in Fig. 6, the storage size after our compression is larger than RAHT, since each segment requires one vector storage to store its median base and (quantized) delta values. Although the 48 speedup brought by our proposal is promising in terms of performance gain, the observed 2 compression inefï¬ciency needs to be addressed. 3) How to Further Improve the Compression Efï¬ciency for Attributes? : Towards further improving the compression efï¬ciency, one could consider different options. Instead of throwing more compute power, we want to emphasize that, the discussion in this section only focuses on the attribute locality within one frame, which has ignored the potential localities among consecutive frames.",
    "source": "PCcompress.pdf",
    "length": 2162,
    "tokens": 485
  },
  {
    "text": "The library implicitly sep- arates prompt and token phases in the scheduler, so we are also interested to see if multiprocessing and MPS can achieve further GPU utilization in a mature scheduler implementation. 1) vLLM Scheduler Analysis: The scheduler in vLLM is executed at a step granularity where each step corresponds to a single token generation for a batch of inference requests. At each step, the scheduler makes a binary decision to run a batch of requests in the waiting queue , meaning their prompts need to be processed, or to select a batch of requests in the running queue , meaning they are in the middle of token generation. If the prompt phase batch is scheduled, the vLLM engine will then fetch and process the input tokens per request in the batch. If the token generation phase batch is selected, then the corresponding KV caches are fetched and processed. The remaining steps in pre-process, compute, and post-process the steps are identical regardless of the phase selected as they are to generate the next token. These steps are to merge the requests inputs into a single set of inputs tensors, process these merged tensors through the LLM Sampler to generate the next token, and finally separate the Sampler output per request. Before the next scheduler step is invoked, it will determine whether any requests are completed or if any new requests were received. It can be observed that vLLM currently implements contin- uous batching, which is where multiple requests are batched to execute simultaneously, but token and prompt phases are not. Therefore, an approach that conducts mixed batching where both token and prompt phases are scheduled and executed simultaneously would be the next step to improve scheduler throughput. Additionally, it should be noted that vLLM s batching implementation already increases the com- pute per kernel by merging a batch of requests into a single set of input tensors. Therefore, the key takeaways of the scheduler analysis are to explore mixed batching while also leveraging vLLM s batching strategy of merging tensors. 2) Attempt 1: vLLM Multiprocessing: We begin with a coarse-grained approach of running two instances of vLLM on separate processes, each receiving and processing its own set of inference requests. The goal is to let the GPU hardware scheduler manage the execution of these two processes such 3 Fig. 2.",
    "source": "2505.03763v1_Splitwiser_Efficient_LM_inference_with_constrained.pdf",
    "length": 2385,
    "tokens": 490
  },
  {
    "text": "Generality across hardware systems. In our study, we assumed fully connected GPUs with the same interconnect bandwidth between each pair. For a larger-scale system with more GPU nodes, different topologies including Mesh, Torus, and Tree topologies can be used. These topology choices will impact specific runtime but are orthogonal to our core insights, and can be modeled by changing the topology implementation. Further, while we assume full TP for Attention layers and full EP for FFN layers in this study, hybrid parallelism (such as using TP EP for FFN) have been proven to be useful in certain settings. Support can easily be added by incorporating current frameworks for hybrid parallelism [25]. Long sequence lengths. The experiments shown in the paper use a sequence length of 512. Longer sequences introduce new tradeoffs, particularly for Token-to-Expert Prediction. For FFN-based predictors, although computation remains parallelizable and overhead manageable, we observe a lower bound on achievable accuracy as sequence length increases. LSTM-based predictors are theoretically sequence-length agnostic in accuracy but suffer from poor parallelism and can hardly scale across different devices. Therefore, under long-sequence workloads, Distribution-Only Prediction may become more favorable due to its scalability and low complexity. Kernel underutilization at small scale. To enable fast evaluation, we use small batch sizes and short sequences, which can expose low-level inefficiencies in kernel execution, such as insufficient overlap between prologue epilogue and the main MMA loop, which leads to underutilization of Tensor Core FLOPs. Our simulator is throughput-oriented and assumes at least one hardware unit (in this case, compute) is saturated. To validate our findings, we compared simulated results against actual GPU measurements for both model and predictor inference. Although the absolute runtimes differ, the relative overhead between prediction and inference is consistent. Thus, we report and analyze prediction overhead as a ratio to the simulated inference runtime for an accurate estimation. Expert duplication s communication overhead. We also quantify the communication overhead of expert duplication to show that we can hide its latency with the Attention layers. For Mixtral 8 7B, each FP16 expert contains approximately 4,096 14,336 2 2 bytes of weight data.",
    "source": "2506.07366v1_MoE-GPS_Guidlines_for_Prediction_Strategy_for_Dyna.pdf",
    "length": 2401,
    "tokens": 465
  },
  {
    "text": "The physical input range without switching the cell is 0.6V to 0.6V . In order to be able to interpret the computation done by the memristor crossbar, we need to map the resulting output currents back to the desired value space. We introduce a correction factor c with the unit 1 A so that for the difference between the output currents of a cell on the positive bitline I and negative bitline I the following holds for any x in [-1, 1]: I high(x 0.6V ) I low(x 0.6V ) c x (1) I low(x 0.6V ) I low(x 0.6V ) c 0 (2) I low(x 0.6V ) I high(x 0.6V ) c x (3) Table 1: Computation results of 10000 single simulated cells set to perform the given multiplication in a [0,1] normalized space for input and weight. The first three rows show the non- linearity w.r.t input voltage on a single fixed device. Row 4 shows the noise with zero weight due to the paired subtraction of two crossbar columns necessary. Row 5 shows the increased uncertainty when trying to set half conductance states. operation average std-dev min max 1.0 x 1 1.016 0.063 0.707 1.27 0.1 x 1 0.989e 1 0.062e 1 0.688e 1 1.24e 1 0.01 x 1 0.985e 2 0.069e 2 0.721e 2 1.20e 2 1.0 x 0 8.52e 4 4.75e 2 0.2751 0.2455 1.0 x 0.5 0.476 0.094 0.109 1.06 We write Ihigh for a cell in high conductance state, and Ilow for a low conductance state. For the paired memristor model in Synaptogen, the optimal c w.r.t. a quadratic error was 8020 1 A. Given that the memristors can not be precisely set to the desired conductance states, we obtain large deviations in the computa- tions. Table 1 shows the results for the different basic operations.",
    "source": "2505.24721v1_Running_Conventional_Automatic_Speech_Recognition_.pdf",
    "length": 1592,
    "tokens": 474
  },
  {
    "text": "Ultimately, more advanced IP protection schemes are called for. II. BACKGROUND A. LLMs for Hardware Design LLMs have revolutionized hardware design, particularly in the area of Verilog coding [18]. Frameworks such as RTL- Coder [17] leverage GPT to create instruction-code pairs from carefully curated datasets, demonstrating superior performance compared to GPT-3.5 in benchmark evaluations. Domain adaptation has emerged as a key strategy, with approaches like VeriGen [19] running FT with CodeGen-16B [20] on specialized Verilog repositories, or ChipNemo [7] enhancing LLaMA2 [21] by using both public resources and proprietary NVIDIA designs. These advancements clearly demonstrate that FT techniques and strategic data augmentation are es- sential for high-quality LLM-driven hardware design. Recent work has also considered agentic systems [22], [23]. Furthermore, verification has progressed through specialized adaptations [24] [26], while assertion techniques [27], [28] extend LLMs toward formal verification. Strategic prompt engineering [8], [29], [30] enhances performance, with for example [7], [8], [31] automating complex EDA tasks. Eval- uation frameworks address key challenges like reproducibility through metrics, and are utilized in RTLLM [32], VerilogEval [33], [34], and OpenLLM-RTL [35]. B. Security Concerns with LLMs LLMs have shown remarkable proficiency in code genera- tion and other tasks. However, their indiscriminate integration introduces significant vulnerabilities [36]. For example, [13], [14], [37], [38] all show that LLMs can inadvertently expose sensitive information, making privacy a critical concern. There are three types of privacy attacks on LLMs. First, membership inference attacks [15], [39], [40] attempt to determine whether specific code samples were part of an LLM s training dataset. Second, backdoor attacks [11], [41] inject malicious code snippets into the training dataset, compromising the model to generate insecure codes. Third, data extraction attacks [39], [42], [43] extract sensitive information from model outputs or internal representations. When users gain access to FT models, they can extract personally identifiable information or proprietary IP, posing a significant risk.",
    "source": "2503.13116v4_VeriLeaky_Navigating_IP_Protection_vs_Utility_in_F.pdf",
    "length": 2245,
    "tokens": 480
  },
  {
    "text": "In ISWC. IEEE. [59] Mohammad Rostami, Jeremy Gummeson, Ali Kiaghadi, and Deepak Ganesan. 2018. Polymorphic radios: A new design paradigm for ultra- low power communication. In Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication. 446 460. [60] Fernando Moya Rueda, RenÃ© Grzeszick, Gernot A. Fink, Sascha Feld- horst, and Michael ten Hompel. 2018. Convolutional Neural Networks for Human Activity Recognition Using Body-Worn Sensors. Informat- ics (2018). [61] Rachid Saadane, Abdellah Chehri, Seunggil Jeon, et al. 2022. AI-based modeling and data-driven evaluation for smart farming-oriented big data architecture using IoT with energy harvesting capabilities. Sus- tainable Energy Technologies and Assessments 52 (2022), 102093. [62] Alanson P Sample, Daniel J Yeager, Pauline S Powledge, Alexander V Mamishev, and Joshua R Smith. 2008. Design of an RFID-based battery- free programmable sensing platform. IEEE transactions on instrumen- tation and measurement 57, 11 (2008), 2608 2615. [63] Himanshu Sharma, Ahteshamul Haque, and Zainul Abdin Jaffery. 2019. Maximization of wireless sensor network lifetime using solar energy harvesting for smart agriculture monitoring. Ad Hoc Networks 94 (2019), 101966. [64] Zhuoran Song, Bangqi Fu, Feiyang Wu, Zhaoming Jiang, Li Jiang, Naifeng Jing, and Xiaoyao Liang. 2020. DRQ: dynamic region-based quantization for deep neural network acceleration. In ISCA. IEEE. [65] Ben Taylor, Vicent Sanz Marco, Willy Wolff, Yehia Elkhatib, and Zheng Wang. 2018. Adaptive deep learning model selection on embedded systems. ACM SIGPLAN Notices 53, 6 (2018), 31 43. [66] Texas Instrument Micro-controller with FRAM 2022.",
    "source": "Seeker.pdf",
    "length": 1688,
    "tokens": 461
  },
  {
    "text": "Additionally, branching architectures where a model has mul- tiple branches at different stages can improve performance by 7 enabling the model to capture multi-scale features. However, these designs also increase computational complexity and memory demands. In general, more complex network architectures can yield better performance but at the cost of higher computational overhead and memory requirements. When deploying these models on resource-constrained edge devices, there is a need to balance complexity with efficiency. Model structure com- pression aims to reduce the computational burden and memory usage by optimizing the network s design or eliminating redundant components. These techniques focus on the archi- tectural level of the network and are essential for creating more efficient, deployable models. 1) Lightweight Networks: Lightweight networks are neural network models designed to operate efficiently in resource- constrained environments, aiming to reduce model size, com- putational complexity, and memory usage while preserving high performance. Szegedy et al. [45] introduced GoogleNet, which featured the Inception module a multi-scale convolutional layer design that employs 1 1, 3 3, and 5 5 convolutional kernels in parallel. This approach captures multi-scale features while keeping computational cost low. By repeating the Inception module across the network and combining it with pooling layers, they successfully increased both the depth and width of the model without significantly increasing the computational load. Similarly, Iandola et al. [46] proposed SqueezeNet, which uses a novel Fire module to build a lightweight model. The Fire module consists of a squeeze layer (using 1 1 convolutions to reduce feature map dimensions) and an expand layer (combining 1 1 and 3 3 convolutions), making it efficient and compact. The MobileNet series has also gained significant attention for its lightweight design. Howard et al. [47] introduced MobileNet v1, which uses depthwise separable convolutions, splitting traditional convolution into two parts: Depthwise Convolution and Pointwise Convolution. This reduces the number of parameters and computational complexity, while allowing flexibility in latency-accuracy trade-offs through two simple hyperparameters. MobileNet v2 [48] further optimized the network by introducing linear bottlenecks and inverted residuals, enabling deeper yet smaller and faster networks. MobileNet v3 [49] refined MobileNet v2 by removing certain layers, reducing computational overhead without sacrificing accuracy. Han et al.",
    "source": "2505.13461v1_FPGA-based_Acceleration_for_Convolutional_Neural_N.pdf",
    "length": 2592,
    "tokens": 490
  },
  {
    "text": "While beneficial for systematic analysis, this limited search space also creates an opportunity: the dataset size is suitable for developing zero-cost estimation methods to quickly predict architecture robustness against analog noise, facilitating exploration of substantially larger and more diverse search spaces in the future. Second, our benchmark is limited to convolution-based network architectures. Although these architectures naturally align with analog crossbar implementations, in their im2col (Chellapilla et al., 2006) format, transformer-based models have recently gained significant attention due to their extensive matrix-vector multiplications and large parameter counts, which can benefit substantially from AIMC stationary computations, i.e., avoiding weight loading (Spoon et al., 2021; Sridharan et al., 2023). Thus, incorporating a dedicated transformer-based search space into the benchmark is crucial for extracting insights into architectural robustness specific to transformer models. Lastly, all current evaluations assume a fixed hardware configuration, detailed in the appendix, and are primarily based on Phase-Change Memory (PCM) device characteristics for noise analysis. Future extensions should explore mappings of neural architectures onto diverse analog hardware platforms, including Resistive RAM (RRAM), Ferroelectric RAM (FeRAM), Electrochemical RAM (ECRAM), and other emerging devices (Joshi et al., 2019). Additionally, expanding to heterogeneous systems that combine analog and digital hardware components will provide deeper insights into the architecture-hardware interplay. Exploring these variations will significantly broaden the applicability and robustness insights of AnalogNAS-Bench across multiple analog computing scenarios. 6 Conclusion In this paper, we introduced AnalogNAS-Bench, the first NAS benchmark tailored for AIMC, ex- tending NAS-Bench-201 with AIMC-specific constraints to enable a fair evaluation of architectures under analog noise conditions, bridging a critical gap in existing NAS benchmarks that overlook AIMC non-idealities. Through our analysis, we highlighted the limitations of conventional quan- tization techniques in capturing AIMC-specific noise and identified key architectural traits that enhance resilience: increased skip connections, wider layers, and a preference for larger convolu- tional operations over narrower ones. With this benchmark in place, more NAS algorithms and estimators will be developed by researchers, making it more practical to compare them in AIMC scenarios. 9 Acknowledgements.",
    "source": "2506.18495v1_AnalogNAS-Bench_A_NAS_Benchmark_for_Analog_In-Memo.pdf",
    "length": 2588,
    "tokens": 477
  },
  {
    "text": "To break this circular de- pendency, we take inspiration from prior work in HAR [47], and use the highly stable temporal continuity of human ac- tivity (relative to the tens of milliseconds timescales for HAR 10 0 0.05 0.1 0.15 0.2 k 15 k 12 k 10 k 8 Activity Aware Coresets (HAR) Energy Aware Coresets (Bearing) Fixed Coresets Data Volume normalized to raw data (a) Data volume with dynamic coresets 0.5867 0.4106 0.2837 0.9139 0.8516 0.6076 0.8742 0.8073 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 WiFi Office Wifi Home Piezo Daily Movement Completion in Origin (HAR) Completion in Seeker (HAR) Completion in Seeker (Bearing) N A (b) Fraction of inferences completed with different EH sources 6.18 6.96 7.12 21.76 19.46 12.57 31.27 28.76 22.56 34.16 32.95 35.19 6.63 11.87 22.56 0 10 20 30 40 50 60 70 80 90 100 Wifi Office Wifi Home Piezo Daily Movement Power Trace: Memoization DNN 16bit DNN 12 Bit Clustering Importance Sampling (c) Distribution of compute off-load to different components Figure 11: Accuracy and communication efficiency of Seeker with different data sets and its sensitivity towards various EH sources. -0.45 -0.24 -0.02 -0.07 -0.14 -0.19 -0.50 -0.40 -0.30 -0.20 -0.10 0.00 0 50 100 Walking Climbing Cycling Running Jogging Jumping Geo Mean Error Accuracy ( ) Coreset: Compressed Coreset: Reconstructed Reconstructed with Larger Model Seeker Baseline: EAP Baseline: Origin Baseline: Large DNN Error vs Fully Powered (a) Accuracy with MHEALTH dataset -0.26 -0.19 -0.76 -0.28 -0.04 -0.80 -0.60 -0.40 -0.20 0.00 0 20 40 60 80 100 Walking Climbing Cycling Running Jumping Geo Mean Error Accuracy ( ) Coreset: Compressed Coreset: Reconstructed Reconstructed with Larger Model Seeker Baseline: EAP Baseline: Origin Baseline: Large DNN Error vs Fully Powered (b) Accuracy with PAMAP2 dataset Figure 12: Accuracy and communication efficiency of Seeker with different data sets and sensitivity study. inferences) to predict the current activity based on previously completed local inferences.",
    "source": "Seeker.pdf",
    "length": 2006,
    "tokens": 589
  },
  {
    "text": "Contribution II : Score Predictor - We aim to demon- strate that instruction-accurate simulators can be used for per- formance analysis, using autotuning workloads as an example. To the best of our knowledge, we are the first to show how performance analysis can be conducted with fast instruction- accurate simulators. Many open-source implementations of instruction-accurate simulators exist for different architectures, e.g., QEMU [10] or gem5 [11]. Figure 1 II illustrates this idea. A predictor gets statistics from an instruction-accurate simulator as input and calculates a score based on reference values measured on real hardware. The score is then returned to the autotuning framework. Since an instruction-accurate simulator does not provide accurate timing, we do not aim to predict execution latencies. Instead, our approach utilizes scores to evaluate and com- pare different implementations of the same workload. These scores are essential for guiding the autotuning process but are not suitable for comparing different types of workloads. We employ multiple predictors for this task: Multiple Linear Regression (MLR) [12], Deep Neural Networks (DNNs) [13], Bayesian optimization [14], and XGBoost [15]. We tune and compare them to identify the most accurate predictions for common ML kernels. We evaluate our approach on different CPU architectures, namely x86, ARM, and RISC-V. or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. including reprinting republishing this material for advertising or promotional purposes, collecting new collected works for resale 2025 IEEE: Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, PREPRINT - Accepted for publication at the 62th Design Automation Conference (DAC), June 22-25, 2025, in San Francisco. II. BACKGROUND AND RELATED WORK A. Autotuning in TVM Apache TVM [3] is an open-source ML compiler framework designed to optimize computations across various hardware platforms. In TVM, each operation, called kernel in this work, can be expressed in different abstractions, e.g., in Tensor Ex- pression (TE) or in Tensor Intermediate Representation (TIR). For example, a kernel can be a Neural Network (NN) layer.",
    "source": "2505.13357v1_Introducing_Instruction-Accurate_Simulators_for_Pe.pdf",
    "length": 2277,
    "tokens": 491
  },
  {
    "text": "W1 W4 W7 W2 W5 W8 W3 W6 W9 A3 A2 A1 A5 A4 0 A7 0 0 W1A1 W4 W7 W2 W5 W8 W3 W6 W9 A4 A3 A2 A6 A5 A4 A8 A7 0 C3 C2 C1 C5 C4 B6 C7 B9 B8 D1 C3 C2 C6 C5 C4 C8 C7 B9 t 0 t 1 t 7 t 6 W1 W4 W7 W8 W5 Placing I-Txn-Var in the SA Grid Tile Level Acclerator Level Global Scratchpad Banked 1 - 4 T12 T13 T14 T15 T8 T9 T10 T11 T4 T5 T6 T7 T0 T1 T2 T3 Weights Output Stream Banked 1 - 4 Input Stream Input Stream Input Stream Input Stream Banked 1 - 4 T12 T13 T14 T15 T8 T9 T10 T11 T4 T5 T6 T7 T0 T1 T2 T3 Weights Output Stream Banked 1 - 1 Input Stream Input Stream Input Stream Input Stream Global Scratchpad Banked 1 - 1 Banked 1 - 1 Power Level -1 Power Level -2 Power Level -3 Tile n W1 W4 W7 X2 W5 W8 X3 X6 X9 A3 A2 A1 A5 A4 0 A7 0 0 X1 X4 X7 X8 X5 W1 W4 W7 W2 W5 W8 W3 W6 W8 A3 A2 A1 A5 A4 0 A7 0 0 W1 W4 W7 W8 W5 Tile 2 W1 W4 W7 V2 W5 W8 V3 V6 V9 A3 A2 A1 A5 A4 0 A7 0 0 V1 V4 V7 V8 V5 Tile 1 A3 A2 A1 I-Txn-00 A6 A5 A4 I-Txn-01 A9 A8 A7 I-Txn-02 Tile 0 W1 W4 W7 U2 W5 W8 U3 U6 U9 A3 A2 A1 A5 A4 0 A7 0 0 U1 U4 U7 U8 U5 Input Stream Global Scratchpad Banked 1 - 16 Banked 1 - 16 T12 T13 T14 T15 T8 T9 T10 T11 T4 T5 T6 T7 T0 T1 T2 T3 Weights Output Stream Banked 1 - 16 Input Stream Input Stream Input Stream Input Stream C1 C2 C3 C4 A5 A6 C7 A8 A9 B1 B2 B3 B4 A5 A6 B7 A8 A9 A1 A2 A3 A4 A5 A6 A7 A8 A9 W1 W2 W3 W4 W5 W6 W7 W8 W9 Weight Stationary Convolution Toy Example PE Level Fig. 6: Weight stationary compute mapping. The PE-level shows how the input ï¬‚ows and the convolutions are computed with a 3x3 convolution toy example.",
    "source": "Usas.pdf",
    "length": 1523,
    "tokens": 679
  },
  {
    "text": "A for-loop can be vectorized if and only if all of its callbacks can be vectorized. A vectorization scheme is legal if and only if all of its for-loops can be vectorized. Vector extensions such as Arm SVE [62] provide instructions to vectorize most callbacks in embedding operations, making the space of legal vectorization schemes large. However, prior work has demonstrated that the most efficient vectorization scheme for sparse-dense tensor multiplication is inner-loop vectorization, as- suming the dense tensor is in row-major order and has rows which are larger than the vector length [61]. Embedding operations gen- erally satisfy these assumptions (Section 2). Hence, similarly to the MLIR sparsifier [8], Ember only attempts inner-loop vectorization. If the inner for-loop is legal, Ember vectorizes its access and execute code in two steps. Ember starts by vectorizing the inner for-loop and its streams. As the stream-to-value operations in the callbacks expect stream types, the algorithm adds a temporary cast operation for source materialization. Then, during the callback vec- torization step, Ember recursively vectorizes the uses of these cast operations, producing full SLCV code. To keep the conversion step simple, load and store operations into callbacks are firstly converted to vector gather and scatter operations, with vector indices and masks coming from its parent vectorized for-loop operation. A fur- ther transformation pass simplifies these operations into contiguous vector loads and stores. 7.2 Bufferization Bufferization allows to marshal and compute embedding vectors as compound types. As shown in Figure 14c, the access unit pushes, in the control queue, one ğ‘’ğ‘’(embedding-vector end) token for each embedding vector and, in the data queue, the position of the output embedding vector and all of its values. As the length of the embedding vector (emb_len) is constant, once the core reads an ğ‘’ğ‘’token, it pops emb_len elements with a vectorized for-loop. Bufferization greatly improves marshaling and compute efficiency, especially for long embedding vectors.",
    "source": "2504.09870v1_Ember_A_Compiler_for_Efficient_Embedding_Operation.pdf",
    "length": 2096,
    "tokens": 461
  },
  {
    "text": "E CLIP Benchmark Full Result In this section we provide a detailed breakdown on the accuracy of each dataset for CarbonCLIP and the evaluated baselines in Table 6. F Carbon Footprint Breakdown We provide a breakdown of the carbon footprint for each variant of the CarbonCLIP family model. As shown in Figure 11, as the model increases in size, the proportion of operational carbon in the overall carbon footprint of the model increases from 20 to over 40 . The CarbonCLIP-XL model has 3 the number of parameters and almost 3 the latency of CarbonCLIP-XS, but the selected hardware architecture only has double the number of compute PEs. Therefore, the operational carbon increases proportionally more than the increase in embodied carbon. This highlights need of co-optimizing the model and hardware architecture to maintain an intricate balance between operational and embodied carbon, keeping the overall carbon footprint of the system low. As such, the expected lifetime and source of power are also important factors that need to be taken into account during the optimization process. 18 Table 4 The hardware and model architecture configuration found by each optimization metric at each accuracy point. Hardware configurations are specified in the format of: {TC, pex, pey, L2, L2bw, glb}.",
    "source": "2505.01386v2_Carbon_Aware_Transformers_Through_Joint_Model-Hard.pdf",
    "length": 1294,
    "tokens": 257
  },
  {
    "text": "Modest adaboost- teaching adaboost to generalize better. In Graphicon, pages 987 997, 2005. [78] Jasper A Vrugt and Bruce A Robinson. Treatment of uncertainty using ensemble methods: Comparison of sequential data assimilation and bayesian model averaging. Water Resources Research, 43(1), 2007. [79] Cheng Wang, Bhuvan Urgaonkar, Neda Nasiriani, and George Kesidis. Using burstable instances in the public cloud: Why, when and how? SIGMETRICS, June 2017. [80] Wei Wang, Jinyang Gao, Meihui Zhang, Sheng Wang, Gang Chen, Teck Khim Ng, Beng Chin Ooi, Jie Shao, and Moaz Reyad. Raï¬ki: machine learning as an analytics service system. Proceedings of the VLDB Endowment, 12(2):128 140, 2018. [81] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, et al. Huggingface s transformers: State-of- USENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation 1055 the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. [82] Carole-Jean Wu, David Brooks, Kevin Chen, Douglas Chen, Sy Choud- hury, Marat Dukhan, Kim Hazelwood, Eldad Isaac, Yangqing Jia, Bill Jia, et al. Machine learning at facebook: Understanding inference at the edge. In 2019 IEEE International Symposium on High Performance Computer Architecture (HPCA), pages 331 344. IEEE, 2019. [83] Neeraja J. Yadwadkar, Francisco Romero, Qian Li, and Christos Kozyrakis. A case for managed and model-less inference serving. In Proceedings of the Workshop on Hot Topics in Operating Systems, New York, NY, USA, 2019. Association for Computing Machinery. [84] Tien-Ju Yang, Andrew G. Howard, Bo Chen, Xiao Zhang, Alec Go, Vivienne Sze, and Hartwig Adam.",
    "source": "cocktail.pdf",
    "length": 1741,
    "tokens": 489
  },
  {
    "text": "[5] Tixiao Shan, Brendan Englot, Drew Meyers, Wei Wang, Carlo Ratti, and Daniela Rus. LIO-SAM: Tightly- coupled Lidar Intertial Odometry via Smoothing and Mapping. In Proceedings of the IEEE RSJ International Conference on Intelligent Robots and Systems (IROS), pages 5135 5142, October 2020. [6] Han Wang, Chen Wang, Chun-Lin Chen, and Lihua Xie. F-LOAM: Fast LiDAR Odometry and Mapping. In Proceedings of the IEEE RSJ International Conference on Intelligent Robots and Systems (IROS), pages 4390 4396, September 2021. [7] Xingyi Zhou, Vladlen Koltun, and Philipp KrÃ¤henbÃ¼hl. Tracking Objects as Points. In Proceedings of the European Conference on Computer Vision (ECCV), pages 474 490, August 2020. [8] Tianwei Yin, Xingyi Zhou, and Philipp KrÃ¤henbÃ¼hl. Center-Based 3D Object Detection and Tracking. In Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11784 11793, June 2021. [9] Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 652 660, July 2017. [10] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas Guibas. PointNet : Deep Hierarchical Feature Learning on Point Sets in a Metric Space. In Proceedings of the Advances in Neural Information Processing Systems (NIPS), pages 5099 5108, December 2017. [11] Yiqun Lin, Zizheng Yan, Haibin Huang, Dong Du, Ligang Liu, Shuguang Cui, and Xiaoguang Han. FPConv: Learning Local Flattening for Point Convolution. In Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4293 4302, June 2020. [12] Mutian Xu, Runyu Ding, Hengshuang Zhao, and Xiaojuan Qi.",
    "source": "2506.00438v1_PointODE_Lightweight_Point_Cloud_Learning_with_Neu.pdf",
    "length": 1774,
    "tokens": 491
  },
  {
    "text": "12( c) shows direct quantization severely degrades output quality, producing results substantially different from full-precision outputs. Our analysis reveals that the issue occurs because microscaling NVFP4 quantization requires the scale factor to be represented in E4M3 FP8 format [7], rather than the FP32 data type typically used for scale factors. This causes accuracy loss when the scale factor is directly converted to E4M3 format. To better understand this accuracy loss, we analyze the data distribution of eP and its scale factors in Fig. 3. Since eP is computed using online softmax [8], the values in each microscaling block ePij fall [0, 1]. Consequently, the scale factor (scale factor max(ePij) 6) ranges between 0 and 0.167. This narrow range leads to inefficient usage of E4M3 s representable range, increasing accuracy loss. To reduce accuracy loss by fully utilizing E4M3 s range, we propose a two-level quantization method for the eP matrix. Specifically, we first quantize each row of eP to [0, 448 6]. Then we apply the standard FP4 quantization Ï• for the quantized eP. The two-level quantization can be formulated as follows: sP1 rowmax(eP) (448 6), eP2 eP sP1, sP2, Ë†P2 Ï•(eP2) (eP Ë†P2 sP2 sP1), O FP4MM(Ë†P2, sP2, Ë†V, sV) sP1 (5) Where eP, eP2, and sP1 are in FP32 data type. sP2 and sV are in FP8 data type. Ë†P2 and Ë†V are in FP4 data type. Empirical results: As shown in Fig. 3, our two-level quantization maximizes the E4M3 range utilization for sP, thereby reducing both the numerical representation error of sP and the quantization error of eP. A more formal theoretical analysis is provided in the Appendix. Table 1(b) shows the accuracy of 4 two-level quantization against naive direct quantization, using real Q, K, V from layers of CogVideoX.",
    "source": "2505.11594v1_SageAttention3_Microscaling_FP4_Attention_for_Infe.pdf",
    "length": 1775,
    "tokens": 490
  },
  {
    "text": "[14] K. Han, Z. Fang, P. Diefenbaugh, R. Forand, R. R. Iyer, and D. Newell, Using Checksum to Reduce Power Consumption of Display Systems for Low-motion Content, in 2009 IEEE International Conference on Computer Design, 2009, pp. 47 53. [15] K. Han, A. W. Min, N. S. Jeganathan, and P. S. Diefenbaugh, A Hybrid Display Frame Buffer Architecture for Energy Efï¬cient Display Subsystems, in International Symposium on Low Power Electronics and Design (ISLPED), 2013, pp. 347 353. [16] T. HARDWARE, Magic Leap One Powered by Nvidia Tegra TX2, Available Summer. , 2019. [17] B. Haynes, A. Mazumdar, A. Alaghi, M. Balazinska, L. Ceze, and A. Cheung, LightDB: A DBMS for Virtual Reality Video, Proc. VLDB Endow., pp. 1192 1205, 2018. [18] J. He, M. A. Qureshi, L. Qiu, J. Li, F. Li, and L. Han, Rubiks: Practical 360-Degree Streaming for Smartphones, in Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services, 2018, pp. 482 494. [19] HEADJACK, The Best Encoding Settings For Your 4k 360 3D VR Videos FREE Encoding Tool, encoding-settings-resolution-for-4k-360-3d-vr-videos . [20] C. Heather Bellini, W. Chen, M. Sugiyama, M. Shin, S. Alam, and D. Takayama, Virtual and Augmented Reality. innovation-folder virtual-and-augmented-reality report.pdf , 2016. [21] L. F. Hodges, Tutorial: Time-multiplexed Stereoscopic Computer Graphics, IEEE Computer Graphics and Applications, pp. 20 30, 1992. 252 [22] A. Holdings, White Paper: 360-Degree Video Rendering. blog posts white-paper-360-degree-video-rendering , 2019.",
    "source": "DejaView.pdf",
    "length": 1551,
    "tokens": 485
  },
  {
    "text": "5.1 Random Matrices We evaluate the performance of the proposed algorithm on random ğ‘š ğ‘šmatrices. In this subsection, to facilitate comparison with the Hcmvmalgorithm, we adopt a convention from [3] where a ğ‘ğ‘¤-bit random matrix is generated by sampling integers uniformly from [2ğ‘ğ‘¤ 1 1, 2ğ‘ğ‘¤ 1]. ğ‘‘ğ‘ 1 (no delay constraint) ğ‘‘ğ‘ 0 ğ‘‘ğ‘ 2 da4ml Hcmvm [3] da4ml Hcmvm [3] da4ml Hcmvm [3] N step adder cpu [ms] step adder cpu [ms] step adder cpu [ms] step adder cpu [ms] step adder cpu [ms] step adder 2 3.3 8.7 0.1 4.4 8.2 1.0e1 3.1 9.9 0.1 3.1 8.8 1.0e1 3.3 8.7 0.1 3.7 8.2 4 6.1 29.3 0.3 7.8 27.6 4.8e2 4.1 37. 0.3 4.1 32.1 4.7e2 5.9 30. 0.3 5.7 28.1 6 8.4 59. 0.6 10. 57.3 3.3e3 5. 77.8 0.8 5. 66.8 3.8e3 6.7 62.6 0.6 7. 58.2 8 9.4 98. 1.3 11.9 96.3 1.5e4 5.1 130.9 2. 5.1 117.2 1.7e4 7. 102.3 1.4 7.1 99.5 1 10.8 146.6 2.7 13.2 143.5 5.4e4 6. 195.6 4.2 6. 157.7 8.2e4 7.8 152.8 2.8 8. 146.9 12 11.6 203.6 4.8 14.6 200.4 1.7e5 6. 271.8 7.9 6. 241.6 1.7e5 8. 214.9 5.2 8.",
    "source": "2507.04535v1_da4ml_Distributed_Arithmetic_for_Real-time_Neural_.pdf",
    "length": 964,
    "tokens": 501
  },
  {
    "text": "13 SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired Symbolic Reasoning B AST Template The following section demonstrate how applying AST templates transforms the code and apply optimization patterns. 1 module example_raw 2 ( parameter BW 8) 3 ( 4 input [BW-1:0] a, 5 input [BW-1:0] b, 6 input [BW-1:0] c, 7 input [BW-1:0] d, 8 output [BW-1:0] s1 9 ); 10 assign s2 a b; 11 assign s3 a b d; 12 assign s4 c d b a; 13 assign s5 a - b; 14 assign s6 (b 1) a d c -b; 15 assign s1 a 23; 16 endmodule Listing 4: Example Test Case: dead code elimination. 1 module example_raw 2 (parameter BW 8) 3 ( 4 input [BW-1:0] a, 5 input [BW-1:0] b, 6 input [BW-1:0] c, 7 input [BW-1:0] d, 8 output [BW-1:0] s1 9 ); 10 assign s1 a 23; 11 endmodule Listing 5: Example Test Case: dead code elimination after applying the Dead Code Elimination AST template. 1 module example_raw 2 ( parameter BW 8) 3 ( 4 input [BW-1:0] a, 5 input [BW-1:0] b, 6 input [BW-1:0] c, 7 input [BW-1:0] d, 8 output [BW-1:0] s1, 9 output [BW-1:0] s2, 10 output [BW-1:0] s3, 11 output [BW-1:0] s4, 12 output [BW-1:0] s5, 13 output [BW-1:0] s6 14 ); 15 assign s1 a b; 16 assign s2 a b; 17 assign s3 a \\ b d; 18 assign s4 c d b a; 19 assign s5 a - b; 20 assign s6 (b 1) a d c -b; 21 endmodule Listing 6: Example Test Case: subexpression elimination.",
    "source": "2504.10369v1_SymRTLO_Enhancing_RTL_Code_Optimization_with_LLMs_.pdf",
    "length": 1320,
    "tokens": 483
  },
  {
    "text": "Symp. Biomed. Imag., 2024, pp. 1 5. [14] O. B. Demirel, et al., 20-fold accelerated 7T fMRI using referenceless self-supervised deep learning reconstruction, in Proc. 43rd Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. (EMBC), 2021, pp. 3765 3769. [15] L. Vizioli, et al., Lowering the thermal noise barrier in functional brain mapping with magnetic resonance imaging, Nature Communications, vol. 12, no. 1, pp. 1 15, 2021. [16] K. P. Pruessmann, M. Weiger, M. B. Scheidegger, and P. Boesiger, SENSE: Sensitivity encoding for fast MRI, Magn. Reson. Med., vol. 42, no. 5, pp. 952 962, 1999. [17] Y. Ma, Y. Cao, S. Vrudhula, and J.-s. Seo, Automatic compilation of diverse CNNs onto high-performance FPGA accelerators, IEEE Trans. Comput.-Aided Design Integr. Circuits Syst., vol. 39, no. 2, pp. 424 437, Feb. 2018. [18] Y. Ma, N. Suda, Y. Cao, S. Vrudhula, and J.-s. Seo, ALAMO: FPGA acceleration of deep learning algorithms with a modularized RTL compiler, Integration, vol. 62, pp. 14 23, 2018. [19] C. Zhang, G. Sun, Z. Fang, P. Zhou, P. Pan, and J. Cong, Caffeine: Toward uniformed representation and acceleration for deep convolutional neural networks, IEEE Trans. Comput.-Aided Design Integr. Circuits Syst., vol. 38, no. 11, pp. 2072 2085, Nov. 2019. [20] H.-s. Suh, J. Meng, T. Nguyen, V. Kumar, Y. Cao, and J.-S. Seo, Algorithm-hardware co-optimization for energy-efficient drone detec- tion on resource-constrained FPGA, ACM Trans. Reconfig. Technol. Syst., vol. 16, no.",
    "source": "2506.03183v1_Edge_Computing_for_Physics-Driven_AI_in_Computatio.pdf",
    "length": 1475,
    "tokens": 496
  },
  {
    "text": "Table 22: NEXAI IP blocks in Edge SoCs Aspect Value Units Performance (dense) 12,583 TFLOPS Target CMOS process TSMC N2 node Logic density 313 MTr mm2 Weights and activations format: FP4 4 bits Primary NEXAI clock 1.5 GHz Processing Element (PE) area 0.77 Âµm2 HILT unit cell area 0.012 Âµm2 HILT area overhead (including latch tree) 22 CASCADE local clock speed 12 GHz Rest of NEXAI IP block clock speed 1.5 GHz Batch size x input token length in HILT 4,096 B x L Active CASCADE array columns 512 columns Spare CASCADE columns for CREST 8 columns Columns per CASCADE array 520 columns Rows per CASCADE array 64 rows CASCADE arrays in NEXAI IP block 16 arrays Total CASCADE rows NEXAI IP block 1,024 rows PEs in NEXAI IP block 532,480 PEs Active PEs in NEXAI IP block 524,288 PEs Weight bits in CASCADE PEs 2,097,152 bits Activations HILT bits 16,777,216 bits Output sums HILT bits 16,777,216 bits Number of SanDisk HBF NAND Flash stacks 1 stack Capacity of HBF stacks 512.0 GBytes Likely bandwidth of HBF stacks 1.2 TB s CASCADE array chip area 0.41 mm2 Activations HILT chip area 0.25 mm2 Output sums HILT chip area 0.25 mm2 Total chip area for NEXAI IP block 0.91 mm2 Total NEXAI IP block memory 4.46 MBytes CASCADE system power consumption 1.43 Watts Transformer inferenced DeepSeek V3 R1 Typical weights activated per MoE inference 37 billion Input token sequence 2,048 tokens CASCADE limited transformer inference time 9.0 ms HBF limited transformer inference time 17.0 ms Max inference rate, limited by HBF 59 tokens sec 49 29 Design Validation 29.1 Validation Requirements and Open Questions The following critical aspects require experimental validation before the ZettaLith architecture can be considered viable: PE Timing Closure: SPICE simulation using TSMC A16 PDK to verify 12 GHz operation Thermal Feasibility: CFD analysis of 321 W cmÂ² heat dissipation with JETSTREAM cooling WSSCB Mechanical Integrity: FEA of silicon spring stress relief under thermal cycling CASCADE Array Functionality: RTL simulation of reduced-scale array CREST Effectiveness: Statistical analysis of fault coverage under realistic defect models 29.2 Design Knobs ZettaLith s performance targets result from pushing the envelope along multiple axes, with the expectation that each axis may be relaxed during detailed design and simulation to recover margin elsewhere. The major knobs include: PE Clock Frequency Nominal: 12 GHz Reduced: 8 GHz (or intermediate) Margin gained: timing-closure headroom, 2 3 dynamic power Chiplet size: 11 mm x 13 mm is somewhat arbitrary. 11 mm matches the HBM HBF size, and 13 mm is chosen to fit 6 per reticle.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2630,
    "tokens": 665
  },
  {
    "text": "\\label {Eq_Uniform_Dequant} (2) Here, x is the original floating-point input, xq is the quan- tized integer representation, k is the bit-width, s is the scale factor, and z is the zero point. The rounding function en- sures proper discretization. Uniform quantization is widely adopted due to its straightforward hardware implementa- tion, allowing integer arithmetic to replace floating-point operations, leading to higher efficiency and lower compu- tational cost. For highly skewed data distributions, the log2 quantizer provides a more effective alternative, as it assigns quantization levels based on powers of two, offering higher precision for small values: x _{q} \\ text { c l am p} \\ left ( \\left \\lfloor -\\log _{2}\\frac {x}{s} \\right \\rceil , 0, 2 k - 1 \\right ) \\label {Eq_Log_Quant} (3) x \\ a p prox \\hat {x} s \\cdot 2 {-x_{q}} \\label {Eq_Log_Dequant} (4) Log2 quantization is particularly beneficial for hardware, as it enables multiplications to be replaced by bit shifts, im- proving computational speed and energy efficiency. 3.1.2. Quantization Granularity Quantization operates at varying levels of granularity, in- troducing a trade-off between computational efficiency and quantization effectiveness. The two most common ap- proaches are per-tensor and per-channel quantization. Per- Tensor quantization employs a single scale and zero-point across an entire weight or activation tensor, reducing com- putational complexity and memory overhead. However, it struggles with large inter-channel variations, leading to suboptimal quantization performance. Per-Channel quan- tization assigns individual quantization parameters to each output channel, effectively mitigating distribution variance across channels. However, it requires storing more quanti- zation parameters, increasing memory usage and data trans- fer costs. 3.1.3.",
    "source": "2503.03088v2_AHCPTQ_Accurate_and_Hardware-Compatible_Post-Train.pdf",
    "length": 1846,
    "tokens": 441
  },
  {
    "text": "BACKGROUND A. Clock Meshes Clock meshes have been extensively adopted in high- performance microprocessors by leading semiconductor com- panies [1] [6]. These companies rely on clock meshes to achieve minimal skew, enhance timing reliability, and im- prove tolerance to process, voltage, and temperature (PVT) variations. However, the significant area and power overhead associated with traditional clock meshes have limited their widespread adoption in general ASIC design, necessitating research into more efficient implementations. Several methodologies have been proposed to address the power and area challenges of clock meshes. MeshWorks [7] introduced an automated framework for clock mesh synthesis, enabling efficient planning, optimization, and tradeoff bal- ancing between skew and power consumption. It integrates mesh planning and optimization algorithms, which reduce buffer area, wire length, and power. MeshWorks also intro- duced mesh reduction techniques, which selectively remove redundant mesh segments while maintaining variation toler- ance. This approach significantly reduces power and resource consumption without degrading performance. There have been other works to optimize clock meshes [8] [11], which have further improved upon these results. All of these results, however, have typically relied on restricted models of delay or heuristics instead of delay modeling. In addition, none of the approaches have considered clock signal slew which can have a large impact on timing performance. B. Graph Neural Networks Deep learning over grid structured data like images has shown promising results using Convolutional Neural Networks (CNNs). These same CNNs, however, have been problematic on irregular structured data like graphs because of graph isomorphism (i.e. same graphs with different node order- ings). Graph Neural Networks (GNNs), however, have been a promising technique to solve this by using localized mes- sage passing between neighboring nodes and combining the resulting features into graph, node, or edge predictions. GNNs struggle with deep layers because of the over- smoothing problem [12] where after multiple steps of the ag- gregation operation the node features become indistinguishable from each other. This is particularly problematic on graphs with large spans since communication between distant nodes is limited. Transformers are a recent approach to aid such global communication within a single layer [13], but scale poorly for large graphs. Jumping Knowledge (JK) connections [14] are another approach to dynamically aggregate node representa- tions from both different layers of a GNN to capture both local and global information.",
    "source": "2507.05681v1_GATMesh_Clock_Mesh_Timing_Analysis_using_Graph_Neu.pdf",
    "length": 2691,
    "tokens": 497
  },
  {
    "text": "The results indicate that it does not offer meaningful Unsigned (0,1) (-1, 1) 0 1 0 0 0 1 AND 0 0 0 0 0 1 Don't care count (Number of \"0\"s) 4 RBVM Number of \"1\"s - Number of \"-1\"s 2 popcount - data width Don't care count 0 2-bit vector 6-bit binary datapack 0 1 0 0 0 1 1 -1 1 -1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 -5 1 -4-1-2 3 previous RBMM that computes Number of \"-1\"s Number of \"0\"s Number of \"1\"s popcount 1 Signed (-1,1) (-1, 1) 0 1 0 0 0 1 XNOR 0 0 0 1 0 1 2-bit vector 6-bit binary datapack -1 1 -1-1-1 1 1 -1 1 -1 1 1 1 0 1 0 1 1 data width - popcount 4 Number of \"-1\"s Number of \"1\"s popcount 2 RBVM Number of \"1\"s - Number of \"-1\"s 2 popcount - data width -2 data width - popcount 5 1 1 1 1 binarization Fig. 4: A 6-bit example of our RBVM. ğ‘¨ ğ‘«ğ’‚ğ’•ğ’‚ğ’‘ğ’‚ğ’„ğ’Œ ğ‘© ğ‘«ğ’‚ğ’•ğ’‚ğ’‘ğ’‚ğ’„ğ’ŒğŸ ğ‘© ğ‘«ğ’‚ğ’•ğ’‚ğ’‘ğ’‚ğ’„ğ’Œğ‘µğ’‘ğ’† A B On Chip Memory RBMM Engine ğ‘¶ğ’–ğ’•ğ’‘ğ’–ğ’•ğŸğ‘¨ ğ‘¶ğ’–ğ’•ğ’‘ğ’–ğ’•ğŸğ‘© Off-Chip DDR RAM A B M2 M3 Output Return DC Return LayerNorm Unit Local Buffer AXI BUS ğ‘¹ğ‘©ğ‘´ğ‘´ ğ‘·ğ‘¬ğ‘µğ’‘ğ’† Npe ğ‘¹ğ‘©ğ‘´ğ‘´ ğ‘·ğ‘¬ğŸ ğ‘¹ğ‘©ğ‘´ğ‘´ ğ‘·ğ‘¬ğŸ COBRA Control Internal Read Write Control Don t care INPUT bias Threshold COBRA Control RBMM Engine On Chip Memory Fig.",
    "source": "2504.16269v2_COBRA_Algorithm-Architecture_Co-optimized_Binary_T.pdf",
    "length": 1071,
    "tokens": 563
  },
  {
    "text": "4 Token-wise Adaptive Activation Quantization As mentioned in Section 3.1, the significantly large activation size in the Protein Structure Prediction Model (PPM) presents a ma- jor limitation. To address this, we propose Token-wise Adaptive Activation Quantization (AAQ), which optimally quantizes Pair Representation activations by fully leveraging their characteristics. By integrating dynamically adjusted precision and adaptive outlier handling into token-wise quantization, AAQ effectively mitigates activation size issues in PPM while ensuring accurate inference. 4.1 Baseline Token-wise Quantization AAQ specifically targets activations in Pair Representation dataflow, the main bottleneck of PPM as mentioned in Section 3.1. To maxi- mize model accuracy, we use a 16-bit fixed-point format for weights without quantization. For activations, quantized inlier values are represented using INT4 or INT8 formats, and outliers are repre- sented in an INT16 format to minimize information loss. Token-wise Quantization. In an attention-based model, in- cluding PPM, most computations, including linear layers or layer normalization, are performed token-wise. Therefore, supporting dynamic token-level parallelism is critical for efficient processing in attention-based models [61]. However, conventional attention- based models employing quantization often use channel-wise quan- tization for accuracy. This approach necessitates the dequantization of individual values within a token before every operation to enable token-wise parallel computations, making the process highly inef- ficient [35, 64]. In contrast, PPM also exhibits a pattern in which large values are concentrated in specific tokens. To exploit this property, we adopt a token-wise quantization. By applying token- wise quantization and setting the scaling factor dynamically at runtime, where each token is adjusted with a unique scaling factor, we achieve superior quantization accuracy. Dynamic Outlier Handling. Although activations in PPM can be quantized token-wise, handling outliers remains another chal- lenge. In typical attention-based models, channel-wise quantiza- tion allows predetermination of quantization parameters based on dataset analysis. However, since the number of tokens varies sig- nificantly depending on input, predefining thresholds for outlier classification is not feasible in a token-wise manner.",
    "source": "2505.05893v1_LightNobel_Improving_Sequence_Length_Limitation_in.pdf",
    "length": 2400,
    "tokens": 490
  },
  {
    "text": "(a) (b) Normalized Computation Cost (10K) 2 4 8 14 6 10 12 10 1 2 3 4 5 6 7 8 9 LightNobel LightNobel Baseline PPM Baseline PPM LightNobel Baseline PPM LightNobel Baseline PPM 128 72 Sequence Length (K) 43.48 Decreased Normalized Computation Cost (10K) 2 4 8 14 6 10 12 10 1 2 3 4 5 6 7 8 9 LightNobel Baseline PPM 128 72 Sequence Length (K) 43.48 Decreased 10 Normalized Memory Footprint (100K) 5 10 15 20 25 1 2 3 4 5 6 7 8 9 1,966 508 LightNobel LightNobel Baseline PPM Baseline PPM LightNobel Baseline PPM Sequence Length (K) 74.10 Decreased 10 Normalized Memory Footprint (100K) 5 10 15 20 25 1 2 3 4 5 6 7 8 9 1,966 508 LightNobel Baseline PPM Sequence Length (K) 74.10 Decreased (a) (b) Normalized Computation Cost (10K) 2 4 8 14 6 10 12 10 1 2 3 4 5 6 7 8 9 LightNobel Baseline PPM 128 72 Sequence Length (K) 43.48 Decreased 10 Normalized Memory Footprint (100K) 5 10 15 20 25 1 2 3 4 5 6 7 8 9 1,966 508 LightNobel Baseline PPM Sequence Length (K) 74.10 Decreased Figure 16: (a) Computational cost of PPM and (b) memory footprint of PPM across various sequence lengths. compared to A100 and H100 in this experiment. For short proteins, the kernel overhead constitutes a significant portion of the overall latency, leading to relatively large speedup gains. However, as the sequence length increases, this overhead becomes less dominant. Although the absolute speedup is relatively modest, LightNobel s speedup becomes more stable and consistent, demonstrating a high degree of scalability with respect to sequence length. 8.3 In-Depth Analysis Peak Memory Requirement. To evaluate LightNobel s benefit on peak memory requirement, we measure the peak memory require- ments across various datasets. Figure 15(a) shows the peak memory requirement of baseline PPM and LightNobel.",
    "source": "2505.05893v1_LightNobel_Improving_Sequence_Length_Limitation_in.pdf",
    "length": 1784,
    "tokens": 488
  },
  {
    "text": "\"\", 2024. [120] Octopai. Octopai: Automated data lineage, data catalog and discovery. com . (Accessed on 09 12 2023). [121] Julius Odede and Ingo Frommholz. Jaybot aiding university students and admission with an llm- based chatbot. In Proceedings of the 2024 Conference on Human Information Interaction and Retrieval, pages 391 395, 2024. [122] Journal of the American Medical Informatics Association. Efficient healthcare with large language models: optimizing clinical workflow and enhancing patient care. Oxford Academic, 2024. [123] OpenAI. Openai chatgpt. \"\", 2024.",
    "source": "NSF_LLM_Medium_Proposal.pdf",
    "length": 571,
    "tokens": 140
  },
  {
    "text": "This work con- tributes quantifiable performance metrics for edge AI deployment in privacy-preserving and energy-constrained applications. Future work should investigate power consumption characteristics, multi-keyword architectures, and robust- ness evaluation under varying acoustic conditions. REFERENCES [1] Shalbbya Ali, Safdar Tanweer, Syed Sibtain Khalid, and Naseem Rao. Mel frequency cepstral coefficient: a review. ICIDSSD, 2020. [2] Justice Amoh and Kofi M Odame. An optimized recurrent unit for ultra-low-power keyword spotting. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 3(2):1 17, 2019. [3] Gianmarco Cerutti, Lukas Cavigelli, Renzo Andri, Michele Magno, Elisabetta Farella, and Luca Benini. Sub-mw keyword spotting on an mcu: Analog binary feature extraction and binary neural networks. IEEE Transactions on Circuits and Systems I: Regular Papers, 69(5):2002 2012, 2022. [4] Guoguo Chen, Carolina Parada, and Tara N Sainath. Query- by-example keyword spotting using long short-term memory networks. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 5236 5240. IEEE, 2015. [5] Cristian Cioflan, Lukas Cavigelli, Manuele Rusci, Miguel De Prado, and Luca Benini. On-device domain learning for keyword spotting on low-power extreme edge embedded systems. In 2024 IEEE 6th International Conference on AI Circuits and Systems (AICAS), pages 6 10. IEEE, 2024. [6] Juan Sebastian Piedrahita Giraldo and Marian Verhelst. Laika: A 5uw programmable lstm accelerator for always-on keyword spotting in 65nm cmos. In ESSCIRC 2018-IEEE 44th European Solid State Circuits Conference (ESSCIRC), pages 166 169. IEEE, 2018. [7] Alexander Gruenstein, Raziel Alvarez, Chris Thornton, and Mohammadali Ghodrat. A cascade architecture for keyword spotting on mobile devices. arXiv preprint arXiv:1712.03603, 2017.",
    "source": "2506.08911v1_Implementing_Keyword_Spotting_on_the_MCUX947_Micro.pdf",
    "length": 1893,
    "tokens": 494
  },
  {
    "text": "The accuracy is evaluated on the test set of the dataset. II is 1 for all designs shown. Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 15 Strategy Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] Latency 76.9 42 (57.6 ns) 16,081 57 26,484 729.4 DA 31 (44.1 ns) 12,682 0 19,056 702.2 Latency 76.6 32 (46.1 ns) 9,347 28 15,216 694.0 DA 26 (37.4 ns) 7,392 0 11,462 695.9 Latency 76.5 35 (67.2 ns) 8,548 30 14,418 520.8 DA 25 (35.4 ns) 6,448 0 10,109 707.2 Latency 76.3 33 (51.8 ns) 6,667 15 11,184 637.3 DA 22 (30.2 ns) 5,019 0 7,682 729.4 Latency 76.0 33 (48.3 ns) 4,471 22 7,279 683.5 DA 23 (32.6 ns) 3,602 0 5,693 706.2 Latency 75.9 31 (42.8 ns) 4,005 17 6,305 723.6 DA 21 (24.9 ns) 2,908 0 4,720 844.6 Table 6. Resource utilization and latency of the high-level feature jet tagging network with and without da4ml generated with hls4ml, marked with DA and Latency for strategy, respectively. The FPGA part is xcvu13p-flga2577-2-e with 1 GHz target clock frequency. Delay constraint is set to 2 for the da4ml optimized designs for each CMVM operation. The accuracy is evaluated on the test set of the dataset. II is 1 for all designs shown. 5.2.2 SVHN Classification Network. We show the out-of-context results after place route for the SVHN classification network [12] on the VU9P FPGA. The network is a LeNet-like [22] convolutional network with dense classification head from [2], and the architecture is shown in Figure 7.",
    "source": "2507.04535v1_da4ml_Distributed_Arithmetic_for_Real-time_Neural_.pdf",
    "length": 1475,
    "tokens": 496
  },
  {
    "text": "With less operating points (i.e., skipping configurations) delivering accuracy above ğ‘ğ‘ğ‘ğ‘šğ‘–ğ‘›, our runtime has less operating points available to adapt the inference and ends up with a speedup over the the other two baselines smaller than in the other evaluation cases. Another important factor for edge deployment is the power efficiency (see right-most columns of Table I). The last column highlights the power efficiency delivered by our zero-overhead, controllable skipping. Even though SkipNet achieves the best results for the ResNet-110 on CIFAR-100, in all the other evaluation cases, our gated models achieve better efficiency and, contrarily to SkipNet, is constantly better than the original ResNet-20 and ResNet-100 models. original ResNet-20 and ResNet-100 models. V. CONCLUSIONS Based on the observation that CNNs become more resilient to arbitrary skipping when trained with the stochastic depth, our framework trains CNNs that can be leveraged for fully- controllable, zero-overhead, runtime adaptive inference as gated CNNs. However, selecting which layers to skip (i.e., the skipping design space) is a combinatorial problem which quickly becomes intractable. To cope with this, we compute a sensitivity list of the model s layers to approximate the Pareto front of skipping configurations. At runtime, the generated operating points are exploited to adapt the inference processing. ACKNOWLEDGMENTS This work was funded in part by the AI competence center ScaDS.AI Dresden Leipzig in Germany (01IS18026A-D) and by the EU Horizon Europe Programme under grant agreement No 101135183 (MYRTUS). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union. Neither the European Union nor the granting authority can be held responsible for them. REFERENCES [1] X. Wang, F. Yu, Z.-Y. Dou, T. Darrell, and J. E. Gonzalez, Skipnet: Learning dynamic routing in convolutional networks, in ECCV, 2018, pp. 409 424. [2] S. Laskaridis, S. I. Venieris et al., HAPI: hardware-aware progressive inference, in ICCAD. IEEE, 2020, pp.",
    "source": "2505.17626v1_Leveraging_Stochastic_Depth_Training_for_Adaptive_.pdf",
    "length": 2098,
    "tokens": 500
  },
  {
    "text": "However, FL faces signif- icant challenges, including heterogeneous data distributions, security vulnerabilities, and optimization constraints. Handling Data Heterogeneity in FL. One major limitation of FL is non-identically distributed (non-IID) data across edge devices, which can degrade model perfor- mance. To address this, meta-learning-based learning-to-learn techniques improve generalization across different data distributions [109, 110]. Additionally, personal- ized FL techniques train models that adapt to specific device distributions, enhancing performance in heterogeneous settings [111, 112]. Security and Robustness in FL. Federated learning remains vulnerable to adver- sarial attacks, particularly Byzantine failures, where malicious nodes send corrupted updates. Tao et al. [113] introduced Byzantine-resilient FL, which integrates dis- tributed gradient descent with gradient compression to mitigate adversarial influences while ensuring convergence. These techniques enhance security without compromising learning efficiency. Optimizing Local Model Updates. FL s communication overhead can be reduced through gradient compression and adaptive update strategies. Wang et al. [114] pro- posed an optimization algorithm that balances local update frequency and global aggregation, improving computational efficiency. Additionally, Split Federated Learn- ing (SFL) [115] partitions models between devices and servers to reduce memory and processing burdens, achieving a 43 reduction in completion time while maintaining model accuracy. Federated learning serves as the foundation for decentralized AI, but it assumes a relatively stable learning environment. In real-world applications, onboard AI must continuously adapt to evolving data distributions, requiring continual learning mechanisms. 4.2 Continual Adaptation in Decentralized Systems: Continual Learning Continual Learning (CL) enables AI models to incrementally acquire new knowledge while preserving past information, making it critical for real-time onboard learn- ing. Unlike traditional FL, which assumes fixed training tasks, CL allows models to evolve dynamically. However, CL faces two key challenges: catastrophic forgetting and computational efficiency in resource-limited environments. 13 Mitigating Catastrophic Forgetting. Standard CL methods suffer from catastrophic forgetting, where previously learned knowledge deteriorates as new tasks are intro- duced. To address this, LightCL [116, 117] freezes the lower and middle layers of a pre-trained model while updating only higher layers, reducing memory consumption while preserving past knowledge.",
    "source": "2505.08793v1_Onboard_Optimization_and_Learning_A_Survey.pdf",
    "length": 2642,
    "tokens": 477
  },
  {
    "text": "After the MAC operation, the outputs are transferred directly through the NoC to the pipeline register of a target module (such as the softmax MEADOW: Memory-efficient Dataflow and Data Packing for Low Power Edge LLMs PE1 PE2 PE3 PE4 PE5 PE6 PE7 PE8 PE9 PE10 SM1 SM2 LN1 LN1 NL1 NL2 Input BRAM Weight BRAM Max EXP Stage ğ‘šğ‘ğ‘¥ - EXP LUT ğ‘’!\" ! Buffer Buffer DIV Stage MAX Stage after F cycles after F cycles Pipeline register Pipeline register ğ‘¥ ğ‘’!\" ! ğ‘¥ ğ‘šğ‘ğ‘¥ PREG MAC GEMM Pipeline GEMM Pipeline Input RF Weight RF Output RF RF: Register File PREG: Pipeline Reg. NOC NOC Weight BRAM Input BRAM NOC NOC PREG Output BRAM (a) (d) (b) WILU Output BRAM NOC PE1-8 Parallel PE; PE9-10 Broadcasting PE Softmax (SM) Module Off-chip DRAM ğ‘Š! ğ‘Š\" ğ‘Š ğ‘Š Input Parallel MAC PE x x x x Broadcasting MAC PE x x x x 4 cycles ğ‘‘ '( ğ‘‘ '( 14 13 9 5 1 16 15 14 13 4 3 2 1 8 7 6 5 a b d a b c d 10 6 2 16 12 8 4 Weight a a a 4 cycles (c) Figure 2. (a) Tiled architecture of MEADOW containing parallel and broadcasting processing elements (PEs), pipelined softmax (SM) module, modules for layer normalization (LN) and non-linear activation functions like ReLU GeLU (NL). (b) The hybrid PE architecture capable of operating in GEMM and pipelined modes. (c) Architecture and execution flow of a parallel and broadcasting MAC PE. (d) The pipelined softmax (SM) module. unit or another PE) in the subsequent pipeline stage. Parallel and Broadcasting PE: MEADOW s tiled archi- tecture contains a mix of Parallel MAC and Broadcasting MAC PEs (for example PE1-8 Parallel and PE9-10 are Broadcasting MAC PEs as shown in Fig. 2a). As shown in Fig.",
    "source": "2503.11663v1_MEADOW_Memory-efficient_Dataflow_and_Data_Packing_.pdf",
    "length": 1606,
    "tokens": 498
  },
  {
    "text": "The baseline FlashAttention kernel performs exponent and multiplication operations separately. Exponent function evaluation is implemented with piece-wise linear approximation in the reduced input range inherent to attention and used also for the ExpMul operators. In practice, the total cost of the unrolled hardware serving multiple query vectors in parallel will be the cost for one query multiplied by the number of parallel query vectors served. The query vectors are preloaded separately in the architecture, while the key and value vectors are loaded and broadcasted to all parallel blocks. Both hardware blocks were implemented in C (publicly available at [15]) and synthesized into Verilog using Catapult HLS with a 28-nm standard-cell library. Both designs operate at the same pipelined latency with a clock frequency of 500 MHz. Increasing the hidden dimension of each attention accelerator increases also the latency of computation. For the examined sizes of the hidden dimension, for one key and value vectors pipelined latency with initiation interval of 1, ranges between 8 and 12 cycles. Next, Verilog was synthesized using the Cadence digital implementation flow, while power consumption was estimated with the PowerPro power analysis and optimization tool. The reported power consumption represents the average power measured after executing attention kernels for the T5 Large Language Model and GLUE benchmarks utilizing the PromptBench workflow. Figs. 3 and 4 show the area and power of the FlashAttention- 2 kernel with and without ExpMul hardware operators, for the two examined floating-point data type and for three hidden dimension sizes d {16, 64, 256}. Power estimation ex- cludes memory power and focuses solely on the average power consumption of the computation kernel. The memory power in both cases is expected to be identical, as both approaches implement the same FlashAttention-2 algorithm using the same computation order and data flows. The difference lies solely on how the computation kernel is executed internally. As shown in Fig. 3, utilizing ExpMul operators reduces the hardware area by more than 28.8 on average in all examined cases. These savings come from the simplified computation Fig. 3. The hardware area at 28 nm for the FlashAttention-2 kernel, evaluated across various hidden dimension sizes, with and without ExpMul operators, for computing attention on a single query using FP32 and BFloat16 floating- point formats. Fig. 4.",
    "source": "2505.14314v2_Low-Cost_FlashAttention_with_Fused_Exponential_and.pdf",
    "length": 2481,
    "tokens": 478
  },
  {
    "text": "In the algorithm, we denote the fixed-point numbers by quantized interval, namely, by their low value, high value, and the step size, [ğ‘™,â„,ğ›¿]. For a generic fixed-point number in the form of fixed ğ‘†, ğ‘Š, ğ¼ , we have ğ‘™ ğ‘† 2(ğ¼ ğ‘†), â„ 2ğ¼ ğ‘† 2 ğ‘Š ğ¼, and ğ›¿ 2 ğ‘Š ğ¼. This notation is useful to track the required bitwidths when accumulating a large number of fixed-point numbers, as otherwise one carry bit will always need to be added to prevent overflow. The notation we use in the algorithm is summarized in Table 1. In particular, ğ‘€[ğ‘‘ğ‘–ğ‘›,ğ‘‘ğ‘œğ‘¢ğ‘¡], ğ‘ğ‘–ğ‘›ğ‘¡ğ‘–ğ‘›[ğ‘‘ğ‘–ğ‘›], ğ‘‘ğ‘’ğ‘ğ‘¡â„ğ‘–ğ‘›ğ‘¡[ğ‘‘ğ‘–ğ‘›], and ğ‘‘ğ‘are the input parameters to the algorithm for a single CMVM operation. 3.2 Overview The da4ml is a CSE-based hybrid algorithm. Like other CSE-based algorithms, the algorithm operates on a discrete representation of the constant matrix. In this work, following [3, 4, 9, 15], we adopt the canonical signed digit (CSD) [6] representation. CSD is a signed digit representation of a number that never has two consecutive non-zero digits, and the number of non-zero digits is guaranteed to be minimal. Hence, for a number with ğ‘¥digits, the CSD representation has at most ğ‘¥ 2 1 non-zero digits, which is 1 3 of the total number of digits on average. As ğ‘€contains only fixed-point integers, the algorithm first normalizes it by applying bit-shifts across the rows and columns such that no row column has all entries even except for zeros. The resultant scaling factors are recorded and will be applied to the input output vectors.",
    "source": "2507.04535v1_da4ml_Distributed_Arithmetic_for_Real-time_Neural_.pdf",
    "length": 1492,
    "tokens": 500
  },
  {
    "text": "Our work demonstrates that through loop tiling, on-chip buffering, and parallel unrolling, even a resource-constrained FPGA can significantly accelerate the ma- trix multiplication operations that underpin Transformer attention mechanisms. Standalone GEMM benchmarks and the acceleration Tiled MatMul Accelerator for Transformer Self-Attention Conference 17, July 2017, Washington, DC, USA of Q, K, and V projections achieved a notable 7 speedup over PyTorch CPU execution and marked improvements in energy effi- ciency. While the overall end-to-end DistilBERT performance currently exhibits a 2 speedup, this result is largely influenced by the trans- mission delays inherent to the current PYNQ overlay and custom forward pass. These unignorable overheads clearly indicate an op- portunity for further optimization. Future work focused on reducing data transfer latencies and integrating additional components such as softmax and FFN layers along with enhanced system-level opti- mizations is expected to yield even greater performance gains. In summary, this project not only confirms the viability of FPGA-based acceleration for key Transformer operations but also establishes a strong foundation for future advancements in efficient, scalable deep learning inference on power-constrained edge devices. ACKNOWLEDGMENTS The authors would like to thank Professor Sitao Huang from the University of California, Irvine, for his invaluable guidance and insightful discussions throughout this project. Special thanks are also extended to Sicheng Chen for his excellent collaboration. REFERENCES [1] Siyuan Lu et al. 2020. Hardware Accelerator for Multi-Head Attention and Position- Wise Feed-Forward in the Transformer. arXiv:2009.08605. 08605. [2] Hyoukjun Kwon, Prasanth Chatarasi, Vivek Sarkar, and Tushar Krishna. 2020. MAESTRO: A Data-Centric Approach to Understand Reuse, Performance, and Hard- ware Cost of DNN Mappings. IEEE Micro, 40(3): xx yy, 2020. 1109 MM.2020.2985963. [3] Hyoukjun Kwon, Prasanth Chatarasi, Michael Pellauer, Angshuman Parashar, Vivek Sarkar, and Tushar Krishna. 2019.",
    "source": "2503.16731v3_Design_and_Implementation_of_an_FPGA-Based_Hardwar.pdf",
    "length": 2096,
    "tokens": 484
  },
  {
    "text": "With FP4 precision, weights can have any of 16 different values. 5 The Wafer-Scale Silicon Circuit Board (WSSCB) Figure 1 illustrates ZettaLith implementation on a 300 mm silicon wafer-scale silicon circuit board (WSSCB), accommodating an array of SCB modules. The central portion has 156 systolic array compute modules, with 8 1 arrays of CPU modules above and below. TSV connections lead to 800 GbE and PCIe 6.0 PCBs, facilitating high-speed external communication. This heterogeneous architecture enables a complete large-scale computing system on a single WSSCB, with data fabric connections providing cohesive operation. WSSCB solves the yield, thermal stress, physical stress, breakage and testing problems with large silicon interposers, and solves the high current power supply problem by integrating many PSU PCBs using column grid array attachment. The WSSCB has Î¼m-scale routing pitches, mechanical and thermal stress-relief structures, and integrated redundancy for each wire. Consequently, high defect densities can be tolerated with no loss of function. The result is a high-yield, passive and robust large silicon substrate providing the interconnections, power distribution, and mechanical support for a large array of active chiplet stacks. The WSSCB uses near-full-thickness silicon. This is viable because the TSVs are not used for high speed signals within the array only for power supply and relatively low speed signals. This, in turn, is because the WSSCB takes the role of silicon- 6 performance PCB, not silicon interposer. Multiple PCBs are connected to the one silicon substrate, as opposed to multiple chips being attached to one PCB. This makes the silicon thickness irrelevant to high speed signal propagation, keeping all high speed signals contained to the front surface RDL of the WSSCB, the TRIMERA stacks, and the HBM stacks. 5.1 WSSCB testing A WSSCB is a passive silicon device with literally tens of millions of short wire segments connecting pairs of microbumps. It is untestable by conventional semiconductor ATE. This paper describes a simple MEMS probe with tens of thousands of integrated MEMS elastic spring probes that can test an entire WSSCB with 100 coverage in a few minutes. As there are no active components on the WSSCB, the test system is very simple only testing for wire opens and shorts.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2343,
    "tokens": 497
  },
  {
    "text": "Cogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations, 2025. [15] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Junkun Yuan, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yanxin Long, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Daquan Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: A systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 11 [16] Genmo Team. Mochi 1. 2024. [17] Black Forest Labs. Flux. 2023. [18] Stability AI. Introducing stable diffusion 3.5. introducing-stable-diffusion-3-5, 2023. [19] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2024. [20] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza, Luca Wehrstedt, Jeremy Reizenstein, and Grigory Sizov. xformers: A modular and hack- able transformer modelling library. 2022. [21] NVIDIA.",
    "source": "2505.11594v1_SageAttention3_Microscaling_FP4_Attention_for_Infe.pdf",
    "length": 1540,
    "tokens": 485
  },
  {
    "text": "2025 IEEE International Symposium on High-Performance Computer Architecture (HPCA) Exploring the Performance Improvement of Tensor Processing Engines through Transformation in the Bit-weight Dimension of MACs Qizhe Wu1, Huawen Liang1, Yuchen Gui1, Zhichen Zeng1,2, Zerong He1, Linfeng Tao1, Xiaotian Wang1,3, Letian Zhao1 Zhaoxi Zeng1, Wei Yuan1, Wei Wu1 and Xi Jin1 1Department of Physics, University of Science and Technology of China, 2University of Washington, 3Raytron Technology Abstract General matrix-matrix multiplication (GEMM), serving as a cornerstone of AI computations, has positioned ten- sor processing engines (TPEs) as increasingly critical components within existing GPUs and domain-specific architectures (DSA). Our analysis identifies that the prevailing architectures primarily focus on dataflow or operand reuse strategies, when consid- ering the combination of matrix multiplication with multiply- accumulator (MAC) itself, it provides greater optimization space for the design of TPEs. This work introduces a novel perspective on matrix multiplication from a hardware standpoint, focus- ing on the bit-weight dimension of MACs. Through this lens, we propose a finer-grained TPE notation, using matrix triple loops as an example, introducing new methods and ideas for designing and optimizing PE microarchitecture. Based on the new notation and transformations, we propose four optimization techniques that achieve varying degrees of improvement in timing, area, and power consumption. We implement our design in RTL using the SMIC-28nm process. Applying our methods to four classic TPE architectures (include systolic array [20], 3D- Cube [27], multiplier-adder tree [48], and 2D-Matrix [30]), we achieved area efficiency improvements of 1.27 , 1.28 , 1.56 , and 1.44 , and 1.04 , 1.56 , 1.49 , and 1.20 for energy efficiency respectively. When applied to a bit-slice architecture, we achieved a 12.10 improvement in energy efficiency and 2.85 in area efficiency compared to Laconic [38]. Our Verilog HDL code, along with timing, area, and power reports for circuit synthesis in URL: Tensor-Processing-Engines. I.",
    "source": "2503.06342v1_Exploring_the_Performance_Improvement_of_Tensor_Pr.pdf",
    "length": 2138,
    "tokens": 493
  },
  {
    "text": "III. EMBEDDABLE U-NET-BASED SEMANTIC SEGMENTATION OF AERIAL IMAGES A. Semantic Segmentation of Aerial Images Our research is situated within the context of earth ob- servation, focusing primarily on two application domains: satellites and UAVs. These platforms are pivotal in acquiring high-resolution terrestrial imagery, offering spatial resolutions ranging from 0.2 to 10 meters, which are critical for numer- ous remote sensing applications [7], [16], [20]. The primary limitation lies in the downlink capacity, as satellites and UAVs lack the capability to transmit all captured images to ground stations. Consequently, on-board analysis becomes essential to ensure that only relevant data is transmitted to Earth, optimizing both bandwidth and data relevance [10]. In this context, semantic segmentation is indispensable as it enables precise on-board analysis of the high-resolution imagery acquired by satellites and UAVs. We employ the Inria Aerial Image Labeling Dataset provided by Inria, renowned for its utility in benchmarking the generalization capabilities of semantic segmentation methodologies [17]. This dataset includes 180 colored satellite photographs, each measuring 5000x5000 pixels (25 Megapixels). The primary task of the dataset involves semantic segmentation, which entails classi- fying each pixel of an input image into a specific category; in our case, this means distinguishing every pixel as either building or not building . This classification results in a segmentation map. Figure 3 illustrates this process. To op- timize for training and model embeddability, we dissect these images into smaller segments of 256x256 pixels, maintaining slight overlaps. These segments are subsequently merged to reconstruct the original 5000x5000 segmentation map post- inference. B. U-Net Architecture We selected a U-Net architecture for our workflow compar- ison. The U-Net [28], initially proposed for biomedical image segmentation, has since become a widespread neural network architecture. It features a low number of parameters, a small memory footprint, and fewer MAC operations compared to other semantic segmentation networks, while still maintaining high accuracy. Additionally, it is designed to be trained with a limited amount of data, a common scenario in the embed- ded domain. These characteristics make the U-Net an ideal candidate for an embedded neural network. However, we modified the U-Net to enhance its embed- dability.",
    "source": "2503.08700v1_Real-Time_Semantic_Segmentation_of_Aerial_Images_U.pdf",
    "length": 2465,
    "tokens": 493
  },
  {
    "text": "We theorize that this is due to the increased utilization of the processing modules, as more computation is required for the larger input height and width. TABLE II: Performance Evaluation on Generative Model Layers. Model OC KS IH IW IC OPs Latency (ms) CPU (ms) Speedup (vs CPU) GOPs GOPs W DCGAN 1 512 5 4 1024 420M 46.26 166.56 3.60 9.07 15.64 DCGAN 2 256 5 8 512 420M 33.97 141.05 4.15 12.35 15.03 DCGAN 3 128 5 16 256 420M 35.86 149.70 4.17 11.70 14.92 DCGAN 4 3 5 32 128 20M 4.67 10.71 2.29 4.21 0.87 FCN 21 4 1 21 14K 0.22 0.22 1.00 0.06 0.01 StyleTransfer 1 64 3 64 128 604M 164.62 304.48 1.85 3.67 23.22 StyleTransfer 2 32 3 128 64 604M 282.83 460.23 1.63 2.14 23.65 StyleTransfer 3 3 9 256 32 1020M 264.27 1045.36 3.96 3.86 40.49 FSRCNN 2 9 32 32 11M 5.21 12.47 2.39 2.04 0.51 Fig. 7: Percentage of cropped outputs for the various TCONV problems benchmarked in Figure 6. C. TCONV Model Layer Evaluation We performed an extensive evaluation across specific TCONV layers commonly found in popular generative mod- els [1], [2], [15], [17]. Table II contains the specific layer details, the performance of our accelerator compared to the PYNQ CPU s single-threaded execution, overall throughput, and energy efficiency. On average, we achieve a 2.8 speedup compared to the CPU implementation while achieving an average throughput of 5.5 GOPs, with an average power to performance ratio of 14.9 GOPs W. As realized in the synthetic benchmark, the MM2IM accelerator takes advantage of the larger Ic dimension seen within the DCGAN [15] layers, achieving up to a 4.2 speedup in some layers.",
    "source": "2507.07683v1_Accelerating_Transposed_Convolutions_on_FPGA-based.pdf",
    "length": 1593,
    "tokens": 471
  },
  {
    "text": "To address this issue, we propose a fine-grained interleaved pipeline that further divides each tile into subtiles, as shown in Fig. 10 (right). The core idea is to overlap the rasterization computation of the current subtile with the memory access required for sorting the next subtile. This fine-grained processing of subtiles is essential because each tile contains a variable number of GS, but the depth buffer size is limited. Fig. 10 (right bottom) illustrates how the proposed pipeline is mapped onto the hardware. For the first subtile, DRAM transfers the depths of GS to the depth buffer, which then outputs the depths to the PE array for sorting. The resulting ğ¹(ğ‘‘) values are written back to the depth buffer. Subsequently, the PE array performs sorting using the ğ¹(ğ‘‘) provided by the depth buffer, while the depth buffer simultaneously receives the depths for the next subtile, thereby overlapping rasterization with memory access. This process repeats, with the latency of depth access being completely hidden except for the first subtile. Throughput (GOPS) Intensity (MACs Byte) Memory- Bound Compute- Bound Sorting Rasterization Fine-grained Interleaved 38.4GB s 1536 Entire Sorting Tile1 Tile2 Tilen ... Low PE Utilization Naive Pipeline High PE Utilization Roofline Model Analysis Fine-grain Interleaved Pipeline ... PE Array T Buffer Access d Access GS d Rasterization Sorting Time Schedule Rasterization Sorting High PE Utilization DRAM Output d Input F(d) Input dnext Output F(d) Figure 10: Roofline model analysis and pipeline comparison. 5.3 ğœ‹ Trajectory Tile Schedule Our architecture adopts a GS-feature cache, where each cache line is tagged using a 28-bit GS ID. An additional 4 bits indicate the number of tiles intersected by each GS. The cache prioritizes the replacement of less important GS. These 4 bits, together with the 28-bit ID, form a 32-bit aligned storage. The cache design effectively reduces off-chip accesses due to the spatial locality of GSs. However, different tile scheduling trajectories affect the cache hit rate. As shown in Fig. 11(a), the baseline implementation scans tiles line by line, exploiting horizontal locality but lacking vertical and hierarchical locality.",
    "source": "2506.07069v1_Accelerating_3D_Gaussian_Splatting_with_Neural_Sor.pdf",
    "length": 2219,
    "tokens": 499
  },
  {
    "text": "We verified that the FPGA outputs matched CPU computation exactly for small test matrices and remained within quantization error for end-to-end model outputs. Tiled MatMul Accelerator for Transformer Self-Attention Conference 17, July 2017, Washington, DC, USA zynq_ultra_ps_e_0 Zynq UltraScale MPSoC M_AXI_HPM0_FPD M_AXI_HPM1_FPD S_AXI_HP0_FPD S_AXI_HP1_FPD S_AXI_HP2_FPD maxihpm0_fpd_aclk maxihpm1_fpd_aclk saxihp0_fpd_aclk saxihp1_fpd_aclk saxihp2_fpd_aclk pl_ps_irq0[0:0] pl_resetn0 pl_clk0 pl_clk1 mmult_accel_0 Mmult_accel (Pre-Production) s_axi_control m_axi_gmemA m_axi_gmemB m_axi_gmemC ap_clk ap_rst_n interrupt rst_ps8_0_99M Processor System Reset slowest_sync_clk ext_reset_in aux_reset_in mb_debug_sys_rst dcm_locked mb_reset bus_struct_reset[0:0] peripheral_reset[0:0] interconnect_aresetn[0:0] peripheral_aresetn[0:0] axi_smc_1 AXI SmartConnect S00_AXI S01_AXI S02_AXI M00_AXI M01_AXI M02_AXI aclk aresetn axi_smc AXI SmartConnect S00_AXI S01_AXI M00_AXI aclk aresetn Figure 3: Vivado Block Design 6 PERFORMANCE EVALUATION 6.1 Experimental Setup We benchmarked the accelerator on the KV260 board running Ubuntu with PYNQ support. The host s CPU consists of 4 Arm Cortex-A53 1.5GHz and 2 Cortex-R5F 600MHz, which we used for running baseline software matrix multiplication (NumPy and PyTorch) and orchestrating FPGA execution.",
    "source": "2503.16731v3_Design_and_Implementation_of_an_FPGA-Based_Hardwar.pdf",
    "length": 1340,
    "tokens": 518
  },
  {
    "text": "The for- ward pass computes activations via the formula Amuv C 1 c 0 H 1 i 0 W 1 j 0 X(u i)(v j)c Kmijc, with results stored in the double-buffered output feature map SRAM. The backward pass emphasizes gradient computation through backpropaga- tion, which is crucial for weight updates. The gradient of the loss function concerning the weights is computed through the formula L Kmi jc U 1 u 0 V 1 v 0 L Amuv X(u i)(v j)c. This gra- dient computation, fundamental for learning, is meticulously mapped across the systolic array, ensuring precise and efï¬cient backpropagation. Memory accesses are optimally managed via the double-buffered SRAM structures, providing timely data availability for the MAC units. The 8 8 systolic array in each tile executes multiply-accumulate operations in a pipelined and parallel fashion, abiding by the Weight Stationary approach, thereby optimizing the throughput and efï¬ciency of the train- ing operations within this hardware architecture. Power Control Logic: Power emergency prediction in Us. as is always conservative, and the solar power predictor has a mean accuracy of 92 , limiting false positives and helping the control unit select appropriate tile counts. The system needs at least 512 cycles of advanced notice to ï¬‚ush compute and enable a compute migration. Fig. 5a shows the block diagram of the mesh interconnect, and Fig. 5b shows the power-down sequence and signal states. The network works at a super-tile (STile) granularity and each arbiter node uses an 8x8 priority- mux. The network only gets activated when it gets a w-pdown warning signal from the predictor. This signal starts a graceful power-down sequence for the required number of tiles. The w-pdown triggers the backup signal and the system goes into pwr-warning state (other states being on, off, invalid and X). In the pwr-warning phase the system ï¬nishes the remaining compute of the systolic arrays (which can take up to 64 cycles), 898 Authorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply. and starts ï¬‚ushing the results for backup.",
    "source": "Usas.pdf",
    "length": 2138,
    "tokens": 500
  },
  {
    "text": "d) Clause-Level Feedback Optimization: In the proposed Algorithm 5: Group Clause TA Update Input: PT A Update (TA update probability), Feedback (loaded Clause level feedback), L (loaded literal), l mask (literal mask), TA (loaded unsigned TA state), action (TA action of the loaded TAs), LT A rand (random number length), TA rand (random number), cl (loaded clause), cl buffer mask (the clause buffer mask), x (literal width of clause computation matrix), y (clause width of clause computation matrix) 1 TAFD(x, y, L, TA, cl, TA rand, PT A Update, Feedback, l mask, cl buffer mask, action) 2 foreach i y do 3 foreach j x do 4 if cl buffer mask[i] l mask[j] then 5 if Feedback[i] 2 b01 then 6 if cl[i] (cl[i] L[j]) then 7 if PT A Update TA rand then 8 if TA[i][j] 0 then 9 TA[i][j] TA[i][j] 1; 10 else if cl[i] L[j] then 11 if PT A Update TA rand then 12 if TA[i][j] 2LT A 1 then 13 TA[i][j] TA[i][j] 1; 14 else if Feedback[i] 2 b10 then 15 if cl[i] L[j] action[i][j] 1 then 16 if TA[i][j] 2LT A 1 then 17 TA[i][j] TA[i][j] 1; 18 return TA; Algorithm 6: Optimized TA Update Input: Feedback (loaded feedback for the current group of clauses) 1 TAFD_OPT(x, y, L, TA, action, cl, TA rand, Feedback, PT A Update, l mask, cl buffer mask) 2 while TA update not finished do 3 if i y such that Feedback[i] 2 b00 then 4 TAFD(x, y, TA, action, TA rand, PT A Update, L, Feedback, cl, l mask, cl buffer mask); 5 else 6 Allocate TA RAM address to the start of the next group of clauses; 7 return TA; architecture, the TAs in a group of clauses are updated concur- rently. As the model converges while training, the frequency of feedback to clauses reduces [38]. DTM skips loading of all TA slices from BRAM and updation when there is no clause-level feedback to a particular group of clauses, saving PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 10 Fig.",
    "source": "2504.19797v1_Dynamic_Tsetlin_Machine_Accelerators_for_On-Chip_T.pdf",
    "length": 1876,
    "tokens": 571
  },
  {
    "text": "To address this limitation, FlashAttention introduces partial Softmax, which processes blocks incrementally while maintaining running statistics (maximum values and exponential sums). For each new block, these statistics are updated and used to compute partial results, enabling numerically equivalence to standard Softmax while significantly reducing memory overhead. This online compu- tation approach not only ensures numerical stability but also eliminates the need to materialize the full attention matrix in memory. C. Execution Model on Snitch Cluster Baseline Softmax: The baseline kernel is written in C without leveraging Snitch s extended ISA (FREP, SSR and SIMD). Data is transferred via DMA from HBM to the local SPM with double buffering to mask data marshalling latency while the eight Snitch cores process sequences in parallel. The division in Softmax is performed by the FPU s division block, and the exponential function is based on math.h library and uses a piecewise polynomial approximation method with software LUTs. Baseline FlashAttention-2: Following the approach in [5], we adapt FlashAttention-2 to the Snitch cluster architecture with an optimized tiling strategy. The implementation first loads a Q tile to SPM via DMA, then iteratively transfers and processes corresponding K and V tiles. To maximize throughput, we employ double buffering for efficient overlap between memory transfers and computation. The tile size is optimized based on SPM capacity under double buffering constraints. Within each tile, both GEMM and partial Softmax computations are parallelized across the cluster cores. The partial Softmax computation is parallelized by having the eight cluster cores simultaneously compute multiple row statistics. The GEMM implementation leverages Snitch s specialized instruction-level optimizations as detailed in [5], which serves as the foundation for all GEMM operations in this work. D. Exponential Approximation Algorithm For efficient exponential computation, we adopt Schrau- dolph s method [15], which exploits the memory arrangement of floating-point numbers to approximate ex with few basic operations. The input x is scaled to the base-2 domain as x x ln(2), then decomposed into integer and fractional parts: int(x ) x and frac(x ) x x . The approximation is reconstructed as exp(x) 2int(x ) (1 frac(x )). Based on the method proposed by Belano et al.",
    "source": "2504.11227v1_VEXP_A_Low-Cost_RISC-V_ISA_Extension_for_Accelerat.pdf",
    "length": 2405,
    "tokens": 497
  },
  {
    "text": "baseline InterHolo IntraHolo InterIntraHolo Energy (J) HoloCompute Overhead 3.68 1.40 1.28 (c) Energy consumption (J). Figure 7: (a) Average power consumption, (b) execution latency, and (c) energy consumption with different configurations and video inputs. 0 2000 4000 6000 2 4 6 8 10 12 14 16 Power (mW) Depth-planes CPU SoC GPU Mem (a) Power breakdown. 23.6 19.8 7.1 6.7 0 10 20 30 40 bike book bottle cup laptop shoe Avg. bike book bottle cup laptop shoe Avg. bike book bottle cup laptop shoe Avg. bike book bottle cup laptop shoe Avg. baseline InterHolo IntraHolo InterIntraHolo Avg. depthPlanes (b) Avg. number of depth planes. Figure 8: (a): Profiling the power breakdown on the edge GPU prototype [36]; and (b): Average number of depth planes required for four design configurations. of depth planes required by the Inter-Holo scheme is reduced from 23.6 to 19.8, and even further to 7.1 and 6.7, by the Intra-Holo and Inter-Intra-Holo schemes, respectively. The above observations from these two figures explain the power benefits of our proposed designs. Execution Latency: Clearly, the reduction in the number of depth planes when using our approximation schemes can reduce the hologram execution latency as well. As shown in Fig. 7b, overall, the Inter-Holo scheme provides a 1.15 speedup compared to the baseline. Further, a 2.42 speedup is achieved when employing Intra-Holo (with only 0.44 overhead), and 2.68 when employ- ing Inter-Intra-Holo (with only 0.14 overhead). Recall that the number of depth planes for each hologram object affects the ex- ecution latency dramatically as shown in Fig. 4b; thus, these per- formance benefits come from the speedup brought by the reduced depth planes by approximation in our schemes. Another interest- ing observation is that, Intra-Holo saves more execution time than Inter-Holo.",
    "source": "HoloAR.pdf",
    "length": 1838,
    "tokens": 461
  },
  {
    "text": "E.1.2 Extending to Complex Compute: In order to perform multiplication-addition in ReRAM x-bars, two arrays of weights and inputs are used. The inputs are fed to the x-bar, which is a two-dimensional array of ReRAM crossbar arrays. The crossbar arrays are composed of a set of row and column wires that intersect at a set of ReRAM devices (refer Figure 5b). The ReRAM devices are programmed to have different resistance values, which are used to store the weights. During the multiplication-addition operation, the input signals are applied to the rows of the x-bar, and the weights are applied to the columns. The output of each ReRAM device is the product of the input and weight signals, which are added together using the crossbar wires. This results in a single output signal that represents the sum of the weighted inputs. To perform convolution, ReRAM x-bars use a similar approach, but with a more complex circuit. The input signal is applied to the x-bar in the same way, but the weights are now applied in a more structured way. Specifically, the weights are arranged in a way that mimics the convolution operation, such that each weight corresponds to a specific location in the input signal. To perform the convolution operation, the input signal is applied to the rows of the x-bar, and the weights are applied to the columns in a structured way. The output signal is obtained by summing the weighted input signals over a sliding window, which moves across the input signal to compute the convolution. At the circuit level, the ReRAM x-bar for multiplication-addition typically includes several com- ponents, such as digital-to-analog converters (DACs), analog-to-digital converters (ADCs), shift registers, and hold capacitors. The DACs and ADCs are used to convert the digital input and weight signals into analog signals that can be applied to the rows and columns of the x-bar. The shift registers are used to apply the weight signals in a structured way, and the hold capacitors are used to store the analog signals during the multiplication-addition operation. Similarly, for performing convolution, the ReRAM x-bar typically includes additional components, such as delay lines and adders.",
    "source": "NexUME.pdf",
    "length": 2208,
    "tokens": 480
  },
  {
    "text": "III. MACHINE LEARNING FOR NEURAL INTERFACES ML techniques, including traditional and deep learning (DL) models, have transformed neural data analysis by uncovering hidden brain patterns while enhancing real-time neural decod- ing, brain function modeling, and feature-level interpretability. A. Traditioinal Machine Learning Traditional ML models, including linear, tree-based, and probabilistic methods, are widely used in neural signal pro- cessing, neurological symptom detection and motor decoding. Distance-based techniques and window discrimination offer computational efficiency, memory savings, and real-time pro- cessing, making them well-suited for neural interfaces [5], [31]. Methods such as K-means and window discrimination classify data by proximity to centroids, shown to be efficient solutions in simple ML tasks, such as spike detection and sorting [31], [32]. K-Nearest Neighbors (KNN) is a non- parametric, flexible method suited for emotion recognition and early seizure detection, adapting to individualized neural activity [33], [34]. It offers low-complexity training without weight optimizations, but can be computationally expensive during inference on large datasets [35]. Linear Discriminant Analysis (LDA) maximizes class sep- arability while reducing dimensionality, offering an inter- pretable and low-complexity solution for motor decoding [20]. Support Vector Machines (SVMs) effectively decode neural signals, with linear SVMs performing well in simple tasks and kernel SVMs handling complex applications like seizure detection. While SVMs provide efficient inference and low memory usage, the training complexity increases with data size. Alternatively, Gradient Boosted Decision Trees (GBDTs) excel in non-linear, high-dimensional neural data modeling, making them effective for cognitive state classification and seizure detection [12], [18], [36]. They use hierarchical de- cision rules, reducing computational overhead and hardware complexity, while requiring minimal memory to store tree structures and split thresholds [37]. GBDTs offer high inter- pretability, enabling neuro-marker identification and providing clear decision paths for neural decoding analysis. Hidden Markov Models (HMMs) and Kalman Filters (KFs) enable sequential neural decoding through probabilistic modeling, iteratively updating hidden states (e.g., motor intentions) by estimating the most probable state based on observed neural data [10], [38].",
    "source": "2505.02516v1_Machine-Learning-Powered_Neural_Interfaces_for_Sma.pdf",
    "length": 2463,
    "tokens": 498
  },
  {
    "text": "[77] R. Shumaker and L. Stephanie, Virtual, Augmented and Mixed Reality: Designing and Developing Augmented and Virtual Environments: 6th International Conference, VAMR 2014, Held as Part of HCI International 2014, Heraklion, Crete, Greece, June 22-27, 2014, Proceedings, Part I. Springer, 2014. [78] J. Sun, Y. Xie, S. Zhang, G. Zhang, H. Bao, and X. Zhou, You Don t Only Look Once: Constructing spatial-temporal memory for integrated 3d object detection and tracking, ICCV, 2021. [79] M. A. S. Teixeira, H. B. Santos, A. S. d. Oliveira, L. V. Arruda, and F. Neves, Robots perception through 3d point cloud sensors, in Robot Operating System (ROS). Springer, 2017, pp. 525 561. [80] TheAILearner Blogger, Image Processing Nearest Neigh- bour Interpolation, , 2018. [81] TopoDOT Blogger, Using Point Clouds for Augmented and Virtual Reality, , 2022. [82] C. Tu, E. Takeuchi, A. Carballo, and K. Takeda, Point cloud compression for 3d lidar sensor using recurrent neural network with residual blocks, in 2019 International Conference on Robotics and Automation (ICRA), 2019, pp. 3274 3280. [83] D. Valenzuela-Urrutia, R. Mu noz-Riffo, and J. Ruiz-del Solar, Virtual reality-based time-delayed haptic teleoperation using point cloud data, Journal of Intelligent Robotic Systems, vol. 96, no. 3, pp. 387 400, 2019. [84] veesus.com, INTRODUCING ZAPPCHA MOBILE POINT CLOUD CAPTURE AND CLOUD-BASED STORAGE, , 2021. [85] Vision Lab, Nanjing University, Multiscale Point Cloud Geometry Compression, , 2020. [86] Z.-R. Wang, C.-G. Yang, and S.-L. Dai, A fast compression framework based on 3d point cloud data for telepresence, Int. J. Autom.",
    "source": "PCcompress.pdf",
    "length": 1633,
    "tokens": 498
  },
  {
    "text": "The number of channels is doubled after the GConv, thus allowing the pruned model to have the same data dimension as the baseline model. The LWC shows a far better hardware mapping than its corresponding block in the baseline model. We leverage the Kullback-Leibler divergence loss forcing the pruned model to learn similar class distributions as the baseline model: Ldistr T 2 N N X i 1 C X j 1 ybj(Xi; T)log ybj(Xi; T) ypj(Xi; T) (7) where ybj(Xi; T) and ypj(Xi; T) are the softened probability corresponding to class j of the baseline and the pruned model, given the input image Xi, i.e., the pre-softmax divided by a given temperature T. The total loss function is as follows: L (1 Î»)Lce Î»Ldistr (8) where Î» is the hyperparameter controlling the balance between the KD loss Ldistr and the cross-entropy loss Lce. Concretely, we set T 8 and Î» 0.5 for later experiments. Assume that the model contains Nb blocks, we gradually prune them in the direction from block Nb to block 1. For each stage, we replace each block by a LWC block such that the output dimension is kept unchanged. Without loss of generality, we also notice that here a block may contains a single or several layers. The pruned model is initialized with weights of the previous stage and then retrained with the aforementioned loss function. The complete pruning procedure is presented in Algorithm 1. Algorithm 1 Block pruning algorithm for BNNs.",
    "source": "2505.13462v1_End-to-end_fully-binarized_network_design_from_Gen.pdf",
    "length": 1417,
    "tokens": 355
  },
  {
    "text": "Time per Device N Num. of Devices Îµ Îµ ÎµÎµ Pessimistic: T (1 Îµ (N-1)) All errors on one device Figure 5: Modeling the impact of prediction errors on end-to-end system runtime. Three scenarios for the same prediction error rate Ïµ: (1) Optimistic errors do not affect load balancing; (2) Typical errors are evenly dis- tributed across devices, leading to moderate slowdown; (3) Pessimistic errors concen- trate on one device, causing worst-case load imbalance. We model the system-level effects of imperfect pre- diction by analyzing how different error distributions affect FFN computation load balance and communi- cation cost. First, we model FFN compute s runtime with predic- tion error rate Ïµ, which is 1 accuracy for Token- to-Expert Prediction, and averaged L1 distance over number of experts for Distribution-Only Prediction. Figure 5 shows three potential outcomes for the same prediction accuracy for FFN compute s runtime. As- sume that Ïµ 0.1: Optimistic: Errors still result in perfect load balancing; e.g. predicting 85 of tokens instead of 75 for Expert 1 in Figure 2. Typical: Errors are uniformly distributed across GPUs, leading to moderate imbal- ance. The most loaded GPU processes up to (1 Ïµ) avg_tokens. This is the default model used in our runtime simulations. Pessimistic: All errors occur on a single GPU, leading to worst-case imbalance where the bottleneck GPU handles up to N (1 Ïµ) avg_tokens. While unlikely to happen, this scenario represents an upper bound on performance degradation. We apply the same typical-case assumption when modeling communication overhead under Expert Parallelism (EP) for Token-to-Expert Prediction. Unlike compute, however, communication costs always increase with prediction errors, as misrouted tokens inevitably trigger additional inter-GPU data transfers. Optimistic cases do not exist in this context. 3.4 Network Performance Simulation We conduct performance evaluations using an extended version of LLMCompass [36], a block- level simulator for large language model inference, validated with silicon measurements. This simulation-based approach allows us to easily explore design tradeoffs across hardware and software configurations without requiring access to real clusters, which are often costly and impractical for exhaustive testing.",
    "source": "2506.07366v1_MoE-GPS_Guidlines_for_Prediction_Strategy_for_Dyna.pdf",
    "length": 2301,
    "tokens": 491
  },
  {
    "text": "Cloud programming sim- plified: A berkeley view on serverless computing. arXiv preprint arXiv:1902.03383 (2019). [34] Ram Srivatsa Kannan, Lavanya Subramanian, Ashwin Raju, Jeongseob Ahn, Jason Mars, and Lingjia Tang. 2019. GrandSLAm: Guaranteeing SLAs for Jobs in Microservices Execution Frameworks. In EuroSys. [35] Kate Keahey, Jason Anderson, Zhuo Zhen, Pierre Riteau, Paul Ruth, Dan Stanzione, Mert Cevik, Jacob Colleran, Haryadi S. Gunawi, Cody Hammock, Joe Mambretti, Alexander Barnes, FranÃ§ois Halbach, Alex Rocha, and Joe Stubbs. 2020. Lessons Learned from the Chameleon Testbed. In Proceedings of the 2020 USENIX Annual Technical Conference (USENIX ATC 20). USENIX Association. [36] Bernhard Korte and Jens Vygen. 2018. Bin-Packing. In Combinatorial Optimization. Springer, 489 507. [37] JÃ¶rn Kuhlenkamp, Sebastian Werner, and Stefan Tai. 2020. The ifs and buts of less is more: a serverless computing reality check. In 2020 IEEE International Conference on Cloud Engineering (IC2E). IEEE, 154 161. [38] Anup Mohan, Harshad Sane, Kshitij Doshi, Saikrishna Edupuganti, Naren Nayak, and Vadim Sukhomlinov. 2019. Agile cold starts for scalable serverless. In 11th {USENIX} Workshop on Hot Topics in Cloud Computing (HotCloud 19). [39] Edward Oakes, Leon Yang, Dennis Zhou, Kevin Houck, Tyler Harter, Andrea Arpaci-Dusseau, and Remzi Arpaci-Dusseau. 2018. SOCK: Rapid Task Provisioning with Serverless-Optimized Containers. In USENIX ATC. [40] Haoran Qiu, Subho S Banerjee, Saurabh Jha, Zbigniew T Kalbarczyk, and Ravishankar K Iyer. 2020.",
    "source": "kraken.pdf",
    "length": 1545,
    "tokens": 483
  },
  {
    "text": "The devices are spread along the x-axis depending on their year of release, while the y-axis represents the number of DSP slices 10SEEs manifest as Single-Event Upsets (SEUs) soft errors altering memory or as Single-Event Latch-ups (SELs) potentially destructive hard errors requiring a power reset. Refer to the supplemental material for developed space constraints. Manuscript submitted to ACM 12 CÃ©dric LÃ©onard, Dirk Stober, and Martin Schulz 2006 2008 2010 2012 2014 2016 2018 2020 Product release [Year] 64 128 256 512 1024 2048 4096 8192 DSP available per device 12 XC7Z020 6 XCZU7EV 6 XC7VX690T 3 XCZU3EG 3 XCZU9EG 2 XCK26 2 XC7A35T 2 XC6VLX240T 2 XC7K325T 2 XCKU040 XC7Z045 1 XCZU15EG 1 XC3SD1800A 1 5CSXC6 1 XC7Z035 1 XC7Z100 1 XC7A200T 1 U280 1 5CSEMA5F31C6 1 XQRKU060 FPGA family Zynq Virtex Kintex Spartan Kria Alveo Artix Cyclone Fig. 3. Distribution of FPGAs used across articles. Unlike other figures tables, it shows FPGA products used per article (46) instead of per experiment (66). indicates additional boards used in the studies, absent from the experiments reported in Table 3. present on the board relevant for arithmetic computations, like ML. Except for one experiment using the Cyclone V FPGA from Intel, we observe that all considered studies deploy hardware from AMD (formerly Xilinx). The most common family with 60 are Zynq FPGAs, which package FPGA programmable logic with CPUs on a single die to support networking and general-purpose tasks. Such products will be referred to as System-on-Chip (SoC) FPGAs. In contrast, datacenter FPGAs, such as the Alveo Series, are often offered as PCIe cards that can be inserted into a server.",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 1662,
    "tokens": 470
  },
  {
    "text": "2020. CASP14. casp14 . [12] Protein Structure Prediction Center. 2022. CASP15. casp15 . [13] Protein Structure Prediction Center. 2024. CASP16. casp16 . [14] Shenggan Cheng, Xuanlei Zhao, Guangyang Lu, Jiarui Fang, Tian Zheng, Ruidong Wu, Xiwen Zhang, Jian Peng, and Yang You. 2024. FastFold: Optimizing AlphaFold Training and Inference on GPU Clusters. In Proceedings of the 29th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming (Edinburgh, United Kingdom) (PPoPP 24). Asso- ciation for Computing Machinery, New York, NY, USA, 417 430. [15] NVIDIA Corporation. 2007. NVIDIA Nsight Systems. com nsight-systems. [16] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. 2022. Flashat- tention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems 35 (2022), 16344 16359. [17] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale. [18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. [19] Jacques Dubochet, Marc Adrian, Jiin-Ju Chang, Jean-Claude Homo, Jean Lepault, Alasdair W McDowall, and Patrick Schultz. 1988. Cryo-electron microscopy of vitrified specimens. Quarterly reviews of biophysics 21, 2 (1988), 129 228. [20] facebookresearch. 2007. esm.",
    "source": "2505.05893v1_LightNobel_Improving_Sequence_Length_Limitation_in.pdf",
    "length": 1576,
    "tokens": 457
  },
  {
    "text": "Cong has a financial interest in AMD REFERENCES [1] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H.-W. Hon, Unified language model pre-training for natural language understanding and generation, Advances in neural information processing systems, vol. 32, 2019. [2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929, 2020. [3] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., Llama 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288, 2023. [4] M. A. K. Raiaan, M. S. H. Mukta, K. Fatema, N. M. Fahad, S. Sakib, M. M. J. Mim, J. Ahmad, M. E. Ali, and S. Azam, A review on large language models: Architectures, applications, taxonomies, open issues and challenges, IEEE Access, 2024. [5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Å. Kaiser, and I. Polosukhin, Attention is all you need, Advances in neural information processing systems, vol. 30, 2017. [6] H. Chen, N. Zhang, S. Xiang, Z. Zeng, M. Dai, and Z. Zhang, Allo: A programming model for composable accelerator design, Proceedings of the ACM on Programming Languages, vol. 8, no. PLDI, pp. 593 620, 2024.",
    "source": "2502.08807v2_InTAR_Inter-Task_Auto-Reconfigurable_Accelerator_D.pdf",
    "length": 1417,
    "tokens": 496
  },
  {
    "text": "124 132. doi:10.1109 CODESISSS.2015.7331375 [111] Christos Profentzas, Magnus Almgren, and Olaf Landsiedel. 2023. MiniLearn: On-Device Learning for Low-Power IoT Devices. In Proceedings of the 2022 International Conference on Embedded Wireless Systems and Networks (, Linz, Austria,) (EWSN 22). Association for Computing Machinery, New York, NY, USA, 1 11. [112] AÃ«l QuÃ©lennec, Enzo Tartaglione, Pavlo Mozharovskyi, and Van-Tam Nguyen. 2023. Towards On-device Learning on the Edge: Ways to Select Neurons to Update under a Budget Constraint. arXiv:2312.05282 [cs.LG] [113] Jathushan Rajasegaran, Vinoj Jayasundara, Sandaru Jayasekara, Hirunima Jayasekara, Suranga Seneviratne, and Ranga Rodrigo. 2019. DeepCaps: Going Deeper with Capsule Networks. arXiv:1904.09546 [cs.CV] [114] Vinay V. Ramasesh, Ethan Dyer, and Maithra Raghu. 2020. Anatomy of Catastrophic Forgetting: Hidden Representations and Task Semantics. CoRR abs 2007.07400 (2020). arXiv:2007.07400 [115] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. 2016. XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks. arXiv:1603.05279 [cs.CV] [116] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Xiaojiang Chen, and Xin Wang. 2020. A Comprehensive Survey of Neural Architecture Search: Challenges and Solutions. CoRR abs 2006.02903 (2020). arXiv:2006.02903 [117] Crefeda Faviola Rodrigues, Graham Riley, and Mikel Lujan. 2018. Fine-Grained Energy and Performance Profiling framework for Deep Convolutional Neural Networks.",
    "source": "2505.12523v1_Energy-Aware_Deep_Learning_on_Resource-Constrained.pdf",
    "length": 1539,
    "tokens": 471
  },
  {
    "text": "Yet, the root cause for the transient execution has not been mitigated in the hardware. Therefore, we argue that different applications can potentially still be vulnerable and be exploited using different transient execution mechanisms. Since finding new software applications vulnerable to tran- sient execution is not part of the scope of this work, we adopt the original Meltdown setup. For the proof of concept, we disable KPTI on the Linux kernel. Using the ÂµRL generated instructions sequence shown in Figure 11: Proof of concept code that demonstrates the use of RL-generated MMX-x87 transient execution mechanism for reading the physical memory. Figure 11, we trigger transient execution of the following memory access in line, which is an illegal access to kernel memory. Speculative load from the kernel memory is then encoded into the cache so that we can decode it later using Flush Reload. In this experiment, we were able to read a pre-chosen secret value from the kernel memory without using a fault handler or TSX instruction set. This result demonstrates how ÂµRL generated instructions sequences can be exploited, and they can be used as alternatives to the known attack vectors. 12 9 Discussion 9.1 Limitations While our RL framework demonstrates promising results in discovering microarchitectural vulnerabilities, it is important to acknowledge its current limitations: Exploration Focus. While the framework is effective at gen- erating instruction sequences that trigger vulnerabilities, it does not account for the entire system s attack surface. For example, vulnerabilities involving interactions between hard- ware and software, such as operating system mitigations or compiler optimizations, are not explicitly explored. In this work, we did not consider the impact of other system configurations such as Hyperthreading, TSX, SGX, AVX, HW prefetch, previous mitigations, Kernel Samepage Merging, ASLR, page table layout, etc. on the ÂµArch vulnerabilities. We leave this for future work. Sparse and Delayed Rewards. Within the search space, only a small fraction of instruction sequences would indicate a vulnerability, assuming the design went through a thorough security review previously. The delayed nature of rewards, which depends on the cumulative effect of multiple instruc- tions, further complicates learning. Therefore, reward signal is sparse and delayed, making it challenging for the agent to learn the optimal policy. Incomplete CPU State Observability. Our framework relies on performance counters and partial state observations to in- fer microarchitectural behavior.",
    "source": "2502.14307v1_Î¼RL_Discovering_Transient_Execution_Vulnerabilitie.pdf",
    "length": 2610,
    "tokens": 498
  },
  {
    "text": "Offline outlier threshold profiling. To separate outliers from inliers, the topK operation is typically used to maintain a constant ratio of outliers [22]. While this approach results in minimal quan- tization loss, the topK operation, essentially a sorting with a time Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV Cache Quantization ISCA 25, June 21 25, 2025, Tokyo, Japan Middle Group Inlier Dense Outlier Sparse 4-bit 5-bit 7-bit 4-bit 8-bit 16-bit 16-bit Outer Group Frequency Inner Group 0 Middle Group Middle Group Outer Group Outer Group idx idx idx g g g idx g idx idx idx g g g idx g (a) (b) (c) s s s s s s s s Figure 7: Oaken s quantization algorithm consisting of three components: (a) threshold-based online-offline hybrid quantization, (b) group-shift quantization, and (c) fused dense-and-sparse encoding. complexity of ğ‘‚(ğ‘›logğ‘›), introduces significant overhead when per- formed during inference, degrades end-to-end performance. Oaken employs offline outlier threshold profiling, leveraging the consis- tent characteristics in the distribution of KV cache discussed in Section 4.1 to avoid expensive online operations. The criteria for splitting the KV cache into three groups are based on four thresholds determined through offline threshold profiling: ğ‘‡ğ‘œ ğ‘™ğ‘œ,ğ‘‡ğ‘– ğ‘™ğ‘œ,ğ‘‡ğ‘– â„ğ‘–, andğ‘‡ğ‘œ â„ğ‘–.",
    "source": "2503.18599v2_Oaken_Fast_and_Efficient_LLM_Serving_with_Online-O.pdf",
    "length": 1321,
    "tokens": 373
  },
  {
    "text": "2023. The Sparse Abstract Machine. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3 ( conf- loc , city Vancouver city , state BC state , country Canada country , conf-loc ) (ASPLOS 2023). Association for Computing Machinery, New York, NY, USA, 710 726. doi:10.1145 3582016.3582051 [22] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: datasets for Ember: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures machine learning on graphs. In Proceedings of the 34th International Conference on Neural Information Processing Systems ( conf-loc , city Vancouver city , state BC state , country Canada country , conf-loc ) (NIPS 20). Curran Associates Inc., Red Hook, NY, USA, Article 1855, 16 pages. [23] Mohamed Assem Ibrahim, Onur Kayiran, and Shaizeen Aga. 2022. Efficient Cache Utilization via Model-aware Data Placement for Recommendation Models. In Proceedings of the International Symposium on Memory Systems (Washington DC, DC, USA) (MEMSYS 21). Association for Computing Machinery, New York, NY, USA, Article 2, 11 pages. doi:10.1145 3488423.3519317 [24] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convolutional Architecture for Fast Feature Embedding. arXiv preprint arXiv:1408.5093 (2014). [25] Manas R. Joglekar, Cong Li, Mei Chen, Taibai Xu, Xiaoming Wang, Jay K. Adams, Pranav Khaitan, Jiahui Liu, and Quoc V. Le. 2020. Neural Input Search for Large Scale Recommendation Models. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery Data Mining (Virtual Event, CA, USA) (KDD 20).",
    "source": "2504.09870v1_Ember_A_Compiler_for_Efficient_Embedding_Operation.pdf",
    "length": 1836,
    "tokens": 490
  },
  {
    "text": "Figure 1 captures the different integration strategies proposed in the literature. Case 1 in figure 1 shows the IDS integrated as CAN Transreceiver CAN Controller Rx Tx ECU IDS Strategy 1 : IDS task on ECU, Sharing critical compute CAN Transreceiver CAN Controller Tx Rx Strategy 3 : ECU(PS) IDS(PL) on same SoC CAN Transreceiver Tx Rx Proposed : SecCAN : IDS embedded inside CAN Controller CAN Controller IDS CAN Transreceiver CAN Controller Rx Tx Strategy 2 : Dedicated IDS node on the network Hybrid ECU ECU IDS SoC IDS ECU Fig. 1: The figure illustrates conventional integration strategies for CAN IDSs reported in the literature, and the proposed case for embedding IDS within the controller. a software task on an existing ECU, while case 2 shows a dedicated IDS accelerator where the IDS could be deployed as the lone software task on an ECU, GPU (edge device or a standard GPU) or a microcontroller platform like Raspberry Pi [9]. Case 3 shows a coupled accelerator, where the ECU of- floads IDS to the accelerator on the same SoC once the packet is received from the CAN controller. In all the above cases, the CAN message has to be completely received by the controller and subsequently read by the coupled ECU processing element before the ML model can perform IDS checks. In this letter, we propose SecCAN, an extended CAN controller that integrates an IDS accelerator into its receive side datapath, as shown in figure 1 case 4. This critical design choice allows the IDS accelerator to directly extract CAN bus data (ID and payload) from within the receive path of the CAN controller for overlapping IDS execution with the current packet s reception. Complementing this with a compact 4-bit quantised multi-layer perception (QMLP) IDS model deployed as an unrolled dataflow accelerator, SecCAN completely hides the latency of IDS within the reception window of the current CAN frame. This is a departure from existing ML-based IDS solutions in the research literature, where IDS execution is triggered after the message is transferred to the ECU from the CAN controller. We evaluate SecCAN controller on a Zynq Ultrascale platform with the ARM cores on the Zynq device acting as the coupled ECU.",
    "source": "2505.14924v1_SecCAN_An_Extended_CAN_Controller_with_Embedded_In.pdf",
    "length": 2209,
    "tokens": 488
  },
  {
    "text": "[135], who improve energy efficiency and latency by assigning optimal bitwidths per layer. Similarly, we see potential in using AutoML, particularly Neural Architecture Search (NAS), to identify DNN architectures tailored to hardware constraints. Approaches like EfficientNet [116] or MobiletNetv3 [51] are designed using platform-aware NAS, but hardware-aware NAS (HW-NAS) is still a young field with significant room for growth [10]. As GPUs increasingly integrate dedicated CNN units, such techniques become even more relevant for FPGAs, which must exploit their configurability to remain competitive. FPGA-aware NAS offers a promising path to design performant models well-matched to FPGA deployment, a research direction supported by datasets like HW-NAS-Bench [72] that estimate FPGA performance. Trust in and Interpretability of Onboard AI. Another underexplored area in this survey is Uncertainty Quantification (UQ). UQ methods estimate the confidence of model predictions to improve interpretability and support more reliable decision-making onboard. Only Myojin et al. [84] apply UQ techniques, using Monte Carlo (MC) Dropout [29] to estimate model uncertainty through multiple stochastic forward passes. Other established methods remain unexploited in this context and deserve further investigation; for a comprehensive overview of UQ in Deep Learning, we refer readers to Gawlikowski et al. [32]. Practical benefits of UQ include uncertainty rejection, where predictions exceeding a confidence threshold are discarded, potentially easing downlink stress and increasing reliability. Similarly, explainable AI (xAI) is crucial for critical decision-making onboard, such as alert triggering or data prioritization. Yet, only Ieracitano et al. 23 Manuscript submitted to ACM 28 CÃ©dric LÃ©onard, Dirk Stober, and Martin Schulz [55] employ xAI tools to interpret model decisions. We believe that lightweight xAI and UQ methods can significantly increase the trustworthiness and adoption of AI solutions for onboard processing [50]. 6.3 Recommendations for Future Work Remote Sensing Good Practices. We encourage researchers to use open-source data or to publish their datasets. When a full release is not possible due to confidentiality agreements, studies should detail, as rigorously as possible, sensor specifications, selected scenes, ground truth, data splits, etc.",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 2376,
    "tokens": 495
  },
  {
    "text": "Despite its elevated power consumption and design complexity, it is well suited for systems requiring robust performance over varying conditions [36]. The DBPMixer features a fully differential structure that suppresses signal leakage and improves isolation, at the cost of signal loss and a strong local oscillator drive requirement [37]. The SBAMixer includes an amplification stage preceding the switching core to enhance signal strength and reduce noise, offering a balanced performance trade-off with increased power consumption and limited spurious rejection [30]. The SBPMixer employs a minimalist switching structure to perform frequency translation without active gain, enabling low power operation in applications with relaxed performance demands [38]. The parameters and performance metrics for these mixer topologies are listed in Table 7. (a) DPAMixer (b) DBPMixer (c) SBAMixer (d) SBPMixer Figure 9: Schematic diagrams of the four Mixer topologies. Table 7: Mixer topologies with parameter sweep ranges, sample sizes, and performance metrics. Dataset Type Topology (Code) of Samples Parameter Sweep Range Performance Metrics (Unit) Mixer DBAMixer (4) 42k C [1 10] pF DCP (W) CGain (dB) NF (dB) VSwg (V) R [1 10] kâ„¦ WN1 [10 30] Âµm WN2 [5 25] Âµm DBPMixer (5) 42k C [100 500] fF R [100 600] â„¦ WN [10 30] Âµm SBAMixer (6) 52k C [1 15] pF R [0.7 2.1] kâ„¦ WN1 [10 30] Âµm WN2 [10 20] Âµm Itail [3 10] mA SBPMixer (7) 44k C [1 30] pF R [1 30] kâ„¦ WN [5 29.5] Âµm B.3 Power Amplifiers (PAs) Power amplifiers (PAs) are the most power-intensive components in radio-frequency (RF) systems and serve as the final interface between transceiver electronics and the antenna. Given their widespread use and the stringent demands of modern communication standards, PA design requires careful trade-offs across key performance metrics [39].",
    "source": "2505.21923v1_FALCON_An_ML_Framework_for_Fully_Automated_Layout-.pdf",
    "length": 1830,
    "tokens": 489
  },
  {
    "text": "9a), and ResNet-50 (A3 pa in Fig. 9b), respectively. Thus, XAI-Gen leads to almost 7 , 2.6 , 2 , and 2 lower energy consumption in approximate LeNet5, ResNet18, ResNet-34, and ResNet-50, respectively. Interestingly, some approximate architectures achieve up to 6 higher accuracy with approximation. For example, A6 pa and A5 pa in the case of approximate LeNet5 and ResNet- 18. This is because XAI assigns the importance score to the neurons according to the target classes. When we remove the non-critical neurons that do not significantly impact the image classification, the accuracy of AxDNNs is slightly improved in some cases. Table I presents the most suitable configura- tions of skipping thresholds and approximate multipliers for AxDNN architectures generated by the XAI-Gen algorithm. It is worth noticing that despite 70 of the skipped neurons and approximate multiplier with the highest approximation error (0.52 ) in a layer, the AxDNN architectures generated by XAI-Gen only suffer from 1 -2 accuracy loss only. This shows that XAI-Gen carefully skips the non-critical neurons and applies approximation to the non-critical layers. D. XAI-NAS: XAI-guided Neural Architecture Search Since XAI-Gen adapts the layer conductance threshold for an appropriate selection of the appropriate approximate multi- pliers instead of an exhaustive hit-and-trial method, integrating XAI-Gen in NAS can expedite the search process. XAI- NAS takes advantage of XAI-Gen to incorporate heterogenous approximate multipliers in the AxDNN layers. This helps achieve higher energy efficiency than NAS. Fig. 10 shows different generations (g) of the AxDNN search process for the Table I: Best parameter configurations for approximate LeNet5 (L5), ResNet18 (R1), ResNet-34 (R2), and ResNet-50 (R3) architectures generated with XAI-Gen. M1 is an accurate multiplier.",
    "source": "2503.16583v1_Explainable_AI-Guided_Efficient_Approximate_DNN_Ge.pdf",
    "length": 1854,
    "tokens": 427
  },
  {
    "text": "1 FPGA-based Acceleration for Convolutional Neural Networks: A Comprehensive Review Junye Jiang , Yaan Zhou , Yuanhao Gong , Haoxuan Yuan, and Shuanglong Liu Abstract Convolutional Neural Networks (CNNs) are fun- damental to deep learning, driving applications across various domains. However, their growing complexity has significantly increased computational demands, necessitating efficient hard- ware accelerators. Field-Programmable Gate Arrays (FPGAs) have emerged as a leading solution, offering reconfigurability, parallelism, and energy efficiency. This paper provides a compre- hensive review of FPGA-based hardware accelerators specifically designed for CNNs. It presents and summarizes the performance evaluation framework grounded in existing studies and explores key optimization strategies, such as parallel computing, dataflow optimization, and hardware-software co-design. It also compares various FPGA architectures in terms of latency, throughput, compute efficiency, power consumption, and resource utilization. Finally, the paper highlights future challenges and opportunities, emphasizing the potential for continued innovation in this field. Index Terms Convolutional Neural Networks (CNNs), Field Programmable Gate Arrays (FPGAs), Hardware Accelerator, Compute Efficiency, Parallel Computing, Design Space Explo- ration (DSE) I. INTRODUCTION In recent years, Deep Learning (DL) has gained significant attention in computer science [1], with Convolutional Neural Networks (CNNs) serving as a cornerstone of this progress. CNNs have achieved remarkable success across various appli- cations, including image classification [2], object detection [3], and semantic segmentation [4]. Their transformative impact has led to widespread adoption by industry leaders, recog- nizing CNNs as a vital tool for innovation and competitive advantage. The strength of CNNs lies in their ability to process high- dimensional data through convolutional operations, pooling, and non-linear activations, enabling automated multi-level feature extraction. However, this capability comes at the cost of increased computational complexity and resource demands. With their adoption in mobile and edge devices like drones and smartphones [5], CNNs face critical challenges in achieving high performance under resource constraints. To address these challenges, extensive research has been conducted on hardware acceleration for CNNs [6] [9].",
    "source": "2505.13461v1_FPGA-based_Acceleration_for_Convolutional_Neural_N.pdf",
    "length": 2439,
    "tokens": 475
  },
  {
    "text": "Task Description Source Count RTL Code Generation Understanding Line Level DeepRTL2 341310 Module Level (Detailed Specification) DeepRTL2 45519 MG-Verilog 10035 DeepCircuitX 32809 Module Level (High-Level Description) DeepRTL2 46876 RTLCoder 25001 MG-Verilog 10037 DeepCircuitX 38179 Natural Language Code Search N A DeepRTL2 59700 Functionality Equivalence Checking Equal Pairs DeepRTL2 9532 Unequal Pairs DeepRTL2 23330 Performance Prediction Area DeepRTL2 18766 Delay DeepRTL2 18766 Table 6: The overall dataset statistics for all evaluated tasks. type d corresponds to the case where some rewritten code snippets, free of syntax er- rors, are functionally equivalent to the original code, while others are not. For all four types of data samples, we convert them into contrastive learning samples as follows: type a: {\"query\": original_code, \"pos\": origi- nal_text, \"neg\": None} {\"query\": original_text, \"pos\": origi- nal_code, \"neg\": None} type b: {\"query\": original_code, \"pos\": origi- nal_text, \"neg\": None} {\"query\": original_text, \"pos\": origi- nal_code, \"neg\": None} {\"query\": original_code, \"pos\": equiva- lent_code, \"neg\": None} {\"query\": equivalent_code, \"pos\": origi- nal_code, \"neg\": None} type c: {\"query\": original_code, \"pos\": origi- nal_text, \"neg\": inequivalent_code} {\"query\": original_text, \"pos\": origi- nal_code, \"neg\": None} type d: {\"query\": original_code, \"pos\": origi- nal_text, \"neg\": inequivalent_code} {\"query\": original_code, \"pos\": equiva- lent_code, \"neg\": inequivalent_code} {\"query\": original_text, \"pos\": origi- nal_code, \"neg\": None} {\"query\": equivalent_code, \"pos\": origi- nal_code, \"neg\": inequivalent_code} In each of the contrastive learning samples above, the key pos refers to the positive instance of the query code text, while the key neg refers to the hard negative instance.",
    "source": "2506.15697v1_DeepRTL2_A_Versatile_Model_for_RTL-Related_Tasks.pdf",
    "length": 1823,
    "tokens": 529
  },
  {
    "text": "Loop unrolling by a factor of 4 is applied to the segments loop. ------------------------------------------------------------------------- for (int seg 0; seg 16; seg 4) { for (int j_in_o 0; j_in_o 4; j_in_o ) { uint32_t j_off j_in_o tile_dim; uint32_t acc_addr; will hold the computed accumulator address for preload --- Unrolled segment \"seg\" ------------------------------------------------- { uint32_t block_id seg 4; uint32_t sub_offset (seg 4) tile_dim; uint32_t A_seg_addr current_A_buffer block_id (tile_dim 4) sub_offset; If this is the very first compute for this sub -tile , use overwrite mode if (first_compute[j_in_o ]) { acc_addr cur_acc_base j_off; first_compute[j_in_o] 0; } else { acc_addr (cur_acc_base j_off) 0x40000000; } preload(new_B_base seg (tile_dim 4) j_off , acc_addr , tile_dim , tile_dim , tile_dim , tile_dim); compute_preloaded(A_seg_addr , ( uint32_t)0, tile_dim , tile_dim , tile_dim , tile_dim); } --- Unrolled segment \"seg 1\" ------------------------------------------------- { uint32_t block_id (seg 1) 4; uint32_t sub_offset ((seg 1) 4) tile_dim; uint32_t A_seg_addr current_A_buffer block_id (tile_dim 4) sub_offset; preload(new_B_base (seg 1) (tile_dim 4) j_off , (cur_acc_base j_off) 0x40000000 , tile_dim , tile_dim , tile_dim , tile_dim); compute_preloaded(A_seg_addr , ( uint32_t)0, tile_dim , tile_dim , tile_dim , tile_dim); } --- Unrolled segment \"seg 2\" ------------------------------------------------- { uint32_t block_id (seg 2) 4; uint32_t sub_offset ((seg 2) 4) tile_dim; uint32_t A_seg_addr current_A_buffer block_id (tile_dim 4) sub_offset; preload(new_B_base (seg 2) (tile_dim 4) j_off , (cur_acc_base j_off) 0x40000000 , tile_dim , tile_dim , tile_dim , tile_dim); compute_preloaded(A_seg_addr , ( uint32_t)0, tile_dim , tile_dim , tile_dim , tile_dim); } Figure 20: Example from Fig. 19, continued.",
    "source": "2505.18574v2_Autocomp_LLM-Driven_Code_Optimization_for_Tensor_A.pdf",
    "length": 1855,
    "tokens": 651
  },
  {
    "text": "These compelling advantages demonstrate that Oaken efficiently addresses the two primary bottlenecks of modern LLM serving. Acknowledgments We thank the anonymous reviewers for their comments and feed- back. This work was supported by the Institute of Information Communications Technology Planning Evaluation (IITP) (No.2018- 0-00503, No.RS-2024-00459797, Development of ML compiler frame- work for on-device AI), IITP under the Graduate School of Artificial Intelligence Semiconductor (IITP-2025-RS-2023-00256472), and the National Research Foundation of Korea (NRF) (RS-2024-00342148), grant funded by the Korea government (MSIT). This work was also partly supported by HyperAccel. ISCA 25, June 21 25, 2025, Tokyo, Japan Kim, Hong, et al. References [1] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani, Alexey Tumanov, and Ramachandran Ramjee. 2024. Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve. arXiv preprint arXiv:2403.02310 (2024). [2] Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh, Zhen Stephen Gou, Phil Blunsom, Ahmet ÃœstÃ¼n, and Sara Hooker. 2023. Intriguing Properties of Quantization at Scale. In Thirty-seventh Conference on Neural Information Processing Systems. [3] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico LebrÃ³n, and Sumit Sanghai. 2023. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.",
    "source": "2503.18599v2_Oaken_Fast_and_Efficient_LLM_Serving_with_Online-O.pdf",
    "length": 1461,
    "tokens": 411
  },
  {
    "text": "RETENTION introduces purity threshold pruning to minimize model complexity while ensuring controlled accuracy degradation for bagging-based models. A tree mapping scheme with two data placement strategies, occurrence-based double reordering and similarity- based path clustering, is proposed to further alleviate memory redundancy for energy-efficient mapping and space-efficient mapping. Experimental results show that implementing the tree mapping scheme alone achieves 1.46 to 21.30 better space efficiency, while the full RETENTION framework yields 4.35 to 207.12 improvement with less than 3 accuracy loss. These results demonstrate that RETENTION is extremely effective in reducing CAM capacity requirement, offering a resource-efficient solution for tree-based model acceleration. By implementing RETENTION, resource retention becomes feasible, bringing CAM-based acceleration closer to practical- ity in resource-constrained environments. REFERENCES [1] Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. Inf. Fusion, 81(C):84 90, May 2022. [2] L eo Grinsztajn, Edouard Oyallon, and Ga el Varoquaux. Why do tree- based models still outperform deep learning on typical tabular data? In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. [3] Scott M. Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jor- dan M. Prutkin, Bala Nair, Ronit Katz, Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee. From local explanations to global understanding with explainable ai for trees. Nature Machine Intelligence, 2(1):56 67, 2020. [4] DARPA. Explainable artificial intelligence (xai). research programs explainable-artificial-intelligence, 2017. [5] Yizhen Zheng, Huan Yee Koh, Jiaxin Ju, Anh T. N. Nguyen, Lauren T. May, Geoffrey I. Webb, and Shirui Pan. Large language models for scientific discovery in molecular property prediction. Nature Machine Intelligence, 2025.",
    "source": "2506.05994v1_RETENTION_Resource-Efficient_Tree-Based_Ensemble_M.pdf",
    "length": 1998,
    "tokens": 460
  },
  {
    "text": "This paper focuses on code gener- ation that aims at producing code from natural language descrip- tions prompts. To facilitate the development of code generation This paper focues on C-based HLS. static void compute_mult_16bit( hls::stream uint16_t inStream1, hls::stream uint16_t inStream2, hls::stream uint16_t outStream, int vSize) { execute: for (int i 0; i vSize; i ) { pragma HLS LOOP_TRIPCOUNT min size max size outStream (inStream1.read() inStream2.read()); } } HLS-based design module multi_16bit( input clk, input rst_n, input start, input [15:0] ain, input [15:0] bin, output [31:0] yout, output done ); reg [15:0] areg; reg [15:0] breg; reg [31:0] yout_r; reg done_r; reg [4:0] i; always (posedge clk or negedge rst_n) if (!rst_n) i 5'd0; else if (start i 5'd17) i i 1'b1; else if (!start) i 5'd0; always (posedge clk or negedge rst_n) if (!rst_n) done_r 1'b0; else if (i 5'd16) done_r 1'b1; else if (i 5'd17) done_r 1'b0; assign done done_r; always (posedge clk or negedge rst_n) begin if (!rst_n) begin areg 16'h0000; breg 16'h0000; yout_r 32'h00000000; end else if (start) begin if (i 5'd0) begin areg ain; breg bin; end else if (i 5'd0 i 5'd17) begin if (areg[i-1]) yout_r yout_r ({16'h0000, breg} (i-1)); end end end assign yout yout_r; endmodule Verilog-based design 0 20 40 60 80 100 HLS Verilog normalized token usage Token Comparison Figure 2: HLS-based and Verilog-based programs.",
    "source": "2502.13921v2_Exploring_Code_Language_Models_for_Automated_HLS-b.pdf",
    "length": 1403,
    "tokens": 518
  },
  {
    "text": "Source-Target Method Compilation Accuracy ( ) Computation Accuracy ( ) CUDA C BANG C HIP C With VNNI CUDA C BANG C HIP C With VNNI CUDA C GPT-4 - 50.6 97.0 84.5 - 7.7 96.4 30.4 OpenAI o1 - 51.8 98.2 85.1 - 48.2 98.2 55.4 QiMeng-Xpiler - 100.0 100.0 100.0 - 91.7 100.0 95.2 BANG C GPT-4 69.0 - 66.1 23.8 6.5 - 6.5 13.1 OpenAI o1 71.4 - 97.0 41.7 10.1 - 7.7 23.2 QiMeng-Xpiler 100.0 - 100.0 100.0 95.8 - 97.0 95.2 HIP GPT-4 97.0 35.1 - 85.1 97.0 5.4 - 24.4 OpenAI o1 98.8 42.3 - 88.7 98.2 9.0 - 30.4 QiMeng-Xpiler 100.0 100.0 - 100.0 100.0 86.9 - 96.4 C With VNNI GPT-4 81.5 41.7 74.7 - 14.3 6.0 12.5 - OpenAI o1 87.5 55.4 97.0 - 51.2 10.7 96.4 - QiMeng-Xpiler 100.0 99.4 100.0 - 98.2 88.7 99.4 - Currently, AI techniques in compilers are mainly focused on improving the optimization sequences within the high- dimensional space of existing compiler frameworks, also known as the Phase Ordering Problem. Yet they struggle to generate an end-to-end compiler that handles both two fundamental tasks for processors. To address the long-term goal of creating an end-to-end compiler capable of both translation and optimization tasks, we have explored the automated compiler tool-chain design methods based on the Software Design Agent, investigating two different approaches: 1) automatically generating compiler backend code.",
    "source": "2506.05007v1_QiMeng_Fully_Automated_Hardware_and_Software_Desig.pdf",
    "length": 1320,
    "tokens": 461
  },
  {
    "text": "Eq. (7) suggests that any prefill stage can only be at one level in L. Different than the prefill phase, the decode phase of a request can be served in multiple decode stages. Thus, we introduce wi,j,k [0, 1] to represent the proportion of the decoding phase of request i executed in the kth decode stage on client j, for i I, j J , and k Kd. The time length of a decode stage is provided by Eq. (8). Let di,j,k {0, 1} be the assignment of decode stage k to request i on client j for decode phase, for i I, j J , and k Kd. Eqs. (9)-(11) together sets the rule that the decoding phase of a request must immediately follow the consecutive prefilling phase or its previous decoding phase. Eqs. (12)-(18) relate the assignment decisions among requests, clients, and bins. Eqs. (19)-(25) define the domain for each set of variables, respectively. Eqs (1)-(25) will hereafter be referred to as the original model . We attempted to solve the original model using com- mercial MIP solvers, such as Gurobi. However, we found it nearly impossible to solve such a large-scale problem directly. The number of requests, denoted by I , is around 1,000 in small cases, while in larger cases, it can be up to 10,000. The number of batch sizes or client numbers, denoted by J , can reach up to 200. The number of exclusive bins, denoted by K , often is on the same order of magnitude as I . To probe the solving cost, we solved a down-scaled toy case model with merely 100 requests and 20 clients, which took Gurobi more than 3,600 seconds to yield a near-optimal solution without fully closing the dual gap. Solving this problem in its original form is hence evidently impractical and cannot be solved in hours. Therefore, it is necessary to decompose this problem into stages and address it sequentially. IV. SOLUTION METHOD A. Method overview The original model, provided by Eqs. (1)-(25), demands a solution method capable of making decisions within 10 milliseconds in an asynchronous cycle, as the decoding time per client batch can be around 50 milliseconds.",
    "source": "2502.15763v1_Hybrid_Offline-online_Scheduling_Method_for_Large_.pdf",
    "length": 2047,
    "tokens": 495
  },
  {
    "text": "[76] 2020 AP2D-Net Custom CNN - Zynq US 55.0 (mIoU) - - Flying-object det. PennSyn2Real [88] [113] 2021 SSD 0.25x SSD VGG16 Zynq US 76.2 (mAP) - 4.04G [113] 2021 SSD 0.5x SSD VGG16 Zynq US 83.9 (mAP) - 15.65G [113] 2021 SSD 1.0x SSD VGG16 Zynq US 88.4 (mAP) - 61.60G UAV RGB (cust.) [87] 2024 YOLOv4 YOLO Darknet53 Zynq US 83.7 (mAP) 245 105.9G [87] 2024 YOLOv4-tiny 3L YOLO Darknet53-tiny Zynq US 72.24 (mAP) 24.10 15.32G Aircraft det. Airbus [31] 2024 ResNet-18 YOLOv2 YOLO ResNet-18 Zynq US 91 (AP) - - GES RGB (sim.) [127] 2021 Improved YOLOv3 YOLO DarkNet53 Zynq US 92 (OA) 8.50 - UAV obstacles det. UAV RGB MMW (cust.) [125] 2024 Lightweight CNN ResNet ResNet-18 Zynq 7000 70.1 (mAP) - 4.6G Railway defect det. FastenerDataset (cust.) [136] 2024 Improved YOLOv4-tiny YOLO Darknet53-tiny Zynq US 95.1 (mAP) - - SAR Ship det. SSDD [73] [132] 2022 YOLO MobileNetv1 Virtex-7 94 (AP) 0.09 0.45G [132] 2022 YOLO MobileNetv2 Virtex-7 93.3 (AP) 0.11 0.56G [132] 2022 YOLO SqueezeNet Virtex-7 92.8 (AP) 0.09 0.6G Regr. SAR Oil spills mon. SAR (sim.) [43] 2022 MLP Shallow NN - Zynq 7000 - - - Task: Regr.",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 1101,
    "tokens": 486
  },
  {
    "text": "Hence, nor- malized by number of requests, bigger VMs would still incur similar costs as smaller VMs. Observation 2: VMs should be used to handle requests during constant arrival rates. Also, the number of concurrent requests which can be executed in VMs should be accurately determined to meet response latency. 0 0.4 0.8 1.2 1.6 Berkley WITS Twitter Wiki Normalized VMs reactive util_aware exascale Figure 4. Over-provisioning of util_aware and exascale, nor- malized to a baseline reactive scheme for four traces. 2.3 Over-provisioning VMs Real-world request arrivals rates are usually not constant as they significantly vary over time (e.g. diurnal, flash-crowds etc.) Therefore, resource procurement and management deci- sions need to be adjusted depending on the resource utiliza- tion load and arrival rates. Public cloud providers leave these major decisions to be Å‚manually handled\" by users, which is very time consuming and strenuous. As a result, the major- ity of application providers use static resource provisioning, which results in poor resource utilization and higher costs. Prior works [3, 9] have tried to solve the resource scal- ing problem with respect to hosting the applications in VMs. They employ autoscaling mechanisms to cope up with dy- namic load. These autoscaling mechanisms can be of two types: (i) spawn VMs if the resource utilization of existing VMs reaches a certain threshold (80 in most cases) [9], and (ii) spawn additional VMs than predicted request demand [6]. We name the former autoscaling scheme as util_aware and the later as exascale. Both these schemes suffer from over- provisioning VMs because (i) we cannot always accurately predict the future load, and (ii) resource utilization is not always the right indicator for increased load. We conduct simulation experiments to compare the schemes, using the profiled values (explained in Section 2.2) for four different well-known request arrival traces. Each request in the trace is associated with an ML inference query, which is randomly picked from our model pool. Figure 4 shows the ratio of over-provisioned VMs compared to a baseline reactive autoscaling mechanism.",
    "source": "PubCloudHet.pdf",
    "length": 2169,
    "tokens": 483
  },
  {
    "text": "Training is typically compute-intensive, inference (particularly the decode phase of LLMs) is often limited by memory bandwidth, and tasks such as autonomous-driving model training involve substantial CPU-side data preprocessing. Fixed node configurations cannot efficiently accommodate this diversity, often leading to over-provisioning or underutilization. To maximize efficiency and adaptability, modern AI infrastructure must enable dynamic, fine-grained composition of heterogeneous resources, e.g., NPUs, CPUs, and memory, adapted to the specific demands of each workload. Challenge 3: Enabling Converged Execution of AI and Data-Intensive Workloads. AI workflows increasingly intersect with traditional data-intensive operations such as data inges- tion, preprocessing, retrieval, analytics, and simulation. Meanwhile, general-purpose workloads, e.g., databases, big data, and HPC, are themselves evolving to incorporate AI capabilities. These converged execution patterns demand high-throughput, low-latency communication and flexible resource orchestration. However, legacy datacenter infrastructures primarily optimized for conven- tional general-purpose workloads struggle to meet these stringent requirements. Enabling efficient convergence of AI and data-intensive tasks requires a fundamentally new infrastructure. Challenge 4: Delivering Memory-class Storage Performance. Modern AI pipelines operate at unprecedented data scales that far exceed the capabilities of traditional storage systems. Tasks, 8 Datacenter Networking (Scale Out) Ultra-High-Performance Networking (Scale Up) Disaggregated NIC Pool One CloudMatrix Supernode NPU Disaggregated NPU Pool CPU Disaggregated CPU Pool Mem Disaggregated Memory Pool Others Disaggregated Other Resources Ultra-High-Performance Networking (Scale Up) Other CloudMatrix Supernodes NIC NIC Fig. 1. Huawei s CloudMatrix architecture vision reimagines AI datacenter infrastructure from the ground up. By dismantling traditional siloed designs, it enables full peer-to-peer disaggregation and pooling of CPUs, NPUs, memory, NICs, and other resources over a unified, ultra-high-performance networking, forming the foundation for scalable, AI-native datacenters.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2216,
    "tokens": 450
  },
  {
    "text": "From the mixed-precision quantization map, highlighted by var- ious colors in Fig. 6, we observe that the EDP-focused models se- lected by CrossNAS tend to use higher weight bit widths in the middle convolutional layers, while the head and tail layers have lower weight bit precision. The activation bit width is generally Figure 6: Block types along with channel numbers (ğµğ‘™ğ‘œğ‘ğ‘˜ ğ¾) selected by CrossNAS for (a) CIFAR-10 dataset with ğ‘¤ğ‘ğ‘ğ‘ 0.99 and (b) ğ‘¤ğ‘ğ‘ğ‘ 0.8; (c) CIFAR-100 dataset with ğ‘¤ğ‘ğ‘ğ‘ 0.99 and (d)ğ‘¤ğ‘ğ‘ğ‘ 0.8; (e) corresponding PIM circuit configuration and quantization color map. AAP denotes adaptive average pooling, WB and AB denote weight and activation bit width, respectively. Underlined blocks indicate ğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘‘ğ‘’ 2. lower in the middle layers, whereas the head and tail layers exhibit higher activation precision. For shallower models, e.g., Fig. 6 (b), there is a tendency to apply the same activation bit width across the entire architecture. From the PIM circuit configuration selected by CrossNAS, shown in Fig. 6 (e), we observe that the EDP-focused models prefer a crossbar size of 256 256. This configuration, combined with an 8-bit ADC, offers the best balance between accuracy and EDP. Most of the EDP-focused models opt for a 2-bit DAC, resulting in minimal accuracy loss, while improving both latency and energy efficiency due to the reduced number of digital-to-analog conversions. 5 Conclusion In this paper, we present CrossNAS, an efficient weight-sharing- based NAS framework for PIM systems. We construct a multi- dimensional cross-layer search space that includes diverse archi- tectures, layer-specific weight and activation bit widths, and PIM parameters such as crossbar size and ADC DAC precisions.",
    "source": "2505.22868v1_CrossNAS_A_Cross-Layer_Neural_Architecture_Search_.pdf",
    "length": 1730,
    "tokens": 478
  },
  {
    "text": "Soft labels encode inter- class relationships, providing richer supervision and enabling the student model to generalize better. However, since well-trained teacher models tend to produce sparse probability distributions with high confidence in the correct class, a temperature parameter C is introduced in the softmax function to soften the distribution and enhance knowledge transfer. Mathematically, given input data X with associated hard labels y for K classes, the soft labels from the teacher model q {qi}K i 1 are computed as: qi exp(zi C) PK j 1 exp(zj C), where zi represents the teacher s logit for class i and C is the temperature parameter. As C increases, q approaches a uniform distribution, enhancing inter-class information. The student model is optimized using a combined loss function that balances hard label loss LHard and soft label loss LSoft: LKD 1 R X X,y ((1 Î³)LHard(X, y) Î³LSoft(X, q)) , (1) where Î³ controls the trade-off between the two loss terms, and R is the total num- ber of input samples. The individual loss functions are defined as: LHard(X, y) PK i 1 yi log pi(X) or LSoft(X, q) PK i 1 qi log pi(X), where pi(X) represents the student model s predicted probability for class i. Hard labels use C 1, whereas soft labels utilize the teacher s temperature. Knowledge distillation is widely applied in onboard learning to improve memory efficiency and reduce computational demands [69 72]. Sepahvand et al. [34] introduced a tensor decomposition-based KD approach, achieving a 265.67 compression rate 7 for ResNet-18 with minimal accuracy loss. In [35], a method named Moonshine is developed with similar student-teacher architecture to reduce resource and memory utilization, which resulted 40 smaller model with 5.02 less error than baseline after compression. Xiao et al. [36] introduced DQN-KD, applying knowledge distilation with reinforcement learning to minimize memory utilization where it achieved 50.4 fewer FLOPs (flops full form floating point operations per second) than baseline with 47.5 parameter reduction.",
    "source": "2505.08793v1_Onboard_Optimization_and_Learning_A_Survey.pdf",
    "length": 2057,
    "tokens": 464
  },
  {
    "text": "FPL, 2019, pp. 397 403. 19 [106] Intel , Intel FPGA Power and Thermal Calculator User Guide , 2024. [Online]. Available: docs programmable 683445 22-2 overview-of-the.html [107] Z. Lin et al., Powergear: Early-stage power estimation in fpga hls via heterogeneous edge-centric gnns, in Proc. DATE, 2022, pp. 1341 1346. [108] S. Dai et al., Fast and accurate estimation of quality of results in high-level synthesis with machine learning, in Proc. FCCM, 2018, pp. 129 132. [109] D. Koeplinger et al., Automatic generation of efficient accelerators for reconfigurable hardware, ACM SIGARCH Computer Architecture News, vol. 44, no. 3, pp. 115 127, 2016. [110] D. Lee, L. K. John, and A. Gerstlauer, Dynamic power and per- formance back-annotation for fast and accurate functional hardware simulation, in Proc. DATE, 2015, pp. 1126 1131. Junye Jiang received the B.Sc. degree from the School of Electrical and Information Engineering, Wuhan Institute of Technology, Wuhan, China, in 2023. He is currently a M.Sc. student in the Computer Vision and High-Performance Computing group at Hunan Normal University. His research interests include the hardware acceleration of con- volutional neural networks (CNNs) and neural ar- chitecture design optimizations applied to computer vision tasks. Yaan Zhou received the B.Sc. degree in Electronic Information Engineering of Central South Univer- sity of Forestry and Technology, Changsha, China, in 2021. He is currently a M.Sc. student in the Computer Vision and High-Performance Computing group at Hunan Normal University. His current research includes the hardware acceleration of con- volutional neural networks (CNNs). Yuanhao Gong received the B.S. degree in Elec- tronic Information Science and Technology from Hunan Normal University, Changsha, China, in 2023. He is currently pursuing the M.S. degree in Electronic Science and Technology with Hunan Nor- mal University. His research interests mainly focus on particle filter methods and efficient hardware architectures for particle filters. Haoxuan Yuan received the B.S.",
    "source": "2505.13461v1_FPGA-based_Acceleration_for_Convolutional_Neural_N.pdf",
    "length": 2068,
    "tokens": 498
  },
  {
    "text": "2.1, the 16 depth planes required by most of the AR applications (typically 10 to 100 depth planes are sufficient) [19, 49] consume more than 300ms, which is 10 larger than the real-time (QoS) requirement. Thus, it can be concluded that, without any optimization, a state- of-the-art edge GPU is only able to compute for 4 depth planes in real-time [36]. These observations motivate us to investigate the reasons behind such low performance on GPU: is it because of the intrinsic software algorithm characteristics, or is it primarily a hardware mapping issue? Towards this, we profiled the hologram processing on the edge GPU [36] using the NVPROF tool [37], and observed the follow- ing: First, the SM utilization for both the steps is very high, i.e., 74 for Forward-Propagation and 90 for Backward-Propagation. This is because the execution is massively parallel at the depth plane level as well as at the pixel level. Moreover, the L1 hit rate for both these steps is as high as 99 . Thus, GPU seems to be one of the reasonable hardware candidates for mapping the hologram application. Second, the four major reasons for instruction stalls in the Forward-Propagation step are: Data Request (21 ), Execu- tion Dependency (19 ), Instruction Fetch (15 ), and Sync (10 ), whereas in the Backward-Propagation step they are Read-only Loads (42 ), Sync (24 ), Data Request (16 ), and Execution Dependency (6 ). These stalls originate mainly from the inter-block and intra- block synchronizations required by the application, as discussed above when explaining Algo. 1. Because of this, recently, alternate hardware-based solutions have been proposed to improve the com- putational efficiency by replacing the expensive transcendental cal- culations with lookup table (LUT) based memoization [35], or miti- gating the data movement overheads by employing a customized buffer on-chip [32], or simply offloading computations to cloud then streaming back [16, 27, 67]. While such an approach improved the computational efficiency and reduced power consumption to some extent, rethinking the design of hologram software hardware con- sidering the unique features of the AR holographic applications (as discussed in Sec.",
    "source": "HoloAR.pdf",
    "length": 2212,
    "tokens": 487
  },
  {
    "text": "However, existing methods primarily rely on local message-passing mechanisms, which limits their ability to model long-range logic dependencies across large-scale AIGs. This often results in the loss of crucial global logic context, leading to reduced accuracy and limited scalability in downstream EDA tasks. While some methods introduce attention mechanisms to enhance global context modeling, they often suffer from significant computational overhead, making them inefficient for large-scale circuit analysis. Therefore, developing efficient and scalable representation methods that preserve and utilize global logic information remains a critical challenge. To address these challenges, we propose FuncGNN, which combines local and global logic infor- mation to effectively address structural heterogeneity and global information loss, thereby enabling more efficient and accurate extraction of AIG logic semantics, which benefits EDA workflows such as design optimization and verification. Specifically, FuncGNN comprises three key components: the Hybrid Feature Aggregation Component, the Global Context Normalization Component and the Multi-Layer Integration Component. The Hybrid Feature Aggregation Component integrates diverse topological patterns, en- hancing circuit property estimation by addressing AIG structural diversity. This approach improves J. ACM, Vol. 1, No. 2, Article 3. Publication date: May 2025. FuncGNN: Learning Functional Semantics of Logic Circuits with Graph Neural Networks 3:3 logic representation accuracy, enabling robust analysis of heterogeneous circuit structures. To further refine logic representations, the Global Context Normalization Component balances circuit-wide gate proportions, improving estimation accuracy. This mitigates uneven gate dis- tributions, addressing structural heterogeneity (thus addressing Challenge 1). Building on this, the Multi-Layer Integration Component synthesizes comprehensive logic relationships across the circuit, enhancing estimation accuracy. This captures global logic information, addressing information loss (thus addressing Challenge 2). To effectively evaluate the model performance, we conducted experiments on a large-scale dataset of 9,933 AIG samples from four circuit benchmark suites to assess FuncGNN s performance in circuit property estimation. The results show FuncGNN s Signal Probability Prediction (SPP) accuracy and Truth-Table Distance Prediction (TTDP) improve by 2.06 and 18.71 , respectively, over the state-of-the-art methods. Additionally, FuncGNN enhances computational efficiency, improving logic representation for accurate circuit property estimation.",
    "source": "2506.06787v1_FuncGNN_Learning_Functional_Semantics_of_Logic_Cir.pdf",
    "length": 2661,
    "tokens": 477
  },
  {
    "text": "IEEE Access, 10:73786 73803, 2022. [3] Chanwoo Kim, Dhananjaya Gowda, Dongsoo Lee, Jiyeon Kim, Ankur Kumar, Sungsoo Kim, Abhinav Garg, and Changwoo Han. A Review of On-Device Fully Neural End-to-End Automatic Speech Recognition Algorithms. In 2020 54th Asilomar Conference on Signals, Systems, and Computers, pages 277 283, 2020. [4] Emil Njor, Mohammad Amin Hasanpour, Jan Madsen, and Xenofon Fafoutis. A Holistic Review of the TinyML Stack for Predictive Main- tenance. IEEE Access, 12:184861 184882, 2024. [5] Maxim Integrated. MAX78000. 2025. [6] Arthur Moss, Hyunjong Lee, Lei Xun, Chulhong Min, Fahim Kawsar, and Alessandro Montanari. Ultra-Low-Power DNN Accelerators for IOT: Resource Characterization of the MAX78000. In Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems, pages 934 940, 2022. [7] Mitchell Clay, Christos Grecos, Mukul Shirvaikar, and Blake Richey. Benchmarking the MAX78000 artificial intelligence microcontroller for deep learning applications. In Real-Time Image Processing and Deep Learning 2022, volume 12102, pages 47 52. SPIE, 2022. [8] Linghe Kong, Jinlin Tan, Junqin Huang, Guihai Chen, Shuaitian Wang, Xi Jin, Peng Zeng, Muhammad Khan, and Sajal K Das. Edge-computing- driven Internet of Things: A survey. ACM Computing Surveys, 55(8):1 41, 2022. [9] Ruijin Wang, Jinshan Lai, Zhiyang Zhang, Xiong Li, Pandi Vijayaku- mar, and Marimuthu Karuppiah. Privacy-preserving Federated Learn- ing for Internet of Medical Things under Edge Computing. IEEE journal of biomedical and health informatics, 27(2):854 865, 2022.",
    "source": "2503.22567v2_Benchmarking_Ultra-Low-Power_Î¼NPUs.pdf",
    "length": 1572,
    "tokens": 470
  },
  {
    "text": "Inputs: - i_clk (Clock signal) - i_rst_b (Active-low asynchronous reset) - i_operand_a (4-bit input operand) - i_operand_b (4-bit input operand) - i_opcode (3-bit input signal to specify the operation) - i_key_in (8-bit input security key) 2. Outputs: - o_result (8-bit result of the operation) Input Prompt 3. Internal Configuration: - A configurable 8-bit internal security key, p_key , with default to 0xAA. 4. Functional Behavior: - If i_key_in matches the internal key, the ALU operations are active and follow the behavior described below: - Addition ( i_opcode 000 ): Perform i_operand_a i_operand_b . - Subtraction ( i_opcode 001 ): Perform i_operand_a - i_operand_b . - Multiplication ( i_opcode 010 ): Perform i_operand_a i_operand_b . - Bitwise AND ( i_opcode 011 ): Perform i_operand_a i_operand_b . - Bitwise OR ( i_opcode 100 ): Perform i_operand_a i_operand_b . - Bitwise NOT ( i_opcode 101 ): Negate i_operand_a (i.e., i_operand_a ). - Bitwise XOR ( i_opcode 110 ): Perform i_operand_a i_operand_b . - Bitwise XNOR ( i_opcode 111 ): Perform (i_operand_a i_operand_b) . - If i_key_in does not match the internal key: - The output o_result should remain 8'b0, and no operation is performed.",
    "source": "2506.14074v1_Comprehensive_Verilog_Design_Problems_A_Next-Gener.pdf",
    "length": 1204,
    "tokens": 409
  },
  {
    "text": "As shown in Table 1, the larger the block, the larger the spatio- temporal locality over keys (higher horizontal CDF part). However, the smaller the block, the lower the compute required. Hence, larger blocks enable better system utilization on flop-oriented machines, Model Input Nodes Edges Layers sizes GNN arxiv [22] 0.2M 1.2M 128 256 256 40 GNN mag [22] 1.9M 21.1M 128 256 349 GNN products [22] 2.4M 61.9M 100 256 256 47 GNN proteins [22] 0.1M 39.6M 8 256 256 112 MP com-Youtube [35] 1.1M 6.0M 128 MP roadNet-CA [35] 2.0M 5.5M 128 MP web-Google [35] 0.9M 5.1M 128 MP wiki-Talk [35] 2.4M 5.0M 128 KG biokg [22] 0.1M 5.1M 512 KG wikikg2 [22] 2.5M 17.1M 512 Table 2: Typical inputs for graph-learning models. and smaller blocks enable lower inference latency on machines with more efficient memory subsystems. 2.2.3 Graph Machine Learning Models. Graph machine learning models such as Graph Neural Networks (GNNs) [22], Message Pass- ing (MP) models [53], and Knowledge Graphs (KGs) [7] embed node or edge features of a graph into vectors. GNNs, for instance, perform inference by chaining layers of deep neural networks and graph convolutions, which gather embeddings from neighbors with the SpMM-like operation shown in Table 1. These SpMM-like em- bedding operations generally have a higher compute-per-lookup Siracusa et al.",
    "source": "2504.09870v1_Ember_A_Compiler_for_Efficient_Embedding_Operation.pdf",
    "length": 1330,
    "tokens": 393
  },
  {
    "text": "By temporarily abstracting away physical constraints, the compiler explores the optimal weight data layout strategies, including the image-to-column (im2col) transformation commonly employed in DNN acceleration. The physical mapping phase then adapts the idealized representation to actual hardware constraints through a se- ries of optimization passes implemented within the MLIR infrastructure. The compiler first applies loop tiling based on resource capacity constraints, then systematically extracts MVM operations from the tiled loops for translation into CIM operations. Through automated analysis, it determines the optimal tile sizes and loop ordering to maximize computational efficiency while respecting resource limitations at each memory hierarchy. Memory access operations are then strategically annotated at appropriate loop levels to minimize data transfer overhead. In the final code generation phase, the optimized IR under- goes conventional compilation techniques, including constant propagation, dead code elimination, and register allocation. The generated instructions adhere to the CIMFlow ISA specification while realizing the optimized resource mapping decisions, ensuring efficient utilization of the CIM hardware resources. D. Simulator Design The CIMFlow simulator provides cycle-accurate perfor- mance analysis through detailed modeling of the digital CIM architecture across multiple abstraction levels, from individual core execution to chip-level coordination. Implemented in SystemC [25], the simulator features a detailed pipeline model to track execution flow and resource utilization within each processing unit, while managing parallel execution across cores connected via NoC. The simulator supports diverse architec- tural configurations through a user-defined configuration file that adheres to the ISA specifications, while its modular design and standardized interfaces allow straightforward integration of custom architectural components. TABLE I ARCHITECTURE PARAMETERS OF THE DEFAULT ARCHITECTURE. Chip Level Core Level Unit Level Core num. 64 CIM comp. unit 16 MG Macro 512 64 NoC flit size 8 Byte Macro group 8 macro Element 32 8 Global mem. 16 MB Local mem. 512 KB At core level, instruction execution follows a three-stage pipeline comprising instruction fetch (IF), decode (DE), and execute (EX). The EX stage implements detailed execution models for different compute units, each with fine-grained pipelining to enable instruction-level parallelism. Instruction conflicts and resource utilization are efficiently tracked through a bitmap-based scoring board within the instruction scheduler, ensuring accurate modeling of both computation and data movement patterns.",
    "source": "2505.01107v1_CIMFlow_An_Integrated_Framework_for_Systematic_Des.pdf",
    "length": 2718,
    "tokens": 482
  },
  {
    "text": "Due to the close quality distri- bution within the Qwen family, the advantage of larger models is constrained to the top-left corner Figure 26: Comparison of different model sizes on Arena Hard in FUEL. Tile colors indicate the model with the lowest carbon per FU. Tile values are carbon savings ( ) of the greenest size compared to the second greenest. 0 20 40 Qscore 0.00 0.01 0.02 0.03 0.04 0.05 Density 7B (mean 26.8, std 11.4) 14B (mean 27.0, std 10.5) 32B (mean 28.9, std 10.3) 20 0 20 Qscore 7B (mean -4.2, std 12.7) 13B (mean 0.4, std 11.9) (a) Qwen (b) Llama Figure 27: Qscore distribution for different model sizes on HumanEval dataset. (high Qscore, low QPS). In contrast, in the bot- tom right corner, as the quality requirement de- creases and QPS increases, the 7B model becomes the greenest one. C.3 Results on HumanEval The HumanEval dataset (Chen et al., 2021) is a benchmark designed to evaluate the code genera- tion ability of LLMs. It consists of Python coding problems and requires LLMs to implement the spe- cific functions. Figure 27 shows the Qscore distribution for dif- ferent model sizes on the HumanEval dataset. Qs- core distribution for the Qwen models is much closer on this dataset. This is consistent with their technical report (Qwen et al., 2025), which also highlights similar performance across models in the HumanEval evaluation. However, for the Llama models, the gap between the 7B and 13B model remains large, as expected.",
    "source": "2502.11256v2_Unveiling_Environmental_Impacts_of_Large_Language_.pdf",
    "length": 1464,
    "tokens": 391
  },
  {
    "text": "Our analysis reveals several key findings. First, both 1-bit and 2-bit cells show similar accuracy degradation at equivalent SAF rates, suggesting that cell precision does not significantly influence SAF sensitivity. However, network complexity strongly affects SAF tolerance - ResNet-50 on ImageNet loses accuracy even at minimal SAF rates, while VGG8 and ResNet18 maintain limited functionality only at the lowest SAF rates. Notably, SAF errors cause more severe accuracy degradation compared to other noise sources like D2D variation or conductance drift. This heightened sensitivity to SAF errors aligns with previous research findings [42], highlighting the critical importance of minimizing stuck faults during device fabrication. 4) MAC Output Noise: circuit-level noise sources during array computation can be captured as statistical variations in the post-ADC MAC outputs. To demonstrate NeuroSim V1.5 s flexible noise modeling capabilities, we evaluated inference accuracy across four different CIM macros, incorporating both SPICE-characterized circuit data and silicon measurements from fabricated chips. CIM A and B [43] employ 2b FeFET technology with different computing modes: CIM A performs current-mode computation with FeFETs directly, while CIM B implements charge-based computation using FeFET-modulated capacitors. Their noise characteristics were derived from Monte-Carlo SPICE simulations. CIM C [7] provides real silicon validation through direct measurements from a fabricated RRAM-based CIM chip. For these three designs, each MAC output level is characterized by unique mean and standard deviation values. CIM D [18], [33] uses 28 nm nvCap with charge-mode computation, where SPICE simulations revealed thermal noise as the dominant source, here, a uniform variation across output levels was used. Figs. 8(a-d) illustrate the statistical distribution of actual versus ideal MAC outputs for representative cases. While CIM A, B, and D show tighter error bounds compared to CIM C, all designs achieve similar accuracy on smaller networks. Fig. 8(e) compares inference accuracy across architectures of in- creasing complexity. VGG8 and ResNet18 maintain reasonable accuracy across all implementations, with CIM C showing slightly higher degradation ( 2 ) due to increased output variation. For ResNet50, only CIM B and D - which achieve lower standard deviations in their MAC errors - maintain high accuracy.",
    "source": "2505.02314v1_NeuroSim_V15_Improved_Software_Backbone_for_Benchm.pdf",
    "length": 2433,
    "tokens": 482
  },
  {
    "text": "Speedup over the Original baseline (right-most plots) for SkipNet and Ours. ResNet-110 on top plots, ResNet-20 on bottom plots. Dashed curves for CIFAR-100 dataset, solid curves for CIFAR-10. Note that original and SkipNet models are shown for comparison and do not follow the number of skipped blocks in the x-axis. (delivering 81.51 ). Similar behavior is observed across the two evaluated datasets and ResNets. The gains from our skipping approach are especially clear for speedup and energy. 2) Inference Time: Despite Skipnet s high accuracy, we can see that the overhead introduced by its decision gates can become an issue for inference time for some models and datasets. For instance, for ResNet-20, SkipNet ends up slowing down the inference when compared to the original model (speedups of 0.96 and 0.92 on CIFAR-10 and CIFAR-100, respectively). This is primarily due to SkipNet s low number of skipping blocks on ResNet-20 (from none to just three blocks are skipped). Therefore, the overhead from the layers in the decision gates become more apparent. Our approach, on the other side, presents significant speedup levels over the original ResNet-20 on CIFAR- 10 (up to 3.33 ) and CIFAR-100 (up to 3.13 ). For the larger ResNet-110, SkipNet improves over the original model (1.50 speedup on CIFAR-10). For this model and dataset, SkipNet runs also faster than our approach for skipping configurations of up to 14 skipped blocks at which point SkipNet delivers an accuracy 1.18 higher than our approach (Figure 6). However, for configurations of over 14 skipped blocks, our approach delivers inferences faster than both baselines (up to 19.7 over the original ResNet-110 on CIFAR-10). See the zoom-in from none to 20 skipped blocks in the top-center plot. Additionally, recall that the SkipNet curves in Figure 6 give the result over the full test set and due to its input-dependent behavior the inference time can vary drastically from input to input. On the evaluated models and datasets, we observe that SkipNet inference time can vary as much as 13 (between skipping all or executing all blocks), which severely impacts SkipNet s predictability.",
    "source": "2505.17626v1_Leveraging_Stochastic_Depth_Training_for_Adaptive_.pdf",
    "length": 2159,
    "tokens": 496
  },
  {
    "text": "2 Background and Motivation In this section, we first introduce the background of Multi- LoRA serving with caching, then investigate the inefficiency of current Multi-LoRA serving systems. 2.1 Multi-LoRA Serving with Caching Multi-LoRA. LoRA is a popular method for efficiently fine- tuning pre-trained LLMs by adding lightweight adapters to original weights [19]. Instead of updating all parameters of a model, LoRA only learns a pair of small low-rank matrices that modify the original weights. These matrices are much smaller than the original weight matrix, which can reduce the computational cost and memory usage. For the Multi-LoRA scenario, the pre-trained base model is loaded once, and multiple pairs of low-rank matrices are introduced, each corresponding to a specific task [20,51]. For each task t, a unique pair of low-rank matrices At and Bt is learned, and the original weight matrix W is updated as: W t W Wt W AtBt (1) For Multi-LoRA serving, based on the query s task, the corresponding LoRA matrices are loaded into the HBM for usage before inferencing. Queries using different LoRAs can be processed in a single batch using Segmented Gather Matrix- Vector multiplication (SGMV) [6, 37], improving both effi- ciency and throughput. KV Caches for Multi-LoRAs. The LoRAs need to be loaded into the HBM for usage during the inference [19, 2 37]. Moreover, most LLMs use a decoder-only transformer to predict the next token with KV caches computed from previous tokens [8, 15]. When a query matches an existing prefix, the stored KV caches are reused to reduce HBM usage and eliminate redundant computations, such as in multi-turn dialogues [16, 17]. Therefore, maintaining the history KV caches in HBM can maximize the reuse. Each LoRA adds a low-rank branch to the original weights that participate in the KV cache computation.",
    "source": "2505.03756v1_Improving_the_Serving_Performance_of_Multi-LoRA_La.pdf",
    "length": 1845,
    "tokens": 431
  },
  {
    "text": "(Gop s W) Resource EFF. (GOp s DSP) [91] fpgaConvNet Streaming Structure Xilinx XC7Z020 12.73 7.27 - [14] Angel-Eye Single Engine Xilinx XC7Z045 XC7Z020 XC7Z045: 137 XC7Z020: 84.3 XC7Z045: 14.2 XC7Z020: 24.1 - [94] Snowflake Single Engine Xilinx XC7Z045 GoogLeNet: 116.5 GoogLeNet: 12.3 - [92] fpgaConvNet Streaming Structure Xilinx XC7Z045 AlexNet: 134.10 VGG16: 123.12 - AlexNet: 0.18 VGG16: 0.14 [93] fpgaConvNet Streaming Structure Xilinx XC7Z045 AlexNet: 197.40 VGG16: 155.81 GoogLeNet: 165.30 ResNet-152: 188.18 DenseNet-161: 155.57 AlexNet: 49.35 VGG16: 38.95 GoogLeNet: 41.32 ResNet-152: 47.04 DenseNet-161: 38.9 AlexNet: 0.22 VGG16: 0.17 GoogLeNet: 0.184 ResNet-152: 0.209 DenseNet-161: 0.173 [95] f-CNNx Streaming Structure Xilinx XC7Z045 VGG16: 75.00 VGG16 : 18.75 - [96] FMM-X3D Streaming Structure Xilinx ZCU102 X3D-M: 119.83 - X3D-M: 0.055 [97] HARFLOW3D Streaming Structure Xilinx ZCU102 VC709 ZCU102: C3D: 393.37 Slowonly: 177.05 R(2 1)D-18: 173.91 (R(2 1)D-34): 184.29 X3D-M: 43.78 VC709: C3D: 424.14 Slowonly: 229.01 R(2 1)D-18: 185.13 (R(2 1)D-34: 206.39 X3D-M: 56.14 - ZCU102: C3D: 0.156 Slowonly: 0.07 R(2 1)D-18: 0.069 (R(2 1)D-34: 0.073 X3D-M: 0.017 VC709: C3D: 0.117 Slowonly: 0.063 R(2 1)D-18: 0.051 (R(2 1)D-34: 0.057 X3D-M: 0.015 [98] fpgaHART Streaming Structure Xilinx ZCU102 C3D: 130.84 Slowonly: 144.44 R(2 1)D-18: 39.59 (R(2 1)D-34: 34.26 X3D-M: 85.96 - C3D: 0.052 Slowonly: 0.057 R(2 1)D-18: 0.015 (R(2 1)D-34: 0.013 X3D-M: 0.034 [23] Pflow Streaming Structure Xilinx XCZU3EG XCVU13P XCZU3EG: 272.64 XCVU13P: 3686.4 XCZU3EG: 46.5 XCVU13P: 59.4 XCZU3EG: 0.71 XCVU13P: 0.6 TABLE VI COMPARISON OF EXISTING DSE METHODS DSE Method Local Optimum Large Parameter Space Real-Time Low Complexity Low Parameter Dependency Memory Bandwidth Limitations BruteForce Simulated Annealing Rule-Based Method Genetic Algorithm performance (e.g., latency) based on hardware configuration and algorithm characteristics. These models replace the time- consuming task of running CNNs on actual hardware by providing estimates based on hardware parameters.",
    "source": "2505.13461v1_FPGA-based_Acceleration_for_Convolutional_Neural_N.pdf",
    "length": 2065,
    "tokens": 886
  },
  {
    "text": "You are an expert at refining code challenge datapoints. Analyze the provided datapoint and improve it , focusing ONLY on enhancing the prompt field. Improvements should be subtle and nuanced , and should not change the overall meaning of the datapoint , but should make the datapoint more precise and helpful in solving the problem. The input , output , categories , and harness fields MUST remain unchanged and are provided only for reference to help you understand the task better. Return a complete datapoint JSON structure with an additional reasoning_prompt field that explains your improvements , along with ambiguity_score and consistency_score fields that evaluate the quality of the original datapoint. You should also include a category_match_score field that evaluates how well the category tag in the original datapoint matches the category tag in the categories field , where 1 means no match and 10 13 means a perfect match. With this , include a reasoning_category_match field that explains your reasoning for the category match score. Additionally , provide a behavioral_match_score field that evaluates how well the logic and behavior described in the prompt matches the actual logic and behavior in the output reference solution and what is checked in the test harness. Include a behavioral_match_reasoning field explaining your assessment of this behavioral alignment. I need help refining this code challenge datapoint (ID: {id}). Here is the original datapoint: json {json.dumps(datapoint , indent 2)} IMPORTANT CONSTRAINTS : - You can ONLY modify the prompt field - The following fields MUST remain exactly as they are in the original: input : The input to the code challenge output : The expected output of the code challenge (the \"reference solution \") categories : Includes the difficulty (\" easy\", \"medium\", \"hard \") and the category tag. The category tags below are the only ones that are allowed: cid002 : \"RTL Code Completion: Input must be skeleton code , and output must be the complete RTL code .\" cid003 : \" Specification to RTL Translation : Input must be a natural language specification , and output must be the complete RTL code .\" cid004 : \"RTL Code Modification : Input must be existing RTL code and natural language specification of the changes to make , and output must be the modified RTL code .\"",
    "source": "2506.14074v1_Comprehensive_Verilog_Design_Problems_A_Next-Gener.pdf",
    "length": 2339,
    "tokens": 484
  },
  {
    "text": "LLM Decoder LLMs (e.g., GPT, DeepSeek) Task 3 RTL Code Reasoning Generated RTL Code module test ( input [3:0] a, input [3:0] b, output [7:0] Result); wire n1; wire [7:0] Result_1; wire [7:0] Result_2; assign Res_1 a b; assign Res_2 a b; assign n1 (a b) ? 1'b1 : 1'b0; assign Result n1 ? Res_2 : Res_1; endmodule AOI22 U16(.A(n0) ) NOR3 U01(.A(n2) ) OAI21 U21(.A(n3) ) ... MUX2 U5 (.S(n10) ) Netlist Text GenEDA Netlist Graph â‘ netlist (.v) â‘¡question_prompt (.txt) â‘¢ground-truth spec (.txt) Evaluation: NLP scores GPT score â‘£ground-truth RTL testbench (.v) Evaluation: Simulation GPT score Proposed netlist generative reasoning benchmark Existing LLM generation GenEDA generation Generated Spec Fig. 3: Proposed netlist generative reasoning benchmark. For Tasks 1 and 2, the netlist and question prompt are processed by models for specification reasoning, and evaluated using NLP scores and GPT scores. For Task 3, the models reconstruct RTL code from the netlist, which is evaluated via simulation and GPT scores. approaches typically extract subcircuits from the netlist and match them against components in a golden library via exhaustive formal verification. However, they are time-consuming, dependent on library completeness, and incapable of recognizing functional variants. GNN-based prediction. Recently, GNN-based methods [36], [37], [7], [23], [10] have been used for predictive netlist functional reasoning, focusing on gate-level function classification. While effective in identifying roles of known components, they require annotated labels and cannot generalize to unseen functionality or reason about the full circuit behavior. Functionality generation by GenEDA. GenEDA moves beyond gate-level prediction to full-circuit generative reasoning.",
    "source": "2504.09485v1_GenEDA_Unleashing_Generative_Reasoning_on_Netlist_.pdf",
    "length": 1758,
    "tokens": 479
  },
  {
    "text": "To address this challenge, the orthogonal array (OA) strategy is adopted. The orthogonality of OA ensures that sampled configurations are evenly distributed throughout the design space [21]. Since the design space for the soft processor is fixed within our framework, the OA can be pre-generated using online methods, further simplifying implementation. C. Incorporating Evaluation Time Cost Into Optimization Due to the proposed accelerated soft processor evaluation method, the evaluation time of a configuration depends on how similar it is to previously evaluated configurations stored in the checkpoint database. Therefore, we incorporate the consideration of evaluation time in the BO to enable more efficient configuration selection and reduce the total evaluation time. To achieve this, we incorporate a cost-aware cooling mech- anism into the BO acquisition function, steering configuration selection in the BO process for optimized overall evaluation time. The cost function, Ë†c(x), is defined as the minimum weighted Euclidean distance between the candidate configura- tion and all previously evaluated configurations stored in the checkpoint database, same as Equation 7. Inspired by [22], the modified cost-aware acquisition function is defined as: Î±cool(x, t) Î±(x) Î»(t) Ë†c(x) (9) where Î±(x) is the original acquisition function (Expected Improvement in this project), and Î»(t) denotes a cooling schedule that evolves over the current BO iteration t. In this work, we adopt a simple decaying schedule designed to impose a high penalty on distant configurations in the early optimization stages. This encourages the algorithm to remain in regions that are quick to evaluate. As the optimization progresses, the penalty is gradually reduced, allowing the algorithm to explore more distant, potentially expensive but promising configurations. The cooling schedule is defined as: Î»(t) Î»0 exp( kt) (10) where Î»0 is the initial cost sensitivity, and k is the decay rate; both are determined empirically through experiments. TABLE IV RISC-V BENCHMARKS USED IN THIS PROJECT.",
    "source": "2506.06817v1_ASPO_Constraint-Aware_Bayesian_Optimization_for_FP.pdf",
    "length": 2079,
    "tokens": 426
  },
  {
    "text": "[40] Haoran Qiu, Subho S Banerjee, Saurabh Jha, Zbigniew T Kalbarczyk, and Ravishankar K Iyer. 2020. {FIRM}: An Intelligent Fine-grained Resource Management Framework for SLO-Oriented Microservices. In 14th {USENIX} Symposium on Operating Systems Design and Imple- mentation ({OSDI} 20). 805 825. 166 Kraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC 21, November 1 4, 2021, Seattle, WA, USA [41] Mohammad Shahrad, Jonathan Balkind, and David Wentzlaff. 2019. Architectural implications of function-as-a-service computing. In Pro- ceedings of the 52nd Annual IEEE ACM International Symposium on Microarchitecture. 1063 1075. [42] Mohammad Shahrad, Rodrigo Fonseca, ÃÃ±igo Goiri, Gohar Chaudhry, Paul Batum, Jason Cooke, Eduardo Laureano, Colby Tresness, Mark Russinovich, and Ricardo Bianchini. 2020. In 2020 {USENIX} Annual Technical Conference ({USENIX}{ATC} 20). 205 218. [43] Paulo Silva, Daniel Fireman, and Thiago Emmanuel Pereira. 2020. Prebaking Functions to Warm the Serverless Cold Start. In Proceedings of the 21st International Middleware Conference. 1 13. [44] Arjun Singhvi, Kevin Houck, Arjun Balasubramanian, Mo- hammed Danish Shaikh, Shivaram Venkataraman, and Aditya Akella. 2019. Archipelago: A scalable low-latency serverless platform. arXiv preprint arXiv:1911.09849 (2019). [45] Davide Taibi, Nabil El Ioini, Claus Pahl, and Jan Raphael Schmid Niederkofler. 2020. Patterns for Serverless Functions (Function-as- a-Service): A Multivocal Literature Review.. In CLOSER. 181 192. [46] Ali Tariq, Austin Pahl, Sharat Nimmagadda, Eric Rozner, and Siddharth Lanka. 2020.",
    "source": "kraken.pdf",
    "length": 1632,
    "tokens": 485
  },
  {
    "text": "Dataset Type Topology (Code) of Samples Parameter Sweep Range Performance Metrics (Unit) LNA CGLNA (0) 52k C1 [100 600] fF DCP (W) PGain (dB) S11 (dB) NF (dB) BW (Hz) C2 [50 300] fF Cb [250 750] fF Ld [80 580] pH Ls [0.5 5.5] nH WN [12 23] Âµm CLNA (1) 62k C1, C2 [50 250] fF Ld [140 300] pH Lg [0.4 2] nH Ls [50 250] pH WN1 [3 5] Âµm WN2 [7 9] Âµm CSLNA (2) 39k C [100 300] fF Lg [4 6] nH Ls [100 200] pH WN [2.5 4] Âµm Vgs [0.5 0.9] V DLNA (3) 92k C1 [100 190] fF C2 [130 220] fF Ld [100 250] pH Lg [600 900] pH Ls [50 80] pH WN1 [4 9.4] Âµm WN2 [5 14] Âµm 15 B.2 Mixers Mixers are fundamental nonlinear components in RF systems, responsible for frequency translation by combining two input signals to produce outputs at the sum and difference of their frequencies. This functionality is essential for transferring signals across frequency domains and is widely used in both transmission and reception paths [35]. To capture diverse mixer architectures, we implement four representative topologies in this work double-balanced active mixer (DBAMixer), double-balanced passive mixer (DBPMixer), single-balanced active mixer (SBAMixer), and single-balanced passive mixer (SBPMixer) as shown in Figure 9. The DBAMixer integrates amplification and differential switching to achieve conversion gain and high port-to-port isolation. Despite its elevated power consumption and design complexity, it is well suited for systems requiring robust performance over varying conditions [36]. The DBPMixer features a fully differential structure that suppresses signal leakage and improves isolation, at the cost of signal loss and a strong local oscillator drive requirement [37].",
    "source": "2505.21923v1_FALCON_An_ML_Framework_for_Fully_Automated_Layout-.pdf",
    "length": 1662,
    "tokens": 491
  },
  {
    "text": "The proportion of training data varies from as low as 0.3 (4 samples) to 10 (135 samples) and performance is evaluated on the remaining dataset. As shown in Fig. 5, both integration methods outperform random initialization, particularly in low-data regimes, demonstrating that Lib2Vec effectively transfers cell knowledge to aid circuit learning. Notably, the model-based integration achieves approximately 80 accuracy in logic output prediction and 0.236 RMSE (Root Mean Square Error) with only 4 training samples, comparing to the 65 accuracy and 0.378 RMSE of random initialization. It highlights Lib2Vec s potential for enabling few-shot learning. Further investi- gation is needed to fully understand and expand this capability. 5 Fig. 4. Visualization of cell representations. Fig. 5. Impacts of integrating Lib2Vec into ML models for different prediction tasks. 6 IX. CONCLUSION In this paper, we introduce Lib2Vec, a novel self-supervised frame- work for learning meaningful vector representations of library cells, enabling ML models to capture functional and electrical relationships. Future work includes exploring Lib2Vec for circuit foundation models and systematically evaluating its impact on downstream tasks. REFERENCES [1] E. C. Barboza et al., Machine learning-based pre-routing timing prediction with reduced pessimism, in Proceedings of Design Automation Conference (DAC), 2019, pp. 1 6. [2] Z. Xie et al., Preplacement net length and timing estimation by customized graph neural network, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), vol. 41, no. 11, pp. 4667 4680, 2022. [3] Z. Guo et al., A timing engine inspired graph neural network model for pre-routing slack prediction, in Proceedings of Design Automation Conference (DAC), 2022, pp. 1207 1212. [4] Y. Zhang, H. Ren, and B. Khailany, GRANNITE: Graph neural network inference for transferable power estimation, in Proceedings of Design Automation Conference (DAC), 2020, pp. 1 6. [5] A. Fayyazi et al., Deep learning-based circuit recognition using sparse mapping and level-dependent decaying sum circuit representations, in Proceedings of Design, Automation Test in Europe Conference Exhibition (DATE), 2019, pp.",
    "source": "2503.22900v1_Learning_Library_Cell_Representations_in_Vector_Sp.pdf",
    "length": 2231,
    "tokens": 498
  },
  {
    "text": "Nevertheless, due to the distinctive nature of processor chip design, applying LLMs and agents to automated processor chip design faces four principal challenges: knowledge representa- tion gap, data scarcity, correctness guarantee, and enormous solution space. First, the knowledge representation gap: critical processor chip design data employs graph structures, such as abstract syntax trees (ASTs), data flow diagrams (DFGs), and control flow diagrams (CFGs). Graph data exhibits an inherent semantic gap with the sequential text that LLMs typically process, constraining the capacity for domain knowledge rep- resentation and limiting the processor chip design capabilities of LLMs. Second, the data scarcity: unlike the vast petabyte- scale text corpora available on the Internet for training general- purpose LLMs, processor chip design data are orders of magnitude smaller, with merely terabyte-scale in open-source communities like GitHub, severely constraining the develop- ment of domain-specialized LLMs for processor chip design. Third, the correctness guarantee: processor design demands rigorous verification standards, which fundamentally conflict with the probabilistic nature of LLMs. For example, Intel s Pentium 4 processor required 99.99999999999 accuracy in functional verification [9]. Finally, the enormous solution space: processor design spans multiple abstraction stages from foundational software to physical layouts, thus, modeling the design space directly at the raw bitstream level suffers from a dimensionality explosion. For example, the solution space for a 32-bit CPU reaches 1010540. This enormous solution space poses extreme challenges for deriving both functionally- correct and performance-optimized processor designs. To address the aforementioned challenges and pioneer a Large Processor Chip Model Bottom-up Top-down Automated HDL Generation Automated Front-end Design Module Decomposition Module Generation Hardware Design Agent Automated OS Configuration Optimization Performance Optimization Automated Compiler Tool-Chain Design Automated Tensor Program Transcompiler Automated High-Performance Library Generation Function Adaptation Software Design Agent Fig. 1. Overview. QiMeng consists of three layers, a domain-specialized Large Processor Chip Model (LPCM) in the bottom-layer, Hardware Design Agent and Software Design Agent enabling automated hardware and software design based on LPCM in the middle-layer, and various processor chip design applications in the top-layer. transformative paradigm, we propose QiMeng1, a novel system for fully automated hardware and software design for pro- cessor chips.",
    "source": "2506.05007v1_QiMeng_Fully_Automated_Hardware_and_Software_Desig.pdf",
    "length": 2657,
    "tokens": 494
  },
  {
    "text": "4 Figure 2: In Stage 1, an MLP classifier selects the most suitable circuit topology from a library of human-designed netlists, conditioned on the target performance specification. Table 1: Classification performance on topology selection. Metric Score ( ) Accuracy 99.57 Balanced Accuracy 99.33 Macro Precision 99.27 Macro Recall 99.33 Macro F1 99.30 Micro F1 99.57 Each datapoint is represented by a 16-dimensional performance vector of key analog RF metrics1. We normalize features using z-scores computed from the training set. Missing metrics (e.g., oscillation frequency for amplifiers) are imputed with zeros, yielding zero-centered, fixed-length vectors that retain task-relevant variation. Dataset splits are stratified to preserve class balance across training, validation, and test sets. We assume each target vector is realizable by at least one topology in T , though the library can be extended with new designs. Model Architecture and Training. We train a 5-layer MLP with hidden size 256 and ReLU activations for this problem. The model takes the normalized performance vector ytarget R16 as input and outputs a probability distribution over 20 candidate topologies. The predicted topology is selected as T arg maxTk T MLP(ytarget)k. We train the model using a cross-entropy loss and the Adam optimizer [55], with a batch size of 256. An overview of this process is shown in Figure 2. Evaluation. We begin by assessing the quality of the input representation used for topology classifi- cation. Normalized performance vectors encode rich semantic information about circuit behavior. To validate this, we project them into a two-dimensional t-SNE space [56] (Figure 3(a)). The re- sulting clusters align closely with topology labels, indicating that performance specifications reflect underlying schematic structure and are effective inputs for supervised classification. We assess classification performance using accuracy, balanced accuracy, macro precision, macro recall, macro F1, and micro F1 scores on the test set. As summarized in Table 1, the classifier achieves an overall accuracy of 99.57 , with macro F1 of 99.30 and balanced accuracy of 99.33 , demon- strating strong generalization across all 20 circuit topologies.",
    "source": "2505.21923v1_FALCON_An_ML_Framework_for_Fully_Automated_Layout-.pdf",
    "length": 2245,
    "tokens": 478
  },
  {
    "text": "[6] propose FINN-L, an extension of FINN to deploy parametrisable quantised LSTMs on FPGAs. They implement a Bi-LSTM model for OCR recognition tasks on the ZCU104 FPGA platform. This has, however, remained a custom implementation not integrated with the general FINN workflow. Ribes et al. [15] propose solving the deployment constraints posed by stacked LSTM models on FPGA by altering their computational structure optimising memory requirements. Ioannou and Fahmy [16] propose a flexible overlay architecture for LSTMs on hybrid FPGAs. They propose a streaming dataflow arrangement ex- ploiting the capabilities of DSP blocks while also mitigating external memory overheads. While the above implementations introduce novel deployment architectures for LSTMs, they do not provide a generalized mixed precision development and deployment workflow, limiting the broader adoption of efficient LSTM models on FPGAs. C. FINN FINN facilitates the rapid development and deployment of quantised neural networks (QNNs) on both edge and datacen- ter FPGAs [2]. Widely adopted by the open-source research community, it has evolved into an efficient toolchain for acceleration of quantised machine learning models. A distin- guishing feature of this framework is its use of a thresholding operator to implement quantised activation functions. This operator maps input values to integers in the interval [0, n] by comparing the input to a set of threshold values and returning an integer value that specifies the number of threshold values that the input is greater than or equal to. By adjusting the spacing between threshold values, this operator can model any monotonically increasing activation function, such as, ReLU, sigmoid, and tanh. The efficient, hardware-friendly implementation of the latter two relying on comparisons rather than the explicit computation of exponentials is key for an efficient implementation of LSTMs. Figure 3 illustrates how to operator models the tanh activation at three different INT quantisation levels. We demonstrate that our extensions to the FINN framework encompass all essential components, including FINN compiler transformations that map the LSTM compute graph to pre-built hardware building blocks, along with support for converting custom operators such as mapping sigmoid and tanh acti- vations to comparison based thresholding operations available through finn-hlslib.",
    "source": "2506.20810v1_FINN-GL_Generalized_Mixed-Precision_Extensions_for.pdf",
    "length": 2407,
    "tokens": 493
  },
  {
    "text": "This paper presents a work-in-progress of adapting and deploying the MatMul-free language model from Zhu 1 arXiv:2503.18002v2 [cs.NE] 25 Mar 2025 Published at ICLR 2025 Workshop (SCOPE) et al. (2024) to Intel Loihi 2, opening a pathway that bridges neuromorphic computing with state- of-the-art efficient LLMs. This required a microcode implementation to map the MMF model to Loihi 2 s architecture, along with a detailed ablation study to evaluate the optimal bit-precision for all operators in the MMF model. We are able to run the MMF model fully on-chip, using fixed-point arithmetic to optimize for energy and latency. 2 MODEL ARCHITECTURE The model architecture is based on the 370M parameter1 MatMul-free language model (Zhu et al., 2024). It uses a combination of ternary weights and specialized layers to replace all matrix multipli- cations with additions, bit shifts, and elementwise operators. The overall model architecture follows the Metaformer (Yu et al., 2022) paradigm, consisting of alternating token mixers and channel mix- ers. Figure 1 provides a high-level overview of this structure. HGRN block MLP block HGRN block RMSNorm BitLinear Linear lm_head Block 1 Embedding Block N Block Matrix operation Element-wise operation RMSNorm RMSNorm MLP block Wg Wu Wd Wg Wf Wc Wo SiLU Ïƒ SiLU RMS ht 1 ht SiLU Figure 1: Model architecture of the MatMul-free language model from Zhu et al. (2024). The model introduces two key innovations. The BitLinear Layer combines a ternarized linear transformation with a preceding RMSNorm operation, to stabilize the activation distribution (Ma et al., 2024; Zhang et al., 2023; Zhu et al., 2024).",
    "source": "2503.18002v2_Neuromorphic_Principles_for_Efficient_Large_Langua.pdf",
    "length": 1647,
    "tokens": 413
  },
  {
    "text": "VERR achieved up to 207 Kb s leakage rate with 4-bit leakage granularity, only in Rap- tor Lake. The relation between the iteration and leakage rates are given in Figure 9. 1 ;leaks through VERW 2 rep N 3 VERW word [R15] 4 LZCNT EDX, dword [R15] 5 endrep 6 7 ;leaks through VERR 8 rep N 9 VERR AX 10 endrep Listing 4: Instruction sequences with leakage through VERW and VERR. CLMUL Instructions. The CLMUL instruction set is de- signed to accelerate cryptographic operations by performing carry-less multiplication on 128-bit operands. This is particu- (a) VERW (b) VERR Figure 9: Leakage rates for VERW and VERR instructions mea- sured in Raptor Lake. In (b) Darker lines are running mean and the shades are rolling variance around the mean. larly useful in cryptographic algorithms like AES and GCM, where polynomial multiplications over GF(2128) are required, and carry propagation is not needed. Listing 5 shows an RL- generated sequence which includes PCLMULQDQ which is one of the CLMUL instructions. We observed leakage only in Raptor Lake with up to 90 Kb s with 4-bit granularity as shown in Figure 10. 1 ;leaks through PCLMULQDQ 2 rep N 3 PCLMULQDQ XMM4 , [R15], 2 4 VCVTPS2PH XMM0 , YMM0 , 2 5 LOCK CMPXCHG16B [R15] 6 endrep Listing 5: Instruction sequence with leakage through CMUL Miscellaneous. We observed leakage with the com- bination of other instructions such as LSL RDSCP, LAR RDSCP, LAR MULX, LAR ADCX CMOVNL, LAR PREFETCHWT1, etc. However, to save space, we give some of them in the Appendix C. 8 Exploitability of Discovered Transient Exe- cution Mechanisms In this section, we show how Meltdown-like vulnerabilities can be exploited without having TSX or exception handling, thanks to discovered instruction sequences by ÂµRL.",
    "source": "2502.14307v1_Î¼RL_Discovering_Transient_Execution_Vulnerabilitie.pdf",
    "length": 1749,
    "tokens": 473
  },
  {
    "text": "This envisions a supernode constructed from distinct, specialized node types: NPU-centric nodes densely packed with AI accelerators, and CPU-centric nodes offering substantial general-purpose compute, memory capacity, and I O capabilities. These heterogeneous node types would be interconnected via the high-bandwidth, low-latency UB network plane, enabling granular, flexible, and scalable resource pooling at the supernode level. The motivation for physical disaggregation arises from the rigidity of conventional CPU-NPU pairings in fixed node configurations, where static NPU-to-CPU ratios constrain the system s ability to match workload demands. For example, some inference workloads require intensive CPU pre post-processing or large memory-backed caching, resulting in CPU bottlenecks despite idle NPUs. Conversely, training workloads might saturate NPUs while leaving CPU resources underutilized. In such cases, tightly coupled CPU-NPU configurations lead to suboptimal hardware utilization and inflexible scaling. Although CloudMatrix384 s peer-to-peer UB topology already decouples logical resource as- signment, enabling flexible CPU-NPU matching across the supernode, physically separating CPU and NPU resources into dedicated resource pools unlocks further advantages: 1) Independent and Optimized Scaling: Physically separate NPU-centric nodes (e.g., with a minimal local CPU for basic management but maximized NPU density) and CPU-centric nodes (e.g., with many CPU cores, large DRAM capacities, and rich I O options, serving as the supernode s primary CPU and memory resource pool) could be developed. This allows the NPU compute capacity and the general-purpose CPU memory capacity of the supernode to be scaled independently and more economically. Datacenter operators could then compose supernodes with highly variable NPU-to-CPU-and-memory ratios, precisely tailored to the dominant workloads (e.g., NPU-rich for training, CPU memory-rich for data-intensive pre-processing or large-scale EMS caching). 2) Enhanced Resource Utilization and Specialization: Specialized node designs allow for hardware optimization specific to the primary resource type. NPU nodes could focus on power delivery and cooling for accelerators, while CPU memory nodes could optimize for memory density, I O bandwidth, or specific CPU instruction sets.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2348,
    "tokens": 489
  },
  {
    "text": "Datasets Full Power AP PT iNAS PT Stateful ePerceptive DynBal NExUME FMNIST 98.70 71.90 79.72 83.68 85.40 86.25 87.50 88.90 CIFAR10 89.81 55.05 62.00 66.98 68.50 70.20 71.75 76.29 MHEALTH 89.62 59.76 65.40 71.56 73.80 74.95 76.10 80.75 PAMAP 87.30 57.38 65.77 70.33 72.20 73.35 74.50 75.16 AudioMNIST 88.20 67.29 73.16 75.41 76.80 77.95 78.60 80.01 Table 1: Accuracy comparison on TI MSP board using piezoelectric energy harvesting. As observed in Table 1, NExUME consistently outperforms the state-of-the-art methods across all datasets. For instance, on CIFAR10, NExUME achieves an accuracy of 76.29 , which is approximately 4.54 higher than DynBal, the next best method. This improvement is significant in the context of energy-harvesting intermittent systems, where achieving high accuracy under strict energy constraints is challenging. The superior performance of NExUME can be attributed to its unique integration of energy variability awareness directly into both the training (DynFit) and inference (DynInfer) processes. Unlike other methods that either focus on modifying the DNN architecture or optimizing inference configurations, NExUME adapts the DNN s computational complexity in real-time based on instantaneous energy availability, leading to more efficient use of scarce energy resources and improved accuracy. Dataset Platform Energy Source Stateful ePerceptive DynBal NExUME FMNIST MSP430FR5994 Piezoelectric 20.1 20.8 21.5 23.4 CIFAR10 Arduino Nano Thermal 16.0 16.5 17.0 18.5 MHEALTH ESP32 S3 Eye Piezoelectric 18.5 19.0 19.6 21.0 PAMAP STM32H7 Thermal 16.5 17.0 17.5 19.0 AudioMNIST Raspberry Pi Pico Piezoelectric 20.5 21.0 21.7 23.2 Table 2: Energy efficiency comparison on different hardware platforms.",
    "source": "NexUME.pdf",
    "length": 1728,
    "tokens": 494
  },
  {
    "text": "While partitioning strategies effectively reduce the number of required arrays, they proportionally increase the number of cycles, resulting in constant energy consumption across different partitioning schemes. MEMHD distinguishes itself by enabling associative search with just one computation in a single 128x128 array. This unique architecture allows MEMHD to achieve not only single-cycle inference but also significantly reduced energy 0 20 40 60 80 100 0 20 40 60 80 100 of Cycles of Arrays Normalized Energy Consumption (AM) FMNIST BasicHDC 10240x10 BasicHDC 1024x100 (P 10) SearcHD 8000x10 [14] SearcHD 800x100 (P 10) QuantHD 1600x10 [13] QuantHD 160x100 (P 10) LeHDC 400x10 [15] LeHDC 100x40 (P 4) MEMHD 128x128 1 Total Number of Arrays Energy Consumption Computation Cycles Fig. 7: Normalized energy of AM and cycles with array usage. MEMHD (128x128) achieves comparable accuracy to base- lines with higher dimensions: BasicHDC (10240D), SearcHD (8000D), QuantHD (1600D), and LeHDC (400D). consumption. Consequently, MEMHD demonstrates remark- able efficiency gains: it is 80 more efficient than Basic HDC in terms of energy consumption. Moreover, compared to LeHDC, which represents the state-of-the-art in accuracy, MEMHD is 4 more efficient. V. CONCLUSION This paper introduced MEMHD, a Memory-Efficient Multi- centroid HDC framework that optimizes HDC for IMC archi- tectures. MEMHD effectively trains multi-centroid AM through clustering-based initialization and quantization-aware iterative learning. As a result, this approach significantly outperforms binary HDC baselines, achieving up to 13.69 higher accuracy with the same memory usage or 13.25 more memory effi- ciency at the same accuracy level. By enabling full utilization of IMC arrays and one-shot (or few-shot) associative search, MEMHD reduces computation cycles by up to 80 and array usage by up to 71 compared to the partitioning method on 128 128 IMC arrays, substantially improving energy efficiency and cycles. These advancements pave the way for more efficient implementation of HDC in resource-constrained environments.",
    "source": "2502.07834v1_MEMHD_Memory-Efficient_Multi-Centroid_Hyperdimensi.pdf",
    "length": 2105,
    "tokens": 492
  },
  {
    "text": "V) and reached the following conclusions: the primary reason behind their performance inefï¬ciencies is what can be termed as under-parallelism , i.e., not being able to fully exploit parallelism during compression. Especially, many levels of dependencies (i.e, various regularities of locks) exist in their pipelines e.g., the entire octree needs to acquire a macro lock before inserting a point and updating the tree (as shown in Fig. 5), during the intra-frame geometry compression; similarly, in the attribute compression shown in Fig. 6, the points at one layer in the octree have dependencies with those at other layers, thus their processing requires acquiring locks at a layer granularity . Even with the optimizations in [33], where the octree construction stage can be performed in parallel, there are still several synchronization points, resulting to limited parallelism. To summarize, the performance inefï¬ciencies in prior works can be primarily attributed to the lack of parallelism of these algorithms. Motivated by this observation, we next plan to improve the compression performance by exploiting various parallelism opportunities, which have been ignored, to the best of our knowledge, by the prior research but are essential in employing PCC in edge device settings. B. What are the potential opportunities? Increasing Geometry Compression Parallelism Using Morton Code: As mentioned earlier, the reason why the sequential update is necessary is that, during the interme- diate stages, the global Octree (the ï¬nal tree constructed at the last step) is unknown until the last point is inserted in the tree. To relax this constraint, if the PCs can be sorted based on a geometrical order, then the topographic structure of the global tree can be known at the beginning, thus ï¬xing the tree structure and not requiring to be updated in a point-by-point fashion. As a result, these points can processed in parallel. In fact, there is a mathematical concept called Morton Code [30] (essentially, a space ï¬lling curve that maps a multidimensional data to one dimension while preserving the locality of the data points), which describes the geometrical location relationships between points, and thus can serve this purpose perfectly.",
    "source": "PCcompress.pdf",
    "length": 2247,
    "tokens": 465
  },
  {
    "text": "[20] facebookresearch. 2007. esm. [21] Timothy R. Fallon, Vikram V. Shende, Igor H. Wierzbicki, Amanda L. Pendlet on, Nathan F. Watervoort, Robert P. Auber, David J. Gonzalez, Jenni fer H. Wisecaver, and Bradley S. Moore. 2024. Giant polyketide syn- thase enzymes in the biosynthesis of giant marine polyether toxins. Science 385, 6709 (2024), 671 678. [22] Python Software Foundation. 2007. python. [23] Walter Friedrich, Paul Knipping, and Max Laue. 1913. Interferenzerscheinungen bei roentgenstrahlen. Annalen der Physik 346, 10 (1913), 971 988. [24] Mu Gao and Jeffrey Skolnick. 2021. A general framework to learn tertiary structure for protein sequence characterization. Frontiers in bioinformatics 1 (2021), 689960. [25] Cong Guo, Jiaming Tang, Weiming Hu, Jingwen Leng, Chen Zhang, Fan Yang, Yunxin Liu, Minyi Guo, and Yuhao Zhu. 2023. Olive: Accelerating large language models via hardware-friendly outlier-victim pair quantization. In Proceedings of the 50th Annual International Symposium on Computer Architecture. Association for Computing Machinery, New York, NY, USA, 1 15. [26] Tae Jun Ham, Sung Jun Jung, Seonghak Kim, Young H. Oh, Yeonhong Park, Yoonho Song, Jung-Hun Park, Sanghee Lee, Kyoung Park, Jae W. Lee, and Deog- Kyoon Jeong. 2020. AË† 3: Accelerating attention mechanisms in neural networks with approximation. In 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE, IEEE, Piscataway, NJ, USA, 328 341. [27] Seunghee Han, Seungjae Moon, Teokkyu Suh, JaeHoon Heo, and Joo-Young Kim. 2024. BLESS: Bandwidth and Locality Enhanced SMEM Seeding Acceleration for DNA Sequencing.",
    "source": "2505.05893v1_LightNobel_Improving_Sequence_Length_Limitation_in.pdf",
    "length": 1634,
    "tokens": 480
  },
  {
    "text": "10 8.1 Detailed process flow A detailed manufacturing process flow for a WSSCB has been developed. This is out of scope this this paper but is available in supplementary material. 11 9 TRIMERA The ZettaLith TRIMERA stacks are CASCADE arrays of FP4 processing elements. Other systems using ZettaLith construction can use different TRIMERA stacks, such as BitNet b1.58 CASCADE arrays, FP8 CASCADE arrays, HPC stacks or DSP stacks for various applications. The FP4 TRIMERAs are designed as a Simple Hybrid Array of Processing Elements (SHAPE). They contain edge-to-edge CASCADE arrays of FP4 PEs. This achieves maximum performance and simplicity. The SLD contains 203 million FP4 PEs, each being 505 transistors. There are no bond pads, no TSVs, no SRAM, no analog, and nothing that requires synthesis or standard cells. All connections to any other circuitry is via hybrid bonding to the HILT die. The SLD can be designed for a new process without waiting for standard cells, SRAM, or analog mixed- signal qualification, or IP blocks for complex designs such as processors or high speed interfaces. All such circuits are in the mainstream process BID or the HILT dies, which can potentially remain unchanged over multiple generations of SOTA process nodes. While back-side power is scheduled to be available for the A16 node, this is not used. Power is delivered via hybrid bonding to the front side of the wafer. The SLD is intentionally very simple and highly repetitive. This is to make it extremely fast to design, and to port to new processes. It also substantially reduces mask calculation time and cost, which is significant at SOTA logic nodes. 9.1 Main signal interconnects of TRIMERA Figure 5 illustrates the fundamental signal interconnect architecture within an SCB module of a WSSCB, showing how high-bandwidth memory (HBM) interfaces, logic processing, and I O functions are integrated through interconnection paths. The HBM stack connects to the BID through HBM channels in the WSSCB. The SLD is integrated with the HILT via very high density face-to-face hybrid bonds providing millions of high- density, low-latency vertical connections between the SLD and the HILT.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2181,
    "tokens": 490
  },
  {
    "text": "Autoscaling in Public Cloud: There are several research works that optimize the resource provisioning cost in pub- lic cloud. These works are broadly categorized into: (i) multiplexing the different instance types (e.g., Spot, On- Demand) [12, 23, 34, 41, 42, 68, 79], (ii) proactive resource provisioning based on prediction policies [34,36,40,41,69,86]. Cocktail uses similar load prediction models and auto-scales VMs in a distributed fashion with respect to model ensem- bling. Swayam [34] is relatively similar to our work as it han- USENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation 1043 Baseline(BL) NASLarge IRV2 Xception DNet121 NASMob Models 10 8 7 5 2 BL_Latency 311(ms) 152(ms) 120(ms) 100(ms) 98(ms) E_Latency 152(ms) 120(ms) 103(ms) 89(ms) 44(ms) Table 3: Comparing latency of Ensembling (E_Latency) with single (baseline) models. dles container provisioning and load-balancing, speciï¬cally catered for single model inferences. Cocktail s autoscaling policy strikes parallels with Swayam s distributed autoscaling; however, we further incorporate novel importance sampling techniques to reduce over-provisioning for under-used models. Table 2 provides a comprehensive comparison of Cocktail with the most relevant works across key dimensions. 2.3 Pros and Cons of Model Ensembling In this section, we quantitatively evaluate (i) how effective ensembles are in terms of accuracy and latency compared to single models, and (ii) the challenges in deploying en- semble frameworks in a cost-effective fashion on a public cloud. For relevance in comparison to prior work [27, 83] we chose image inference as our ensemble workload. While ensembling is applicable in other classiï¬cation workloads like product recommendations [24,53], text classiï¬cation [71] etc, the observations drawn are generic and applicable to other applications.",
    "source": "cocktail.pdf",
    "length": 1880,
    "tokens": 468
  },
  {
    "text": "Compounding this, emerging batching techniques such as chunked prefill [16], and disaggregated prefill-decode [17] introduce additional variables into the design space. We define a single hardware client as hardware cluster combined with scheduler for request scheduling. The hardware cluster includes hardware, memory, and other physical components combined with software optimization technique specific to a particular hardware. For, e.g. 2xH100 GPU running vLLM is considered a single client, a separate ASIC device for running RAG steps would be another client. Current simulation frameworks, Table I, lack the fidelity to model these dynamics. For example, while Vidur [14] supports multiple homogeneous clients(for existing system only), it cannot simulate heterogeneous client configurations or disag- gregated prefill and decode hardware, particularly for systems that are not yet accessible. Similarly, LLMServingSim [13] fail to capture both chunked and disaggregated batching. Both these works falls short at modeling advanced multi-stage requests, rendering them inadequate for evaluating modern inference pipelines. To bridge this gap, we introduce HERMES, a high-fidelity simulator designed for multi-stage, heterogeneous LLM serving systems. HERMES uniquely models diverse request stages like KV cache retrieval, RAG, reasoning, prefill, and de- code across a hierarchy of hardware components, including accelerators, memory nodes, and disaggregated clusters. Unlike prior works, HERMES supports multiple heterogeneous clients servicing distinct models and request types simultaneously, while integrating advanced batching strategies (e.g., chunked prefill and disaggregated batching) and multi-level memory hierarchies with offload capabilities. Furthermore, HERMES uses hardware traces with linear regression to model real hardware systems. At the same time, HERMES leverages open-source hardware modeling frameworks to simulate future systems, and provide recommendation for future hardware inference system design. HERMES empowers computer architects to answer pivotal questions like: How does adding a parallel reasoning thoughts affect end-to-end latency under memory constraints? What is the optimal balance between chunked, continuous, and disaggregated batching for hybrid RAG-decode pipelines? Through various case studies, we demonstrate HERMES s various capabilities.",
    "source": "2504.09775v3_Understanding_and_Optimizing_Multi-Stage_AI_Infere.pdf",
    "length": 2394,
    "tokens": 468
  },
  {
    "text": "Emerging ML-AI Techniques for Analog and RF EDA Zhengfeng Wu Drexel University Philadelphia, Pennsylvania, USA Nnaemeka Achebe Drexel University Philadelphia, Pennsylvania, USA Ziyi Chen Drexel University Philadelphia, Pennsylvania, USA Vaibhav V. Rao Drexel University Philadelphia, Pennsylvania, USA Pratik Shrestha Drexel University Philadelphia, Pennsylvania, USA Ioannis Savidis Drexel University Philadelphia, Pennsylvania, USA Abstract This survey explores the integration of machine learning (ML) into EDA workflows for analog and RF circuits, addressing challenges unique to analog design, which include complex constraints, non- linear design spaces, and high computational costs. State-of-the-art learning and optimization techniques are reviewed for circuit tasks such as constraint formulation, topology generation, device mod- eling, sizing, placement, and routing. The survey highlights the capability of ML to enhance automation, improve design quality, and reduce time-to-market while meeting the target specifications of an analog or RF circuit. Emerging trends and cross-cutting chal- lenges, including robustness to variations and considerations of interconnect parasitics, are also discussed. Keywords Analog EDA, RF Design Automation, Machine Learning, AI in EDA 1 Introduction to ML-AI Techniques for Analog RF Design Automation Analog design remains a cornerstone of modern integrated circuits, accounting for approximately 20 of the chip area and 40 of the total IC design effort [1]. In addition, analog circuits contribute to approximately 50 of the costly design iterations that occur during development [1]. As analog and RF systems evolve toward higher frequencies and greater levels of integration, traditional knowledge- driven methods struggle to address the increasing computational and design complexities [2]. Electronic design automation (EDA) has transformed the design of an IC that allow for high-level circuit implementation strate- gies. However, while digital design has achieved high levels of abstraction and automation, analog design automation lags behind as analog circuits are highly customized [3]. Analog synthesis and physical design typically follows a hierarchical flow that includes topology generation, device sizing, layout generation, and post- layout verification. The challenges of automating analog design stem from the highly non-linear design space, computational com- plexity, and stringent performance and manufacturing constraints, which result in complex multi-objective optimization problems that require intricate trade-offs between competing circuit objectives [3].",
    "source": "2506.00007v1_Emerging_ML-AI_Techniques_for_Analog_and_RF_EDA.pdf",
    "length": 2635,
    "tokens": 499
  },
  {
    "text": "This utility function effectively balances the benefits of participation against the associated costs and future oppor- tunities, guiding sensors to make strategic decisions that optimize their long-term contributions to the network. Nash Equilibrium and Stability: A Nash equilibrium (NE) represents a stable action profile a (t) where no sensor can unilaterally improve its utility by deviating from its current strategy: Ui(a i (t), a i(t)) Ui(ai(t), a i(t)) ai(t), i. 4 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 Achieving an NE ensures that sensor participation patterns are stable; once equilibrium is reached, no single sensor ben- efits from changing its participation decision independently. This stability is critical for maintaining consistent network performance and energy sustainability over time. Distributed Best-Response Algorithm: To realize the NE, we propose a distributed best-response algorithm where each sensor iteratively adjusts its action based on the current state and the expected actions of others. The algorithm operates as follows: Algorithm 1 Distributed Best-Response Participation Algo- rithm 1: Input: Current energies Bi(t), predicted harvest Ë†Ei(t 1), parameters Î³, Î´, Î·, Î², and energy costs ecap( ), einf, ecomm. 2: At each inference event: 3: Each sensor si receives a solicitation from the lead sensor and forms an estimate of Ai(t) given potential SNR choices and expected actions of others. 4: For each action candidate ai(t) {P, NP}, the sensor computes the expected utility: U ai(t) i E[Ri(t)] E[ei(t)] Î²E[Vi(t 1)], where the expectations are taken over uncertainties in correctness, SNR impact, and future energy. 5: If U P i U NP i and Bi(t) ecap(SNRi(t)) einf ecomm, then si chooses P. Otherwise, it chooses NP.",
    "source": "ParticipationGamesICML25.pdf",
    "length": 1936,
    "tokens": 467
  },
  {
    "text": "Restrictions apply. TABLE IV: Comparison against Potluck and MCDNN mAP Latency Energy MCDNN 36.9 33 35 Potluck 51.6 63 61 This Work 50.3 35 34 in Fig. 8d and Fig. 9d, DeepCache can only save 38 and 45 on execution time for YOLOv3 and YOLOv4-tiny, respectively, which are less than both FI SI (e.g., 52 for YOLOv3; 53 for YOLOv4-tiny) and FI SI PI (e.g., 55 for YOLOv3; 61 for YOLOv4-tiny) schemes. Another hardware-based optimization has been proposed in Euphrates [9], as discussed in Sec. II-B3. We acknowledge that, compared to our proposal, Euphrates yields around 10 additional energy savings when the window size is set to be as large as 8 (i.e., always skipping 7 frames). We want to emphasize however that, such static (pre-deï¬ned) window-size works well only when there are few movements in the videos. On the other hand, when the video has more dynamic behavior, this static skipping jeopardizes the quality of applications which demand high accuracy. To give an example, we selected a segment of frames (i.e., Frame 6750 to Frame 6850) from V1 [33], in which the objects move more aggressively than other segments. In this scenario, our scheme ï¬gures out that more movements exist from large MV blocks, and dynam- ically adjusts the inference decisions. Hence, our scheme does not lose any accuracy, and still saves 43 energy. However, due to the static window size employed, Euphrates [9] leads to as much as 6 accuracy drop in this video segment, which is hardly acceptable by most applications. Apart from the above mentioned optimizations at the layer- level (DeepCache) and the frame-level (Euphrates), recall that, in Table I, we indicated (in the last column) the Decision Making Logic (DM) the previously proposed optimization strategies employ. Now, in Table IV, we present the mAP, latency and energy efï¬ciency results with those prior schemes.",
    "source": "PCframeSim.pdf",
    "length": 1865,
    "tokens": 470
  },
  {
    "text": "To overcome these limitations, we propose COBRA, a hardware software co-designed Binary Transformer Acceler- ator optimized for edge FPGAs. COBRA introduces several innovations, including the Shifted Polarized Softmax (SPS) for hardware-efficient attention, a true 1-bit binary multiplication method for 1, 0, and 1 matrices, and integer packing to maximize bandwidth. Additional optimizations, such as efficient popcount units, operation fusion, processing element reuse, and parallelism tuning, further enhance throughput, latency, and resource efficiency. Our evaluations show that COBRA operates efficiently on both mid-range edge FPGA (ZCU102) and low-power edge FPGA (KV260), making it portable and well-suited for resource-limited edge deployment on various devices. The contributions of this work are sum- marized as follows: We propose COBRA, an algorithm-architecture co- optimized hardware accelerator for efficient inference of binary Transformer models. COBRA achieves 3.5Ë† speedup over state-of-the-art binary transformer acceler- arXiv:2504.16269v2 [cs.AR] 24 Apr 2025 ators. At hardware architecture level, we propose a real 1-bit binary matrix multiplication engine, RBMM. It achieves high computational efficiency with real 1-bit operations, using bitwise XNOR and popcount operations with a unique don t care (DC) count mechanism. RBMM is designed to support serveral variations of binary opera- tions, making it reusable and resource-efficient for edge devices, and it could be extended to other binary models. At algorithm level, we propose a hardware-friendly binary attention mechanism, shifted polarized softmax, SPS, which enables efficient hardware implementation with negligible impact on transformer models inference ac- curacy. At system level, we propose a series of optimizations including 6:3 compressor-based popcount, quantization- fused multiplication, and long bitwidth datapack repre- sentation that further enhance the system performance of COBRA. II.",
    "source": "2504.16269v2_COBRA_Algorithm-Architecture_Co-optimized_Binary_T.pdf",
    "length": 1989,
    "tokens": 431
  },
  {
    "text": "This improvement is attributed to shorter total lengths reducing the KV cache space required per request, which in turn allows for larger batch sizes. Moreover, as the TPOT SLO becomes more stringent, from 50 ms to 15 ms, CloudMatrix- Infer adjusts the batch size accordingly to meet latency targets. Under a relaxed SLO of 50 ms, CloudMatrix-Infer supports a batch size of 96 and achieves a throughput of 1,943 tokens s per NPU while satisfying the latency constraint. As the SLO tightens to 30 ms and 15 ms, the batch sizes reduce to 24 and 8 respectively, resulting in lower throughputs of 974 and 538 tokens s per NPU. These findings demonstrate CloudMatrix-Infer s ability to meet diverse latency constraints by dynamically scaling batch sizes, all while maintaining high decoding throughput even under stringent real-time demands. 5.3 Accuracy To comprehensively assess the inference accuracy of DeepSeek-R1 when quantized to INT8 and deployed on CloudMatrix384, hereafter referred to as DeepSeek-R1 (INT8) for brevity, we conduct extensive tests based on widely used benchmarks. Our evaluation focuses on comparing the accuracy of the INT8 quantization implemented by SilliconFlow ( 4.5) against results from the official DeepSeek-R1 API [10] and results published in its technical report [13]. Given that the original DeepSeek-R1 technical report does not fully disclose all testing parameters for each benchmark, which can lead to variations in direct replication, we adopt a side-by-side evaluation 42 Table 5. Accuracy comparison of DeepSeek-R1 with INT8 quantization on Ascend 910, the official DeepSeek- R1 API [10], and results reported in the DeepSeek-R1 technical report [13] across multiple benchmarks (Results from benchmarks with testing configurations deemed inconsistent have been excluded.).",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 1813,
    "tokens": 392
  },
  {
    "text": "To address these limitations, recent advances in deep learning, particularly Graph Neural Net- works [23] (GNNs), have opened new avenues for circuit representation learning. GNNs excel at adaptively capturing complex dependencies between nodes in circuit graphs, enabling low- dimensional vector representations of logic gates. These representations have shown remarkable performance in various EDA tasks, including congestion prediction, power estimation, and testa- bility analysis. In the context of AIG representation learning, GNN-based methods have gained traction by transforming circuit netlists into logic representations. For instance, DeepGate [17] uses AIG-based representation with signal probability supervision for functional modeling. Similarly, GAMORA [29] employs a multi-task framework to model circuit structure and logic function for gate-level recognition. Additionally, PolarGate [18] introduces polarity embeddings and logic oper- ators to enhance message passing. Furthermore, DeepGate3 [25] combines GNNs and transformers with circuit-specific pretraining to enhance scalability. Although existing methods have made notable progress, the growing integration density and functional complexity of modern integrated circuits have led to increasingly intelligent and large- scale logic designs. These trends introduce new demands on structural modeling and functional abstraction, giving rise to the following two challenges for existing methods: Challenge 1: Facing structural heterogeneity in AIGs. Due to the wide variation in circuit complexity and gate arrangements in real-world scenarios, AIGs exhibit significant structural het- erogeneity with diverse topologies and uneven gate distributions. Such heterogeneity complicates capturing consistent structural characteristics for AIG representation. Due to significant structural heterogeneity, existing methods struggle to capture consistent characteristics, making it difficult for existing GNN-based methods to generate stable and reliable representations [4, 15, 33]. Thus, achieving effective AIG representation to overcome structural heterogeneity remains a critical challenge. Challenge 2: Lacking efficient capture of global structural information in AIGs. With the increasing integration density of modern Field Programmable Gate Arrays (FPGA) designs, accurate circuit analysis increasingly depends on understanding global logic interactions across the entire AIG [2, 32]. However, existing methods primarily rely on local message-passing mechanisms, which limits their ability to model long-range logic dependencies across large-scale AIGs. This often results in the loss of crucial global logic context, leading to reduced accuracy and limited scalability in downstream EDA tasks.",
    "source": "2506.06787v1_FuncGNN_Learning_Functional_Semantics_of_Logic_Cir.pdf",
    "length": 2771,
    "tokens": 495
  },
  {
    "text": "Latency is in Cycles. Based on the ARAB dataset, the dimension of the input is 13 and the series length is 50. ğ‘ğ‘¥ 10 20 30 40 50 Latency 16,344 62,032 162,125 412,452 740,512 BRAM 8 8 11 21 37 OMS DSP 18 18 12 19 19 FF 7,822 10,941 15,422 12,529 14,325 LUT 8,631 11,080 13,490 11,522 12,761 Latency 14,474 63,649 170,557 435,699 789,574 BRAM 8 8 12 26 50 RMS DSP 19 19 12 20 19 FF 7,768 10,900 15,455 12,510 14,351 LUT 8,284 10,615 12,805 10,643 11,734 Latency 5,628 21,448 47,468 83,688 130,108 BRAM 2 2 2 5 9 DPRR DSP 5 5 5 5 5 FF 1,212 1,815 2,284 2,667 3,171 LUT 1,465 1,836 2,086 2,395 2,725 5.2 Comparison of Classification Accuracy with Existing Machine Learning Methods Next, we compared the accuracy of the proposed DFR with DPRR (DFR_DPRR), to those of the DFR using DRS (DFR_DRS) and other machine learning methods [8]. Comparison with DFR_DRS corresponds to comparison with existing studies of DFR [2, 16]. This comparison of accuracy is followed by a comparison of hardware usage. As can be seen from the comparison in Section 5.1, DFR with OMS or RMS has the same level of accuracy as DFR_DPRR, but is less computationally efficient than DFR_DPRR, therefore it is not included in the comparison. However, DFR_DRS is less accurate than DFR_DPRR, but is more computationally efficient, therefore it is worth evaluating. The DFR models were implemented using Python. The hyperparameters listed in Table 5 were used for each dataset. In some datasets, the length of the input series varied. For data with a shorter length ğ‘‡than the maximum series length ğ‘‡max, the inputs were filled with zeros when using methods other than DFRs.",
    "source": "2504.11981v1_Hardware-Friendly_Delayed-Feedback_Reservoir_for_M.pdf",
    "length": 1639,
    "tokens": 502
  },
  {
    "text": "We analyze the accuracy-latency trade offs of each strategy and show their benefits in different scenarios. A. Appliance Energy Prediction The appliance energy prediction data-set predicts the energy usage of home appliances, given the environmental parameters, 3 Algorithm 1: Training and Inference Pseudocode function TRAIN(Sensor Data, NodeID, CloudID) Edge for each node do prep data(data,node); pre-process the data at edge if Privacy Aware then train model() Locally train the model send model(cloudID) else send data(cloudID); Send raw data to cloud Cloud for each node do if Privacy Aware then sample trees(nodeID); Sample trees from each node else merge data(); merge raw data from all nodes train(); end function function INFERENCE(Data, NodeID) Edge for each node do Predict() if Accuracy Threshold then Send data(CloudID); Send data to cloud for accuracy else Send results(CloudID); Send the inference result Run Prediction at Cloud end function 0 0.02 0.04 0.06 0.08 0.1 0.12 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 Edge Cloud (Shared) Cloud (Privacy) latency (ms) Correlation Pearson Correlation Latency (ms) (a) Data from 2 homes 0 0.02 0.04 0.06 0.08 0.1 0.5 0.55 0.6 0.65 0.7 0.75 Edge Cloud (Shared) Cloud (Privacy) Latency (ms) Correlation Pearson Correlation Latency (ms) (b) Data from 4 homes Fig. 2: Accuracy comparison of different policies on a simulated distributed setup. The data set is divided into 2 chunks creating a two home set up and the similar is done for a 4 home setup. such as temperature and humidity of different regions of the home as well as the locality (from weather station data [3] with 14803 training samples and 4932 testing samples). This data set directly fits our use case for two reasons - 1. In the real world, these sensors would be distributed in different homes, and each home will have its own idiosyncrasies. 2. The home owners may or may not be willing to share the sensor measurements of their home for privacy reasons.",
    "source": "EdgeClourRF.pdf",
    "length": 1977,
    "tokens": 484
  },
  {
    "text": "If increasing loaded tile size , update base scratchpad addresses to fit new tile size Figure 16: The list of rules provided during both the planning and code implementation phases, as described in Sec. 3.2. 20 C Code Examples In this section, we discuss in greater depth what optimizations Autocomp applies in our evaluations and how Autocomp is able to achieve significantly better performance than hand-optimized code. C.1 12544x64x256 GEMM Fig. 17 contains the unoptimized Exo-generated code, used as the starting point for search. Fig. 18 contains the code generated by Exo after hand-optimization by Ikarashi et al. [26]. Figs. 19 to 21 contain the result of Autocomp optimization on Exo Unoptimized code. While the code is substantially transformed from the original code, some aspects remain the same. For example, in this case the configuration instructions and loop ordering remain largely the same. Of course, many optimizations have been applied to the code. We briefly summarize the optimization menu options selected and plans generated during the optimization process for this code. We also include the speedup after each respective optimization. 1. 1.67 : initial speedup of Exo Unoptimized code over Gemmini s software library before any optimization. 2. 1.93 after hoist redundant operations out of loops . This plan hoists constants like tile_dim 16 and loop-invariant expressions like ko 64 and k 16 out of inner loops. These precomputed values are reused inside Gemmini ops in each iterations, so we should reduce the number of times they must be calculated. 3. 1.95 after double buffering . This plan defines two buffer regions for matrices A and B in the scratchpad. A buffer_toggle flag is introduced to alternate between these buffers each iteration. All mvin, preload, and compute instructions are updated to use the active buffer based on the toggle. Data loading for the next iteration is scheduled earlier to overlap with current computation. Address calculations are adjusted to include buffer offsets accordingly. 4. 2.15 after pipeline operations to better overlap computation and data movement . Moves mvin2 (A tile load) to immediately after compute_preloaded in the ko loop to overlap A prefetch with current compute. Moves mvin3 (B tile load) earlier in the k loop, before the next compute, to overlap B prefetch with current compute.",
    "source": "2505.18574v2_Autocomp_LLM-Driven_Code_Optimization_for_Tensor_A.pdf",
    "length": 2370,
    "tokens": 499
  },
  {
    "text": "In 2022 30th European Signal Processing Conference (EUSIPCO). 1801 1805. EUSIPCO55093.2022.9909727 [44] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu. 2020. GhostNet: More Features From Cheap Operations. In Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition. 1580 1589. [45] Song Han, Huizi Mao, and William J. Dally. 2016. Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. arXiv:1510.00149 [cs] [46] Shintaro Hashimoto, Yohei Sugimoto, Ko Hamamoto, and Naoki Ishihama. 2019. Ship Classification from SAR Images Based on Deep Learning. In Intelligent Systems and Applications, Vol 1 (Advances in Intelligent Systems and Computing, Vol. 868), K Arai, S Kapoor, and R Bhatia (Eds.). 18 34. [47] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 770 778. [48] Wenjing He, Yuesong Yang, Shaohui Mei, Jian Hu, Wanqiu Xu, and Shiqi Hao. 2023. Configurable 2D 3D CNNs Accelerator for FPGA-Based Hyperspectral Imagery Classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 16 (2023), 9406 9421. [49] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the Knowledge in a Neural Network. arXiv:1503.02531 [stat] [50] Adrian HÃ¶hl, Ivica Obadic, Miguel Ãngel FernÃ¡ndez Torres, Hiba Najjar, Dario Oliveira, Zeynep Akata, Andreas Dengel, and Xiao Xiang Zhu. 2024. Opening the Black-Box: A Systematic Review on Explainable AI in Remote Sensing.",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 1634,
    "tokens": 469
  },
  {
    "text": "IV. DECENTRALIZED TRAINING A. Hierarchical Decentralized Training Federated learning has demonstrated its potential for decen- tralized training, with both cross-silo and cross-device settings studied for various user scenarios. However, its practicality and effectiveness might decrease when deploying it for clients with poor or unreliable communication. This challenge becomes FL FL Organizations, Institutions, and Clients with Reliable Connections Parties with Communication or Geometric Restrictions FL Phase1: Hybrid Decentralized Training Phase2: Model Merging Fig. 2: The vision and overview of our proposed framework for the future of AI-assisted hardware design. even more pronounced in extra-large-scale collaborative train- ing settings, where geometric and infrastructural restrictions might affect deployment feasibility. Additionally, most feder- ated learning approaches assume a shared network architecture for locally trained models, which becomes impractical in the context of LLM due to the high computational and memory requirements for LLM training on client devices. To promote the broader adoption of decentralized training for foundation models in AI-assisted hardware design, this paper proposes a hierarchical decentralized training scheme. As illustrated in Fig. 2 and Algorithm 1, our hierarchical decentralized training consists of two tiers. The first tier is referred to as hybrid decentralized training. For clients or organizations with reliable communication channels, feder- ated learning is employed for collaborative training. Multiple clients will employ federated learning within each group independently, resulting in several separately trained federated models. Meanwhile, for parties with isolated environments due to geographical or infrastructural limitations, individual local training is performed. In the second tier, different models with diverse domain knowledge, learned via either federated or local training, are combined together using model merging techniques. This hierarchical, two-tier decentralized training framework enables efficient utilization of private domain data regardless of physical or regulatory restrictions. B. Metric-based Aggregation Adaptive methods like client selection and quality-aware aggregation [35] have been explored in federated learning and model merging, primarily for classification and segmentation tasks. Their effectiveness in generative AI remains underex- plored, largely due to challenges in evaluating generated con- tent. Metrics like perplexity depend on reference outputs and often fail to capture functional equivalence e.g., semantically identical programs with different styles may score differently.",
    "source": "2506.00002v1_Advancing_AI-assisted_Hardware_Design_with_Hierarc.pdf",
    "length": 2703,
    "tokens": 481
  },
  {
    "text": "We look into data memoization as one such opportunity. For two instances of the same class, there should be a very high correlation in the sensor data. We empirically measure this by testing for correlation between the sensor signatures of different classes. Conservatively, we choose a correlation coefficient 0.95 to predict that the two activities are the same, and hence skip the inference altogether and just com- municate only the results to the host. We store ground truth sensor data pattern for all possible labels, and when new data arrives, we find the correlation of the sampled data against the ground truth data, and if any of the correlation coefficient 6 Power-Pred Decision Logic (MCU) Correlation Sensor Data 16bit DNN (x-bar) 12bit DNN (x-bar Coreset: Imp Smp Clust. Wireless Communication H S C M Harvestor Sensor Node EH Sense Compute EH Sense Compute EH Sense Compute Host Seeker Ecosystem Coreset Reconstruct DNN for Recovery DNN for Inference Ensemble Engine Cluster Recovery Classfied Results Figure 5: Overall system design of Seeker comes out to be 0.95, we choose to ignore further infer- ence computation and only communicate the classification result to the host for further processing. Note that choosing the correlation threshold entirely depends on the application and user preference. 3.2.2 Recoverable Coreset Construction: The primary reason the accuracy of inferring on coreset data is lower than that of the original model is the loss of features. Typically, the sensor data are low dimensional, and hence even with a good quality of coreset construction, it is difficult to preserve all the features. However, while inferring at the host, if we are able to recover the data or reconstruct it with minimum error, the accuracy can easily be increased. Clustering Coreset Recovery: Clustering preserves the geometry of the original data by representing them as a set of N-spherical clusters represented with a center and a ra- dius. In the process of coreset construction we only preserve the coordinates of the centers and the radii of the clusters, and hence miss the coordinates of the points inside the clus- ters.",
    "source": "Seeker.pdf",
    "length": 2154,
    "tokens": 451
  },
  {
    "text": "Dean, Distilling the Knowledge in a Neural Network, 3 2015. [Online]. Available: 1503.02531v1 [12] G. Yang, G. Hao, L. Weijia, W. Qinghua, S. Chen, and N. Zhang, An Attentive Pruning Method for Edge Computing, ACM International Conference Proceeding Series, pp. 6 10, feb 2020. [13] F. Aguirre, A. Sebastian, M. Le Gallo, W. Song, T. Wang, J. J. Yang, W. Lu, M. F. Chang, D. Ielmini, Y. Yang, A. Mehonic, A. Kenyon, M. A. Villena, J. B. Rold an, Y. Wu, H. H. Hsu, N. Raghavan, J. Su n e, E. Miranda, A. Eltawil, G. Setti, K. Smagulova, K. N. Salama, O. Krestinskaya, X. Yan, K. W. Ang, S. Jain, S. Li, O. Alharbi, S. Pazos, and M. Lanza, Hardware implementation of memristor-based artificial neural networks, Nature Communications 2024 15:1, vol. 15, no. 1, pp. 1 40, mar 2024. [14] M. Kantharimuthu, P. Selvaraj, H. Sankar, and G. Brindavanam, Effi- cient Parallel Median Filter for Image Denoising: Implementation and Performance Evaluation, Traitement du Signal, vol. 41, no. 05, pp. 2403 2414, oct 2024. [15] A. Burman, J. Sol e-Casals, and S. E. Lew, Robust and memory-less median estimation for real-time spike detection, PLOS ONE, vol. 19, no. 11, p. e0308125, nov 2024. [16] W. Wang and P. Lu, An efficient switching median filter based on local outlier factor, IEEE Signal Processing Letters, vol. 18, no. 10, pp. 551 554, 2011.",
    "source": "2502.10089v1_A_Hybrid_Edge_Classifier_Combining_TinyML-Optimise.pdf",
    "length": 1337,
    "tokens": 476
  },
  {
    "text": "We evaluate the impact of different N:M patterns on the perplexity with a LLaMA3-8B model in Figure 6(b) for target sparsity of 50 . To demonstrate the impact of static N:M choices, we performed ablations with two N:M patterns, namely 2:4 and 4:8, where the weights are pruned based on their importance scores. We find that the model pruned with 2:4 sparsity yields a significant increase in perplexity over the dense baseline. 4:8, though it improves the performance, still falls significantly behind the baseline dense. However, enabling flexibility for N, i.e., N:4 {1:4, 2:4, and 4:4 (dense)} alleviates some of the perplexity degradation from a static N:M. More importantly, enabling flexibility for both N and M as proposed in FLOW i.e., N:M {1:2, 1:4, 2:4, 1:8, 2:8, 4:8, 8:8 (dense)}, we achieve the lowest perplexity that is closest to the dense baseline. This shows the non-uniformity of LLM layers to sparsity that can be catered to only with a flexible selection of N and M. TABLE II: Comparison of FlexCiM with baselines at 28nm . , in a column identify the presence and absence of a feature, respectively. Architecture Component (Area (mm2)) Total Area (mm2) Flexible N:M Flexibility Overhead Compute Density (Peak TOPS mm2) VEGETA [14] Input Output buffers (0.27) 3.28 15.4 2.36 Weight buffer (0.18) PE array (2.46) Sparsity support (0.38) SDP [35] Input Output buffers (0.27) 0.98 3.5 7.65 Weight buffer (0) DCiM array (0.68) Sparsity support (0.024) FlexCiM (Ours) Input Output buffers (0.27) 1.03 5.9 7.28 Weight buffer (0) DCiM array (0.72) Sparsity support (0.043) C. FlexCiM: Results and Analysis Area comparison.",
    "source": "2504.14365v1_Accelerating_LLM_Inference_with_Flexible_NM_Sparsi.pdf",
    "length": 1634,
    "tokens": 484
  },
  {
    "text": "IEEE, 2017, pp. 550 555. [52] MPEG, G-PCC codec description v2, 3nN43C8 , 2019. [53] MPEG, MPEG Point Cloud Compression, pcc.org , 2022. [54] MPEG, Point Cloud Video: Loot, , 2022. [55] MPEG, Point Cloud Video: Redandblack, 3NyQq2R , 2022. [56] MPEGGroup, Geometry based point cloud compression (G- PCC) test model, tmc13 , 2021. [57] New Farmer Blogger, 3D points clouds for immersive real es- tate and telepresence experiences, , 2015. [58] Nvidia Corporation, Jetson AGX Xavier Developer Kit, , 2018. 297 Authorized licensed use limited to: Penn State University. Downloaded on August 10,2023 at 18:50:20 UTC from IEEE Xplore. Restrictions apply. [59] Park, Jounsup and Chou, Philip A. and Hwang, Jenq-Neng, Rate-utility optimized streaming of volumetric media for augmented reality, 2018. [60] W. A. Pearlman and A. Said, Entropy coding techniques. Cambridge University Press, 2011, p. 41 76. [61] Pix4D SA, PIX4Dcatch: Turn your mobile device into a professional 3D scanner using the power of photogrammetry, , 2021. [62] Point Cloud Library Contributors, Module kdtree - Point Cloud Library (PCL), group kdtree.html , 2022. [63] Point Cloud Library Contributors, Module octree - Point Cloud Library (PCL), group octree.html , 2022. [64] Point Cloud Library Contributors, Pcl gpu octree, https: github.com PointCloudLibrary pcl tree master gpu octree , 2022. [65] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, Frustum pointnets for 3d object detection from rgb-d data, in Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.",
    "source": "PCcompress.pdf",
    "length": 1579,
    "tokens": 465
  },
  {
    "text": "The proposed designs deliver better accuracy, as the average loss of the EvoApprox8b configurations is 23 , while in terms of energy, they provide 2 gains. This comparison highlights the advantage of studying the approximations at a lower DNN level, i.e., filter or kernel. Namely, the fine-grained use of multipliers with different approximation strength outperforms the conventional approximation of all the layer multiplications. Lessons Learnt: According to our multi-level design space exploration and evaluation, we experimentally prove that: (i) the first convolutional layers are more sensitive in appro- ximations, i.e., less error resilient, than the final ones. (ii) the approximation of multipliers based on a non-uniform fine-grained filter kernel-level approach delivers better accu- racy than the coarse-grained layer-level approximation. V. CONCLUSION AND FUTURE WORK In this work, we presented the MAx-DNN framework to explore the efficiency of multi-level arithmetic approximation in DNN hardware accelerators. MAx-DNN extends prior-art TABLE I COMPARISON OF APPROXIMATE RESNET-8 HARDWARE ACCELERATORS Approximation Approach Config. Energy Gain Accuracy Loss1 Proposed ROUP [2] FLAM-3clas. 2 1 1 49 18 FLAM-3clas. 2 2 1 52 20 KLAM-chan. 1 0 1 46 17 KLAM-chan. 2 0 2 53 21 KLAM-chan. 1 1 2 50 19 KLAM-chan. 2 1 2 54 21 KLAM-row 2 1 1 50 19 KLAM-row 2 1 2 52 20 Uniform Evo [5] Evo mul8 2AC 23 20 Evo mul8u 2HH 23 23 Evo mul8u NGR 32 23 Evo mul8u ZFB 39 23 Evo mul8u 7C1 20 24 1 It is reported compared to the full accurate model. The baseline quantized model, where we apply our approximations, already has an accuracy loss of 17 . design approaches considering approximate multiplier hete- rogeneity, not only to each DNN layer, but also to each filter and kernel.",
    "source": "2506.21371v1_MAx-DNN_Multi-Level_Arithmetic_Approximation_for_E.pdf",
    "length": 1782,
    "tokens": 468
  },
  {
    "text": "EMS tightly integrates with the disaggregated prefill and decode pipeline: Prefill Reuse and Store: Upon receiving a new request, the prefill engine queries EMS with a hash of the input prefix to identify reusable KV cache blocks. If found, these blocks are fetched via the UB plane and loaded directly into NPU memory, bypassing redundant computation. The engine then processes the remaining suffix and generates the corresponding KV cache blocks. These new blocks are asynchronously stored back to EMS, enabling reuse in future requests without stalling ongoing computation. Decode Selective Cache Storage: KV cache generated during the decode phase can be reused for non-reasoning models, but not for reasoning models like DeepSeek-R1. These reasoning models emit intermediate reasoning tokens followed by final response tokens. Intermediate tokens are typically not re-ingested in subsequent turns, and hence final response tokens shift in position when included in later prompts. Such positional changes disrupt cache validity due to position-sensitive attention. As a result, decode-generated caches are usually excluded from storage. However, if the system adopts approximate KV reuse techniques that tolerate positional shifts, selectively storing final response tokens cache blocks can offer performance benefits. 4.4.3 Model Caching Modern LLM serving infrastructures must efficiently support a diverse portfolio of models varying in size, architecture, and task specializations. These infrastructures must also accommodate dy- namic model switching in response to fluctuating service demands and continuous model updates. However, loading multi-billion-parameter LLMs from persistent storage, e.g., object storage service (OBS), into NPU memory incurs significant latency. For example, loading a DeepSeek-R1 model with 671B parameters from OBS, assuming a standard 2.5 GB s access bandwidth per bucket, takes over five minutes. This delay severely limits the practicality of dynamic model switching and impairs service responsiveness, particularly during model updates or A B testing. Thus, a fast caching mechanism is essential not only to mitigate these overheads but also to ensure responsive, agile model deployment. 36 Table 1. Performance comparison of model loading strategies for loading a 671B INT8 model (approximately 671GB data size) into 8 model instances within a CloudMatrix384 (The model is originally stored in an OBS bucket with 2.5GB s bandwidth.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2476,
    "tokens": 488
  },
  {
    "text": "Habana Labs Purpose-Built AI Inference and Training Processor Architectures: Scaling AI Training Systems Using Standard Ethernet With Gaudi Processor. IEEE Micro 40 (2020), 17 24. [46] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer Sentinel Mixture Models. arXiv preprint arXiv:1609.07843 (2016). [47] Microsoft. 2024. AzurePublicDataset. AzurePublicDataset. [48] Seungjae Moon, Junsoo Kim, Jung-Hoon Kim, Junseo Cha, Gyubin Choi, Seong- min Hong, and Joo-Young Kim. 2023. HyperAccel Latency Processing Unit (LPUTM) Accelerating Hyperscale Models for Generative AI . In 2023 IEEE Hot Chips 35 Symposium (HCS). IEEE Computer Society, Los Alamitos, CA, USA, 1 1. doi:10.1109 HCS59251.2023.10254693 [49] NVIDIA. 2020. NVIDIA A100 Tensor Core GPU Architecture. ampere-architecture-whitepaper.pdf. [50] NVIDIA. 2023. NVIDIA TensorRT-LLM. LLM. [51] Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim, Beom- seok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee. 2024. LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Effi- cient Inference in Large-Scale Generative Language Models. In ICLR. https: openreview.net forum?id gLARhFLE0F [52] Jaehyun Park, Jaewan Choi, Kwanhee Kyung, Michael Jaemin Kim, Yongsuk Kwon, Nam Sung Kim, and Jung Ho Ahn. 2024. AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (La Jolla, CA, USA) (ASPLOS 24).",
    "source": "2503.18599v2_Oaken_Fast_and_Efficient_LLM_Serving_with_Online-O.pdf",
    "length": 1587,
    "tokens": 491
  },
  {
    "text": "T LB tp td . (32) C. Online requests and iteration scheduling In this online part of the LLM inference optimization problem, two critical considerations arise. First, we need to determine which request to send to an available client once the previous request is completed. Second, when a round of decoding stage is finished, we must decide whether to send a preemptive prefill stage or continue this decode stage to the LLM workers for the subsequent time frame. The first issue presents an online scheduling problem, as illustrated by Eqs. (14) (15) and (16) (17). The primary decision in this context is whether to select a new request to override the original assignment in order to achieve better machine utilization. A sorting and online preemptive method is illustrated in Algorithm 1. This online algorithm first selects the future available requests, denoted as Ij, for client j J . The set Ij is sorted by N p i N d i , that is, N p i1 N d i1 N p i2 N d i2 i1 i2 Ij. Then, for each client, the algorithm calculates future requests and counts the expected remaining tokens remain token(j) for j J to be processed. Idle clients then greedily select the longest request from busy clients to process. This algorithm utilizes offline information on request assignment to provide timely online assignment decisions. Algorithm 1 Sorting and Online Preemptive Method Require: {Ij {i xij 1}, j J } for client j in clients J do if queue for client j is empty and Ij then pop Ij to client j remain token(j) remain token(j) (N p i N d i ) else if max(remain token(j)) 0 then pop arg max(remain token) to client j remain token(j) remain token(j) (N p i N d i ) end if end for Continuing from the previous discussion, the second prob- lem involves a sequential decision-making process, as outlined by Eqs. (2) (11). The main challenge here is to deliver timely and efficient decisions in real time. As previously mentioned, each round of decoding takes approximately 50 milliseconds. Thus, it is essential to ensure that decisions are made within 10 milliseconds to sustain system efficiency. To achieve this, we employ the following method to integrate quick decision- making into the process.",
    "source": "2502.15763v1_Hybrid_Offline-online_Scheduling_Method_for_Large_.pdf",
    "length": 2189,
    "tokens": 492
  },
  {
    "text": "(a) Diagram of the MatMul-free PE array, operating under an output- stationary dataflow. (b) Individual PE, which uses a bit shift in place of a multiplication. (c) Output PE (OPE) module, which performs input output rescaling as well as bias addition and ReLU activation. TCN processing scheme. Each cycle, it receives 16 4-bit unsigned uniform ReLU-based activations, either from the input memory (first layer) or activation memory (subsequent layers), along with 16 16 log2-quantized weights from the weight memory. We use 4-bit signed log2 weights, as they offer the same dynamic range as 8-bit signed integers while halving storage, reducing power and energy footprints [32], and maintaining floating-point accuracy [32]. Each individual PE (Fig. 10(b)) first left-shifts the horizontally broadcast input by the weight s exponent and then applies sign correction, producing a 12-bit signed output. These are summed vertically and accumulated in 18-bit signed uniform registers within output PEs (OPEs), detailed in Fig. 10(c). The OPE performs input rescaling (for residuals), bias addition, ReLU activation, and output rescaling before its output is written back to the activation memory. With inference being learning-free now, we also make learning fully multiplication-free by adapting Equation (6) for a log2 formulation, allowing for efficient PN parameter extraction. Since all weights are log2 (including sj i), the square in the bias term simplifies, thereby fully replacing multiplication by bit shifts: 8 Legend 4 4 Core logic 256b 112b 56b 128 rows Bias memory 5 128b 64b 64b 512 rows OPEs 16 16 high throughput eff. 64b 64b Always on Off during 4 4 Power domains Weight memory 256 rows 56b 16 16 Virtually stack adjacent memories to increase of rows 4 4 low-leakage, small nets (a) (b) PE array Act. memory 16b Fig. 11. (a) Comparison of simulated real-time KWS power and peak TOPS W estimates for different PE array sizes. (b) Data layout for activation, bias, and weight memories to support both 4 4 and 16 16 PE array usage, allocating LSBs of weight memories to the top-left 4 4 section.",
    "source": "2505.24852v2_Chameleon_A_MatMul-Free_Temporal_Convolutional_Net.pdf",
    "length": 2109,
    "tokens": 491
  },
  {
    "text": "Here, n represents the number of heads, fixed at 10 in our experiments. 3) Model Inference: During inference, all models are loaded in float16 format. CodeLlama-based models are configured with a max- imum token length of 8192, while CodeT5p-based models are limited to 2048 tokens. For speed evaluation, each prompt is processed using two decoding methods: greedy decoding and sampling decoding at a temperature of 0.8. For quality evaluation of the generated Verilog code, 20 responses are sampled per prompt at temperatures of 0.2, 0.4, 0.6, and 0.8. The final result for each prompt is determined by selecting the output with the highest accuracy across all temperatures. B. Evaluation Benchmark and Metric We use RTLLM [16] and VGen [22] as evaluation benchmarks. Specifically, we employ low-level prompts from VGen that align with the format of our training data. These prompts describe the module s function along with its header, including the module name and the input and output types, which are the most challenging cases. 1) Speed Evaluation: In addition to prompts from RTLLM and VGen, we utilize GPT-4 to generate additional prompts for the Verilog code generation task based on the input prompt formats of RTLLM and VGen, aiming to enhance testing accuracy by increasing the diversity of prompts used during evaluation. Ultimately, the generation speed of the fine-tuned models is assessed using a total of 575 input prompts. For each prompt, the model generate outputs using both greedy decoding and sampling decoding methods, with Please act as a professional Verilog designer. Create a simple Verilog module named \"data_register\" that takes a 4-bit input data_in and assigns it to a 4-bit output data_out using a non-blocking assignment on the positive edge of the clock.",
    "source": "2503.14153v1_Speculative_Decoding_for_Verilog_Speed_and_Quality.pdf",
    "length": 1789,
    "tokens": 393
  },
  {
    "text": "In contrast, communicating the data, often after little preprocessing, although popular, is not cheap in terms of power requirement, and often poses a challenge for remotely deployed and ultra low power WSNs. Prior works, trying to tackle this conflict between computa- tion, communication, power-requirement and quality of ser- vice (QoS), have pursued three major approaches: inference effort partitioning optimizations [22, 30, 31, 50], mitigation of energy provisioning limitations [24, 40, 43, 47, 56], and minimizing communication overheads [32, 33, 36, 37, 45]. One of the most emerging line of work aims to solve the energy provisioning problem at the edge by integrating en- ergy harvesting (EH) to the sensor nodes while making them more capable performing complex compute intermittently, which has given rise to energy harvesting wireless sensor networks (EH-WSNs). Specifically, recent works [24, 43] pro- posed EH, along with compiler runtime optimizations and leveraging non-volatile processors (NVP) [40, 56], to increase local compute at the edge. EH as a solution has been partic- ularly interesting as a means to address the sustainability issue of battery backing trillions of future devices. More importantly, EH can help us build sustainable distributed sensing monitoring infrastructure at virtually inaccessible places like oil-wells, mines, and even satellite orbits [14, 49]. However, harvested energy is fickle in nature, and typically arXiv:2408.14379v1 [cs.AR] 26 Aug 2024 harvested sources only deliver scant microwatts of power (see Figure 1b for an overview). The sporadic nature of har- vested energy and the lossy nature of EH based storage and charging circuits calls for using the harvested energy di- rectly to perform intermittent compute rather than storing energy for some distant future use. On this front, recent works [43, 44, 47] have specifically optimized DNN infer- ence execution at the EH-edge nodes by utilizing adaptive dynamic check pointing, intelligent scheduling and ensem- ble learning. Given the limitations of the EH budget, such approaches typically end up dropping many samples and not inferring from them locally. Importantly, they are of- ten incapable of transmitting the raw data due to a lack of sufficient energy; for sensing tasks with modest inference requirements, performing inference and transmitting the result can take less energy than transmitting raw data.",
    "source": "Seeker.pdf",
    "length": 2430,
    "tokens": 503
  },
  {
    "text": "DFG Retrieval. The DFG is composed of signal-level variables and relationships. As a result, it is useful when signal-level information is needed and the utilization method can vary greatly for different downstream tasks. In this work, we utilize the signal-level flow to enhance code completion and code debugging tasks, as illustrated in Figure 2. There are two primary operations for DFG retrieval of different tasks, which are Signal Traverse and Similarity-based Extract: Blocks Modules Signals Query Gen. Query Gen. Internal Eval. Internal Eval. Bench- mark Step 2 Step 3 RTL Repo Input Filtered Repo Manual Filtering Step 1 Fig. 5. HDLSearch benchmark generation flow. 1) Debugging. For debugging tasks, if a signal mismatch is detected, the debugging process can iteratively traverse the DFG upstream with Signal Traverse operation from the faulty signal, inspecting each node (e.g., operators, multiplexers, or instance outputs) to identify where the dataflow diverges from expected behavior. This approach guides LLM debugging by focusing only on the subgraph directly influencing the problematic signal, filtering out irrelevant code regions. By extracting the immediate upstream nodes and their associated code blocks, the system generates a concise, context-rich error candidate set. 2) Completion. While we want to retrieve the similar code with the unfinished code, some reference code may look different but still having the similar functionality because the hardware (i.e. dataflow) described are very similar. Graph embedding offers a viable approach for Verilog code com- pletion by translating code s structural and semantic relation- ships into a unified mathematical framework. By leveraging GraphSAGE [39], these graphs are compressed into low- dimensional vector representations that preserve contextual patterns, such as recurring HDL constructs (e.g., finite state machines, pipelined operations) or common coding idioms (e.g., non-blocking assignments in clock-driven blocks). When a developer writes partial code, the corresponding subgraph is embedded and compared against historical embeddings using similarity metrics, enabling the system to infer likely comple- tions even with incomplete structures by prioritizing nodes critical to the current context. This allows real-time retrieval of relevant patterns from large codebases while adhering to Verilog-specific constraints.",
    "source": "2505.15701v1_HDLxGraph_Bridging_Large_Language_Models_and_HDL_R.pdf",
    "length": 2408,
    "tokens": 477
  },
  {
    "text": "2019. Distance-based protein folding powered by deep learning. Proceedings of the National Acad- emy of Sciences 116, 34 (Aug. 2019), 16856 16865. [66] Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas Moshovos. 2020. Gobo: Quantizing attention-based nlp models for low latency and energy ef- ficient inference. In 2020 53rd Annual IEEE ACM International Symposium on Microarchitecture (MICRO). IEEE, IEEE, Piscataway, NJ, USA, 811 824. [67] Ali Hadi Zadeh, Mostafa Mahmoud, Ameer Abdelhadi, and Andreas Moshovos. 2022. Mokey: Enabling narrow fixed-point inference for out-of-the-box floating- point transformer models. In Proceedings of the 49th Annual International Sym- posium on Computer Architecture. Association for Computing Machinery, New York, NY, USA, 888 901. [68] Jinnian Zhang, Houwen Peng, Kan Wu, Mengchen Liu, Bin Xiao, Jianlong Fu, and Lu Yuan. 2022. Minivit: Compressing vision transformers with weight multiplexing. In Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition. IEEE, Piscataway, NJ, USA, 12145 12154. [69] Yang Zhang and Jeffrey Skolnick. 2005. TM-align: a protein structure alignment algorithm based on the TM-score. Nucleic acids research 33, 7 (2005), 2302 2309. [70] Feiwen Zhu, Arkadiusz Nowaczynski, Rundong Li, Jie Xin, Yifei Song, Michal Marcinkiewicz, Sukru Burc Eryilmaz, Jun Yang, and Michael Andersch. 2024. ScaleFold: Reducing AlphaFold Initial Training Time to 10 Hours. In Proceedings of the 61st ACM IEEE Design Automation Conference (San Francisco, CA, USA) (DAC 24). Association for Computing Machinery, New York, NY, USA, Article 265, 6 pages.",
    "source": "2505.05893v1_LightNobel_Improving_Sequence_Length_Limitation_in.pdf",
    "length": 1629,
    "tokens": 453
  },
  {
    "text": "Reducing the SLO, in turn, can potentially reduce the batch sizes of functions as well. Moreover, the reduced SLO target results in increased SLO violations across all policies. However, Kraken is able to maintain at least 99.5 SLO guarantee and spawns 50 , 34 and 15 less containers compared to Arch, Fifer and Xanadu, respectively. It can be seen that the difference in SLO compli- ance between Kraken, Comm Only, and Conn Only increases due to the reduced target SLO. This difference, in terms of percent of SLO violations, changes from being at most 0.1 to being between 0.1 to 0.35 . This is a result of Kraken being more resilient at the tail of the response time distribution as it uses both Commonality and Connectivity while spawning containers. In comparison, Comm Only and Conn Only fail to spawn enough containers for each important function as they do not consider both these parameters, resulting in increased tail latency and exacerbates the SLO violations. 7 Concluding Remarks Adopting serverless functions for executing microservice- based applications introduces critical inefficiencies in terms of scheduling and resource management for the cloud provider, especially when deploying Dynamic DAG Applications. To- wards addressing these challenges, we design and evalu- ate Kraken, a DAG workflow-aware resource management framework, for efficiently running such applications by uti- lizing minimum resources, while remaining SLO-compliant. Kraken employs proactive weighted scaling of functions, where the weights are calculated using function invocation probabilities and other parameters pertaining to the appli- cation s DAG structure. Our experimental evaluation on a 160-core cluster using Deathstarbench workload suite and real-world traces demonstrate that Kraken spawns up to 76 fewer containers, thereby improving container utilization and cluster-wide energy savings by up to 4 and 48 , respec- tively, compared to state-of-the art schedulers employed in serverless platforms. 8 Acknowledgement We are indebted to the anonymous reviewers for their in- sightful comments. This research was partially supported by NSF grants 1931531, 1955815, 1763681, 2116962, 2122155 and 2028929. We also thank the NSF Chameleon Cloud project CH-819640 for their generous compute grant. All product names used here are for identification purposes only and may be trademarks of their respective companies.",
    "source": "kraken.pdf",
    "length": 2417,
    "tokens": 493
  },
  {
    "text": "Then, during planning for new GEMMs with the same aspect ratios or with two shared dimensions, rather than exploring the full menu, we prompt the LLMs to select specifically the menu options used in our recorded schedule, one at a time. As we are not exploring the full menu, we can use a smaller beam width and sample count, reducing both LLM calls and search time. After completing this lightweight search, we take the best-performing code so far and further refine it by invoking the full Autocomp search for a small number of iterations. This resembles the classic exploration-exploitation trade-off in optimization: by reusing a schedule we exploit a known high-quality schedule and avoid the initial exploration cost for a new workload. 4 Evaluating Autocomp Without Schedule Reuse We evaluate the effectiveness of Autocomp on three distinct types of workloads 1: 1) matrix multipli- cation (GEMM) derived from ResNet-50, 2) convolution derived from ResNet-50, and 3) robotics code used for model-predictive control. For all experiments, we ensemble OpenAI s o3-mini and gpt-4o (via the OpenAI API Platform) for both phases, with temperature 1.0. Menu options are dropped out with 70 probability. 4.1 Hardware Platform We use Gemmini [17] to generate two different accelerators for evaluation. Gemmini is an accelerator generator that can generate systolic array- and vector-style tensor accelerators with a wide range of data types and sizes. Gemmini is ideal for evaluating Autocomp as it: 1) generates accelerators that deliver performance comparable to commercial ones, 2) is open-source, enabling instantiation of different accelerator instances, user modifications to the software toolchain, and extraction of fine-grained performance feedback, and 3) supports fast and cycle-accurate hardware simulation via FireSim [31]. Like other accelerators, its low-resource nature eliminates data contamination and makes directly prompting LLMs challenging. We used AWS EC2 F1 instances and local AMD Alveo U250 FPGAs to run FireSim. For the GEMM and convolution benchmarks in Secs.",
    "source": "2505.18574v2_Autocomp_LLM-Driven_Code_Optimization_for_Tensor_A.pdf",
    "length": 2085,
    "tokens": 456
  },
  {
    "text": "We use polynomial regression models decode runtime with mean square error(MSE) 4.09e-07. Prefill runtime is modeled using past token count, prefill token count, batch size, and token2, with MSE 6.49e-05. HERMES can also leverage analytical simulators LLMCompass [44], GenZ [45] to model unavailable system configuration(e.g. Nvidia Rubin [46] or Google Ironwood [47]). Additional framework like HW Simu- lators [27], [48], [49], [50] can be used to model individual operators dataflow microarchitectures. We can apply the same ML modeling approach to predict run times generated by analytical simulators. While this approach models a particular model configuration, we can collect data points with different optimizations (such as quantization [51], [52], [53], pruning [54], [55], [56], [57], speculative decoding [58], [59], [60], [61], flash attention [62], [63]) and use it as to train the ML model. ML modeling provides a 20 50 simulation speedup compared to analytical simulation. 2) RAG cluster:: RAG HW cluster required to i) convert input query into search space embedding, ii) retrieve related documents, iii) Re-rank top k documents. In practice, the embedding and retrieval workloads can run on different devices. For instance, the embedding model may be deployed on NPUs or GPUs, while retrieval and reranking steps are more suited to CPUs. For embedding model, we use the embedding model prefill time for a give query. The time is either calculated from real trace or simulator as described in previous subsection. For modelling the retrieval and reranking steps are more suited to CPUs we implement IVF-PQ modelling equations described in RAGO-SERVE [34]. 3) KV Retrieval:: HERMES models KV cache retrieval as a multi-level memory hierarchy, similar in spirit to CPU cache systems. Each level in the hierarchy is characterized by its capacity, lookup latency (ranging from nanoseconds to milliseconds), bandwidth, and cache hit rate. However, unlike CPU caches where a miss leads to DRAM access, a miss in prefix caching may result in the need to recompute the entire context using the LLM, which is significantly more expensive.",
    "source": "2504.09775v3_Understanding_and_Optimizing_Multi-Stage_AI_Infere.pdf",
    "length": 2144,
    "tokens": 497
  },
  {
    "text": ". , ğ‘ğ‘¥). (28) The above ğ‘ğ‘¥ (ğ‘ğ‘¥ 1) values were used as the proposed DPRR. These features are represented as vectors. Using ğ’™ (ğ‘˜) [ğ’™(ğ‘˜), 1], DPRR can be represented as follows: ğ’“ vec( ğ‘‡ ğ‘˜ 1 ğ’™(ğ‘˜)ğ’™ (ğ‘˜ 1)). (29) DPRR is expected to be implemented more efficiently than OMS or RMS because it can be calculated by using only matrix multiplications. 4.2 Fully Digital DFR for Hardware Implementation Here, we describe a DFR model that uses the proposed reservoir representation. In contrast to existing DFRs that use a mixture of digital and analog signals, our model operates entirely digitally making it particularly easy to implement in FPGAs. In addition, the proposed model incorporates the following: 1) the reservoir layer applies a lightweight nonlinear function without storing weights that require a large amount of memory; 2) the masking method suppresses the accuracy variation and supports multivariate time series inputs; and 3) the learning method of an output layer based on ridge regression with a constant term improves classification accuracy. Algorithms 1 and 2 show pseudo codes for the algorithms used in the nonlinear element part of the DFR. Algorithm 1 Calculate ğ‘“ Input: ğ‘¥, ğ‘— 1: ğ‘¡ (ğ‘¥ ğ›¾ ğ‘—) 2: return ğœ‚ğ‘¡ [1 ğ‘¡2] ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. 000:10 Ikeda et al. Algorithm 2 Calculate MackeyGlass Input: ğ’™(ğ‘˜ 1) [ğ‘¥(ğ‘˜ 1)1,ğ‘¥(ğ‘˜ 1)2, . . . ,ğ‘¥(ğ‘˜ 1)ğ‘ğ‘¥] Input: ğ’‹(ğ‘˜) [ğ‘—(ğ‘˜)1, ğ‘—(ğ‘˜)2, . . .",
    "source": "2504.11981v1_Hardware-Friendly_Delayed-Feedback_Reservoir_for_M.pdf",
    "length": 1449,
    "tokens": 487
  },
  {
    "text": "Nonvolatile processor architectures: Efficient, reliable progress with unstable power. IEEE Micro, 36(3):72 83, 2016. Kaisheng Ma, Xueqing Li, Jinyang Li, Yongpan Liu, Yuan Xie, Jack Sampson, Mahmut Taylan Kandemir, and Vijaykrishnan Narayanan. Incidental computing on iot nonvolatile processors. In Proceedings of the 50th Annual IEEE ACM International Symposium on Microarchitecture, pp. 204 218, 2017. Kiwan Maeng and Brandon Lucia. Adaptive dynamic checkpointing for safe efficient intermittent computing. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), pp. 129 144, 2018. Hashan Roshantha Mendis, Chih-Kai Kang, and Pi-cheng Hsiu. Intermittent-aware neural architecture search. ACM Transactions on Embedded Computing Systems (TECS), 20(5s):1 27, 2021. Cyan Subhra Mishra, Jack Sampson, Mahmut Taylan Kandemir, and Vijaykrishnan Narayanan. Origin: Enabling on-device intelligence for human activity recognition using energy harvesting wireless sensor networks. In 2021 Design, Automation Test in Europe Conference Exhibition (DATE), pp. 1414 1419. IEEE, 2021. Cyan Subhra Mishra, Jack Sampson, Mahmut Taylan Kandemir, Vijaykrishnan Narayanan, and Chita R Das. Usas: A sustainable continuous-learning framework for edge servers. In 2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA), pp. 891 907. IEEE, 2024. Alessandro Montanari, Manuja Sharma, Dainius Jenkus, Mohammed Alloulah, Lorena Qendro, and Fahim Kawsar. eperceptive: energy reactive embedded intelligence for batteryless sensors. In Proceedings of the 18th Conference on Embedded Networked Sensor Systems, pp. 382 394, 2020. Keni Qiu, Nicholas Jao, Mengying Zhao, Cyan Subhra Mishra, Gulsum Gudukbay, Sethu Jose, Jack Sampson, Mahmut Taylan Kandemir, and Vijaykrishnan Narayanan.",
    "source": "NexUME.pdf",
    "length": 1809,
    "tokens": 481
  },
  {
    "text": "One main task of analog design automation is circuit link prediction, which involves inferring missing component interconnections from incomplete netlists. There are two conventional approaches for general link prediction: heuris- tic methods and learning-based approaches. Heuristic approaches can be divided into two categories: local similarity metrics, which include the Jac- card index, common neighbors, preferential attachment, and resource allo- cation, and global similarity metrics, including the Katz index and Sim- Rank [2]. Although heuristic methods offer simplicity and interpretability through predefined topological features, they have limited generalizability due to their dependence on handcrafted structural assumptions about link formation mechanisms [3]. Therefore, they are not commonly used in circuit link prediction. In the Electronic Design Automation (EDA) field, where circuit netlists represent graph-structured data, machine learning (ML) approaches have gained more prominence. Genssler et al. proposed a novel method using brain-inspired hyperdimensional computing (HDC) for encoding and recog- nizing gate-level netlists [4]. However, the high-dimensional vector opera- tions involved in HDC led to significant computational and storage over- 2 head, especially when dealing with large-scale graph data [5]. Luo et al. proposed a novel neural model called Directional Equivariant Hypergraph Neural Network (DE-HNN) for effective representation learning on directed hypergraphs, particularly for circuit netlists in chip design [6]. However, DE- HNN performing hypergraph diffusion or tensor operations could exponen- tially increase computational costs as the hyperedge order increases, making it challenging to scale the approach for large chip design scenarios [7]. GNNs can learn from circuit netlists inherent graph structure, making them well-suited for circuit link prediction. However, a critical challenge lies in the lack of annotated training data caused by high human workload and data sensitivity [8]. Furthermore, the diversity of netlist formats poses significant challenges in analog design automation. Additionally, customized netlist formats are often created by modifying standard formats. Conse- quently, different datasets are often created in different formats, complicating the training processes for machine learning methods.",
    "source": "2504.10240v2_GNN-ACLP_Graph_Neural_Networks_based_Analog_Circui.pdf",
    "length": 2382,
    "tokens": 457
  },
  {
    "text": "Flowtune: End-to-end automatic logic optimization exploration via domain-specific multiarmed bandit. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), 2022. [61] Rongjian Liang, Jinwook Jung, Hua Xiang, Lakshmi Reddy, Alexey Lvov, Jiang Hu, and Gi-Joon Nam. Flowtuner: A multi-stage eda flow tuner exploiting parameter knowledge transfer. In International Conference on Computer-Aided Design (ICCAD), 2021. [62] Chen Bai, Qi Sun, Jianwang Zhai, Yuzhe Ma, Bei Yu, and Martin DF Wong. Boom-explorer: Risc-v boom microar- chitecture design space exploration framework. In International Conference on Computer-Aided Design (ICCAD), 2021. [63] Hung-Yi Liu and Luca P Carloni. On learning-based methods for design-space exploration with high-level synthesis. In Design Automation Conference (DAC), 2013. [64] Benjamin Carrion Schafer and Zi Wang. High-level synthesis design space exploration: Past, present, and future. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), 2019. [65] Yi-Chen Lu, Siddhartha Nath, Vishal Khandelwal, and Sung Kyu Lim. RL-sizer: VLSI gate sizing for timing optimization using deep reinforcement learning. In Design Automation Conference (DAC), 2021. [66] Yi-Chen Lu, Wei-Ting Chan, Deyuan Guo, Sudipto Kundu, Vishal Khandelwal, and Sung Kyu Lim. Rl-ccd: Concurrent clock and data optimization using attention-based self-supervised reinforcement learning. In Design Automation Conference (DAC), 2023. [67] Ruizhe Zhong, Xingbo Du, Shixiong Kai, Zhentao Tang, Siyuan Xu, Hui-Ling Zhen, Jianye Hao, Qiang Xu, Mingxuan Yuan, and Junchi Yan. Llm4eda: Emerging progress in large language models for electronic design automation. arXiv preprint arXiv:2401.12224, 2023.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 1756,
    "tokens": 489
  },
  {
    "text": "It delivers up to a 2.8 increase in first- attempt functional correctness compared to baseline models while outperforming existing state-of-the-art methods across multiple benchmarks. This improvement is particularly notable in smaller parameter models. Remarkably, the framework achieves 83.1 on VerilogEval-Machine, even surpassing much larger models including GPT-4 Turbo. 2 Background Recent years have seen a surge of interest in applying large language models (LLMs) to hardware design, particularly for generating Register-Transfer Level (RTL) code in Verilog. [7, 9, 2, 1, 20, 17, 25, 11, 10, 18, 13, 4]. Previous reseach have shown significant potential for LLM-based RTL generation in automating parts of the hardware design process using finetuning, or using differnt prompting techniques. However, it has also revealed several key challenges of produce correct and efficient hardware designs reliably. Challenges in LLM-based RTL generation tasks Researchers investigated fine-tuning LLMs using domain-specific data and techniques to improve their performance [9, 14]. For example, Liu et.al introduced ChipNeMo [7], which fine-tunes a general-purpose LLM on internal NVIDIA datasets for various chip design tasks. Similarly, Thankur et.al developed VeriGen [17] to improve Verilog generation capabilities. Subsequent works, such as RTLCoder [9] is trained based on automatically generated datasets. BetterV [14], finetunes LLM by converting Verilog code to the C language. While effective, these methods face challenges of scalability and generalizability due to their high demand for high quality instruction-code pairs. The inherent limitation of lack of real RTL code and the low quality of generated code make it hard to make further improvement. Moreover, LLM generated Verilog code often face the issue of hallucination. Prompt-based methods [1, 2] rely heavily on the quality and clarity of the input prompts, facing difficulties in consistently aligning complex, multi-step circuit specifications with the generated code. These methods often suffer from hallucinations or syntactically correct yet functionally incorrect outputs due to inadequate contextual understanding. Moreover, they inherently lack iterative refinement capabilities, making them incapable of progressively improving RTL code quality.",
    "source": "2505.11849v1_VeriReason_Reinforcement_Learning_with_Testbench_F.pdf",
    "length": 2326,
    "tokens": 478
  },
  {
    "text": "However, existing bench- marks for LLM-generated hardware designs typically evaluate the generated code based only on syntax and functional correctness, overlooking resource-related issues. To address this limitation, we introduce ResBench, the first FPGA-resource-focused benchmark specifically designed to eval- uate LLM-generated designs based on resource usage. Unlike pre- vious benchmarks that focus primarily on syntax and functional correctness, ResBench highlights how well LLMs generate Verilog code optimized for FPGA resource utilization. Our key contribu- tions are: A resource-focused benchmark featuring 56 problems across 12 categories, covering real-world FPGA workloads such as combinational logic, state machines, AI accelerators, and financial computing applications. (Section 3) An open-source automated evaluation framework that per- forms LLM querying, functional correctness testing, FPGA synthesis, and resource measurement1. The framework auto- matically generates Verilog code using LLMs and evaluates its correctness and resource usage. (Section 4) A detailed study of nine LLMs, comparing their performance in functional correctness and FPGA resource usage. The results reveal substantial differences in how various models generate resource-conscious designs. (Section 5) 1Code repository: arXiv:2503.08823v2 [cs.AR] 21 Mar 2025 HEART 25, May 26 28, 2025, Kumamoto, Japan C. Guo and T. Zhao By integrating FPGA resource awareness into benchmarking, ResBench provides a practical evaluation of LLM-generated HDL. This benchmark establishes a foundation for advancing AI-driven FPGA design, encouraging the development of more resource- efficient models optimized for FPGAs. 2 Background This section explores the evolution of large language models (LLMs) from general code generation to hardware design generation using hardware description languages (HDLs). We examine their capa- bilities and limitations, as well as existing benchmarks for LLM- generated hardware design. 2.1 Code-specialized and HDL-specialized LLMs Language models have seen significant advancements, particularly with the introduction of Transformers [36]. Large-scale pre-trained models such as BERT [8], the GPT series [5, 28, 44], and PaLM 2 [7] have expanded their capabilities across various tasks, including code generation.",
    "source": "2503.08823v2_ResBench_Benchmarking_LLM-Generated_FPGA_Designs_w.pdf",
    "length": 2331,
    "tokens": 482
  },
  {
    "text": "It can be seen that functions which are common to a larger number of paths are invoked at a higher rate by such a request arrival pattern. Therefore, common functions have a higher chance of experiencing increased load due to be- ing present in multiple paths. Consequently, higher weights have to be assigned to such functions to ensure resilience in the presence of varying application usage patterns. Opportunity 2: Although proactive provisioning combined with probability-based scaling is useful, it is essential to iden- tify critical and common functions in each DDA and assign them higher weights in comparison to standard functions. Hence, rather than simply measuring the weights only in terms of function invocation frequency, we also need to account for DAG specific factors like Commonality and Con- nectivity. The above discourse motivates us to rethink the design of serverless RM frameworks to cater to DDAs as well. One key driver for the design lies in a Probability Estimation Model for individual functions, which is explained below. 3 Function Probability Estimation Model As elucidated in Opportunity-1, to specifically address the container over-provisioning problem for DDAs, we need to estimate the weights to be assigned to their composite func- tions, a key component of which is the function invocation probability. In this section, we model the function probability estimation problem using a Variable Order Markov Model (VOMM) [21]. VOMMs are effective in capturing the invo- cation patterns of functions within each application while simultaneously isolating the effects of other applications that share them. This aids us in the calculation of function invocation probabilities. Wherever appropriate, we draw in- spiration from related works that model user web surfing 156 Kraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC 21, November 1 4, 2021, Seattle, WA, USA 0 0.25 0.5 0.75 1 Hit Rate (a) Social Network. 0 0.25 0.5 0.75 1 Hit Rate (b) Media Service. 0 0.25 0.5 0.75 1 Hit Rate (c) Hotel Reservation. Figure 4: Function Hit Rate for an Evenly Distributed Load across all Paths in each Application.",
    "source": "kraken.pdf",
    "length": 2183,
    "tokens": 458
  },
  {
    "text": "Probability: As alluded to in Section 2, one of the factors used in function weight estimation is its invocation probabil- ity. The procedure in Section 3 describes how the transition probabilities of the states associated with functions are com- puted through repeated matrix multiplications of the Transi- tion Matrix,ğ‘‡with the Probability Vector, ğ‘ƒ.ğ¶ğ‘œğ‘šğ‘ğ‘¢ğ‘¡ğ‘’_ğ‘ƒğ‘Ÿğ‘œğ‘, 158 Kraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms SoCC 21, November 1 4, 2021, Seattle, WA, USA in Algorithm 1, first estimates the invocation probabilities of a function s immediate predecessors and uses it along with system log information and load measurements of the function to calculate its invocation probability. Connectivity: In addition to function invocation probabil- ities, it is necessary to also account for the effects of cold starts on DDAs while estimating function weights. Cold start spillovers (that often occur due to container underprovision- ing), as described in Section 2, can impact the response la- tency of applications harshly. Provisioning critical functions with more containers helps throttle this at the source. To this end, Kraken makes use of a parameter called Connec- tivity, while assigning function weights. The Connectivity of a function is defined as the ratio of number of its descen- dant functions to the total number of functions. The ğ¶ğ‘œğ‘›ğ‘› procedure in Algorithm 1 makes use of this formula. For ex- ample, in Figure 1c, the Connectivity of ğ¶â„ğ‘’ğ‘ğ‘˜_ğ‘…ğ‘’ğ‘ ğ‘’ğ‘Ÿğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘› is 2 5 since it has two descendants and there is a total of five functions. Bringing Connectivity into the weight estimation process helps Kraken assign a higher weight to critical func- tions, in turn, ensuring that more containers are assigned to them, resulting in improved response times for the functions themselves, as well as their descendants.",
    "source": "kraken.pdf",
    "length": 1868,
    "tokens": 471
  },
  {
    "text": "Each tagged entry includes a prediction counter, a partial tag, and a usefulness counter. The final prediction is selected from the tagged compo- nent with the longest matching history. TAGE-SC-L enhances this core design with two additional components: (i) a statistical correc- tor (SC) that aggregates predictions from biased, global, and local history tables to override low-confidence TAGE outcomes, and (ii) a loop predictor (L) optimized for detecting constant-iteration loops [6]. TAGE-SC-L was the winning submission in CBP-2016 and serves as the baseline for the predictor proposed in this work. 2.2 The Perceptron-Based Predictor Perceptron-based predictors take a fundamentally different ap- proach by framing branch prediction as a form of single-layer neu- ral inference. Each prediction is computed as a weighted sum of recent branch outcomes combined with a bias term; the sign of the resulting sum determines whether the branch is predicted as taken or not taken [1]. Weights are updated at retirement based on the correctness of the prediction, allowing the perceptron to learn long-term correlations that traditional counter-based predic- tors typically miss. While perceptrons are well-suited for linearly separable patterns, they introduce higher latency and storage costs, especially as history lengths grow and weight vectors expand. 2.3 Why use H2P-Tailored Predictors? While continued process scaling has enabled significantly larger branch predictor budgets, idealized studies reveal diminishing re- turns in accuracy per kilobyte as predictor size increases [5]. Even with sophisticated designs like TAGE-SC-L, detailed execution traces show that a small number of hard-to-predict (H2P) branches account for a disproportionate share of mispredictions, limiting overall IPC gains [2]. These insights motivate hybrid predictor architectures, such as the one proposed in this work, that retain TAGE-SC-L s low-latency, high-throughput backbone while inte- grating lightweight perceptron models explicitly targeted at these high-impact, difficult-to-model branches. 3 Bullseye: High-Level Design Overview Figure 1: A high-level overview of the architecture of the Bullseye prediction system. Figure 1 shows the control flow through Bullseye s H2P pipeline.",
    "source": "2506.06773v1_Taming_Wild_Branches_Overcoming_Hard-to-Predict_Br.pdf",
    "length": 2279,
    "tokens": 489
  },
  {
    "text": "This paves the way for new opportunities in multi-modal sensing enabling smaller, more efficient deep learning models that can be implemented in devices such as wearables while preserving battery life. VI. CONCLUSION This work demonstrates the feasibility of a hybrid digital- analogue architecture for edge image classification through the integration of optimised neural networks with RRAM- based ACAM hardware accelerators. The proposed digital methodology utilising knowledge distillation achieves substan- tial efficiency improvements while maintaining accuracy, with our resultant student model achieving 82.22 accuracy on CIFAR-10 using only 1.45 of the teacher model s parameters and reducing MAC operations by a factor of 800. For analogue classification, template pattern matching maintains 70.91 accuracy using a simple feature count approach, suggesting more complex pattern matching approaches may not yield proportional benefits in classification accuracy. This approach demonstrates significant advantages in terms of energy efficiency, with the ACAM back-end consum- ing only 1.45nJ per classification operation compared with 78.06ÂµJ for the teacher model, while maintaining acceptable accuracy levels for edge applications. These results, combined with our investigation of multiple template strategies, establish the potential of digital-analogue approaches for resource- constrained edge devices. Future research could further ex- plore template generation techniques and novel ACAM cell designs, further optimising classification accuracy and energy efficiency in edge computing applications. REFERENCES [1] W. Rawat and Z. Wang, Deep convolutional neural networks for image classification: A comprehensive review, Neural Computation, vol. 29, no. 9, pp. 2352 2449, sep 2017. [2] D. W. Otter, J. R. Medina, and J. K. Kalita, A Survey of the Usages of Deep Learning for Natural Language Processing, IEEE Transactions on Neural Networks and Learning Systems, vol. 32, no. 2, pp. 604 624, feb 2021. [3] A. Mehrish, N. Majumder, R. Bharadwaj, R. Mihalcea, and S. Poria, A review of deep learning techniques for speech processing, Information Fusion, vol. 99, p. 101869, nov 2023. [4] D. Nagaraju and N. Chandrachoodan, Compressing fully connected layers of deep neural networks using permuted features, IET Computers and Digital Techniques, vol.",
    "source": "2502.10089v1_A_Hybrid_Edge_Classifier_Combining_TinyML-Optimise.pdf",
    "length": 2361,
    "tokens": 504
  },
  {
    "text": "In IEEE ACM International Symposium on Microarchitecture (MICRO), 2023. [22] C. Hong, S. Bhatia, A. Haan, S. K. Dong, D. Nikiforov, A. Cheung, and Y. S. Shao. Llm-aided compilation for tensor accelerators. In 2024 IEEE LLM Aided Design Workshop (LAD), pages 1 14, 2024. doi: 10.1109 LAD62341.2024.10691748. [23] J. Hruska. As chip design costs skyrocket, 3nm process node is in jeopardy, 2018. URL [24] Q. Huang, M. Kang, G. Dinh, T. Norell, A. Kalaiah, J. Demmel, J. Wawrzynek, and Y. S. Shao. CoSA: Scheduling by Constrained Optimization for Spatial Accelerators. In Proceedings of the International Symposium on Computer Architecture (ISCA), 2021. [25] Q. Huang, C. Hong, J. Wawrzynek, M. Subedar, and Y. S. Shao. Learning a continuous and reconstructible latent space for hardware accelerator design. In Proceedings of the International Symposium on Performance Analysis of Systems and Software (ISPASS), 2022. 11 [26] Y. Ikarashi, G. L. Bernstein, A. Reinking, H. Genc, and J. Ragan-Kelley. Exocompilation for productive programming of hardware accelerators. In Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation, PLDI 2022, page 703 718, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450392655. doi: 10.1145 3519939.3523446. URL 3519939.3523446. [27] Intel. Intel advanced matrix extensions overview. URL www us en products docs accelerator-engines advanced-matrix-extensions overview.html. [28] D. Jiang, X. Ren, and B. Y. Lin. LLM-blender: Ensembling large language models with pairwise ranking and generative fusion.",
    "source": "2505.18574v2_Autocomp_LLM-Driven_Code_Optimization_for_Tensor_A.pdf",
    "length": 1610,
    "tokens": 483
  },
  {
    "text": "This data-centric approach is amenable to analytic frameworks such as MAESTRO [2] and other DNN dataflow studies [3]. High-Level Synthesis (HLS) and Easy Integration: Im- plementing the accelerator in C HLS facilitates rapid de- sign space exploration and more accessible customization. The accelerator is integrated via AXI4 and PYNQ overlays, offering a user-friendly interface that can be extended to other matrix-multiply-based workloads with minimal effort. Quantized DistilBERT Integration: We replace standard PyTorch Q, K, and V linear layers with our FPGAQuantizedLinear module and validate end-to-end functionality on a quantized DistilBERT. This demonstrates near-lossless accuracy while providing up to a 7 speedup over PyTorch CPU baselines for these layers. Our open-source release includes scripts for quantization, FPGA invocation, and reproducible bench- marking, potentially lowering the barrier for researchers to adopt FPGA acceleration in transformer applications. Roadmap for Future Scaling: Although we focus on mod- erate matrix sizes (e.g., 64 768, 768 3072) representative of DistilBERT, our tiled architecture offers a blueprint for scal- ing to larger transformer variants. The clear separation of on-chip buffering, tiling logic, and computation modules can be extended to handle more extensive hidden dimensions and different attention heads with minimal redesign. By addressing a critical compute bottleneck within Transformers and providing an open-source HLS-based implementation, we aim to advance both the practical adoption of FPGAs in edge-deployed large language models and future studies on optimized dataflows for deep learning inference. 3 RELATED WORK FPGA Accelerators for Transformers Recent research has produced FPGA-based LLM accelerators such as FlightLLM [6] and SSR [7]. FlightLLM (FPGA 24) maps entire LLM inference flows onto FPGAs, leveraging sparsity and mixed- precision; implemented on a high-end Alveo U280, it achieves 6 higher energy efficiency than an NVIDIA V100 GPU [6] by using sparse DSP chains and always-on-chip decoding. SSR (FPGA 24) explores launching multiple accelerators in parallel versus sequen- tially to balance latency and throughput.",
    "source": "2503.16731v3_Design_and_Implementation_of_an_FPGA-Based_Hardwar.pdf",
    "length": 2211,
    "tokens": 491
  },
  {
    "text": "Table 1: Comparison between InstantFT and baselines (LeNet-5) MNIST SVHN Method Trainable params FLOPs (forward) FLOPs (backward) Memory usage (KB) Trainable params FLOPs (forward) FLOPs (backward) Memory usage (KB) FT-All 61706 0.851M 1.444M 567.8KB 62006 1.321M 1.914M 579.4KB FT-Last 850 0.851M 0.002M 285.8KB 850 1.321M 0.002M 296.1KB FT-Bias 236 0.851M 0.611M 322.0KB 236 1.321M 0.611M 332.3KB LoRA-All 36328 0.923M 0.743M 611.8KB 45480 1.412M 0.762M 695.4KB LoRA-Last 376 0.852M 0.001M 285.4KB 376 1.322M 0.001M 295.8KB InstantFT 10456 0.872M 0.036M 366.2KB 19608 1.360M 0.054M 449.8KB As in Fig. 2 (bottom), we consider two LoRA-based approaches: inserting LoRA adapters into all layers (LoRA-All) or applying LoRA to only the last layer (LoRA-Last). For the i-th layer, LoRA introduces a new matrix Wi 1,i Rd d. It is further decomposed into two trainable matrices A Rr d, B Rd r of rank r d, which are initialized with random Gaussian values and zeros. The modified layer now produces: xi Wi Wi 1,i xi 1 bi Wi Bi 1,iAi 1,i xi 1 bi. The trainable parameters in LoRA-All and LoRA-Last are denoted as Ai 1,i, Bi 1,i i 1,...,L and AL 1,L, BL 1,L , respectively. LoRA-All performs backpropagation across the entire network, and therefore needs to store additional LoRA parameters, all forward activations x1, . . . , xL, and their gradients dx1, . . .",
    "source": "2506.06505v1_InstantFT_An_FPGA-Based_Runtime_Subsecond_Fine-tun.pdf",
    "length": 1355,
    "tokens": 490
  },
  {
    "text": "The Npe 32 COBRA implementa- tion on mid-end ZCU102 delivers the best performance in both energy efficiency and throughput among all evaluated imple- mentations. Specifically, it achieves 3.5 throughput and 2.2 energy efficiency improvements, compared to BAT [19], the state-of-the-art binary transformer accelerator. Furthermore, our design on the low-power ZCU102 platform achieves 2.5Ë† higher throughput than COSA [23] and TransFRU [24] designs on high-performance datacenter-grade FPGAs, while our de- sign consumes significantly lower power and has reduced LUT or DSP resource utilization compared to theirs. The resource breakdown of COBRA Npe 32 is shown in Table IV. Our RBMM Engine design uses 42,926 LUTs for efficient quantization-fused RBMM. The LayerNorm unit occupies 78 DSPs for computing the standard deviation and normalization. Notably, the data packing conversion units consume significant resources due to the LUT-based intermediate buffers in the transpose operation for V output matrix to datapacks (details in the M3. MHA Context Output section). Fig. 7 shows the implementation layout of the COBRA accelerator on the ZCU102 FPGA. Major components are highlighted and labeled in the figure. C. Ablation Study of COBRA Hardware Design We performed an ablation study on our proposed optimiza- tion techniques to better understand their impact on the overall accelerator performance. Table V shows the results. 1) SPS instead of Softmax: As discussed in Section III-A2, the Softmax hardware module is inefficient for hardware. In contrast, our proposed SPS-fused RBMM achieves a through- put improvement of 564Ë† compared to the original Softmax unit, which requires additional 36K LUTs and 26 DSPs for the computation of attention scores across h heads after RBMM. 2) 6:3 Compressor-Based Popcount instead of CPU- Optimized Popcount: The design of the Popcount unit is a critical consideration for the RBMM Engine, as it is replicated h times in each RBMM PE. The CPU-optimized popcount, as introduced in [25], leads to additional 12K LUTs and 1536 DSPs due to the inclusion of multipliers.",
    "source": "2504.16269v2_COBRA_Algorithm-Architecture_Co-optimized_Binary_T.pdf",
    "length": 2110,
    "tokens": 489
  },
  {
    "text": "In this 3 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning work, for CPU execution (source platform), we use TACO (Kjolstad et al., 2017), a domain-specific language and a compiler designed for sparse tensor algebra. Considering our target accelerators, SPADE has its own tile-based open instruction set architecture (ISA) to leverage different vari- ations of SpMM and SDDMM operations. For GPU, we employ SparseTIR (Ye et al., 2023), a sparse tensor com- pilation framework developed as an enhancement to TVM Tensor IR (Chen et al., 2018a). 2.3. ML-based cost models Learned Cost Models. Cost models act as fast cost- effective proxies for executing workloads on real hardware. Their primary goal is to accurately estimate the execution time of workloads as they would perform on real hardware. To achieve this, these cost models can be trained on data sam- ples with various program configurations and then be used to predict the program configuration that will deliver the op- timal performance. Hence, generally, the training objective of cost models is tied with minimizing t CM t , where t is the runtime of the true optimal program configuration and t CM is the runtime of the best program configuration suggested by the cost model (accuracy objective)(detailed Appendix A). Finding the best configuration suggested by the cost model is usually done using auxiliary intelligent search techniques such as simulated annealing, Monte Carlo tree search, and reinforcement learning. There have been numerous works on learned cost models to predict the run- time of workloads targeting different hardware platforms (Chen et al., 2018b; Adams et al., 2019). These techniques range from simple XGBoost (Chen Guestrin, 2016) based cost models (Chen et al., 2018b;a) to sophisticated deep neu- ral network based models (Baghdadi et al., 2021; Kaufman et al., 2021; Zhai et al., 2023; Zheng et al., 2020). WACO s Cost Model. WACO (Figure 3(a)) (Won et al., 2023) introduced a learned cost model specifically built for sparse tensor programs, which utilizes sparsity patterns as raw input data.",
    "source": "2506.00424v2_COGNATE_Acceleration_of_Sparse_Tensor_Programs_on_.pdf",
    "length": 2134,
    "tokens": 476
  },
  {
    "text": "C. Optimized Softmax Kernel To speed up the execution of the Softmax function on the enhanced Snitch cluster, we develop optimized software routines that exploit the underlying ISA of the Snitch cores, TABLE I SNITCH RISC-V ENCODINGS FOR FEXP AND VFEXP Format Encoding (32-bit) FEXP rd, rs1 001111100000{rs1}000{rd}1010011 VFEXP rd, rs1 101111100000{rs1}000{rd}1010011 NORM Loop for N: flh ft1, 0(a2) fdiv.h ft2, ft1, sum fsh ft2, 0(a2) addi a2, a2, 2 addi a3, a3, -1 bnez a3, loop for(i 0;i N;i ){ y[i] sum; } for(i 0;i N;i ){ y[i] exp(x[i]-max) sum y[i]; } for(i 0;i N;i ){ if(x[i] max_val) max x[i];} MAX Loop for N: flh ft1,0(a2) fmax.h max,ft1,max addi a2,a2,2 addi a3,a3,-1 bnez a3, loop EXP Loop for N 8: ssr ft1 read double ssr ft2 write double frep N 8,8 vfsub.h ft3,ft1,max vfsub.h ft4,ft1,max vfexp.h ft3,ft3 vfexp.h ft4,ft4 vfsgnj.h ft2, ft3 vfsgnj.h ft2, ft4 vfadd.h ft3, ft3 vfadd.h ft4, ft4 EXP Loop for N: Initialization flh ft0, 0(a0) fsub.h ft1, ft0, ft5 ... Exp approximation srli a2, ft1, 20 andi a2, a2, 2047 bgeu a2,1067,overflow fmul.d ft2, const1, ft1 fadd.d ft2, ft2, const2 fmul.d ft2, ft2, const3 fcvt.h.d ft1, ft2 Update y[i], sum Solve overflow ... MAX Loop for N 16: ssr ft0 read double frep N 16,4 vfmax.h ft3,ft3,ft0 vfmax.h ft4,ft4,ft0 vfmax.h ft5,ft5,ft0 vfmax.h ft6,ft6,ft0 NORM Loop for N 16: fdiv.h (1 sum),1,sum ssr ft0 read double ssr ft1 write double frep N 16,4 vfmul.h ft1,(1 sum),ft0 vfmul.h ft1,(1 sum),ft0 vfmul.h ft1,(1 sum),ft0 vfmul.h ft1,(1 sum),ft0 Baseline C code Baseline Assembly Optim Assembly Fig. 4. Code comparison of Baseline and Optimized Softmax implemen- tations.",
    "source": "2504.11227v1_VEXP_A_Low-Cost_RISC-V_ISA_Extension_for_Accelerat.pdf",
    "length": 1624,
    "tokens": 723
  },
  {
    "text": "To accelerate the execution of sizing algorithms, techniques have been proposed that allow parallelization. When the simulation time for a given set of design variables is constant, parallel execution is leveraged for algorithms compatible with multi-core simulators, whereas those relying on sequential decision-making remain serial. A batch-constrained Bayesian optimization (BO) methodology is proposed in [20] that utilizes a multi-objective acquisition function ensemble as a substitute of the sequential execution of traditional BO. An asynchronously parallel batch optimization method for analog sizing is proposed in [52] that utilizes a ranking approxima- tion method to select between cheap and expensive simulations of circuit parameters. Another emerging research direction is layout-driven device siz- ing [52], which includes an iterative design loop between schematic- level sizing and layout generation, minimizing the need for addi- tional post-layout adjustments. 3.5 Placement Optimization in Analog Design Analog IC placement involves determining the optimal locations for devices within a given circuit topology to optimize specific perfor- mance metrics. The placement algorithm must adhere to topological constraints, including device matching, symmetry between devices or device groups, and proximity requirements. Depending on the target application, constraints that include alignment, thermal gra- dients, or current signal flows must also be met [3]. Traditional analog circuit placement methodologies have been mostly manual, leading to an increase in circuit development time and cost. ML-based approaches proposed in [53 56] utilize ANNs and deep generative models to provide an optimum placement of a circuit while considering topological constraints. In [53, 54], a non- linear ANN model is proposed that is trained with semi-supervised learning. The ANN outputs the placement coordinates of each cell (sub-block) for a given sizing and set of topological constraints. DeepPlacer, a deep learning generative model, is proposed that performs the placement of multiple amplifier topologies in less than 150 ms across different technology nodes [55], proving the scalability of ML-based placement methodologies. Unsupervised models including AGraph2Seq introduce structure through an attention-based graph-to-sequence encoder-decoder ar- chitecture that enables efficient placement of analog circuit blocks with a minimal set of trainable parameters [56].",
    "source": "2506.00007v1_Emerging_ML-AI_Techniques_for_Analog_and_RF_EDA.pdf",
    "length": 2485,
    "tokens": 469
  },
  {
    "text": "The weight feedback is dependent on the generated clause output for this particular clause and the class- level feedback yc. The weight update is controlled through the clause update probabilities C1 or C2. e) TA level feedback: Type I and Type II feedbacks both iterate through every literal in the selected clause to determine whether to transition it s associated TA. For Type I feedback, there are TA update probabilities 1 s and s 1 s for both TA state value increment and decrement, respectively. These update probability expressions introduce the Sensitivity hyperparameter s. TM algorithms also have a mode that is called boost true positive . When using this mode, the increase in Type I feedback will always occur regardless of the condition S2. Type II feedback is deterministic. The feedback process involves two hyperparameters s and T. The effect of these parameters on TA updates and clause updates, respectively, is explored in [19] for the Vanilla TM. The following section will illustrate how these probabilities are translated to FPGA. III. RELATED ACCELERATOR DESIGNS This section explores training accelerator designs targeting FPGAs for DNNs to address how intrinsic attributes of the TM algorithm can offer reduced complexity2. a) Quantization in Training: One of the algorithmic drawbacks of all DNN-based algorithms is the use of 32-bit floating point weights and computation. Most works address this through quantization of the these floats [4], [20] [26]. Extreme quantization to binary weights and data, referred to as Binary Neural Networks (BNNs), leads to a reduction in the computation of floating point multiplication and addition to XNOR and popcount. This is incredibly effective for high-throughput inference accelerators [4], [20], [21]. Trans- lating the training of these quantized DNNs to accelerators is challenging because the precision of the weights cannot be reduced as significantly. For example, for training, the authors in [22] retain 20-bit weights for their accelerator. This results in substantial Block RAM (BRAM) usage for weight storage and many DSP blocks for their computation. Batched training to improve throughput is seen in [27] (Low Batch CNN). b) Design Approaches for Backpropagation: DNN train- ing requires backpropagation for weight updates during train- ing. This involves computationally expensive partial derivative calculations.",
    "source": "2504.19797v1_Dynamic_Tsetlin_Machine_Accelerators_for_On-Chip_T.pdf",
    "length": 2399,
    "tokens": 490
  },
  {
    "text": "Net- Spectre marks a significant shift from local to remote attacks, making Spectre a threat even to systems where no attacker- controlled code is executed, including cloud environments. SgxPectre attack [7] exploits CPU vulnerabilities to com- promise the confidentiality and integrity of SGX enclaves. By manipulating branch prediction from outside the enclave, attackers can temporarily alter the enclave s control flow, pro- ducing cache-state changes that reveal sensitive information within the enclave. Meltdown [28] bypasses memory isolation by exploiting out-of-order execution in modern processors to access pro- tected kernel memory. This enables attackers to read memory from other processes or virtual machines without permission, posing a severe risk to millions of users. Foreshadow [52] is a microarchitectural attack exploiting speculative execution flaws in Intel processors to breach SGX security. Without needing kernel access or assumptions about enclave code, Foreshadow leaks enclave secrets from the CPU cache. Rogue In-flight Data Load (RIDL) [55] is a speculative execution attack that leaks data across address spaces and privilege boundaries. RIDL retrieves in-flight data directly from CPU components without relying on cache or trans- lation structures, making it uniquely invasive and effective. Fallout [4] reveals that Meltdown-like attacks remain feasible on newer CPUs that are supposedly immune to Meltdown due to hardware fixes. By examining the behavior of the store 3 buffer, they uncover vulnerabilities. ZombieLoad [45] is a Meltdown-type attack that exploits a vulnerability in the processor s fill-buffer logic to leak data across logical CPU cores, even on CPUs with hardware miti- gations against Meltdown and MDS. ZombieLoad uses fault- ing load instructions to transiently access unauthorized data in the fill buffer. Load Value Injection (LVI) [54] is a technique that generalizes injection-based attacks to the memory hier- archy by injecting attacker-controlled values into a victim s transient execution. Downfall [32] is a new class of transient execution attacks that exploit the gather instruction on x86 CPUs to leak sensitive data across security boundaries, in- cluding user-kernel, process, and virtual machine isolation, as well as trusted execution environments.",
    "source": "2502.14307v1_Î¼RL_Discovering_Transient_Execution_Vulnerabilitie.pdf",
    "length": 2322,
    "tokens": 456
  },
  {
    "text": "2024. The Llama 3 Herd of Models. arXiv:2407.21783 [cs.AI] [28] Yufeng Gu, Alireza Khadem, Sumanth Umesh, Ning Liang, Xavier Servot, Onur Mutlu, Ravi Iyer, and Reetuparna Das. 2025. PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference. In Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2. ACM, 862 881. 1145 3676641.3716267 [29] Juan GÃ³mez-Luna, Izzat El Hajj, Ivan Fernandez, Christina Giannoula, Geraldo F. Oliveira, and Onur Mutlu. 2022. Benchmarking a New Paradigm: Experimental Analysis and Characterization of a Real Processing-in-Memory System. IEEE Access 10 (2022), 52565 52608. [30] Jiaao He and Jidong Zhai. 2024. FastDecode: High-Throughput GPU-Efficient LLM Serving using Heterogeneous Pipelines. arXiv:2403.11421 [cs.DC] https: arxiv.org abs 2403.11421 [31] Mingxuan He, Choungki Song, Ilkon Kim, Chunseok Jeong, Seho Kim, Il Park, Mithuna Thottethodi, and T. N. Vijaykumar. 2020. Newton: A DRAM-maker s Accelerator-in-Memory (AiM) Architecture for Machine Learning. In 2020 53rd Annual IEEE ACM International Symposium on Microarchitecture (MICRO). 372 385. [32] Yintao He, Haiyu Mao, Christina Giannoula, Mohammad Sadrosadati, Juan GÃ³mez-Luna, Huawei Li, Xiaowei Li, Ying Wang, and Onur Mutlu. 2025. PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding with a Processing-In-Memory-Enabled Computing System. In Proceedings of the 30th ACM International Conference on Architectural Support for Programming Lan- guages and Operating Systems, Volume 2 (Rotterdam, Netherlands) (ASPLOS 25).",
    "source": "2504.17584v1_L3_DIMM-PIM_Integrated_Architecture_and_Coordinati.pdf",
    "length": 1638,
    "tokens": 495
  },
  {
    "text": "The residual frame Rt for each frame is calculated as: Rt Ft predict(Ft 1, Mt) where predict( ) is a function that reconstructs Ft from Ft 1 using the motion vector field Mt. This prediction involves translating the blocks of Ft 1 according to Mt and serves as the predicted frame. The residual frame Rt, which contains only the differences not captured by the motion prediction, is then encoded using the layered neural network. Each layer of the neural network encodes progressively finer details of Rt. If Lk(Rt) represents the k-th layer s encoding of the residual frame, the overall encoding of the frame can be expressed as: Et K X k 1 Lk(Rt) where K is the total number of layers, and each Lk encodes different levels of detail or different regions of the frame, based on the motion information and the prediction error. Algorithm 1 shows the details of the layered neural codec. We implement this neural codec using the FPGA in the CSDs. FPGAs are ideal for this application due to their parallel processing capabilities and the ability to handle multiple data streams concurrently. 9 Algorithm 2 Training Auto-Encoder with Motion Vectors and Stacked Compression while freezing the inference model.",
    "source": "SaLT.pdf",
    "length": 1206,
    "tokens": 259
  },
  {
    "text": "Towards improving verification productivity with circuit- aware translation of natural language to systemverilog assertions. In International Workshop on Deep Learning-aided Verification (DAV), 2023. [190] Mingjie Liu, Minwoo Kang, Ghaith Bany Hamad, Syed Suhaib, and Haoxing Ren. Domain-adapted llms for vlsi design and verification: A case study on formal verification. In VLSI Test Symposium (VTS), 2024. [191] Hanxian Huang, Zhenghan Lin, Zixuan Wang, Xin Chen, Ke Ding, and Jishen Zhao. Towards llm-powered verilog rtl assistant: Self-verification and self-correction. arXiv preprint arXiv:2406.00115, 2024. [192] Rahul Kande, Hammond Pearce, Benjamin Tan, Brendan Dolan-Gavitt, Shailja Thakur, Ramesh Karri, and Jeyavijayan Rajendran. Llm-assisted generation of hardware assertions. arXiv preprint arXiv:2306.14027, 2023. [193] Baleegh Ahmad, Shailja Thakur, Benjamin Tan, Ramesh Karri, and Hammond Pearce. Fixing hardware security bugs with large language models. arXiv preprint arXiv:2302.01215, 2023. [194] Chao Xiao, Yifei Deng, Zhijie Yang, Renzhi Chen, Hong Wang, Jingyue Zhao, Huadong Dai, Lei Wang, Yuhua Tang, and Weixia Xu. Llm-based processor verification: A case study for neuronnorphic processor. In Design, Automation and Test in Europe Conference and Exhibition (DATE), 2024. [195] Jitendra Bhandari, Johann Knechtel, Ramesh Narayanaswamy, Siddharth Garg, and Ramesh Karri. Llm-aided testbench generation and bug detection for finite-state machines. arXiv preprint arXiv:2406.17132, 2024. [196] Jason Blocklove, Siddharth Garg, Ramesh Karri, and Hammond Pearce. Evaluating llms for hardware design and test.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 1628,
    "tokens": 488
  },
  {
    "text": "[28] M. Renze, The effect of sampling temperature on problem solving in large language models, in Findings of the Association for Computa- tional Linguistics: EMNLP 2024, 2024, pp. 7346 7356. [29] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, The curious case of neural text degeneration, 2020. [Online]. Available: [30] Y.-S. Chuang, Y. Xie, H. Luo, Y. Kim, J. Glass, and P. He, Dola: Decoding by contrasting layers improves factuality in large language models, arXiv preprint arXiv:2309.03883, 2023. [31] Y. Su, T. Lan, Y. Wang, D. Yogatama, L. Kong, and N. Collier, A contrastive framework for neural text generation, 2022. [Online]. Available: [32] X. L. Li, A. Holtzman, D. Fried, P. Liang, J. Eisner, T. Hashimoto, L. Zettlemoyer, and M. Lewis, Contrastive decoding: Open- ended text generation as optimization, 2023. [Online]. Available: [33] S. O Brien and M. Lewis, Contrastive decoding improves reasoning in large language models, arXiv preprint arXiv:2309.09117, 2023. [34] B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J. Zhang, B. Yu, K. Lu et al., Qwen2. 5-coder technical report, arXiv preprint arXiv:2409.12186, 2024. [35] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, R. Sauvestre, T. Remez et al., Code llama: Open foundation models for code, arXiv preprint arXiv:2308.12950, 2023.",
    "source": "2507.02226v1_DecoRTL_A_Run-time_Decoding_Framework_for_RTL_Code.pdf",
    "length": 1357,
    "tokens": 472
  },
  {
    "text": "[112] Intel Corporation. Intel Optane Persistent Memory 200 Series Brief. optane-persistent-memory optane-persistent-memory-200-series-brief.html, 2020. [113] Chunghan Lee, Tatsuo Kumano, Tatsuma Matsuki, Hiroshi Endo, Naoto Fuku- moto, and Mariko Sugawara. Understanding Storage Traffic Characteristics on Enterprise Virtual Desktop Infrastructure. In SYSTOR, 2017. [114] Gala Yadgar, Moshe Gabel, Shehbaz Jaffer, and Bianca Schroeder. SSD-Based Workload Characteristics and their Performance Implications. TOS, 2021. [115] Brian F Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell Sears. Benchmarking Cloud Serving Systems With YCSB. In SOCC, 2010. [116] MLCommons. MLPerf Storage Benchmarks. benchmarks storage , 2024. [117] Hasan Al Maruf, Hao Wang, Abhishek Dhanotia, Johannes Weiner, Niket Agar- wal, Pallab Bhattacharya, Chris Petersen, Mosharaf Chowdhury, Shobhit Kanau- jia, and Prakash Chauhan. TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory. In ASPLOS, 2023. [118] Donghyun Gouk, Miryeong Kwon, Hanyeoreum Bae, Sangwon Lee, and My- oungsoo Jung. Memory Pooling With CXL. IEEE Micro, 2023. [119] Huaicheng Li, Daniel S Berger, Lisa Hsu, Daniel Ernst, Pantea Zardoshti, Stanko Novakovic, Monish Shah, Samir Rajadnya, Scott Lee, Ishwar Agarwal, et al. Pond: CXL-Based Memory Pooling Systems for Cloud Platforms. In ASPLOS, 2023. [120] Kaiyang Liu, Jun Peng, Jingrong Wang, Boyang Yu, Zhuofan Liao, Zhiwu Huang, and Jianping Pan. A Learning-Based Data Placement Framework for Low Latency in Data Center Networks. In TCC, 2019. [121] Sangjin Yoo and Dongkun Shin.",
    "source": "2503.20507v2_Harmonia_A_Multi-Agent_Reinforcement_Learning_Appr.pdf",
    "length": 1603,
    "tokens": 495
  },
  {
    "text": "Accessed: 2025-04-12. Radcolor. n.d. Radcolor ARM-linux-gnueabi: Bleeding edge GNU gcc toolchains (cc only) built from sources with latest binutils and glibc (for arm). https: github.com radcolor arm-linux-gnueabi. GitHub. 10 Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery data mining, pages 3505 3506. Red Hat. 2022. Arm vs x86: What s the difference? Accessed: 2025-05-19. Reuters. 2025. Arm expects its share of data center cpu market to surge as sales rocket 50 this year. xpects-its-share-data-center-cpu-marke t-sales-rocket-50-this-year-2025-03-31 . Accessed: 2025-04-12. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, and 1 others. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950. Phillip Rust, Jonas Pfeiffer, Ivan Vuli c, Sebastian Ruder, and Iryna Gurevych. 2020. How good is your tokenizer? on the monolingual performance of multilingual language models. arXiv preprint arXiv:2012.15613. Richard L Sites, Anton Chernoff, Matthew B Kirk, Maurice P Marks, and Scott G Robinson. 1993. Binary translation. Communications of the ACM, 36(2):69 81. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: En- hanced transformer with rotary position embedding. Neurocomputing, 568:127063. Hanzhuo Tan, Qi Luo, Jing Li, and Yuqun Zhang.",
    "source": "2506.14606v1_Guaranteed_Guess_A_Language_Modeling_Approach_for_.pdf",
    "length": 1621,
    "tokens": 487
  },
  {
    "text": "The timeline of cross-stage aligned encoders is demonstrated in Figure 9 (b). Cross-stage alignment via contrastive learning. Contrastive learning-based alignment has been effectively used for bridging different design stages by learning stage-invariant circuit rep- resentations. CircuitEncoder [102] and CircuitFusion [97] focus on RTL-to-netlist alignment by integrating structural and functional representations through self-supervised contrastive learning. These models enforce similarity constraints between functionally equivalent circuits across design stages, ensuring that embeddings capture both high-level design intent and low-level implemen- tation details. NetTAG [113] extends this approach beyond RTL and netlist, incorporating layout information to enable RTL-netlist-layout alignment. By leveraging cross-modal contrastive learn- ing, NetTAG [113] aligns representations across all three design stages, facilitating more accurate early-stage predictions of post-layout circuit characteristics. Cross-stage alignment via mask-reconstruction. Mask-reconstruction-based alignment, on the other hand, focuses on recovering masked portions of a circuit while maintaining logical and structural consistency across design stages. MGVGA [112] is designed for RTL-to-AIG alignment, where it employs Masked Gate Modeling to selectively mask gate-level details in AIG representations while preserving functional equivalence. This ensures that the learned embeddings retain both RTL-level semantics and AIG-level logic properties. DeepCell [114] also employs this technique for AIG-to-netlist alignment, incorporating a self-supervised masking strategy to reconstruct standard cell representations from their lower-level gate descriptions. This approach enhances the model s ability to understand structural variations while preserving functional equivalence across abstraction levels. A Survey of Circuit Foundation Model: Foundation AI Models for VLSI Circuit Design and EDA 31 5 FOUNDATION MODEL AS A CIRCUIT DECODER Another major paradigm of circuit foundation models is circuit decoders, which leverage LLMs for the automated generation of circuit-related content. These models facilitate the creation of RTL code (e.g., Verilog or VHDL), HLS code (e.g., SystemC or C ), design scripts (e.g., Tcl or Python), design descriptions, etc.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 2346,
    "tokens": 484
  },
  {
    "text": "10): First, a performance-efficiency trade- off is evident, with mid-sized models like Qwen 3-8B achieving an optimal balance (EC 0.96, 20s), yielding a composite metric (EC Response Time) of 0.048, outperforming smaller (Qwen 2.5-0.5B: EC 0.8, 28s) and larger models (Qwen 2.5-72B: EC 0.995, 75s). Second, generational advancements show newer models, such as Qwen 3-8B (EC 0.96, 20s) and Llama- 3-8B (EC 0.93, 21s), surpassing predecessors like Qwen 2.5- 7B (EC 0.93, 20s) and Llama-2-7B (EC 0.89, 22s), reflecting architectural enhancements. Third, efficiency peaks at 7B-14B parameters, with models like Qwen 2.5-7B and Llama-2-13B maintaining stable response times ( 20s) before scaling penalties emerge. These results highlight Qwen 3-8B as the superior choice for high-demand tasks, with Qwen 2.5-7B and Llama-2-13B as efficient alternatives. VI. CONCLUSION This research introduces an automated workflow-based framework called D2S-FLOW that harnesses LLMs to address the critical challenge of parameter extraction from chip datasheets and the generation of SPICE models for circuit design. By integrating three innovative mechanisms AGDF, HDER, and HNEN the framework overcomes limitations of traditional RAG systems, including semantic confusion from broad retrieval, inefficient processing of redundant data, and inconsistencies in terminology across diverse document sources. The framework s effectiveness is evidenced by rigorous experimental evaluations, achieving an EM score of 0.86, an F1 score of 0.92, and an EC score of 0.96 outperforming the strongest baseline by 19.4 , 5.7 , and 13.1 , respectively. Additionally, it reduces API token consumption by 38 and minimizes the irrelevant information ratio to 4 , demonstrating superior resource efficiency. Validation experiments further confirm the practical utility of the generated SPICE models, with a 100 success rate in functional simulations across various device types, reinforcing the framework s value in EDA.",
    "source": "2502.16540v2_D2S-FLOW_Automated_Parameter_Extraction_from_Datas.pdf",
    "length": 1984,
    "tokens": 482
  },
  {
    "text": "5s, pp. 1 24, 2023. [35] A. Parashar, P. Raina, Y. S. Shao, Y.-H. Chen, V. A. Ying, A. Mukkara, R. Venkatesan, B. Khailany, S. W. Keckler, and J. Emer, Timeloop: A systematic approach to dnn accelerator evaluation, in 2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), 2019, pp. 304 315. [36] R. Prabhakar, S. Jairath, and J. L. Shin, Sambanova sn10 rdu: A 7nm dataflow architecture to accelerate software 2.0, in 2022 IEEE International Solid-State Circuits Conference (ISSCC), vol. 65. IEEE, 2022, pp. 350 352. [37] M. R. Santoro and M. A. Horowitz, Spim: a pipelined 64 64-bit iterative multiplier, IEEE journal of solid-state circuits, vol. 24, no. 2, pp. 487 493, 1989. [38] S. Sharify, A. D. Lascorz, M. Mahmoud, M. Nikolic, K. Siu, D. M. Stuart, Z. Poulos, and A. Moshovos, Laconic deep learning inference acceleration, in Proceedings of the 46th International Symposium on Computer Architecture, 2019, pp. 304 317. [39] M. Shi, V. Jain, A. Joseph, M. Meijer, and M. Verhelst, Bitwave: Ex- ploiting column-based bit-level sparsity for deep learning acceleration, in 2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 2024, pp. 732 746. [40] M. Sjalander and P. Larsson-Edefors, High-speed and low-power mul- tipliers using the baugh-wooley algorithm and hpm reduction tree, in 2008 15th IEEE international conference on electronics, circuits and systems. IEEE, 2008, pp. 33 36. [41] Taiwan Semiconductor Manufacturing Company, Tsmc 2023 annual report, 2023, accessed: 2024-06-27. [Online].",
    "source": "2503.06342v1_Exploring_the_Performance_Improvement_of_Tensor_Pr.pdf",
    "length": 1576,
    "tokens": 497
  },
  {
    "text": "Furthermore, the MG-Verilog dataset (Zhang et al., 2024), despite featuring multi-level descriptions alongside code samples, is limited in size and its reliance on LLaMA2-70B-Chat for annotations raises quality concerns about the dataset. These alignment issues may hinder the fine-tuned LLMs performance, leading to the generation of non-synthesizable or non-functional hardware source code. To address the limitations of previous studies, we introduce a novel high-quality dataset that aligns natural lan- guage with Verilog code at multiple levels: line, block, and module. It includes both detailed and high-level descriptions, integrating open-source and proprietary code to enhance its diversity and applicability. Unlike prior efforts focused solely on Verilog generation, we are the first to consider the crucial task of Verilog understanding. This comprehensive dataset enables the development of a unified representation model, DeepRTL, which excels in both Verilog understanding and generation, paving the way for significant advancements in hardware design automation. Curriculum Learning. Curriculum learning is a training strategy inspired by human learning, where models are exposed to simpler tasks before advancing to more complex ones. This approach has been shown to accelerate convergence and improve model performance, particularly for tasks with hierarchical difficulty levels. Initially introduced in Bengio et al. (2009), curriculum learning has been applied to various domains, including natural language processing (Xu et al., 2020), com- puter vision (Wang et al., 2023b), and reinforcement learning (Narvekar et al., 2020). Recent work has demonstrated its efficacy in fine-tuning LLMs, where progressively increasing task complexity helps the models better capture intricate patterns (Campos, 2021). Notably, Na et al. (2024) apply curriculum learning to code language models, achieving significant improvements in the accuracy of code execution tasks. In this work, we adapt curriculum learning to train DeepRTL, utilizing our structured dataset with descriptions at varying levels of detail. This approach significantly enhances the model s capabilities in both Verilog understanding and generation.",
    "source": "2502.15832v1_DeepRTL_Bridging_Verilog_Understanding_and_Generat.pdf",
    "length": 2230,
    "tokens": 432
  },
  {
    "text": "Learning to design circuits. arXiv preprint arXiv:1812.02734, 2018. [13] Keertana Settaluri, Ameer Haj-Ali, Qijing Huang, Kourosh Hakhamaneshi, and Borivoje Nikolic. Autockt: deep reinforcement learning of analog circuit designs. In Proceedings of the 23rd Conference on Design, Automation and Test in Europe, DATE 20, page 490 495, San Jose, CA, USA, 2020. EDA Consortium. [14] Yaguang Li, Yishuang Lin, Meghna Madhusudan, Arvind Sharma, Sachin Sapatnekar, Ramesh Harjani, and Jiang Hu. A circuit attention network-based actor-critic learning approach to robust analog transistor sizing. In 2021 ACM IEEE 3rd Workshop on Machine Learning for CAD (MLCAD), pages 1 6, 2021. [15] Wenlong Lyu, Pan Xue, Fan Yang, Changhao Yan, Zhiliang Hong, Xuan Zeng, and Dian Zhou. An efficient bayesian optimization approach for automated optimization of analog circuits. IEEE Transactions on Circuits and Systems I: Regular Papers, 65(6):1954 1967, 2017. [16] Kourosh Hakhamaneshi, Marcel Nassar, Mariano Phielipp, Pieter Abbeel, and Vladimir Stojanovic. Pretraining graph neural networks for few-shot analog circuit modeling and design. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 42(7):2163 2173, 2022. 10 [17] Morteza Fayazi, Morteza Tavakoli Taba, Ehsan Afshari, and Ronald Dreslinski. Angel: Fully-automated analog circuit generator using a neural network assisted semi-supervised learning approach. IEEE Transac- tions on Circuits and Systems I: Regular Papers, 2023. [18] Jintao Li, Haochang Zhi, Ruiyu Lyu, Wangzhen Li, Zhaori Bi, Keren Zhu, Yanhan Zeng, Weiwei Shan, Changhao Yan, Fan Yang, Yun Li, and Xuan Zeng. Analoggym: An open and practical testing suite for analog circuit synthesis. In International Conference on Computer Aided Design, 2024.",
    "source": "2505.21923v1_FALCON_An_ML_Framework_for_Fully_Automated_Layout-.pdf",
    "length": 1779,
    "tokens": 496
  },
  {
    "text": "(13) This provides the combined RMSNorm operation with different scaling parameters g1 and g2. By combining two RMSNorm operations with different scaling parameters, we arrive at a single nor- malization step: z x gcombined Âµx Îµcombined , (14) where: gcombined g1g2 p g2 1 Îµ , Îµcombined Îµ2 g2 1 Îµ. (15) Alternatively, since the denominator depends on Âµx, it may not be possible to express Îµcombined independently without further approximations. A.4 DETAILED HARDWARE RESULTS A.4.1 DETAILED LOIHI 2 RESULTS Single chip experiments As described in Section 4, the energy and throughput metrics for Loihi 2 were estimated based on a preliminary implementation. We first implemented a single MatMul- free LM block on a Oheo Gulch single-chip Loihi 2 system, see Table 3 (1-chip). We measured 13 Published at ICLR 2025 Workshop (SCOPE) the average time per step (TPS, TTPS), i.e., the time that a single execution time step takes. Given the number of time steps per block, Nsteps block, we can compute the total latency of the model, or the time-to-first-token Tttft, as Tttft Nblocks Nsteps block TTPS where Nblocks 24 for the 370M MatMul-free language model. In prefill we use pipelined mode where the TPS is constant over time because equal time bins are enforced (see Appendix A.1.1 for an explanation). We calculate the prefill throughput as fprefill T 1 TPS because a new token is processed in the interval TTPS. We also measure the power of the single- chip system as the sum of a static power and dynamic power component: P 1-chip P 1-chip P 1-chip where P denotes static power and P denotes dynamic power. We estimate the total prefill power as Ë†Pprefill 24 P 1-chip. We finally estimate the energy per token as Ë†Eprefill Ë†Pprefill TTPS. Figure 4 shows the dynamic and static power of the single-chip experiment. Figure 4: Power of one MatMul-free block on a single-chip Loihi 2 system.",
    "source": "2503.18002v2_Neuromorphic_Principles_for_Efficient_Large_Langua.pdf",
    "length": 1889,
    "tokens": 500
  },
  {
    "text": "By contrast, a conventional single-turn LLM inference based on ShareGPT requires only 0.32 Wh (8B) and 2.55 Wh (70B) per request. These figures correspond to a 62.1 136.5 increase in GPU energy per query under agent-based test-time scaling. Based on recent estimates, ChatGPT serves roughly 500 million weekly active users (WAU) [35] (Figure 23), which translates to about 71.4 million daily active users (DAU). Assuming, conservatively, that each user submits only one agentic query per day, Reflexion s daily GPU energy footprint would be roughly 2.97 GWh for the 8B model and 24.89 GWh for the 70B model. While our analysis does not consider LLM request batching [19], [66], our estimate is still conservative because (1) it reflects current DAU with only one query per user, even though AI adoption continues to accelerate often with much heavier per-user usage, and (2) it counts only GPU energy, omitting CPU, memory, networking, storage, and cooling overheads. Even under these modest assumptions, the projected demand rivals the daily electricity consumption of Seattle and its surrounding area (24.8 GWh) [43]. As AI agents become 12 Accuracy ( ) Latency (seconds) Energy (Wh query) Power 71.4 Million Queries day (Watt) Power 13.7 Billion Queries day (Watt) 8B ShareGPT 4.23 (1 ) 0.32 (1 ) 1.0 M 182.7 M Reflexion 38 649.34 (153.7 ) 41.53 (130.9 ) 123.6 M 23.7 G LATS 80 380.90 (90.1 ) 22.76 (71.7 ) 67.7 M 13.0 G 70B ShareGPT 6.40 (1 ) 2.55 (1 ) 7.6 M 1.5 G Reflexion 67 720.00 (112.6 ) 348.41 (136.5 ) 1.0 G 198.9 G LATS 82 305.67 (47.8 ) 158.48 (62.1 ) 471.5 M 90.5 G TABLE III: Energy and power demands of handling an AI agent service request on HotpotQA.",
    "source": "2506.04301v1_The_Cost_of_Dynamic_Reasoning_Demystifying_AI_Agen.pdf",
    "length": 1669,
    "tokens": 486
  },
  {
    "text": "[16] Y. Kwon, Y. Lee, and M. Rhu, Tensordimm: A practical near-memory processing architecture for embeddings and tensor operations in deep learning, in Proceedings of the 52nd Annual IEEE ACM International Symposium on Microarchitecture, 2019, pp. 740 753. [17] V. Gupta, D. Choudhary, P. Tang, X. Wei, X. Wang, Y. Huang, A. Ke- jariwal, K. Ramchandran, and M. W. Mahoney, Training recommender systems at scale: Communication-efficient model and data parallelism, in Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery Data Mining, 2021, pp. 2928 2936. [18] W. Zhao, D. Xie, R. Jia, Y. Qian, R. Ding, M. Sun, and P. Li, Distributed hierarchical gpu parameter server for massive scale deep learning ads systems, Proceedings of Machine Learning and Systems, vol. 2, pp. 412 428, 2020. [19] G. Sethi, B. Acun, N. Agarwal, C. Kozyrakis, C. Trippel, and C.- J. Wu, Recshard: statistical feature-based memory optimization for industry-scale neural recommendation, in Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, 2022, pp. 344 358. [20] W. Zhao, J. Zhang, D. Xie, Y. Qian, R. Jia, and P. Li, Aibox: Ctr prediction model training on a single node, in Proceedings of the 28th ACM International Conference on Information and Knowledge Management, 2019, pp. 319 328. [21] U. Gupta, S. Hsia, V. Saraph, X. Wang, B. Reagen, G.-Y. Wei, H.- H. S. Lee, D. Brooks, and C.-J. Wu, Deeprecsys: A system for optimizing end-to-end at-scale neural recommendation inference, in 2020 ACM IEEE 47th Annual International Symposium on Computer Architecture (ISCA). IEEE, 2020, pp. 982 995. [22] X.",
    "source": "2504.00520v1_SCRec_A_Scalable_Computational_Storage_System_with.pdf",
    "length": 1668,
    "tokens": 487
  },
  {
    "text": "The technique introduces additional outer loops that partition the dataset into smaller chunks. Each chunk is processed independently, allowing it to be efficiently stored in cache and reducing memory access overhead. This segmentation enhances execution speed by minimizing data movement between main memory and processing units. In hardware-accelerated environments, loop tiling is often combined with parallelism and pipelining to further opti- mize performance. In parallel computing architectures such as GPUs, multiple processing units can operate on different tiles concurrently, enabling efficient parallel execution. In FPGAs and custom accelerators, loop tiling can be integrated with hardware-specific architectural features to mitigate memory bottlenecks and enhance overall data throughput. 2) Loop Unrolling: Loop unrolling is a widely used op- timization technique aimed at improving the execution effi- ciency of loops in CNN hardware accelerators, particularly on FPGAs. The key idea is to merge multiple iterations of a loop into a single, larger operation, reducing loop control overhead and increasing parallelism to accelerate execution. In CNN computations, convolution operations often involve deeply nested loops, leading to a high number of iterations when processing large-scale input data. Loop unrolling ad- dresses this inefficiency by transforming a single iteration into multiple parallel operations. By increasing the workload of each iteration, it reduces the need for control logic, such as loop counters and branching decisions, thereby improving instruction execution efficiency and maximizing hardware par- allelism. For instance, in convolution operations, a standard loop structure processes input data elements sequentially. With loop unrolling, multiple elements can be processed in a single iteration, or different convolution tasks can be executed simul- taneously, significantly boosting computational throughput. However, the degree of unrolling must be carefully tuned to match the available hardware resources, such as logic units and memory bandwidth. Excessive unrolling can lead to resource congestion, negatively affecting overall performance. There- fore, in CNN hardware acceleration, loop unrolling must be strategically applied to strike a balance between performance gains and efficient hardware utilization. 3) Parallel Computing: Parallel computing is a fundamen- tal technique in CNN hardware acceleration, particularly for computationally intensive tasks like convolution operations. ... ... F Pf ... ... ... ... ... ... Input Feature Kernel Input Feature Kernel Input Feature Kernel Input Feature Kernel Input Feature Kernel Input Feature Kernel Input Feature Kernel Input Feature Kernel Pc Pc K Pv Pk Fig. 1.",
    "source": "2505.13461v1_FPGA-based_Acceleration_for_Convolutional_Neural_N.pdf",
    "length": 2770,
    "tokens": 493
  },
  {
    "text": "To account for this new range we assign 6 bits for the integer part and the rest for the fraction part. After the clipping and quantization of X, Eq. (6) becomes Log2Exp(X) Ë†X log2 e Since Ë†X is a fixed-point number the multiplication with the constant value log2 e can be approximated by integer shift and add operations [26], [25]. Following the approach presented in [26], we get that: Log2Exp(X) Ë†X Ë†X 1 Ë†X 4 (7) With the help of (7) the ExpMul operator of (4) can be rewritten as: ExpMul(x, V ) 2 Ë†L V, where Ë†L Log2Exp(x). (8) Multiplying the floating-point number V ( 1)SV 2EV b(1 MV ), where SV , EV , MV represent its sign, exponent and mantissa field, respectively, with 2 Ë†L in (8) is equivalent to subtracting Ë†L from the exponent of V . In case of an overflow the resulting FP number is set to 0. More specifically, ExpMul(x, V ) 2 Ë†L V 2 Ë†L ( 1)SV 2EV b (1 MV ) ( 1)SV 2EA b Ë†L (1 MV ) ( 1)SV 2EA b Log2Exp(x) (1 MV ) (9) The result computes both ex V and at the same time returns the result directly in floating-point format without requiring any addition conversion (or dequantization) step. Alg. 3 summarizes the computational steps that are per- formed by the ExpMul operator to calculate ex V and Alg. 4 using (3), (5), and (9) illustrates how the new hardware operator is integrated in FlashAttention-2 kernel.",
    "source": "2505.14314v2_Low-Cost_FlashAttention_with_Fused_Exponential_and.pdf",
    "length": 1330,
    "tokens": 410
  },
  {
    "text": "However, for longer input vectors, this approach requires additional levels to aggregate multiple 8-bit popcount results, ultimately leading to a tree-based adder architecture. A more recent approach leverages 6-input LUTs in mod- ern FPGAs to compress popcount trees, where three LUTs collectively function as a 6-bit popcounter, producing a 3-bit output [10]. Another design, optimized based on ripple carry adders, introduces an additional chain to propagate the sum of each full adder [6]. While this method achieves modest resource savings, it increases latency compared to conventional popcount trees. Further optimizations based on these works have been proposed by sharing logic elements [5], [7]. Most existing adder-based approaches remain within a sim- ilar design space, where improving one metric typically comes at the expense of others. More importantly, the comparison of multiple popcount results an essential operation in applica- tions such as TMs and the output layer of BNNs introduces significant overhead in terms of latency and resource con- sumption when using digital comparators [11]. In ML tasks involving a large number of classes, this comparison (argmax) becomes a major bottleneck. B. Programmable Delay Line (PDL) PDLs have been practically implemented on FPGAs in previous works, utilizing a cascade of programmable delay elements, where each element consists of a single LUT that buffers or inverts an input signal with its delay multiplexed by other inputs [12] [15]. These PDLs are commonly used for arbiter physical unclonable functions (PUFs), which compare signals racing through two symmetric PDLs and generate responses based on cumulative delays of all units in each path. However, PDLs originally for arbiter PUFs cannot be di- rectly applied for time-domain popcount. First, PUF outputs are determined by specific input vector, whereas popcount outputs depend on input Hamming weight; for example, the output should remain the same for input vectors 0...01 and 10...0 in popcount, which is not the case for PUFs. Second, while PUFs exploit intrinsic process variations to generate device-specific responses, popcount must mitigate these variations for consistency and device-independency. A recent study proposed using PDLs specifically for TM popcount followed by asynchronous arbitration for argmax [16].",
    "source": "2505.02181v1_Efficient_FPGA_Implementation_of_Time-Domain_Popco.pdf",
    "length": 2351,
    "tokens": 494
  },
  {
    "text": "EmbBERT-Q achieves Accuracies and Scores comparable to the ones of models with up to 25 its memory footprint, and stands out as the highest-performing solution within the 2 MB memory budget explored in this paper. By leveraging an innovative architectural design specifically tailored for extremely memory- and computationally-constrained environments, EmbBERT-Q effectively balances parameter efficiency and competitive accuracy. Its Embedder Block, Efficient Encoder, and the application of post-training 8-bit quantization significantly reduce the model s memory footprint while maintaining high performance both on the newly proposed TinyNLP benchmark and on the GLUE benchmark, crucially demonstrat- ing the effectiveness and the efficiency of EmbBERT-Q in resource-constrained environments. 14 Future research will explore further compression techniques, novel architectural designs, targeted knowledge distillation, and even more extreme quantizations tailored to emerging hardware accelerators (e.g., 1-bit quantization). Combining these advancements with next-generation hardware has the potential to further optimize model memory and computation footprints while preserving, or even enhancing, performance. Our work establishes a systematic foundation for designing efficient language models capable of operating effectively within the most stringent memory constraints. Acknowledgment This paper is supported by PNRR-PE-AI FAIR project funded by the NextGeneration EU program. A Complete results In this section, we provide the complete results of our experiments, spanning all datasets and models in both pretrained and non-pretrained contexts. The detailed results, concerning model quantization as well, are presented in Tables 7, 8, 9, 10, 11, and 12. A.1 Evaluation on the TinyNLP Benchmark For what concerns non-pretrained models, as shown in Table 7 BERT(2MB) and MAMBA(2MB) demonstrate compatible results with respect to other models, with average accuracies of 83.74 and 83.86 , respectively. Notably, Embedder Conv and Embedder outperform others models, achieving 86.50 and 86.41 average accuracy, respectively. The Embedder Conv model particularly excells on datasets like AG News, scoring 91.50 , the highest across the board for this task. When non-pretrained and evaluated with the MCC score, as shown in Table 8 the Embedder-based models again deliver competitive performances, with Embedder Conv achieving an average MCC of 78.49 .",
    "source": "2502.10001v1_EmbBERT-Q_Breaking_Memory_Barriers_in_Embedded_NLP.pdf",
    "length": 2458,
    "tokens": 498
  },
  {
    "text": "27.3 Power consumption Achieving this level of performance within a PCIe card necessitates careful thermal and power management. Calculated ExaLith total board power is 539 W, under the 600 W limit for PCIe cards. Cooling is envisioned using advanced air-cooling solutions incorporating phase-change heat pipe technology and high-efficiency fans, like those employed in flagship consumer and workstation GPU cards. While demanding, this remains within the established capabilities of desktop workstation thermal design, avoiding the 2-PIC JETSTREAM cooling requirements of the full ZettaLith system. 12V power delivery utilizes the 16-pin 12VHPWR connector from a ATX 3.0 compliant PSU. The 12V input is regulated to the TRIMERA, CPU, HBM, and HBF requirements by an on- board multiphase controller (such as the Infineon XDPE192C4C programable digital multi-phase controller) with 12 interleaved phases driving power stages such as the Infineon TDA21590, Monolithic Power MP86956, or Renesas RAA220105. Multiphase buck converters are selected for their high efficiency and cost-effectiveness at PCIe power levels, compared to the TLVRs chosen for ZettaLith's high current regulation needs. 27.4 Use cases By offering performance equivalent to multi-million-dollar GPU clusters in a single PCIe card, ExaLith fundamentally changes the economics for numerous users. Potential use cases include: SMBs: Running complex, proprietary AI models for data analysis, customer service, or internal process automation without relying on expensive cloud services or compromising data privacy. Researchers Academia: Enabling experimentation and development with state-of-the-art large models on local hardware, accelerating research cycles. While ExaLith can t be used for transformer training, it is well suited to developing agentic and reasoning models using pre-trained LLMs. AI Developers: Providing a powerful local platform for developing, testing, and debugging AI applications destined for various deployment targets. Creative Professionals: Facilitating high-fidelity, low- latency local generation of AI-driven content (images, video, audio, 3D assets) enabling faster creative iteration.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2185,
    "tokens": 464
  },
  {
    "text": "H0 H1 H3 H2 Channel 0 Channel 1 Channel N Req 0 H0 Layer 0 H0 Layer 1 Layer 0 Req 1 Rankset 0 Rankset 1 H0 H0 H0 H0 H0 H0 Layer 1 Req 2 Req 3 H0 H0 H0 H0 H0 H0 H0 H0 H0 Ch0 H1 Ch1 H2 Ch2 H3 Ch3 (a) Communication and computation with the rankset. (b) The data mapping on ranksets for load balancing. offloaded data PCIe CPU comm ... rank rank rank rank rank rank computation ... ... Rankset M Mapping Figure 6: Communication-computation overlapping with load balanced ranksets. 5.1 Optimizing Cross-device Communication L3 introduces dependency-aware communication-computation over- lap, a hardware-software co-design involving three techniques: 1) overlapping the communication and computation on the DIMM- PIM to hide the cross-device communication latencies, 2) achieving data layout load balancing among ranks, and 3) removing most data transfer out of the inference critical path. Concurrent communication and computation. Conventional scheduling treats all DIMM-PIM as a monolithic entity, alternat- ing between communication and computation modes. However, we observe that due to the shared memory buses between ranks, channel bandwidth does not scale with the number of ranks for communication. A single rank can nearly saturate the available channel bandwidth, rendering the simultaneous activation of all ranks for communication inefficient. L3 enables concurrent communication with computation on the DIMM-PIM side. The key insight is that, KV offloading does not require all ranks to operate in the communication mode concurrently. Therefore, we introduce a new hardware extension to use rankset as the basic unit for data transfer, with one rank on each channel forming a rankset, as shown in Fig. 6-a. During a single data transfer, only one rankset (e.g., rankset 0 in Fig. 6-a) is engaged to receive the data, while other ranksets can perform independent computations without being stuck. This hardware extension enables L3 to achieve parallel PIM computation and data transfer at the DIMM-PIM level. Taking the DGX-A100 with four ranks per channel as an example, this design preserves 75 of the DIMM-PIM s computational power during communication.",
    "source": "2504.17584v1_L3_DIMM-PIM_Integrated_Architecture_and_Coordinati.pdf",
    "length": 2163,
    "tokens": 502
  },
  {
    "text": "(11), we follow [54] and randomly sample 128 TABLE IV RESOURCE CONSUMPTION OF OUR DEDICATED ACCELERATOR Resources BRAM DSP LUT FF Available 2016 9024 1304K 2607K Used 513 (25.4 ) 4497 (49.8 ) 420K(32.2 ) 274K(10.5 ) TABLE V PERFORMANCE OF LLAMA-2-7B [4] ON THE WIKITEXT-103 DATASET [51] WITH VARYING SEQUENCE LENGTHS UNDER DIFFERENT COMPRESSION ALGORITHMS Method Algorithm Model Size (GB) Perplexity ( ) 3k 4k 5k 6k 7k FP16 - 12.1 6.506 7.455 12.49130.275 62.200 W8A8 SmoothQuant 6.03 6.778 7.743 13.09032.478 66.430 W4A8KV4 QoQ 3.08 7.142 8.186 13.70733.729 67.240 2:4 Pruning SparseGPT 6.60 13.77516.30927.96665.122116.967 W2A8KV4 2:4 Pruning Î›-Shaped Attention Ours 1.53 8.038 8.524 9.316 9.512 9.869 TABLE VI PERFORMANCE OF LLAMA-2-7B [4] ON THE WIKITEXT-2 DATASET [51] WITH VARYING SEQUENCE LENGTHS UNDER DIFFERENT COMPRESSION ALGORITHMS Method Algorithm Model Size (GB) Perplexity ( ) 3k 4k 5k 6k 7k FP16 - 12.1 18.49720.608 30.619 63.461 114.484 W8A8 SmoothQuant 6.03 18.96721.246 31.892 67.059 120.419 W4A8KV4 QoQ 3.08 41.22044.845 62.171 113.396180.845 2:4 Pruning SparseGPT 6.60 54.51667.892102.321194.244317.622 W2A8KV4 2:4 Pruning Î›-Shaped Attention Ours 1.53 10.99211.857 12.101 12.502 12.669 sentences from the training set of WikiText-103 and WikiText- 2 [51], which serve as calibration datasets. Subsequently, we perform dataset-specific LoRA fine-tuning on their respective datasets. Baselines: We compare our compressed algorithm with four counterparts: (1) the half-precision (FP16) baseline, (2) the widely used LLM quantization work, SmoothQuant [20], (3) the SOTA W4A8KV4 LLM quantization framework, QoQ [19], and (4) the widely adopted LLM pruning method, SparseGPT [17], in terms of perplexity on varying sequence lengths and model size after compression.",
    "source": "2505.03745v1_AccLLM_Accelerating_Long-Context_LLM_Inference_Via.pdf",
    "length": 1780,
    "tokens": 669
  },
  {
    "text": "This process of verification 7 and repair is repeated iteratively until a functionally correct design is achieved. Through this iterative functional correct- ness feedback loop, the design s correctness is continuously refined, ultimately approaching 100 functional correctness of the automatically designed processor chips. 2) Performance Feedback: Performance optimization aims to improve the PPA of designed process chips, playing a vital role in designing processor chips and adapting foundational software. Existing automated optimization tools, such as deep learning compilers and DSE methods, typically rely on expert- designed rule-based optimization methodologies or machine learning techniques. While these tools significantly reduce the human labor compared to manual tuning, they still encounter issues such as narrow coverage, suboptimal efficiency, and poor cross-domain transferability. To strengthen the capacity of automated performance optimization, cross-stage collab- orative design is necessary. LPCM should be capable of directly generating chip layouts from functional specifications. Nevertheless, formulating the design issue directly with the raw bitstream input of processor chips leads to the curse of dimensionality. For instance, the solution space for a 32-bit CPU could grow to 1010540. To address the challenges of the enormous solution space, processor chip design has been divided into multiple ab- stract stages. During logic design, functional specifications are translated into high-level HDL. In circuit design, these high- level HDLs are converted into gate-level netlists. While in physical design, gate-level netlists are turned into chip layouts. This workflow follows a progressive coarse-to-fine design process. Each stage incorporates additional implementation details, progressively introducing more constraints, gradually pruning the solution subspaces with lower probability of con- taining optimal solutions, thereby reducing the overall solution space size. Inspired by this, to enable automated performance optimization, LPCM needs to adopt a hierarchical search- based inference mechanism guided by performance feedback. This involves building hierarchical decompositions, where the solution space is pruned based on domain knowledge and performance feedback, effectively reducing its dimensionality. Simultaneously, by leveraging the iterative reasoning and Test- Time Scaling (TTS) benefits demonstrated by LLMs during inference, the solution space can be efficiently explored, ad- dressing the enormous solution space challenge and enhancing the performance of automated design. To implement performance feedback in inference, efficient search techniques must be integrated into the inference process to obtain high-performance outcomes from the vast solution space.",
    "source": "2506.05007v1_QiMeng_Fully_Automated_Hardware_and_Software_Desig.pdf",
    "length": 2822,
    "tokens": 494
  },
  {
    "text": "Figure 1 rep- resents the i-th lane of a vector architecture with N lanes that completes a vector instruction over V L elements in V L N steps. The vector register file is interleaved across lanes and a lane interconnect enables inter-lane communication to support vector reductions, slides, and similar operations [27]. The vector operand buffers are present on each lane to receive vector element operands from the local Vector Register File (VRF) slice in preparation toward the functional units. Vector architectures support configurable vector length which disables computation on vector tail elements by scheduling fewer elements into the operand buffers. In addition, vector architec- tures support predication, which discards the functional unit outputs for specific elements and thus preserve their values. C. Matrix ISAs for CPU Architectures General-purpose CPU architectures are gradually incor- porating matrix multiplication ISA extensions to accelerate Artificial Intelligence (AI) and High-Performance Computing (HPC) workloads [5], [7], [9], [10]. These systems expose matrix multiplication instructions implemented by a Matrix Multiply-Accumulate (MMA) unit, which leverages the data reuse potential of MMA operations and outperform approaches based purely on SIMD Vector units. 1) The Intel Advanced Matrix Extensions (AMX): AMX consists of a dedicated set of eight tile registers with 1KB of storage each, and a tile matrix multiply unit (TMUL) to operate on them [34]. To use the TMUL, software must first configure the shape of each individual tile register, in terms 2 Lane i 1 VRF slice Lane i Write-back buffer VL-N i jN i N i i VL-N i jN i N i i VL-N i jN i N i i FPU ALU Lane interconnect Lane N-1 vd vs1 vs2 Last step VL N Pipeline step 0 Pipeline step 1 Pipeline step j Operand Buffers Functional Units Fig. 1. Structure of the i-th lane of a vector unit with N lanes. The vector register file storage is interleaved across the N lanes. Functional units contain one execution pipeline per lane operating on local elements across V L N steps. The lane interconnect enables communication between lanes. of the number of active rows and the row size in bytes, by populating a 64-byte CSR using a dedicated instruction.",
    "source": "2507.03522v1_A_Flexible_Instruction_Set_Architecture_for_Effici.pdf",
    "length": 2244,
    "tokens": 491
  },
  {
    "text": "The architectural design space for LLM inference accelerators has expanded dramatically as models have grown beyond the capabilities of traditional computing platforms. What began as adaptations of general- purpose GPU architectures have evolved into a diverse ecosystem of specialized hardware with fundamentally different approaches to computation, memory organization, and scaling strategies. Contemporary accelerators range from tensor-core equipped GPUs to systolic array processors, wafer- scale engines, and deterministic pipeline architectures each representing a distinct point in the design space with unique tradeoffs [2]. This paper makes the following novel contributions: 1. We present the first comprehensive cross-architectural analysis of AI accelerators for LLM inference that quantitatively evalu- ates performance across the full spectrum of inference workloads, from single-token generation to high-throughput batch pro- cessing. 2. We introduce a new methodology for evaluating accelerator fitness-for-purpose across six distinct operational regimes, provid- ing system designers with actionable decision criteria for hardware selection based on deployment requirements. 3. We quantify the efficiency and scaling characteristics of four distributed inference strategies, demonstrating previously unreported tradeoffs between parameter capacity, throughput, and latency predictability. 4. We identify critical architectural gaps in current accelerator designs and propose specific hardware innovations needed to bridge these gaps for future-generation models. Unlike previous surveys that focus on training accelerators [3] or general AI hardware [4], our work specifically addresses the unique challenges of LLM inference and provides quantitative architectural comparisons backed by industry benchmarks and direct perfor- mance measurements. The structure of the remaining sections is as follows: Section II presents a detailed overview of LLM inference requirements and exam- ines relevant literature. Section III examines the architectural approaches of major AI accelerators. Section IV analyzes workload-specific performance characteristics. Section V explores scaling strategies for trillion-parameter models. Section VI presents microarchitectural details of leading accelerators. Section VII discusses emerging trends and future directions. Finally, Section VIII concludes the paper. II. Background and Related Work A. Evolution of LLM Size and Computational Requirements Since 2018, large language models have significantly increased in size, demanding more computation and memory. Fig. 1 highlights this growth, from BERT-Large (340M parameters) in 2018 to models with over 5 trillion parameters projected by 2025 [3]. This growth has been accompanied by significant advances in model architecture, training methodologies, and inference optimization techniques.",
    "source": "2506.00008v1_AI_Accelerators_for_Large_Language_Model_Inference.pdf",
    "length": 2893,
    "tokens": 494
  },
  {
    "text": "In 2024 IEEE International Solid-State Circuits Conference (ISSCC), volume 67, pages 210 212, 2024b. doi: 10.1109 ISSCC49657.2024.10454529. Yannan Nellie Wu, Joel S. Emer, and Vivienne Sze. Accelergy: An Architecture-Level Energy Estimation Methodology for Accelerator Designs. In 2019 IEEE ACM International Conference on Computer-Aided Design (ICCAD), pages 1 8, 2019. Hu Xu, Saining Xie, Xiaoqing Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying CLIP data. In The Twelfth International Conference on Learning Representations, 2024. Dan Zhang, Safeen Huda, Ebrahim Songhori, Kartik Prabhu, Quoc Le, Anna Goldie, and Azalia Mirhoseini. A full-stack search technique for domain optimized deep learning accelerators. In Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, page 27 42, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450392051. Yiyang Zhao and Tian Guo. Carbon-efficient neural architecture search. In Proceedings of the 2nd Workshop on Sustainable Computer Systems, HotCarbon 23, page 1 7. ACM, July 2023. doi: 10.1145 3604930.3605708. 15 Yiyang Zhao, Yunzhuo Liu, Bo Jiang, and Tian Guo. CE-NAS: An end-to-end carbon-efficient neural architecture search framework. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Yanqi Zhou, Xuanyi Dong, Tianjian Meng, Mingxing Tan, Berkin Akin, Daiyi Peng, Amir Yazdanbakhsh, Da Huang, Ravi Narayanaswami, and James Laudon. Towards the co-design of neural networks and accelerators. In D. Mar- culescu, Y. Chi, and C. Wu, editors, Proceedings of Machine Learning and Systems, volume 4, pages 141 152, 2022.",
    "source": "2505.01386v2_Carbon_Aware_Transformers_Through_Joint_Model-Hard.pdf",
    "length": 1779,
    "tokens": 485
  },
  {
    "text": "In 2018 51st Annual IEEE ACM International Symposium on Microarchitecture (MICRO). 802 814. [9] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier- Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebas- tian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, ClÃ©ment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark DÃ­az, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhong- tao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wi- eting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu.",
    "source": "2504.17584v1_L3_DIMM-PIM_Integrated_Architecture_and_Coordinati.pdf",
    "length": 1943,
    "tokens": 623
  },
  {
    "text": "When scoring , consider if the task better fits in a different category. The reasoning_category_match field should explain your reasoning for the category match score. The behavioral_match_score should evaluate specifically how well the logic and behavior described in the prompt matches the actual implementation details in the reference solution and what is being checked in the test harness. It focuses on the technical alignment of the expected behavior versus what is actually implemented and tested. The behavioral_match_reasoning field should explain your reasoning for the behavioral_match_score , highlighting any discrepancies or strong alignments between the prompt s behavioral specifications and the actual implementation testing. Your JSON response can be minimal , containing just: json { \"prompt \": \"your improved prompt here\", \" reasoning_prompt \": \"your explanation here\", \" ambiguity_score \": 8, \" reasoning_ambiguity \": \"your explanation for ambiguity_score here\", \" consistency_score \": 8, \" reasoning_consistency \": \"your explanation for consistency_score here\", \" category_match_score \": 8, \" reasoning_category_match \": \"your explanation for category_match_score here\", \" behavioral_match_score \": 8, \" behavioral_match_reasoning \": \"your explanation for behavioral_match_score here\" } Or you can include the full datapoint structure if you prefer. The system will ensure other fields remain unchanged. Return the datapoint as valid JSON. Listing 1: Quality filtering prompt instructions. Original JSONL Dataset Benchmark Runner LLM-Based Quality Judge Ambiguity Score? Consistency Score? Full Datapoint Full Datapoint (Category, Prompt, Context, Harness, Ref. Solution) Behavioral Score? Category Score? Annotated JSON with Scores and Explanations Scored JSONL Dataset Post- Process Script Final JSON Dataset Only Containing High Scoring Datapoints Final JSON Dataset Only Containing High Scoring Datapoints Map mode Figure 6: Quality filtering flow. C Compute Requirements Benchmark infrastructure development and model evaluation were performed on a virtual machine with 12 virtual CPUs and 24 GB of RAM, running Rocky Linux 8.10.",
    "source": "2506.14074v1_Comprehensive_Verilog_Design_Problems_A_Next-Gener.pdf",
    "length": 2157,
    "tokens": 458
  },
  {
    "text": "The sensitivity analysis ranks blocks individually according to their accuracy impact (their importance). Every block is removed (skipped) and the accuracy is evaluated. Then, an ordered list of blocks is created from the lowest accuracy (given by the removal of the most important block) to the highest accuracy (least important block). Regarding the time to build the sensitivity list, when compared to training the models, the list does not seriously increase training time. Precisely, it is necessary to run only one evaluation on the test set for every skippable block. As an example, take the training of ResNet-20 and -100. It took 500 epochs in our experiments, meaning 500 evaluations (plus the many more training iterations). In contrast, it takes only 7 evaluations (just 1 of the evaluations run during training) and 51 evaluations (10 of the training evaluations) for building the sensitivity lists for ResNet-20 and -100, respectively. If one considers also the time spent on the trainings set, the costs of building the sensitivity list decreases even further in comparison with the time for training those models. 4) Pareto Front Generation: With the sensitivity list at hand, all filtered skipping configurations are evaluated on the complete test set according to their accuracy and execution time for deriving an approximation of the Pareto front (Step 4 in Figure 3). By ranging the number of skipped blocks ğ‘from zero (meaning no skipping, ğ‘† [1, 1, .., 1]) to ğ‘ ğµğ‘ (where all blocks are skipped, ğ‘† [0, 0, .., 0]), a initial set of skipping configurations is generated. These configurations are obtained by skipping the least sensible ğ‘blocks. Nevertheless, when evaluated, those configurations are not guaranteed to deliver Pareto operating points. That is, the sensitivity list regards blocks individually, only one is skipped at a time, and it does not evaluates their execution time. Therefore, in this step, the Pareto front is found by evaluating the accuracy versus inference time of the skipping configurations. For example, out of the 51 possible skipping configurations in the evaluated ResNet-110, 32 configurations made into the Pareto front.",
    "source": "2505.17626v1_Leveraging_Stochastic_Depth_Training_for_Adaptive_.pdf",
    "length": 2173,
    "tokens": 460
  },
  {
    "text": "M V LEN RLEN , N min(M, RLEN SEWo ) , K RLEN SEWi (3) Therefore, transposing B makes it possible to determine N by considering SEWo instead of both SEWo and SEWi. Because there is no equation in Formula 3 involving both SEWi and SEWo, the inequality SEWi SEWo does not restrict the full use of the vector register length. For 4 TABLE II 64-BIT CSR FOR THE MATRIX TILE EXTENSION Name Description Bits t[m,n,k] Tile dimension shapes 36 ttype[i,o] Input output matrix tile types 8 rlenb RLEN in Bytes 12 Reserved additional data 8 TABLE III MATRIX TILE EXTENSION INSTRUCTIONS Instruction Arguments Description tss[m,n,k] rd, rs1, ttypeio tile set shape [M,N,K] t{t}l[a,b,c,bt] vd, rs1, rs2 [A,B,C,BT ] tile {transposed} load t{t}sc vd, rs1, rs2 C tile {transposed} store t{f}{w}mul vd, vs1, vs2 tile {FP} {widening} dot product tvmask[a,b,c,bt] vd, rs1 set vector mask instance, an architecture targeting 32-bit matrix operations with V LEN 8192 and RLEN 512 describes a maximum matrix multiplication geometry of 16x16x16 in a uniform precision scenario with 32-bit data types. This scenario fully utilizes the vector register capacity (256 elements) on all operands, according to Formula 2. The same architecture executing mixed-precision operations with SEWo 32 and SEWi 16 describes a maximum geometry of 16x16x32, also using full vector registers capacity (256 output elements, 512 input elements), according to Formula 3. B. Control Status Registers MTE stores all the state defining the tile s geometry in a 64-bit Control Status Register (CSR) described in Table II. The tm, tn, and tk fields store the current hardware geometry settings for the M, N, and K dimensions with a maximum dimension size of 212 4096 elements.",
    "source": "2507.03522v1_A_Flexible_Instruction_Set_Architecture_for_Effici.pdf",
    "length": 1724,
    "tokens": 487
  },
  {
    "text": "The rationale behind it follows the MLPerf inference server standard [21] that defines a request as dropped if it arrives at a busy device. Therefore, in such cases, the runtime increases the number of skipped blocks by one (line 7). This way, the inference processing gradually moves to a faster skipping configurations in the Pareto Front. This adaptation mechanism only sacrifices accuracy when inference requests cannot be served due a lack of processing capabilities. As second outcome, when the device is free (else block in line 8), the runtime can feed the executable with the input and current skipping configuration. However, if the time elapsed since the last request is greater than a pre-defined value (Î”ğ‘Ÿğ‘’ğ‘), the inference processing is assumed to safely go to a slower (but of higher accuracy) configuration (i.e., decrease the number of skipped blocks - line 10). In our experiments, we set Î”ğ‘Ÿğ‘’ğ‘to the time taken by the zero-skipping model and ğ‘šğ‘–ğ‘›ğ‘ğ‘ğ‘to 10 below the accuracy of the original model. We also note that it is possible to incorporate more sophisticated methods in our solution to dynamically adapt the Î”ğ‘Ÿğ‘’ğ‘and ğ‘šğ‘–ğ‘›ğ‘ğ‘ğ‘parameters at runtime (e.g., with reinforcement learning methods). III. METHODOLOGY A. CNNs and Datasets We evaluate our approach on two datasets (CIFAR-10 [22] and CIFAR-100 [22]) and two residual models (ResNet-20 and ResNet-110). All input images follow CIFAR standard sizes of 32x32. All models are trained for 500 epochs. Learning rate starts at 0.1 for the first 250 epochs, 0.01 for the next 175, and 10 4 until the end. All models are trained in PyTorch on an Nvidia Geforce RTX 3080. Accuracy reports are given on TOP-1 accuracy. After training, all models follow the same compilation steps, they are exported to ONNX files and compiled by our modified IREE flow with the same default compiler flags. Skipping configurations are saved as binary files and deployed with the executables.",
    "source": "2505.17626v1_Leveraging_Stochastic_Depth_Training_for_Adaptive_.pdf",
    "length": 1938,
    "tokens": 484
  },
  {
    "text": "on Intelligent Transportation Systems (ITS), vol. 23, no. 11, 2022. [14] Opencorecan, 2017. [15] A. Pappalardo, Xilinx brevitas, 2023. [16] P. Cheng, K. Xu, S. Li, and M. Han, TCAN-IDS: Intrusion Detection System for Internet of Vehicle Using Temporal Convolutional Attention Network, Symmetry, vol. 14, no. 2, p. 310, 2022. [17] A. Anjum, P. Agbaje, S. Hounsinou, and H. Olufowobi, In-Vehicle Net- work Anomaly Detection Using Extreme Gradient Boosting Machine, in Proc. Med. Conf. on Embedded Computing (MECO), pp. 1 6, 2022. [18] S. B. Park, H. J. Jo, and D. H. Lee, G-idcs: Graph-based intrusion detection and classification system for can protocol, IEEE Access, vol. 11, pp. 39213 39227, 2023. [19] M. D. Hossain, H. Inoue, H. Ochiai, D. Fall, and Y. Kadobayashi, LSTM-based Intrusion Detection System for In-Vehicle CAN Bus Communications, IEEE Access, vol. 8, pp. 185489 185502, 2020. [20] S. Khandelwal and S. Shreejith, Exploring Highly Quantised Neural Networks for Intrusion Detection in Automotive CAN, in Proc. Intl. Conf. on Field-Programmable Logic and Applications (FPL), pp. 235 241, 2023. [21] S. Khandelwal and S. Shreejith, Real-time Zero-day Intrusion Detection System for Automotive Controller Area Network on FPGAs, in Proc. Intl. Conf. on Application-specific Systems, Architectures and Processors (ASAP), pp. 139 146, 2023.",
    "source": "2505.14924v1_SecCAN_An_Extended_CAN_Controller_with_Embedded_In.pdf",
    "length": 1348,
    "tokens": 440
  },
  {
    "text": "We systematically characterize resource utilization, latency implications, and energy demands inherent in the iterative execution patterns of AI agents. Our analysis highlights the critical system-level challenges faced when deploying AI agents and identifies opportunities for optimization through architectural improvements, enhanced inference algorithms, and intelligent resource allocation strate- gies. To the best of our knowledge, this work is the first to provide a system-level characterization of dynamic reasoning in AI agents, grounded in empirical analysis of end-to-end infrastructure behavior across diverse agentic workflows. A central contribution and key objective of our study is to quantitatively assess the AI infrastructure costs associated with dynamic reasoning deployments, and to inform and caution the broad research community about the urgent need for sus- tainable, efficient design principles to bridge the gap between advanced algorithmic capabilities and practical, scalable, and sustainable deployment. II. BACKGROUND AND MOTIVATION A. Definition of AI Agents AI agents are inference-time frameworks that extend the capabilities of large language models by enabling multi-step reasoning, adaptive decision-making, and interaction with the external environment. Unlike conventional LLM applications that produce a single output from a static prompt, AI agents operate through iterative internal reasoning and external ac- tions at inference time. At each iteration, the agent may generate an intermediate reasoning result, call an external tool, such as a search engine, calculator, or code interpreter, and incorporate the output into its subsequent decisions. This process allows the agent to refine its strategy, retrieve missing information, and revise its reasoning dynamically in response to evolving task demands. While this adaptivity enhances the agent s ability to handle complex and open-ended problems, it also leads to variability in the number of model calls, tool usage patterns, and overall computational cost. B. Core Components and Workflows of AI Agents As illustrated in Figure 2, AI agents generally consist of four core components (agent core, memory, plan, and tools), and AI agent workflows interconnect these core components through iterative interactions. These workflows orchestrate how the components dynamically collaborate, enabling the agent s adaptive behaviors. The 1 agent core is the central component responsible for advanced reasoning, powered by one or more LLMs Fig. 2: Overview of AI agent structure. configured in specific roles .",
    "source": "2506.04301v1_The_Cost_of_Dynamic_Reasoning_Demystifying_AI_Agen.pdf",
    "length": 2603,
    "tokens": 458
  },
  {
    "text": "Each permutation ğœ‹ Î  corresponds to one order of ablating the parameters from pğ‘ğ‘ğ‘ ğ‘’to pğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡, resulting in a different value for the incremental effect of modifying parameter ğ‘–. 11 A. Nasr-Esfahany et al. Specifically, define pğœ‹(ğ‘—) (pğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ ğœ‹1:ğ‘— ,pğ‘ğ‘ğ‘ ğ‘’ ğœ‹ğ‘— 1:ğ‘‘) to be the ğ‘—ğ‘¡â„microar- chitecture encountered in the ablation study based on order ğœ‹, i.e., parameters ğœ‹1,...,ğœ‹ğ‘—are set to their target values and the rest re- main at the baseline. Let ğ‘˜denote the position of parameter ğ‘–in the order ğœ‹. Then, the incremental effect of parameter ğ‘–in order ğœ‹is: Î”ğœ‹ ğ‘– ğ‘“(x,pğœ‹(k)) ğ‘“(x,pğœ‹(k-1)). To assign an overall attribution to parameter ğ‘–, we take the average over all permutations: ğœ‘ğ‘– 1 Î  ğœ‹ Î  Î”ğœ‹ ğ‘–, (8) where Î  ğ‘‘! is the total number of permutations. The quantity defined in Equation (8) is referred to as the Shapley value [78] in economics. The concept arises in cooperative game theory, where a group of ğ‘€players work together to generate value ğ‘£(ğ‘€). Shapley s seminal work showed that the Shapley value is a fair distribution of ğ‘£(ğ‘€) among the players, in that it is the only way to divide ğ‘£(ğ‘€) that satisfies certain desirable properties (refer to [78] for details).",
    "source": "2503.23076v1_Concorde_Fast_and_Accurate_CPU_Performance_Modelin.pdf",
    "length": 1166,
    "tokens": 479
  },
  {
    "text": "C2hlsc: Leveraging large language models to bridge the software-to- hardware design gap. arXiv preprint arXiv:2412.00214, 2024. [169] Seyed Arash Sheikholeslam and Andre Ivanov. Synthai: A multi agent generative ai framework for automated modular hls design generation. arXiv preprint arXiv:2405.16072, 2024. [170] Yuchao Liao, Tosiron Adegbija, and Roman Lysecky. Are llms any good for high-level synthesis? arXiv preprint arXiv:2408.10428, 2024. [171] Jiahao Gai, Hao Chen, Zhican Wang, Hongyu Zhou, Wanru Zhao, Nicholas Lane, and Hongxiang Fan. Exploring code language models for automated hls-based hardware generation: Benchmark, infrastructure and analysis. In Asia and South Pacific Design Automation Conference (ASP-DAC), 2025. [172] Xufeng Yao, Yiwen Wang, Xing Li, Yingzhao Lian, Ran Chen, Lei Chen, Mingxuan Yuan, Hong Xu, and Bei Yu. Rtlrewriter: Methodologies for large models aided rtl code optimization. arXiv preprint arXiv:2409.11414, 2024. [173] Pablo Antonio MartÃ­nez, Gregorio BernabÃ©, and JosÃ© Manuel GarcÃ­a. Code detection for hardware acceleration using large language models. IEEE Access, 2024. [174] Haocheng Xu, Haotian Hu, and Sitao Huang. Optimizing high-level synthesis designs with retrieval-augmented large language models. In LLM Aided Design Workshop (LAD), 2024. [175] Kiran Thorat, Jiahui Zhao, Yaotian Liu, Hongwu Peng, Xi Xie, Bin Lei, Jeff Zhang, and Caiwen Ding. Advanced large language model (llm)-driven verilog development: Enhancing power, performance, and area optimization in code synthesis. arXiv preprint arXiv:2312.01022, 2023. [176] YunDa Tsai, Mingjie Liu, and Haoxing Ren. Rtlfixer: Automatically fixing rtl syntax errors with large language models.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 1700,
    "tokens": 496
  },
  {
    "text": "It optimizes the code by hoisting data loads shared between kernels (reducing data movement beyond what is possible for the hardware FSM implementation), as well as utilizing fine-grained software pipelining and eliminating blocking operations where possible. This experiment highlights Autocomp s adaptability: we optimize a new benchmark, running on an accelerator with a new size and data type, with highly different performance characteristics from previous 8 Forward Pass Backward Pass 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 Speedup 1.0 1.0 3.0 2.4 1.9 2.4 SW Unopt Autocomp HW FSM Opt Figure 9: Speedup for fine-grained linear algebra benchmarks. Category Base Benchmark Reuse Targets Square 1024 1024 1024 512 512 512, 256 256 256 Column-dominant 12544 256x64 6272 256x64, 12544 128x64 Row-dominant 128 1024 1024 64 1024 1024, 32 1024 1024 Table 2: GEMM benchmarks for schedule reuse experiments. 0 100 200 300 400 500 600 700 Number of Samples 3 4 5 Speedup MM 512x512x512 Reuse MM 512x512x512 w o Reuse 0 100 200 300 400 500 600 700 Number of Samples 2 4 6 Speedup MM 12544 128x64 Reuse MM 12544 128x64 w o Reuse Figure 10: With the same sample budget, Autocomp with reuse (blue line) consistently delivers improved performance over Autocomp without reuse (orange line). experiments, by updating the Accelerator ISA and changing a few lines in the Optimization Menu and Instruction sections of the prompt. 5 Improving Sample Efficiency With Schedule Reuse In this experiment, we illustrate that optimization schedules generated by Autocomp for one tensor operation can be effectively reused to accelerate the optimization of similar tensor operations (as described in Sec. 3.5). For this experiment, we generate additional benchmarks structurally similar (either same aspect ratio or two shared dimensions) to those in Sec. 4.3. For each new benchmark, in the plan reuse phase, we search with beam size B 2, N 2 plan candidates, and K 2 code candidates. Once the reuse sequence is exhausted, we switch to a full Autocomp search with beam size B 6, N 6 plan candidates, and K 2 code candidates.",
    "source": "2505.18574v2_Autocomp_LLM-Driven_Code_Optimization_for_Tensor_A.pdf",
    "length": 2095,
    "tokens": 494
  },
  {
    "text": "The accuracy of both system might be identical (over a large number of test sets), yet for the current test case, both the models are not equally conï¬dent about the classiï¬cation. The question is, how do we measure the conï¬dence of the given classiï¬cation? It is obvious that the most conï¬dent classiï¬cation for the same class would be [1, 0, 0, 0], where the model is 100 conï¬dent on class o1 and the most confused prediction would be [0.25, 0.25, 0.25, 0.25], where the classiï¬er is equally confused between all the classes. Therefore, a good metric for the conï¬dence would be the vari- ance of the output probability vector. The higher the variance the more conï¬dent is the classiï¬cation. Towards this, we build a lookup table by averaging the variance of output vectors of multiple test cases. This table, which we call the conï¬dence matrix, gives us the conï¬dence of each sensor for each class, and can be used as a weight for majority voting. The next challenge is to adapt the conï¬dence matrix for individual users. Each user has unique expressions of behaviour classes reï¬‚ected in the sensor data. For example, gaits of two different people may signiï¬cantly vary, and might be entirely different from the training data. Thus, it is important to keep learning and adapting to the user behavior. Since, we cannot keep re-training the DNNs because of their resource constraints, we choose to periodically update the conï¬dence matrix. The initial conï¬dence matrix, derived from the test cases, would be programmed into the host device. Further, after each successful classiï¬cation, the sensors would send the conï¬dence score for that classiï¬er along with the output class. This conï¬dence score would further update the weight matrix of the host device using a moving average method and keep updating it as the user keeps using the device.",
    "source": "Origin.pdf",
    "length": 1842,
    "tokens": 476
  },
  {
    "text": "We also compare against two ideal approaches: (1) Fast-Only, and (2) Oracle [98] (See 3). Oracle serves as a reference to evaluate the accuracy of Harmonia s decisions as it makes optimal decisions based on knowledge of future access patterns. Table 3: Host System and HSS Configurations Host System AMD Ryzen 7 2700G[110], GHz, 8 64 32 KiB L1-I D, 4 MiB L2, 8 MiB L3, 16 GiB RDIMM DDR4 2666 MHz Storage Devices Characteristics H: Intel Optane SSD P4800X [47] 375 GB, PCIe 3.0 NVMe, SLC, R W: 2.4 2 GB s, random R W: 550000 500000 IOPS M: Intel SSD D3-S4510 [48] 1.92 TB, SATA TLC (3D), R W: 560 510 MB s, random R W: 895000 21000 IOPS L: Seagate HDD ST1000DM010 [108] 1 TB, SATA 6Gb s 7200 RPM Max. Sustained Transfer Rate: 210 MB s LSSD: ADATA SU630 SSD [111] 960 GB, SATA, TLC, R W: 520 450 MB s PMEM (Emulated): Intel Optane Persistent Memory 200 Series [112] 128 GB, Memory Mode R W: 7.45 2.25 GB s (256B) HSS Configurations Devices Performance-Optimized high-end (H) middle-end (M) Cost-Optimized high-end (H) low-end (L) HSS with PMEM Emulated PMEM (PMEM) high-end SSD (H) Tri-HSS high-end (H) middle-end (M) low-end (L) Quad-HSS high-end (H) middle-end (M) low-end SSD (LSSD) low-end HDD (L) Workloads. We select seventeen data-intensive storage workloads from SYSTOR 17 [113], RocksDB traces [114], Yahoo! Cloud Serv- ing Benchmark (YCSB) suite [115], and MLPerf Storage3 [116] from real enterprise and datacenter environments. The average workload size is approximately 50000x the fast device capacity.",
    "source": "2503.20507v2_Harmonia_A_Multi-Agent_Reinforcement_Learning_Appr.pdf",
    "length": 1512,
    "tokens": 490
  },
  {
    "text": "Krizhevsky, A., Sutskever, I., Hinton, G.E. : ImageNet Classification with Deep Convolutional Neural Networks. Communications of the ACM 60(6), 84 90 (May 2017) 12. Kurtic, E. et al. : Sparse Fine-tuning for Inference Acceleration of Large Language Models (2023) 13. Mishra, A. et al. : Accelerating Sparse Deep Neural Networks (2021) 14. Parashar, A. et al. : SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks. In: Proceedings of the 44th Annual International Symposium on Computer Architecture. p. 27 40. ISCA 17, Association for Computing Machinery, New York, NY, USA (2017) 15. Paszke, A. et al. : PyTorch: An Imperative Style, High-Performance Deep Learning Library. In: Advances in Neural Information Processing Systems. vol. 32. Curran Associates, Inc. (2019) 16. Simonyan, K., Zisserman, A.: Very Deep Convolutional Networks for Large-Scale Image Recognition (2015) 17. Soltaniyeh, M., Martin, R.P., Nagarakatte, S.: An Accelerator for Sparse Convolu- tional Neural Networks Leveraging Systolic General Matrix-matrix Multiplication. ACM Transactions on Architecture and Code Optimization 19(3), 1 26 (May 2022) 18. Szegedy, C. et al. : Going Deeper with Convolutions. In: 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). p. 1 9 (Jun 2015) 19. Wen, W. et al. : Learning Structured Sparsity in Deep Neural Networks. In: Ad- vances in Neural Information Processing Systems. vol. 29. Curran Associates, Inc. (2016)",
    "source": "2506.01566v1_FlexiSAGA_A_Flexible_Systolic_Array_GEMM_Accelerat.pdf",
    "length": 1464,
    "tokens": 410
  },
  {
    "text": "Even scaled-down variants like MobileBERT (25.3M parameters) [18] are orders of magnitude too large for deployment on microcontrollers with less than 2 MB memory budgets. To fill this gap, we introduce EmbBERT-Q, a Tiny Language Model (TLM), specifically designed to operate on tiny devices, and under the stringent 2 MB memory budget. In particular EmbBERT-Q, comprises a novel TLM architecture optimized for microcontroller units and other resource-constrained devices. Using techniques such as hardware-compatible 8-bit quantization [19], EmbBERT-Q achieves SotA performance with only 781 kB of memory, a 25 reduction in memory with respect to the SotA models characterized by the smallest memory demands, such as BERT-Tiny [20]. Even when compared with other models specifically adapted to work within the 2 MB constraints, EmbBERT-Q resulted to be both the most effective and the most efficient model. Our contributions include: 1. EmbBERT-Q: We propose a new TLM model specifically designed for tiny devices, combining efficiency and effectiveness. 2. Memory and Computational Analysis: We analytically evaluate the memory usage and compu- tational complexity of EmbBERT-Q and its components, providing a useful tool to evaluate the weights and activations memory trade-offs required to operate within tiny device constraints. 3. Custom Benchmark: We design a specialized benchmark tailored to assess the NLP capabilities of TLMs, enabling consistent and fair evaluations in resource-constrained environments. The remainder of this paper is organized as follows. Section 2 reviews recent work on model compression and training techniques tailored for resource-constrained platforms, setting the stage for our contributions. Section 3 provides a detailed description of EmbBERT-Q, our proposed model for NLP in tiny devices, and includes precise calculations of memory requirements for its layers. In Section 4, we outline the experimental setup, including the training procedures and datasets used to validate our approach. Section 5 presents a comprehensive evaluation of our model on the TinyNLP benchmark suite and GLUE datasets, comparing downscaled versions of BERT and MAMBA, and highlighting the significant performance improvements achieved by EmbBERT-Q. Section 6 delves into an ablation study to assess the individual contributions of the architectural modules within EmbBERT- Q to its overall performance.",
    "source": "2502.10001v1_EmbBERT-Q_Breaking_Memory_Barriers_in_Embedded_NLP.pdf",
    "length": 2422,
    "tokens": 494
  },
  {
    "text": "In Proceedings of the AAAI conference on artificial intelligence. Siyuan Zheng, Zhi Yang, Cedric Renggli, Yuxiang Pu, Zixuan Li, Mohammad Shoeybi, Lin Zhang, Dheevatsa Narayanan, Haotian Zhao, Zhewei Yao, and Tianqi Chen. 2023. vllm: A high-throughput and memory-efficient inference engine for llms. GitHub repository. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient fine- tuning of 100 language models. arXiv preprint arXiv:2403.13372. 11 4k 8k 16k 500k 1M 1.3M 1 2 4 8 float32 bfloat16 int8 int4 75 80 85 90 95 100 Accuracy ( ) 77.58 83.03 87.88 87.88 93.94 98.18 98.18 98.18 98.79 99.39 98.18 98.18 96.96 93.94 Context Window Data Size Beams Quantization Figure 5: Impact of scaling and quantization on Qwen2.5-Coder 1.5B variant evaluated using the code coverage metric on HumanEval with -O0 compiler optimization. A Appendix A.1 Extra Data Analysis Scaling and quantization effect on Qwen2.5- coder models. Figure 5 represents an study to understand where most of the training benefit for our transpiler originates. In particular, we focus on three fundamental modeling aspects and describe their impact on the asm-to-asm transpiler. Our first and most significant result relates to the context window size, and its impact on the transpiler. Recall that a model s context window is the amount of text, in tokens, that the model can consider or remember at any one time. We found that pro- grams do not fully fit in the context window (which includes both the input and output of the model, i.e., the x86 asm and the generated ARM asm), are very likely to not pass all our tests.",
    "source": "2506.14606v1_Guaranteed_Guess_A_Language_Modeling_Approach_for_.pdf",
    "length": 1672,
    "tokens": 459
  },
  {
    "text": "C. Model Performance We experimented with six models. Among them, four are MobileNetV1 [49] based autoencoder models with width mul- viii TABLE II: Architecture of the models. Stage Type Stride M N Output Size Encoder Conv (3 3) s2 1 32 48 50 32 Conv dws (3 3) s1 32 64 48 50 64 Conv dws (3 3) s2 64 128 24 25 128 Conv dws (3 3) s1 128 128 24 25 128 Conv dws (3 3) s2 128 256 12 13 256 Conv dws (3 3) s1 256 256 12 13 256 Conv dws (3 3) s1 256 512 12 13 512 5 Conv dws (3 3) s1 512 512 12 13 512 Conv dws (3 3) s2 512 1024 6 7 1024 Conv dws (3 3) s1 1024 1024 6 7 1024 Avg Pool (6 7) s1 1024 1024 1 1 1024 Decoder ConvTranspose dw (6 7) s1 1024 1024 6 7 1024 ConvTranspose (3 3) s1 1024 1024 6 7 1024 ConvTranspose (3 3) s2 1024 512 12 13 512 5 ConvTranspose (3 3) s1 512 512 12 13 512 ConvTranspose (3 3) s1 512 256 12 13 256 ConvTranspose (3 3) s1 256 256 12 13 256 ConvTranspose (3 3) s2 256 128 24 25 128 ConvTranspose (3 3) s1 128 128 24 25 128 ConvTranspose (3 3) s2 128 64 48 50 64 ConvTranspose (3 3) s1 64 32 48 50 32 ConvTranspose (3 3) s2 32 1 96 100 1 (a) MobileNetV1-CAE(1x) model.",
    "source": "2504.06996v1_Neural_Signal_Compression_using_RAMAN_tinyML_Accel.pdf",
    "length": 1094,
    "tokens": 377
  },
  {
    "text": ". , qk} be a set of QuantaTasks with in- dividual energy requirements Eqi. If P i Eqi Eb, the available energy budget, then tasks can be executed sequentially without interruption. However, if P i Eqi Eb, we aim to fuse tasks to minimize checkpointing overhead. Task fusion is formalized as finding a partition of Q into subsets Q1, Q2, . . . , Qm such that, for each subset Qj, P qi Qj Eqi Eb, and m is minimized. This reduces the number of checkpoints and the overhead associated with task switching. For example, Consider two convolution operations C1 and C2 with energy requirements EC1 and EC2, respectively. If individually EC1, EC2 Eb but EC1 EC2 Eb, we fuse C1 and C2 into a single task. The fused task executes both convolutions atomically within the energy budget, avoiding the overhead of checkpointing between them. Scheduling Problem Formulation: The scheduling problem is formulated with decision variables si (task start times) and binary variables xi {0, 1} (indicating whether a task is scheduled). The energy availability constraint over time is expressed as (subject to energy and task constraints): P i:si t fi Ei Eb(t) The objective is to maximize the total weighted priority of scheduled tasks: max {xi,si} N X i 1 pi Î±Ei Î²(fi Di) xi. (5) Scheduling Performance Assurance: Our scheduling heuristic, Energy-Aware Priority Scheduling, while sub-optimal in the theoretical sense, is designed to perform near-optimally in practice for real- time systems. We ensure its performance by: 1. Empirical Validation: We compare the heuristic s 5 performance with the optimal solution on smaller problem instances using exhaustive search and find that the heuristic achieves within 95 of the optimal task completion rate. 2. Theoretical Analysis: The heuristic prioritizes tasks based on effective priority P eff i pi Ei Ï•i, where Ï•i accounts for deadline urgency. This balances task importance against energy consumption, leading to efficient utilization of available energy. 3. Complexity Analysis: The heuristic has a time complexity of O(N log N) due to sorting tasks based on P eff i , which is acceptable for real-time applications.",
    "source": "NexUME.pdf",
    "length": 2148,
    "tokens": 494
  },
  {
    "text": "In the LFSR version of the master-slave PRNG cluster, the master PRNG will generate and set the seeds for the slave PRNG after 2LLF SR clock cycles, where LLF SR is the length of LFSR. One DSP slice can only afford 16-bits Mid-square based PRNG, hence in this accelerator, LFSR is used as the slave PRNG in the PRNG cluster. The processor supplies and programs the master PRNG seed in real time. D. DTM Control and Data Flows: a) Programming: Before inference or training, the configuration data including the TM type, feature number, clause number and class number are sent to the accelerator, allowing it to calculate clause and weight compute rounds. Upon receiving these data, the necessary clauses and weight masks are derived, as outlined Fig. 9: Accelerator Timing: a) timing diagram for CoTM inference; b) timing diagram for Vanilla TM inference; c) timing diagram for DTM training (same procedure for both Vanilla and CoTM). in IV.For training, hyperparameters like the TA state update probability 1 s and threshold t are provided. The accelerator calculates 1 s, stores t, and initializes the TA states and weights in RAM using PRNGs once the model size is configured. The programming data (model size, PRNG seeds, and hyperpa- rameters) are sent via an AXI-stream channel. The execution mode (inference or training) and the target class index are embedded within the feature data stream. Fig. 10a shows the AXIS integration and how the instruction and feature data flow through the system for the inference and training processes. b) Class Sum Computation: During training, only the target class sum is computed; in inference, class sums for all classes are calculated. The feature data are stored when received; the system uses this to identify the mode (train- ing inference), and loads the target class if in training mode. The TA RAM controller allocates the start clause slice, and the Clause Matrix loads TA actions and literals to compute clauses in block 1. Inference is pipelined: When one class of clause computation completes and the weight computation matrix is idle, the weight RAM controller allocates the weight slice, loading weights, and clauses to compute class sum in block 2. In inference-only mode, once classes are processed, a new data point is handled.",
    "source": "2504.19797v1_Dynamic_Tsetlin_Machine_Accelerators_for_On-Chip_T.pdf",
    "length": 2287,
    "tokens": 496
  },
  {
    "text": "Dfx: A low-latency multi-fpga appliance for accelerating transformer-based text generation. In MICRO, 2022. [12] Minxuan Zhou, Weihong Xu, Jaeyoung Kang, and Tajana Rosing. Transpim: A memory-based acceleration via software-hardware co-design for transformer. In HPCA, 2022. [13] Jaewan Choi, Jaehyun Park, Kwanhee Kyung, Nam Sung Kim, and Jung Ho Ahn. Un- leashing the potential of pim: Accelerating large batched inference of transformer- based generative models. In IEEE CAL, 2023. [14] Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longwriter: Unleashing 10,000 word generation from long context llms. In ICLR, 2025. [15] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A distributed serving system for transformer-based generative models. In OSDI, 2022. [16] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gula- vani, and Ramachandran Ramjee. Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills. arXiv preprint arXiv:2308.16369, 2023. [17] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Inigo Goiri, Saeed Maleki, and Ricardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting. In ISCA, June 2024. [18] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In ICML, 2023. [19] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Lau- rent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling.",
    "source": "2502.15470v2_PAPI_Exploiting_Dynamic_Parallelism_in_Large_Langu.pdf",
    "length": 1598,
    "tokens": 487
  },
  {
    "text": "For instance, constructing a DLRM system with TBs of parameters would require dozens of NVIDIA H100 GPUs, each equipped with 80 GB of HBM, resulting in enormous system costs. The second issue is the underutilization of GPU compute resources for embedding- dominant DLRM applications, leading to inefficiency in the entire system. Lastly, multiple GPU nodes must be intercon- nected to run the entire DLRM system at TB scale in parallel, leading to increased network complexity and overhead. The overall DLRM performance degradation is inevitable due to complex network configuration among GPU nodes, with data communication overhead among them. To be precise, all-to- all communication is required during model inference, where each GPU requests embedding vectors from different GPUs, causing significant system bottlenecks. To mitigate these challenges, recent work [19] has statisti- cally analyzed DLRM characteristics and proposed a sharding technique that stores frequently accessed embedding vectors in the GPU s HBM, while placing rarely accessed embed- ding vectors in the host s DRAM. This approach effectively maintains a comparable system memory bandwidth while reducing the required number of GPUs to run the DLRM. However, challenges such as high system cost, computing resource underutilization, and complexity of network topology still persist in the GPU-based DLRM systems. To address these challenges, we propose SCRec, a scalable computational storage system that can handle TB-scale indus- trial DLRMs while providing high bandwidth requirements. SCRec leverages SmartSSD devices, which integrate an SSD with a field-programmable gate array (FPGA) for near-data processing (NDP), effectively utilizing the SSD s large capac- ity alongside the high bandwidth of DRAM and block RAM (BRAM) on the FPGA. The detailed contributions of our work are as follows. SCRec utilizes a software framework with two pri- mary features statistical sharding and adaptive accel- eration core mapping leveraging a mixed-integer pro- gramming (MIP)-based cost model. By analyzing data EMB_1 EMB_N Embedding Layer Input Data Prediction Feature Interaction Layer Bottom MLP Emb. Vector Emb. Vector Emb. Vector Pooling Pooling Pooling Sparse Features View History ... Dense Features Region Age ... Top MLP Emb. Vector Pooling EMB_3 EMB_2 Item Rate Fig. 2. Architecture of deep learning recommendation model.",
    "source": "2504.00520v1_SCRec_A_Scalable_Computational_Storage_System_with.pdf",
    "length": 2402,
    "tokens": 499
  },
  {
    "text": "1 6, 2020. [18] Pingakshya Goswami, Benjamin Carrion Schaefer, and Dinesh Bhatia. Machine learning based fast and accurate high level synthesis design space exploration: From graph to synthesis. Integration, 88:116 124, 2023. [19] Huiliang Hong, Chenglong Xiao, and Shanshan Wang. Rethinking high-level synthesis design space exploration from a contrastive perspective. In 2024 IEEE 42nd International Conference on Computer Design (ICCD), pages 179 182. IEEE, 2024. [20] Nan Wu, Yuan Xie, and Cong Hao. Ironman-pro: Multiobjective design space exploration in hls via reinforcement learning and graph neural network-based modeling. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 42(3):900 913, 2023. [21] Yuan Yao, Huiliang Hong, Shanshan Wang, and Chenglong Xiao. Decomposition based estimation of distribution algorithm for high-level synthesis design space exploration. Integration, 100:102292, 2025. [22] Md Imtiaz Rashid and Benjamin Carrion Schafer. Fast and inexpensive high-level synthesis design space exploration: Machine learning to the rescue. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 42(11):3939 3950, 2023. [23] Zheyuan Zou, Cheng Tang, Lei Gong, Chao Wang, and Xuehai Zhou. Flexwalker: An efficient multi-objective design space exploration framework for hls design. In 2024 34th International Conference on Field-Programmable Logic and Applications (FPL), pages 126 132, 2024. [24] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [25] Petar VeliË‡ckovi c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. [26] Shengcai Liu, Caishun Chen, Xinghua Qu, Ke Tang, and Yew-Soon Ong. Large language models as evolutionary optimizers.",
    "source": "2504.19649v2_Intelligent4DSE_Optimizing_High-Level_Synthesis_De.pdf",
    "length": 1897,
    "tokens": 495
  },
  {
    "text": "For example, many of the European cities restrict the traffic video data to be streamed to the cloud (www.dlapiperdataprotection.com; Achieving Compliant Data Residency and Security with Azure; Bhardwaj et al., 2022), which enforces performing video Preprint. Under review. arXiv:2410.05435v1 [cs.AR] 7 Oct 2024 Classification Results and Actions Update Model Finetuning Exemplar Selection Inference Model Stacked Encoder Stacked Decoder Compressed Data Quantum-safe Encryption Archival Storage (HDD) Computational Storage FPGA Comptinuous Learning Compute Video Source Neural Encoder Frame Loss Figure 1: Data flow pipeline of continuous learning edge servers with storage and data archival pipeline. The Shown storage pipeline is the preliminary focus of Salient Store . analytics and learning tasks related to urban mobility at the edge. This has lead to a significant development in the direction of enabling video analytics and learning with edge servers. Although efficient algorithms, compute orchestration and hardware have addressed the analytics part, the scaling of such a system becomes a problem primarily due to high energy consumption. Recent works try to solve this problem by augmenting these continuous learning edge servers with application-specific hardware targeted for intermittent computing which could run using solar power. However, all these works focus on the analytics part while overlooking one critical aspect: what happens to all those video data after the analytics? Data Archival: The answer is straightforward, especially for mission-critical public records like urban mobility and surveillance data: these need to be archived in a local storage to avoid under- mining the benefits of edge computation, i.e. minimizing communication and preserving privacy. However, managing such data requires substantial storage infrastructure. For instance, storing a full day s worth of 1080p video at 60fps requires (1920 1080 3 pixels 4 bytes per pixel 60 frames per second 3600 24)117.32 TiB of raw video data, which compresses to approximately 60 GiB to 400 GiB of encoded data per day. Including redundancy, this requires an additional 33 to 100 more storage capacity. Furthermore, given the plug-and-play nature of storage media like HDDs and SSDs, securing this data becomes even more critical. The typical archival process involves three key phases: compression, encryption, and redundancy. Fig.",
    "source": "SaLT.pdf",
    "length": 2424,
    "tokens": 481
  },
  {
    "text": "Text encoders are pruned more aggressively than vision encoders. We observe that, in general, text encoders are pruned more aggressively than vision encoders. This is because vision models have a greater impact on accuracy, as highlighted in Appendix A. We observed that pruning any dimension across the vision encoder resulted in more significant accuracy loss compared to its text encoder counterpart. This is because vision models process higher-dimensional inputs, more aggressive pruning 9 Table 3 The hardware and model architecture properties of each variant of the CarbonCLIP family. Hardware configurations are specified as: {TC, PEx, PEy, L2, L2bw, GLB}. Text and Vision encoders are specified as: {Num Layers, FFN Dim, Hidden Dim, Num Heads}. Name Carbon (kgCO2e) Latency (ms) Hardware Configuration Model Configuration Avg.",
    "source": "2505.01386v2_Carbon_Aware_Transformers_Through_Joint_Model-Hard.pdf",
    "length": 835,
    "tokens": 185
  },
  {
    "text": "[Only the core implementation is shown below due to space constraints] case (i_opcode) 3'b000: ADD o_result i_operand_a i_operand_b; 3'b001: SUB o_result i_operand_a - i_operand_b; 3'b010: MUL o_result i_operand_a i_operand_b; 3'b011: AND o_result {{4'b0},i_operand_a i_operand_b}; 3'b100: OR o_result {{4'b0},i_operand_a i_operand_b}; 3'b101: NOT A o_result {{4'b0}, i_operand_a}; 3'b110: XOR o_result {{4'b0},i_operand_a i_operand_b}; 3'b111: XNOR o_result {{4'b0}, (i_operand_a i_operand_b)}; endcase Golden Solution [Only the core implementation is shown below due to space constraints] case (i_opcode) 3'b000: ADD o_result i_operand_a i_operand_b; 3'b001: SUB o_result i_operand_a - i_operand_b; 3'b010: MUL o_result i_operand_a i_operand_b; 3'b011: Failed to account for different bitwidths between input and output o_result i_operand_a i_operand_b; 3'b100: OR o_result i_operand_a i_operand_b; 3'b101: NOT A o_result i_operand_a; 3'b110: XOR o_result i_operand_a i_operand_b; 3'b111: XNOR o_result (i_operand_a i_operand_b); endcase LLM-Generated Incorrect Solution Figure 5: A failure case on ALU implementation. should be explicitly cleared so that downstream logic can rely on deterministic, intentionally driven zeros. The golden solution makes that intent explicit with {{4 b0}, ...} concatenations.",
    "source": "2506.14074v1_Comprehensive_Verilog_Design_Problems_A_Next-Gener.pdf",
    "length": 1311,
    "tokens": 497
  },
  {
    "text": "over SA 89.34 LLMACO(deepseek-r1) Improv. over GA 85.90 LLMACO(deepseek-r1) Improv. over ACO 88.07 In the experiments, we employ four foundation models (deepseek-r1 [38], gpt-4o [39], gpt-4.1 [40], o3-mini [41]) as surrogate models. In total, we evaluate 12 combinations of LLMMH (3 meta-heuristics 4 LLMs). As shown in Table 7, metaheuristics (GA, SA, ACO) achieve ADRS exceeding 0.5 on the mvt benchmark with large design space, aligning with our theoretical analysis of their exploration limitations. In contrast, our LLMMH framework demonstrates superior performance LLMACO(deepseek-r1) achieves ADRS as low as 0.0339. On small-scale benchmarks such as atax, LLMMH variants maintain comparable solution quality with meta-heuristic methods. Comprehensive experimental results demonstrate that LLMACO(deepseek-r1) achieves the optimal performance with the lowest ADRS, showing improvements of 89.34 , 85.90 , and 88.07 over SA, GA, and GA respectively. While LLMMH algorithms exhibit marginally inferior performance compared to traditional metaheuristics on small-scale design space benchmark due to LLMs inherent hallucination issues (a longstanding challenge in NLP where models generate semantically plausible but technically invalid outputs), holistic evaluation reveals the framework s superior overall performance across metrics. This demonstrates LLMMH s effectiveness in balancing exploration-exploitation tradeoffs despite occasional localized solution quality variations. This quantitatively verifies that our LLMMH framework not only resolves inherent limitations of meta-heuristics but also substantially outperforms them in solution quality. Fig. 7 illustrates convergence curves of high-performing LLMMH variants versus GA, SA, and EA. It can be observed that, for benchmarks with small design spaces, all algorithms demonstrate comparable convergence to Pareto-approximate optimal solutions within limited iterations. However, this comparative analysis quantitatively validates our core hypothesis: traditional meta-heuristics fundamentally underperform in exploring superior configurations under strict iteration budgets for large design spaces, whereas LLM-augmented operators effectively mitigate this limitation through intelligent solution generation, as evidenced by their accelerated convergence trajectories.",
    "source": "2504.19649v2_Intelligent4DSE_Optimizing_High-Level_Synthesis_De.pdf",
    "length": 2333,
    "tokens": 494
  },
  {
    "text": "macro into P components (named as sub-macros), each of dimension X P Y 8. To seamlessly support diverse N:M sparsity patterns with this architecture, we introduce two novel hardware components, namely the distribution and the merging units to orchestrate the working of the P sub-macros. To enable the bit-serial computation, a local input activation (iAct) buffer feeds the activations to the distribution unit (detailed later) at a rate of X 8-bit activations per cycle, which is serialized via the iAct serializer unit in each sub- macro [10], [32]. We employ a two-tier control unit comprising a global controller and column-wise controller to generate control signals that manage pipelining across rows and columns of sub- macros and configure multiplexer select signals. In our demonstrated implementation, we assume X 128,Y 32 for the DCiM bit-cell macro, with partition size of P 4, resulting in an 8Kb sub-macro of dimensions 32 32 8. Each sub-macro column is divided into 32 banks to enable parallel MAC operation. A. Sub-Macro Architecture Memory cell. Figure 4(c), presents a memory word in FlexCiM, with each word composed of 8 bit-cells. To abstract the circuit complexity and demonstrate our proposal to be orthogonal to any SRAM technology, we follow [17] and implement a 28nm, standard cell, latch-based memory structurally resembling a 6T SRAM cell [35]. Each memory cell has two bit-lines BL, BL, shared by all cells in a row, and a word-line (WL), shared per column. All memory cells in a column share a control signal to enable the compute units, namely EN COL. Inspired by [35], we implement a 2:1 multiplexer that is shared by a memory word to select one of the two iActs (via control signal (isel) streamed along the two bit-lines to enable 1:2 structured sparsity as a baseline case. A bit-serial multiplier within each SRAM cell (implemented as NOR gate) performs the multiplication between the selected iAct and stored weight. The memory cell has two modes of operation: memory (MS) and compute (C) mode. Standard read and write operations are performed in MS mode.",
    "source": "2504.14365v1_Accelerating_LLM_Inference_with_Flexible_NM_Sparsi.pdf",
    "length": 2092,
    "tokens": 487
  },
  {
    "text": "arXiv:2506.16903v1 [cs.AR] 20 Jun 2025 1 RCNet: Î£ IADCs as Recurrent AutoEncoders Arnaud Verdant, William Guicquero and J erË†ome Chossat Abstract This paper proposes a deep learning model (RCNet) for Delta-Sigma ( Î£) ADCs. Recurrent Neural Networks (RNNs) allow to describe both modulators and filters. This analogy is applied to Incremental ADCs (IADC). High-end optimizers combined with full-custom losses are used to define additional hardware design constraints: quantized weights, signal saturation, temporal noise injection, devices area. Focusing on DC conversion, our early results demonstrate that SNR defined as an Effective Number Of Bits (ENOB) can be optimized under a certain hardware mapping complexity. The proposed RCNet succeeded to provide design tradeoffs in terms of SNR ( 13bit) versus area constraints ( 14pF total capacitor) at a given OSR (80 samples). Interestingly, it appears that the best RCNet architectures do not necessarily rely on high-order modulators, leveraging additional topology exploration degrees of freedom. Index Terms Incremental Analog to Digital Converter, Delta-Sigma, Recurrent Neural Networks, Compter-Aided Design. I. INTRODUCTION The emergence of versatile deep learning frameworks is leading to the rise of data-driven optimization techniques. The human expertise therefore consists in stating the problem, defining the search space and setting input-output mappings. It also focuses on the computational graph, loss functions and regularization terms. This article deals with Î£ IADC architectures, relying on this hardware-aware design approach. Arnaud Verdant, William Guicquero are with CEA-LETI, Univ. Grenoble Alpes, France J erË†ome Chos- sat is with STMicroelectronics, Grenoble, France. This work is part of the IPCEI Microelectronics and Connectivity and was supported by the French Public Authorities within the frame of France 2030. 2 A. RCNet: the proposed RAE-IADC analogy The presented RCNet methodology relies on the analogy between an IADC based on Î£ modulation [1] [4] and a Recurrent AutoEncoder (RAE) (see Fig. 1).",
    "source": "2506.16903v1_RCNet_Î”Î£_IADCs_as_Recurrent_AutoEncoders.pdf",
    "length": 2085,
    "tokens": 493
  },
  {
    "text": "DIFFERENTIABLE SIMULATION As we describe in detail in Appendix A.3.3, there exist a variety of special purpose field solvers exploiting the ad- joint method dating back decades (Georgieva et al., 2002). There also exist a handful of more general purpose full- wave solvers (Hughes et al., 2018b; 2019) using early research-grade automatic differentiation tools (Maclaurin et al., 2015), and several excellent modern special-purpose tools for specific electromagnetics applications (Laporte et al., 2019). We developed a generic differentiable field solver in Jax (Bradbury et al., 2018), with support for hardware accel- eration on GPU, custom sparse numerical linear algebra primitives with correct sparse vector-Jacobian product im- plementations, batched simulations (varying the data but holding the hardware wave operator fixed), and JIT com- piled simulation to amortize overhead involved in multiple successive simulations. 2.3.3. INVERSE DESIGN OF OPTICAL CLASSIFIER The aggregate forward computational model with parame- ters Î¸ : (Î¸d, Î¸g) of Equation (11) is denoted with: Ë†f(x; Î¸) : D(x; Î¸d)R(A(Î¸g) 1b(x)). (13) 5 Nonlinear Computation with Linear Optics via Source-Position Encoding In multiclass classification, we are given a collection of data D : {(xi, ci)}N i 1 comprised of inputs xi with labels ci. Then we aim to locally solve the following optimization problem, minimizeÎ¸ N X i 1 L(ci, Softmax( Ë†f(xi; Î¸))), (14) with an appropriate objective (e.g., cross-entropy) L. We use gradient based optimization (specifically, Adam (Kingma Ba, 2014)), exploiting FMC-based topology op- timization and differentiable simulation to obtain the deriva- tives with respect to the material distribution parameters Î¸g, and conventional automatic differentiation for the deriva- tives of the decoder parameters Î¸d with respect to the objec- tive. 3.",
    "source": "2504.20401v1_Nonlinear_Computation_with_Linear_Optics_via_Sourc.pdf",
    "length": 1852,
    "tokens": 478
  },
  {
    "text": "REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) 1 Abstract In electronic design, engineers often manually search through extensive documents to retrieve component parameters required for constructing SPICE models, a process that is both labor-intensive and time-consuming. To address this challenge, we present an automated framework called D2S-FLOW that leverages large language models (LLMs) to extract electrical parameters from datasheets and generate SPICE models with high precision and efficiency, significantly reducing the need for manual intervention. Unlike traditional RAG systems, D2S- FLOW employs a workflow to enhance precision in handling unstructured documents and inconsistent naming conventions through three innovative mechanisms: Attention-Guided Document Focusing (AGDF), Hierarchical Document-Enhanced Retrieval (HDER), and Heterogeneous Named Entity Normalization (HNEN). AGDF narrows retrieval to user-selected documents, HDER utilizes document structure for precise parameter localization, and HNEN standardizes terminology via semantic inference. Experimental results demonstrate that the framework achieves an Exact Match (EM) of 0.86, an F1 score of 0.92, and an Exact Correctness (EC) of 0.96, outperforming the strongest baseline by 19.4 , 5.7 , and 13.1 , respectively. Additionally, it reduces API token consumption by 38 and minimizes the irrelevant information ratio to 4 , showcasing substantial improvements in resource efficiency. This research provides an effective automated solution for circuit design. Index Terms Electronic Design Automation, Large Language Model, Chip Datasheets, Parameter Extraction, SPICE Models. I. INTRODUCTION HE rapid advancement of LLMs has significantly propelled the field of natural language processing (NLP), particularly in tasks such as document retrieval and question answering [1], [2]. These models exhibit immense potential for enhancing electronic design automation EDA. LLMs have demonstrated diverse applications within EDA. For example, LLM4EDA [3] automates multiple steps in the EDA process by combining text and multimodal data such as circuit diagrams and code, significantly simplifying the design workflow. ChatEDA [4] further showcases how LLMs can generate processor design scripts and improve EDA efficiency by verifying accuracy This work is funded by, China Postdoctoral Science Foundation (No.",
    "source": "2502.16540v2_D2S-FLOW_Automated_Parameter_Extraction_from_Datas.pdf",
    "length": 2425,
    "tokens": 490
  },
  {
    "text": "While the weight granularity is coarser than that of the partial-sums in cases (ii) and (iv), both share column-wise granularity in cases (i) and (iii). The training delay between weights and partial-sums in case (iii) causes the weights to become overly tuned to full-precision partial-sums during the first stage, hindering performance in the second stage. Moreover, the star marks indicate that case (i) achieves the highest accuracy of the case (ii) with 8.61 less training cost, further validating our approach that the granularity alignment at the column-wise level ensures a more straightforward and efficient training process. E. Evaluation of Variation Robustness Non-idealities in nonvolatile memory, such as device varia- tions, cause accuracy degradation in CIM accelerators. In this section, we conducted a variation analysis on models using our proposed quantization scheme and those of related works. As described in [11], memory device variations are modeled by a log-normal distribution with a mean of zero. To assess 0.00 0.05 0.10 0.15 0.20 0.25 Variation Standard Deviation 55 60 65 70 75 80 85 90 Inference Accuracy( ) Ours Saxena [9] Saxena [8] Bai [6], [7] Kim [5] Fig. 10. Inference accuracy across different standard deviations of memory cell variation, comparing our quantization scheme with those of related works. robustness, we introduced log-normal noise to the weights as follows, wvar w eÎ¸, (5) where Î¸ is the noise, which follows a normal distribution with a zero mean, w is the ideal weight, and wvar is the weight after variation. We evaluated inference accuracy across various standard deviations, as illustrated in Fig. 10. The result shows that models trained with our column-wise quantization method consistently achieve higher inference accuracy across all levels of variation, outperforming other quantization schemes. This gap highlights the robustness of our column-wise quantization approach in preserving model performance under hardware- induced variations, delivering both higher accuracy and greater resilience to memory cell variations. V. CONCLUSION We propose an innovative quantization strategy that aligns weight granularity with partial-sums at the column-wise level.",
    "source": "2502.07842v2_Column-wise_Quantization_of_Weights_and_Partial_Su.pdf",
    "length": 2221,
    "tokens": 481
  },
  {
    "text": "The code provided previously has syntax error. The incorrect module is: Module NAME The syntax error message 1 is: Syntax Error Message 1 from Syntax Checker The incorrect code line 1 is: Context of related incorrect code line Please correct the code of this module. Please give me the complete code of this module. Please only provide code, no explanation required. Regulated Feedback Template Fig. 5. An example of a template for regulated feedback prompting. C. Phase II: Performance and Power Forecasting from Verilog Code In Phase II, the Verilog code generated from Phase I is utilized for performance and power forecasting. In general, the code should have no syntax errors. On the other hand, our method tolerates limited functional errors, i.e., even if the code cannot be synthesized to correct circuits, it can still be applied to provide reasonable performance power estimates. Several previous works have attempted to make performance and power predictions based on Verilog code [29] [30] [31] [33], and we adopt the approach from [29] with one modification. The models of [29] are trained on post-placement analysis data while the models used by Lorecast are trained on post-routing analysis data. Thus, Lorecast is expected to provide a more accurate forecast in capturing the layout impact. As shown in Figure 6, the input Verilog code is first parsed to obtain an Abstract Syntax Tree (AST) using an off-the-shelf software tool [50]. Next, features are extracted from the obtained AST. Then, an XGBoost model is applied to obtain the performance and power forecast for the corresponding Verilog code with the AST features and EDA tool parameters as input. Verilog Code AST Generation AST Feature Extraction 1010 1010 Features XGBoost Model Forecast Fig. 6. Performance and power forecasting from Verilog code. D. Functional Correctness versus Structural Similarity Lorecast requires the LLM-generated Verilog code to be syntac- tically correct but not necessarily functionally correct. Why, then, can Lorecast still be effective despite functional inaccuracies? A key reason lies in the AST structure of the generated code. The Verilog code produced by Lorecast not only achieves a high rate of syntax correctness but also exhibits an AST structure that closely resembles that of functionally correct Verilog code.",
    "source": "2503.11662v2_Lorecast_Layout-Aware_Performance_and_Power_Foreca.pdf",
    "length": 2331,
    "tokens": 490
  },
  {
    "text": "arXiv:2505.24721v1 [cs.LG] 30 May 2025 Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach Nick Rossenbach1,3, Benedikt Hilmes1,3, Leon Brackmann2, Moritz Gunz3, Ralf Schl uter1,3 1Machine Learning and Human Language Technology, RWTH Aachen University, Germany 2Institute for Electronic Materials II, RWTH Aachen University, Germany 3AppTek GmbH, Aachen, Germany Abstract Memristor-based hardware offers new possibilities for energy- efficient machine learning (ML) by providing analog in-memory matrix multiplication. Current hardware prototypes cannot fit large neural networks, and related literature covers only small ML models for tasks like MNIST or single word recognition. Simulation can be used to explore how hardware properties affect larger models, but existing software assumes simplified hardware. We propose a PyTorch-based library based on Synap- togen to simulate neural network execution with accurately captured memristor hardware properties. For the first time, we show how an ML system with millions of parameters would behave on memristor hardware, using a Conformer trained on the speech recognition task TED-LIUMv2 as example. With adjusted quantization-aware training, we limit the relative degra- dation in word error rate to 25 when using a 3-bit weight precision to execute linear operations via simulated analog com- putation. Index Terms: speech recognition, neuromorphic hardware, memristor simulation 1. Introduction Artifical neural networks (ANN) play an important role in natu- ral language processing (NLP) tasks such as automatic speech recognition (ASR). The majority of current ANN architectures used in NLP such as LSTM [1] or Transformer [2] and related derivatives make use of tensor operations such as vector-matrix- multiplication (VMM). VMM-based neural networks are typi- cally executed via graphic processing units (GPU) containing many parallel processors for efficient computation of large VMM operations. By introducing special processor units such as Ten- sor Cores in Nvidia GPUs or dedicated accelerators such as Google TPUs [3], the efficiency further increased for many op- erations [4]. Still, the underlying chip technology is based on complementary metal-oxide semiconductors (CMOS).",
    "source": "2505.24721v1_Running_Conventional_Automatic_Speech_Recognition_.pdf",
    "length": 2283,
    "tokens": 497
  },
  {
    "text": "URL [19] Yao Lu, Shang Liu, Qijun Zhang, and Zhiyao Xie. Rtllm: An open-source benchmark for design rtl generation with large language model. In 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC), pages 722 727. IEEE, 2024. [20] Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: A fully open-source 14b coder at o3-mini level. notion.site DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level, 2025. Notion Blog. [21] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with a 1.5b model by scaling rl. DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL, 2025. Notion Blog. [22] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems, 2024. URL [23] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel CandÃ¨s, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [24] OpenAI.",
    "source": "2505.24183v2_CodeV-R1_Reasoning-Enhanced_Verilog_Generation.pdf",
    "length": 1440,
    "tokens": 490
  },
  {
    "text": "9. 5.23 after loop unrolling . Unrolls the innermost loop (j_in_o) by a factor of 4. 10. 5.53 after fuse loops . Fuses loops by eliminating the loop over j_in_o where we mvin 0s to the accumulator. Instead, use the ability of preload to overwrite the values in the accumulator rather than accumulating, when beginning a new partial sum. 21 From this example, we observe that a diverse set of optimizations is selected, and that speedups are distributed throughout the optimization process rather than concentrated in just one or two steps, showing the importance of a well-designed iterative search process. From here, we summarize the differences between Autocomp-generated code and the previous best code (Exo Opt): Tiling. The Exo Opt code loads 128 256 tiles of A, whereas the Autocomp-generated code loads 32 256 tiles (divided into two 16 256 tiles) of A. While this means there is less reuse for the Autocomp-generated code, there is also less overhead needed for compute instructions to wait for each A tile to be loaded to the scratchpad. In combination with the rest of the optimizations applied by Autocomp, this leads to improved performance. Double-buffering. In the Autocomp-generated code, we see that both the scratchpad and accumulator are explicitly double-buffered. In the schedule, we can see that double buffering is applied 3 times. Initially (in step 3), both the A and B matrices (where matrix multiplication is represented as A B C), are double buffered in the scratchpad. However, after steps 5 and 6, B and A (respectively) are no longer double-buffered as larger tiles are loaded before beginning computation. The accumulator is double-buffered in step 8, resulting in the code below. The Exo Opt code relies on the accelerator s out-of-order execution to handle executing mvin and mvout instructions without dependencies, ahead of the order in which they are issued. Software pipelining. The Autocomp-generated code explicitly issues A mvin instructions before they are needed for computation, whereas as above the Exo Opt code relies on hardware to handle overlapping of data movement and compute.",
    "source": "2505.18574v2_Autocomp_LLM-Driven_Code_Optimization_for_Tensor_A.pdf",
    "length": 2127,
    "tokens": 474
  },
  {
    "text": "Gen: autoregressive generation, Prefill: prefill mode. Llama representative model from Montebovi (2024). Throughput ( tokens sec) Efficiency ( mJ token) Sequence length 500 1000 4000 8000 16000 500 1000 4000 8000 16000 Generate MMF (370M) Loihi 2 41.5 41.5 41.5 41.5 41.5 405 405 405 405 405 MMF (370M) H100 13.4 13.3 13.5 13.2 13.5 10.1k 10.1k 10.0k 9.9k 9.8k TF (370M) H100 22.4 22.9 21.7 21.3 20.9 5.5k 5.6k 6.2k 6.8k 8.2k Llama (400M) Jetson 14.3 14.9 14.7 15.2 12.8 723 719 853 812 1.2k Qwen2 (500M) Jetson 13.4 14.0 14.1 15.4 12.6 791 785 912 839 1.2k Prefill MMF (370M) Loihi 2 6632 6632 6632 6632 6632 3.7 3.7 3.7 3.7 3.7 MMF (370M) H100 11.4k 13.1k 30.6k 51.6k 84.6k 6.1 5.3 2.5 1.4 0.9 TF (370M) H100 21.6k 32.7k 44.3k 55.4k 60.5k 11.3 7.3 5.4 4.3 3.8 Llama (400M) Jetson 849 1620 3153 2258 1440 11.7 7.8 6.8 7.6 11.5 Qwen2 (500M) Jetson 627 909 2639 3861 3617 17.9 13.9 6.7 4.4 5.3 The MatMul-free LM on Loihi 2 was characterized on a 32-chip Alia Point Loihi 2 system (N3C1 silicon) running NxKernel v0.2.0 and NxCore v2.5.8 (accessible to Intel Neuromorphic Research Community members).",
    "source": "2503.18002v2_Neuromorphic_Principles_for_Efficient_Large_Langua.pdf",
    "length": 1099,
    "tokens": 494
  },
  {
    "text": "B. Pipeline Parallelism Pipeline parallelism splits model layers among accelerators, with each device handling a portion and passing activations along. Key characteristics: Implementation: Model layers are partitioned across devices, with activations flowing sequentially through the pipeline. Requirements: Micro-batch processing; balancing compute across pipeline stages; efficient point-to- point communication. Communication pattern: Activation transfers between adjacent pipeline stages. Advantages: Reduced memory requirements per device; lower communication volume than tensor parallelism; effective for very deep models. Best suited for: Very deep models; models that can be easily partitioned into balanced stages. NVIDIA Blackwell, Google TPU v7, Intel Gaudi 3, and Groq LPU provide well-optimized support for pipeline parallelism. AMD MI300X and Cerebras WSE-3 offer more limited support, as their architectures are less optimized for this scaling approach. Pipeline parallelism introduces pipeline bubbles that can reduce hardware utilization, particularly for small batch sizes. Techniques like inter- leaved scheduling and asynchronous pipeline parallelism have been developed to address this limitation, but they add complexity to the implementation. C. Expert Parallelism (MoE) Expert parallelism, specifically for Mixture-of-Experts models, distributes expert networks across devices. Each token activates only a sub- set of experts, reducing compute requirements. Key characteristics: Implementation: Expert modules are distributed across devices, with tokens routed to the appropriate experts based on a learned routing function. Requirements: Hardware support for sparsity; efficient routing mechanism; all-to-all communication for token routing. Communication pattern: All-to-all communication for routing tokens to experts. Advantages: Dramatic parameter scaling with sublinear compute; only activates a fraction of parameters per token; can achieve 5- 10x parameter scaling with minimal latency increase. Best suited for: Trillion-parameter models; applications where model quality trumps latency consistency. NVIDIA Blackwell, Google TPU v7, and Meta MTIA v2 provide excellent support for MoE models. NVIDIA's implementation leverages its general-purpose architecture with optimized libraries, while Google TPU v7 includes dedicated Sparse Core units specifically designed for MoE computation. Meta's MTIA v2 has been heavily optimized for Meta's own MoE-based recommendation models.",
    "source": "2506.00008v1_AI_Accelerators_for_Large_Language_Model_Inference.pdf",
    "length": 2507,
    "tokens": 470
  },
  {
    "text": "Llm4sechw: Leveraging domain-specific large language model for hardware debugging. In Asian Hardware Oriented Security and Trust Symposium (AsianHOST), A Survey of Circuit Foundation Model: Foundation AI Models for VLSI Circuit Design and EDA 61 2023. [203] Baleegh Ahmad, Shailja Thakur, Benjamin Tan, Ramesh Karri, and Hammond Pearce. On hardware security bug code fixes by prompting large language models. IEEE Transactions on Information Forensics and Security (TIFS), 2024. [204] Khushboo Qayyum, Muhammad Hassan, Sallar Ahmadi-Pour, Chandan Kumar Jha, and Rolf Drechsler. From bugs to fixes: Hdl bug identification and patching using llms and rag. In LLM Aided Design Workshop (LAD), 2024. [205] Dipayan Saha, Katayoon Yahyaei, Sujan Kumar Saha, Mark Tehranipoor, and Farimah Farahmandi. Empowering hardware security with llm: The development of a vulnerable hardware database. In International Symposium on Hardware Oriented Security and Trust (HOST), 2024. [206] Mohammad Akyash and Hadi Mardani Kamali. Self-hwdebug: Automation of llm self-instructing for hardware security verification. arXiv preprint arXiv:2405.12347, 2024. [207] Baleegh Ahmad, Shailja Thakur, Benjamin Tan, Ramesh Karri, and Hammond Pearce. On hardware security bug code fixes by prompting large language models. IEEE Transactions on Information Forensics and Security (TIFS), 2024. [208] Marcelo Orenes-Vera, Margaret Martonosi, and David Wentzlaff. From rtl to sva: Llm-assisted generation of formal verification testbenches. arXiv preprint arXiv:2309.09437, 2023. [209] Banafsheh Saber Latibari, Sujan Ghimire, Muhtasim Alam Chowdhury, Najmeh Nazari, Kevin Immanuel Gubbi, Houman Homayoun, Avesta Sasan, and Soheil Salehi. Automated hardware logic obfuscation framework using gpt.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 1763,
    "tokens": 491
  },
  {
    "text": "As a further improvement to the developed pipeline through hardware utilization, a tracker was integrated between detection and recognition stages. This tracker has allowed to avoid repetitive recognition stage to pre-recognized faces. That is, in many frames where no new faces appear, the recognition stage will have a processing time of 0 ms. Therefore, this reduction in recognition stage time has improved the average FPS from 202 FPS to 300 FPS, and the reduced the total power consumption of CPU and GPU from 8,196 mW to 4,359 mW. VI. CONCLUSION This paper presented an efficient and optimized face detection and recognition pipeline that is designed to leverage the throughput of existing pipelines, considering power. This was achieved through pruning the models to improve the processing time, and by evaluating different hardware configurations and tasks allocation on Jetson AGX Orin edge device. Detailed analysis for each of these configurations was carried out, and it is concluded that the best pipeline is achieved by running the Face Detection model on the DLA, and the Face Recognition model on the GPU. Additionally, a further improvement in the throughput and power consumption was achieved by integrating a tracker between the detection and the recognition stages. This tracker was designed to run on the VIC hardware with very low GPU usage, this avoided repetitive recognition stages and reduced the power consumed by the recognition stage. The tracker improved the throughput of the pipeline from 202 FPS to 298 FPS, in addition to GPU power consumption reduction by 500mW and to more than 1000mW reduction in the CPU power consumption. REFERENCES [1] L. Chen, Y. H. Wang, Y. D. Wang, and D. Huang, Face recognition with statistical local binary patterns, Proc. 2009 Int. Conf. Mach. Learn. Cybern., vol. 4, no. May 2004, pp. 2433 2439, 2009, doi: 10.1109 ICMLC.2009.5212189. [2] Kwang In Kim, Keechul Jung, and Hang Joon Kim, Face recognition using kernel principal component analysis, IEEE Signal Process. Lett., vol. 9, no. 2, pp. 40 42, Feb. 2002, doi: 10.1109 97.991133. [3] T. Evgeniou and M. Pontil, Support Vector Machines : Theory and Applications, no. May, 2001, doi: 10.1007 3-540-44673-7.",
    "source": "2505.04524v1_Edge-GPU_Based_Face_Tracking_for_Face_Detection_an.pdf",
    "length": 2225,
    "tokens": 500
  },
  {
    "text": "See Appendix A.1 for more details on Loihi 2. Although LLMs have been dominated by self-attention (Vaswani et al., 2017) with quadratic run- time complexity, LLMs based on state space models (SSMs) offer linear scaling with competitive performance (Gu Dao, 2023). SSMs rely on element-wise recurrence (Gupta et al., 2022), and use stateful neurons that align well with compute-near-memory architectures like Loihi 2. Advances in quantization of LLMs (Dettmers et al., 2022; Frantar et al., 2023; Xiao et al., 2024) have culminated in extreme quantization at scale with LLMs using binary activations (Zhu et al., 2023) or ternary weight matrices as seen in BitNet (Ma et al., 2024), piecewise affine transformers (Kosson Jaggi, 2023), ShiftAddLLM (You et al., 2024), and earlier work on binarized neural networks (Courbariaux et al., 2016). Building on these ideas, Zhu et al. (2024) introduced the MatMul-free LLM, which replaces traditional matrix multiplications (MatMuls) with ternary matrices and element-wise op- erations, while also using a subquadratic SSM layer based on the HGRN model (Qin et al., 2023; 2024). Loihi 2 is optimized for sequence-based processing, element-wise recurrence, low-precision arith- metic, and weight sparsity, all of which are features of the MatMul-free model. These benefits have been demonstrated on signal processing tasks (Orchard et al., 2021b; Shrestha et al., 2024) with orders of magnitude better latency and efficiency than state-of-the-art solutions. This paper presents a work-in-progress of adapting and deploying the MatMul-free language model from Zhu 1 arXiv:2503.18002v2 [cs.NE] 25 Mar 2025 Published at ICLR 2025 Workshop (SCOPE) et al. (2024) to Intel Loihi 2, opening a pathway that bridges neuromorphic computing with state- of-the-art efficient LLMs.",
    "source": "2503.18002v2_Neuromorphic_Principles_for_Efficient_Large_Langua.pdf",
    "length": 1808,
    "tokens": 485
  },
  {
    "text": "Figure 4 illustrates the geomean speedups achieved using multiple techniques: zero-shot inference from the source model (zero-shot), a model trained exclusively on the target hardware using the fine-tuning dataset (no transfer), Wa- coNet with feature augmentation (WACO FA), WacoNet with feature mapping (WACO FM), and COGNATE s per- formance for both the top-1 and top-5 (k-best) predicted program configurations. Our results show that COGNATE consistently outperformed other techniques across both sparse operations and hardware platforms. Specifically for SPADE, COGNATE (Top-1) achieved an average speedup of 1.40 for SpMM, reaching 90 of the optimal speedup of 1.55 . When expanding COGNATE (Top-5), it deliv- ered an average speedup of 1.47 , achieving 95 of the 7 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning optimal speedup. Note that the optimal speedup was deter- mined by exhaustively evaluating all program configurations within the defined constrained search space for each matrix in the test set, and selecting the fastest configuration per matrix to compute the geometric mean. Similarly, for SD- DMM in SPADE, COGNATE (Top-1) achieved an average speedup of 1.27 and COGNATE (Top-5) achieved an aver- age speedup of 1.39 . This emphasizes COGNATE s ability to consistently find near-optimal program configurations with minimal fine-tuning across multiple sparse operations. The speedup gained for zero-shot inference from the source model was significantly lower than the baseline. In contrast, fine-tuning on a few data samples from SPADE led to signifi- cant performance gains showing COGNATE s effectiveness in knowledge transfer. 0 5 10 15 20 25 30 Epochs 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Accuracy Loss Train Loss(PRL) Kendall's TAU Validation Loss(PRL) Ordered Pair Accracy(OPA) Figure 6: Loss and accuracy during training. 4.3. Transferability to GPU COGNATE is generalizable and is not only applicable to one target hardware platform.",
    "source": "2506.00424v2_COGNATE_Acceleration_of_Sparse_Tensor_Programs_on_.pdf",
    "length": 2000,
    "tokens": 476
  },
  {
    "text": "2. Evolutionary tree of foundation AI models for VLSI design and EDA. 4 Wenji Fang, Jing Wang, Yao Lu, Shang Liu, Yuchao Wu, Yuzhe Ma, and Zhiyao Xie works on foundation AI for EDA. We propose referring to this type of work as circuit foundation models (CFMs). Figure 2 illustrates the evolutionary tree of existing circuit foundation models, including both encoder-based and decoder-based paradigms. This paper also covers the potential and challenges of CFMs from our perspective. Structure of Section 1. In this Introduction, we will first propose our own taxonomy of existing AI for EDA techniques in Section 1.1, categorizing all existing AI for EDA techniques into two major types. Then we will briefly introduce the already extensively studied Type I techniques (supervised AI for EDA) in Section 1.2 and elaborate on the emerging Type II techniques (foundation AI for EDA, the focus of our survey) in Section 1.3. After that, in Section 1.4, we will summarize all existing surveys that cover similar topics and elaborate on the contributions of this survey. In Section 1.5, we will introduce the overall structure of this whole survey paper. 1.1 Our Taxonomy of AI for EDA Techniques: Two Different Types In this survey, we propose to categorize existing AI for EDA techniques into two main types, as listed below. Figure 1 summarizes and compares all three paradigms of these two types of works. Type I: Supervised Predictive AI Techniques for EDA. The mainstream paradigm of previous AI for EDA solutions adopts supervised predictive AI models. These supervised predictive models have been developed for various applications, including early-stage design quality prediction, fast design quality simulation, design space exploration, etc. Relevant works have been extensively studied and covered in existing surveys [3, 4] and book [16]. Type II: Foundation AI Techniques for EDA (Circuit Foundation Model). This trend- ing technique is the focus of this survey. The development of foundation AI solutions, according to our proposed definition, involves two phases: 1) Pre-training phase; 2) Fine- tuning phase. The first pre-training step, which is typically self-supervised on a large amount of unlabeled data, enables the AI model to learn more general circuit intrinsic patterns.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 2292,
    "tokens": 489
  },
  {
    "text": "Experimental Setup Algorithm setup. We evaluate NSFlow with three state-of-the-art VSA-based NSAI workloads, i.e., NVSA [17], MIMONet [28], and LVRF [12] on the commonly-used spatial-temporal reasoning datasets - RAVEN [32], I-RAVEN [33], PGM [34], CVR [37], and SVRT [38]. Following [12], [17], [28], we select the training hyperparameters based on the end-to-end reasoning performance on the validation set. Hardware setup. We consider several hardware baselines, includ- ing TX2, Xavier NX, Xeon CPU, RTX 2080, and ML accelerators (TPU, Xilinx DPU). NSFlow framework can be deployed on any type of FPGA board. Tab. III showcases our deployment for 3 algorithms on AMD U250 using Xilinx Vivado and Synopsys Design Compiler. The clock frequency is set to 272MHz. B. NSFlow Performance Mixed-precision performance. We benchmark NSAI model on three spatial-temporal reasoning datasets to first evaluate the effec- tiveness of mixed quantization in NSFlow. As shown in Tab. IV, we can observe that NSFlow mixed precision achieves comparable accuracy with NVSA algorithm [17] while with 5.8 memory footprint savings. Similar results are observed in MIMONet LVRF on CVR SVRT datasets. It is also worth noting that neurosymbolic meth- ods consistently achieve improved cognition and reasoning capability than neural network-based methods and surpass human performance. TX2 NX Xeon CPU RTX 2080 NSFlow 0 Norm.",
    "source": "2504.19323v2_NSFlow_An_End-to-End_FPGA_Framework_with_Scalable_.pdf",
    "length": 1403,
    "tokens": 357
  },
  {
    "text": "One critical yet often overlooked issue in edge LLM deploy- ment is the disproportionate emphasis on decoding throughput, while prefill latency remains largely ignored. For example, [9] demonstrates efficient LLM decoding on embedded FPGAs but neglects the prefill stage entirely. However, prefill latency is not merely a technical detail, it is a primary bottleneck for user experience and safety in latency-sensitive edge AI applications. While prefill overhead may be negligible in cloud environments, on-device deployment places it squarely on the critical execution path. Despite its importance, prefill optimization remains significantly underexplored and demands more serious attention from the community. To address these limitations, we present TeLLMe the Tenary Large Language Model Edge Accelerator the first edge FPGA-based accelerator specifically designed for ternary LLM inference with full support for both prefill and decoding stages. TeLLMe enables low-latency, energy-efficient deploy- ment of LLMs on resource-constrained platforms by target- ing cost-effective FPGAs such as AMD KV260. It supports ternary-quantized weights (1.58-bit) and 8-bit activations. Our design co-optimizes compute, memory, and scheduling efficiency, key contributions are as follows: We develop the first end-to-end edge FPGA accelerator for ternary LLMs supporting both prefill and decoding stages. We propose Table-lookup-based ternary matmul, an effi- cient and resource-saving matrix multiplication unit that arXiv:2504.16266v2 [cs.AR] 24 Apr 2025 specially optimized for FPGA, reusing grouped activa- tions and online computation for ternary matmuls across projection and FFN layers. We introduce a fused attention unit for prefill and de- coding, incorporating a novel reversed attention mecha- nism and fully fused pipeline to minimize off-chip data movement, avoid redundant masked computation, and guarantee the parallelism at the same time. We deliver up to 9.51 tokens sec generation in up to 1024 token contexts while consuming less than 7W power, outperforming mobile SoCs with much lower power budgets.",
    "source": "2504.16266v2_TeLLMe_An_Energy-Efficient_Ternary_LLM_Accelerator.pdf",
    "length": 2113,
    "tokens": 458
  },
  {
    "text": "Low-Rank Adaptation (LoRA) [14] tackles this by freezing the base weights and introducing a small set of trainable adapter parameters, drastically reducing memory and compute requirements for fine-tuning. 2.2 Quantization for LoRA Fine-Tuning Quantized LoRA fine-tuning further cuts memory usage by quantizing the base model weights without hurting performance. QLoRA [9] introduces a NormalFloat format to backpropagate through a 4-bit quantized backbone, while LoftQ [23] and ApiQ [24] jointly optimize quantized base weights and adapter initializations under a unified objective. These advances unlock fine-tuning and deployment of LLMs on low-resource platforms like embedded devices [41, 4] and mobile phones [47, 43]. 2.3 Limitations of Existing Quantized LoRA Methods Despite the early promise of recent work in quantized LoRA like QLoRA and LoftQ, we observe three major limitations that prevent us from fully realizing the potential of memory-efficient LLM fine-tuning. 3 L1: Coarse-Grained Precision Assignment. Existing approaches typically apply a single quantization precision to an entire weight matrix or multiple layers. For instance, QLoRA uses uniform 4-bit quantization across all base weights, while LoftQ adopts a layerwise mixed-precision scheme (e.g., higher precision for earlier layers, lower for later layers). Our findings ( 6) suggest that truly unlocking ultra-low-bit fine-tuning requires a finer-grained assignment strategy, potentially at the sub-layer or sub-matrix level. L2: Discrepancy in Data Distribution. Most quantized LoRA methods use a globally shared data format such as QLoRA s NormalFloat, which assumes a roughly normal distribution. However, Figure 2 reveals that groupwise normalization at a per-channel level often deviates significantly from a global normal distribution. To preserve accuracy, more localized quantization dequantization approaches are needed.",
    "source": "2502.08141v1_LowRA_Accurate_and_Efficient_LoRA_Fine-Tuning_of_L.pdf",
    "length": 1909,
    "tokens": 416
  },
  {
    "text": "When generating new text in auto-regressive generation of an LLM, we have to wait for the token at time t to be output before we can begin processing the next token at time t 1, thus making this a natural fit for Loihi 2 s fall-through mode. 4This is not a hard limit, as one can implement an 8n-bit synapse through n separate 8-bit synapses that are added together with different fixed-point exponents. 5Local states are not restricted in precision, and one may also transmit messages with more than 32 bits in an analogous way to what is described above for synaptic weights. 11 Published at ICLR 2025 Workshop (SCOPE) ğ‘¡0 ğ‘¡1 Input Layer 0 Layer 2 Layer n Fall-through mode ğ‘¡0 ğ‘¡1 Input Layer 0 Layer 2 Layer n Pipelined mode ğ‘¡2 ğ‘¡3 ğ‘¡4 Latency Throughput 1 Latency Throughput 1 Figure 3: Different execution modes on Loihi 2 that either optimize throughput or latency. In the pipelined mode, a new data point is inserted in each time step, to use all processing cores and maximize the throughput at the expense of latency because equal time bins t0 t1 . . . are enforced. In the fall-through mode, a new data points is only provided once the last data point has been fully processed with minimum latency. Only a single neuronal layer is active at any step as data travels through the network. The time per step is thus minimized as traffic is reduced and potentially more complex neuronal layers are not updated. A.2 FIXED-POINT IMPLEMENTATION DETAILS A.2.1 FIXED-POINT IMPLEMENTATION OF THE SIGMOID FUNCTION We employ a look-up-table (LUT) for a fixed-point approximation of the logistic sigmoid function, Ïƒ(x) 1 (1 e x), as discussed in Section 3. Specifically, we scale the floating-point input x by 2xexp where xexp 6 determines the accepted input precision of the fixed-point sigmoid implemen- tation.",
    "source": "2503.18002v2_Neuromorphic_Principles_for_Efficient_Large_Langua.pdf",
    "length": 1805,
    "tokens": 457
  },
  {
    "text": "The feature collection logic zero pads smaller-sized payloads to 8 bytes to generate a uniform feature size for the IDS. This allows us to replay the bus messages as-is during our evaluation without pre-processing the dataset, faithfully replicating an in- vehicle scenario. Once the feature vector is transferred, the IDS processes the frame for potential attacks and asserts the ids output ready signal on completion. This operation over- laps with the CAN protocol checks, where the bit-processor validates the CRC and error flags of the received frame, and waits for the end-of-frame signal before transferring the valid CAN message to the receive buffer (in config registers mod- ule). The IDS output value is wired out of the bit processing module to the top module, where a custom multiplexing logic appends the IDS output to the received CAN message as it is transferred to the receive buffer and subsequently read by the ECU. Figure 3 illustrates the SecCAN operation using a waveform, highlighting how the IDS operations overlap with the CAN signalling checks (CRC, error flagging bit times). Our optimised IDS implementation completes the inference within this window, thus hiding this latency from the ECU. B. Model Dataset for training and testing To arrive at our final QNN model s architecture, we explored different configurations with varying complexity (number of layers and number of neurons in each layer) to find a model that offers high inference accuracy at minimal complexity. We use CAN ID Payload information from each CAN frame as the input feature for the model to perform binary classification. We arrived at a 4-bit (weights activations) quantised multilayer perceptron model (MLP) as the chosen configuration for the IDS, which provided the best validation accuracy during the training process. The QNN model is trained using brevitas, Clock Message Detected IDS Enable ID Value B1 B2 Payload Data B1 B2 B3 B4 B5 Data Enable IDS Output Ready Frame Complete 1. Message Detected on the bus 2. IDS operation started 3. Receive ID Payload Data 4. Start of data write to IDS IDS Operation 6. IDS output sampled at this point 6. Frame reception complete 5. Message check signals (CRC Error checks) Fig. 3: The waveform shows the signalling within the SecCAN controller for a 5-byte CAN message from the bus.",
    "source": "2505.14924v1_SecCAN_An_Extended_CAN_Controller_with_Embedded_In.pdf",
    "length": 2332,
    "tokens": 495
  },
  {
    "text": "We evaluate DeepSeek-R1 [3], DeepSeek-V3 [4], QWQ-32B [34], DeepSeek-R1-Distill-Qwen-32B [3], DeepSeek-R1-Distill-Qwen- 7B [3], Qwen2.5-Coder-32B-Instruct [40], Qwen2.5-Coder-7B-Instruct [40], and GPT-4o [24] on 6 Table 2: Comparison of CodeV-R1-7B on VerilogEval v2 and RTLLM v2. Type Model Open source VerilogEval2-SR ( ) VerilogEval2-CC ( ) RTLLM v2 ( ) Foundation models GPT-4o 64.1 73.7 76.2 57.6 66.1 69.0 56.5 70.3 75.2 DeepSeek-R1-671B 77.5 84.7 87.4 79.1 85.1 87.1 64.7 75.8 79.7 DeepSeek-V3-671B 62.4 71.7 75.0 68.7 76.3 78.2 59.1 71.5 73.3 QWQ-32B 64.2 77.3 80.1 64.0 77.8 80.9 52.9 68.0 71.2 DeepSeek-R1-Distill-Qwen-32B 43.9 63.3 69.2 53.8 69.8 73.8 42.4 62.1 67.0 DeepSeek-R1-Distill-Qwen-7B 0.6 2.2 3.5 2.0 7.0 11.3 0.0 0.0 0.0 Qwen2.5-Coder-32B-Instruct 47.5 60.7 64.7 46.6 59.0 62.8 47.8 63.9 67.8 Qwen2.5-Coder-7B-Instruct 31.3 49.3 54.6 30.5 46.8 52.0 36.1 52.4 57.6 IT Baselines RTLCoder-DS-6.7B 31.1 47.8 52.3 33.7 45.9 49.8 33.6 45.3 49.2 CodeV-R1-7B-Distill 65.2 75.2 77.5 65.5 75.6 78.2 57.2 71.9 77.1 Ours CodeV-R1-7B 68.8 78.2 81.1 69.9 78.2 80.9 68.0 78.2 81.7 We evaluate all models in this table. SR: Specification-to-RTL; CC: Code Completion.",
    "source": "2505.24183v2_CodeV-R1_Reasoning-Enhanced_Verilog_Generation.pdf",
    "length": 1172,
    "tokens": 609
  },
  {
    "text": "The resultant products in each TPE are accumulated downwards across each column of the tensor array. When structured sparsity of 1:4 is enabled, only one of the two multiplexers and multipliers will be activated in each TPE. This process iterates for all incoming rows of matrix A; different rows of A arrive at each TPE in blocks of four consecutive elements, and the stationary weights dictate which ones are selected for computation. III. FAULT DETECTION IN SPARSE TENSOR ARRAYS The first step toward a fault-tolerant sparse tensor array is a fast and light-weight (in terms of both hardware overhead and the impact on application performance) fault-detection mechanism that can trigger an appropriate reaction. We hereby propose an online test technique that periodically checks every time a new group of weights is loaded the systolic array TPEs for permanent errors. For simplicity, let us assume that a permanent fault can only occur within the storage elements, i.e., the registers, of each TPE, as shown in Fig. 3. Hence, a permanent fault may afflict one of the following registers: 1) The so called activation registers, which store the incom- ing blocks of consecutive input elements and propagate them along the horizontal (west-to-east) direction in each row of the array. 2) The N weight registers that store the stationary weights. 3) The weight-index registers that control the multiplexers and select the up to N appropriate input (activation) elements out of the M-element input block. 4) The output registers that store the accumulated sums and propagate them downwards along the vertical direction (north-to-south) in each column of the array. Note that the simplifying assumption that permanent faults may only occur in the above-mentioned registers does not ignore faults manifesting in the remaining logic of a PE. As can be seen in Fig. 3, the micro-architecture of the TPE indicates that such faults will yield an erroneous result in either the multiplication and or the accumulation steps, which will be captured as errors in the output register of each TPE. All paths of the internal TPE logic are funneled into the output register at the bottom right of each TPE. Hence, error coverage of the output register will also detect faults occurring within the remaining TPE logic. Fig. 3.",
    "source": "2504.18628v1_Periodic_Online_Testing_for_Sparse_Systolic_Tensor.pdf",
    "length": 2311,
    "tokens": 485
  },
  {
    "text": "\"\", 2024. [124] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Ja- son Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloun- dou, David Farhi, Liam Fedus, Niko Felix, SimÃ³n Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo- Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Åukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Koko- tajlo, Åukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David MÃ©ly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Eliz- abeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Tur- ley, Jerry Tworek, Juan Felipe CerÃ³n Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Ak- ila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 tech- nical report.",
    "source": "NSF_LLM_Medium_Proposal.pdf",
    "length": 4198,
    "tokens": 1349
  },
  {
    "text": "Recent advances in both commercial (Newsroom; AMD, b; Mesnier, 2022; ScaleFlux, a,b; Eideticom Laboratory; Laboratory) and academic sectors (Torabzadehkashi et al., 2019b; Barbalace Do, 2021; Lukken Trivedi, 2021; Torabzadehkashi et al., 2019a; Sala- mat et al., 2021, 2022; Do et al., 2013) advocate the use of computational storage drives (CSDs) across databases, high-performance computing (HPC), and analytics. AMD and Xilinx have intro- duced specialized tools and libraries designed to harness CSDs (AMD, a; AMD Xilinx), enabling peer-to-peer PCIe transactions that bypass the CPU (AMD Xilinx). The emergence of Compute Express Link (CXL) technology (Sharma, 2022a,b; Jung, 2022) further amplifies the potential of disaggregated storage and memory systems. Decoupling compute tasks needed for storing the data from the host CPU and embedding them directly within storage devices, particularly through CSDs, has demonstrated significant performance and energy benefits. Tasks traditionally performed at the storage controller level are now being offloaded to CSDs, often accelerated using FPGA primitives (Kim et al., 2021; AMD Xilinx; AMD, a). Moreover, CSDs hold the potential to undertake critical machine learning tasks like feature extraction and clustering, streamlining tasks like neural compression. The unique combination of FPGAs parallel processing prowess, the substantial internal bandwidth of SSDs, and their block- accessible nature position CSDs as ideal components for evolving smart storage solutions tailored for machine learning. To cater towards this, in this work, we propose Salient Store a mini computational storage server (we call it edge storage server ) stack for managing the data archival in edge servers. Salient Store provides adaptive data compression using neural codecs and further enhances the data security by providing an accelerated quantum safe data encryption policy, protecting these vulnerable edge storage servers against the store now decrypt later (National Cybersecurity Center of Excellence (NCCoE), 2023) attacks. 3 Data Compression using Neural Codec In this section we go over the overall design and design choices for the neural codec design of Salient Store . We discuss the edge storage architecture followed by the choice of neural codec and their design.",
    "source": "SaLT.pdf",
    "length": 2315,
    "tokens": 494
  },
  {
    "text": "Available: [66] G.-I. Yu, J. S. Jeong, G.-W. Kim, S. Kim, and B.-G. Chun, Orca: A Distributed Serving System for Transformer-Based Generative Models, in Proceedings of the USENIX Symposium on Operating Systems Design and Implementation (OSDI), 2022. [67] Y. Zhong, S. Liu, J. Chen, J. Hu, Y. Zhu, X. Liu, X. Jin, and H. Zhang, DistServe: Disaggregating prefill and decoding for goodput- optimized large language model serving, in Proceedings of the USENIX Symposium on Operating Systems Design and Implementation (OSDI), 2024. [68] A. Zhou, K. Yan, M. Shlapentokh-Rothman, H. Wang, and Y.-X. Wang, Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models, in Proceedings of the International Conference on Machine Learning (ICML), 2024. [69] A. Zhou, K. Yan, M. Shlapentokh-Rothman, H. Wang, and Y.-X. Wang, Official Repo of Language Agent Tree Search (LATS), 2024. [Online]. Available: [70] D. Zhou, N. Sch arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet, Q. Le, and E. Chi, Least-to-Most Prompting En- ables Complex Reasoning in Large Language Models, in Proceedings of the International Conference on Learning Representations (ICLR), 2023. 16",
    "source": "2506.04301v1_The_Cost_of_Dynamic_Reasoning_Demystifying_AI_Agen.pdf",
    "length": 1203,
    "tokens": 367
  },
  {
    "text": "The middle section shows the operand buffer contents. and limited datatype support. For example, AMX and TPU hardware only support up to 16-bit input operands. Extending support to 32-bit or 64-bit inputs would incur significant area overheads, as a multiplier area scales quadratically with mantissa bit width [40], [41]. 2) Vector processor:: To exploit data reuse at the reg- ister level in a vector architecture, we propose breaking down MMA instructions into a sequence of new vector micro-instructions called Component Vector Fused-Multiply- Accumulate (cvfma). The cvfma instructions behave like standard vector FMAs except for the lane control logic, which modifies the operand buffer filling to implement the data reuse patterns required by matrix multiplication. Without loss of generality, this section focuses on supporting tmul; the same method applies to tfmul. The tmul instruction is decoded into K cvfma operations, based on the MTE tk CSR field. Each cvfma operation processes a vector length of M RLEN bits, determined by the MTE tm CSR field, which disables computation on tail elements corresponding to inactive tile rows. To avoid computations on inactive columns across the N dimension, the architecture employs an implicit vector predication mask on cvfma instructions, derived from the MTE tn CSR field. This technique is compatible with software predication masks, combining them with a logical and operation to disable computations on arbitrary elements. Figure 5 illustrates the tmul decomposition and shows the operand buffer contents for the i-th vector lane across all K cvfma instructions. The vd operand buffer, containing the C tile, is filled similarly to regular vector architectures, as indicated in Figure 1. The vs2 operand buffer, mapped to the B tile, holds a single lane-local element for all cvfma steps. This pattern, called implicit broadcast, is already used in scalar- vector arithmetic instructions like the RISC-V V vfmacc.vf instruction [13]. No inter-lane communication is required for the vs2 operand. Finally, the A tile operand requires an access pattern unsupported by standard vector instructions.",
    "source": "2507.03522v1_A_Flexible_Instruction_Set_Architecture_for_Effici.pdf",
    "length": 2153,
    "tokens": 475
  },
  {
    "text": "However, network sparsification may reduce model accuracy [30] Meanwhile, retraining a pre-trained DNN to satisfy accumulator constraints may alter properties of the pre-trained model, such as algorithmic fairness guarantees [42]. We find that enforcing strict bounds on weight magnitude is not necessary for using narrow accumulators. 3.2 Avoiding Transient Overflows Persistent overflow is a true overflow where the final result is simply too large for the accumulator. Transient overflows are temporary and arise when a partial sum overflows but where the final sum may not actually overflow the accumulator. Hence, in the absence of persistent overflow, we should be able to eliminate transient overflows by reordering the summation. 3 Vikas Natesh, H.T. Kung, and David Kong Theorem 3.3. Let ğ‘‹ {ğ‘¥1,ğ‘¥2, ...,ğ‘¥ğ‘˜} be a list of ğ‘˜signed integers, where each ğ‘¥ğ‘–is represented using ğ‘›bits. Let ğ‘¦ Ãğ‘˜ ğ‘– 1 ğ‘¥ğ‘–be the sum of all elements in ğ‘‹, representable using ğ‘š ğ‘› 1 bits without persistent overflow (i.e., 2ğ‘š 1 ğ‘¦ 2ğ‘š 1 1). Then, there exists an ordering of summation for ğ‘‹that avoids transient overflow when using an ğ‘š-bit accumulator. Proof. Suppose ğ‘˜ 2. The list ğ‘‹ {ğ‘¥1,ğ‘¥2} contains ğ‘›-bit numbers, and its sum ğ‘¥1 ğ‘¥2 (or ğ‘¥2 ğ‘¥1) can require at most ğ‘› 1 bits. Since ğ‘š ğ‘› 1, the sum does not overflow ğ‘š-bits, and the theorem holds for ğ‘˜ 2. Let ğ‘™ 2 and assume the theorem holds ğ‘˜ ğ‘™, i.e., there exists an ordering ofğ‘‹ {ğ‘¥1,ğ‘¥2, ...,ğ‘¥ğ‘™} such that the sum of elements ofğ‘‹ w.r.t.",
    "source": "2504.09072v1_MGS_Markov_Greedy_Sums_for_Accurate_Low-Bitwidth_F.pdf",
    "length": 1463,
    "tokens": 497
  },
  {
    "text": "For both datasets, flipping only one vulnerable bit in layer 2 is sufficient to degrade the accuracy. Moreover, GBFA achieves an ASR of 88 on the Cora dataset and 58 on the PubMed dataset by targeting layer 2 at a BER of 1e-1. Table VII presents a comprehensive evaluation of the GBFA attack on the GIN model across individual layers for the Cora and PubMed datasets, under both the minimum BER and BER of 1e-1. For GIN-Cora, the attack demonstrates varying degrees of performance accuracy collapse (PAC) depending on the targeted layer. Notably, Layer 3 exhibits the most significant drop in performance at Min BER, with PAC falling to 33 , while Layer 5 maintains a relatively high PAC of 60 at a higher BER of 1e-1, accompanied by a notably low attack success rate (ASR) of only 5 . This suggests layer-specific resilience differences. In contrast, for GIN-PubMed, the PAC values remain stable at Min BER across all layers, while under a BER of 1e-1, PAC declines moderately (56 58 ) with ASR values ranging from 22 in the earlier layers to 8 in the final layer. These results indicate that the GBFA s impact is highly dependent on both the targeted layer and dataset, with deeper layers in GIN-Cora being more vulnerable and consistent PAC- ASR trade-offs observed in GIN-PubMed. GBFA Effectiveness Across GNN models: The susceptibility of GNN models to the GBFA attack varies significantly based on their architectural design and depth. We observe that model-specific factors such as layer composition, aggregation mechanisms, and parameter count influence the severity of accuracy degradation under bit-flip faults. Among the evaluated architectures, GIN and GraphSAGE consistently exhibit higher vulnerability. In particular, GIN suffers sharp accuracy drops even at low BER values. This heightened sensitivity stems from GIN s reliance on precise weight updates to maintain injective aggregation, making it particularly fragile to targeted bit-level perturbations. GIN models also exhibited layer-dependent vulnerability. The findings demonstrate that middle layers were especially susceptible. The last layer showed reduced responsiveness to bit flips, suggesting that earlier transformations carry more influence over final predictions in GIN. GCN and GAT models demonstrate more moderate vulner- ability in compare to GIN and GraphSAGE. Moreover, both models are vulnerable on the deeper layer.",
    "source": "2507.05531v1_Bit-Flip_Fault_Attack_Crushing_Graph_Neural_Networ.pdf",
    "length": 2405,
    "tokens": 498
  },
  {
    "text": "It is then entered into a one-input, one-output nonlinear (NL) device, whose output is multiplied by ğœ‚ and becomes ğ‘¥(ğ‘¡). Thereafter, ğ‘¥(ğ‘¡) is added to ğ›¾ğ‘—(ğ‘¡) through a feedback loop with a total delay time of ğœ. Virtual nodes with interval ğœƒare serially connected to form a feedback loop in the reservoir. These nodes operate as shift registers. The values stored in the virtual nodes are the features and they collectively form a vector with ğ‘ğ‘¥elements, which is the reservoir state. Therefore, the state that this reservoir holds in the delay elements is expressed as: ğ’™(ğ‘˜) [ğ‘¥(ğ‘˜ğœ ğœƒ),ğ‘¥(ğ‘˜ğœ 2ğœƒ), . . . ,ğ‘¥(ğ‘˜ğœ ğœ)]. (5) Most of the delayed feedback reservoirs use the Mackey-Glass model [11] as a nonlinear element[1, 2, 16], which is expressed as follows: d dğ‘¡ğ‘¥(ğ‘¡) ğ‘¥(ğ‘¡) ğœ‚[ğ‘¥(ğ‘¡ ğœ) ğ›¾ğ‘—(ğ‘¡)] 1 [ğ‘¥(ğ‘¡ ğœ) ğ›¾ğ‘—(ğ‘¡)]ğ‘, (6) where ğ‘is an adjustable parameter. There are two reasons why this model is often used [2]. First, it is easy to implement in analog electronic circuits. Second, the nonlinearity can be adjusted by changing ğ‘. The existing studies often use ğ‘ 7 for ease of hardware implementation. Next, the reservoir representation ğ’“ Rğ‘ğ‘Ÿis obtained for the reservoir states ğ’™(1), ğ’™(2), . . . , ğ’™(ğ‘‡) for use in the output layer. The construction of a reservoir representation is critically important because it significantly affects both the accuracy of the DFR and hardware resource usage. We discuss the implementation of the reservoir representation in the next section.",
    "source": "2504.11981v1_Hardware-Friendly_Delayed-Feedback_Reservoir_for_M.pdf",
    "length": 1460,
    "tokens": 500
  },
  {
    "text": "Rather than explicitly model ev- ery cycle and microarchitectural interaction, these methods learn an approximate model of the architecture s performance from a large corpus of data. A typical approach is to pose the problem as learning a function mapping a sequence of instructions to the tar- get performance metrics. For example, recent work [50, 54, 55, 71] train sequence models (e.g., LSTMs [35] and Transformers [88]) on ground-truth data from a cycle-level simulator to predict metrics such as the program s Cycles Per Instruction (CPI). These methods show promise in providing fast performance esti- mates with reasonable accuracy. However, relying on black-box ML models operating on instruction sequences has several limitations. First, the computational cost of these methods scales proportionally with the length of the instruction sequence, i.e. O(ğ¿) where ğ¿is the instruction sequence length. The O(ğ¿) complexity limits the potential speedup of these methods, e.g., to less than 10 faster than cycle-level simulation with a single GPU [55, 71]. This speedup is mainlyduetoreplacingtheirregularcomputationsofcycle-levelsim- ulation with accelerator-friendly neural network calculations [71]. By contrast, analytical models can be several orders of magnitude faster than cycle-level simulation (and current ML approaches) be- cause they fundamentally operate at a higher level of abstraction, i.e., mathematical expressions relating key statistics (e.g., instruction mix, cache behavior, branch misprediction rate, etc.) to performance. Second, existing ML approaches must learn all the dynamics im- pacting performance from raw instruction-level training data. In many cases, this learning task is unnecessarily complex since it does not exploit the CPU performance modeling problem structure. For example, TAO s Transformer model [71] must learn the perfor- mance impact of register dependencies from per-instruction register information, even though there exist higher-level abstractions (e.g., instruction dependency graphs [61]) that concisely represent depen- dency behavior ( 3.2).",
    "source": "2503.23076v1_Concorde_Fast_and_Accurate_CPU_Performance_Modelin.pdf",
    "length": 2101,
    "tokens": 445
  },
  {
    "text": "Nevertheless, the evaluations demonstrate that our lightweight U-Net outperforms the Inria team s neural network. The lightweight U-Net serves as a baseline for evaluating the five workflows explored in this paper. TABLE I: Evaluation metrics of our lightweight U-Net on the validation set Model IoU Accuracy Lightweight U-Net (ours) 0.7108 0.9546 FCN MLP (Inria) [17] 0.6467 0.9442 Figure 3 shows an example of our U-Net s prediction quality compared to the ground truth on a 256x256 image. The buildings are generally well-predicted by the neural network, even if the contours of the predicted buildings are somewhat blurred, a similar effect was noticed in the original Inria publication [17]. IV. PLATFORMS AND WORKFLOWS A. Platforms for Real-Time Inference We selected two COTS platforms, specifically designed for embedded applications, to deploy our U-Net model. The Xilinx Zynq UltraScale MPSoC, equipped with four ARM Cortex-A53 processor cores and programmable logic (com- monly referred to as an FPGA), has proven effective in both UAV [12] and space domains [24]. For our implementation, we utilized three Xilinx Zynq UltraScale boards Ultra96, ZCU102, and ZCU104 each equipped with the same pro- cessor but featuring varying FPGA sizes, to host the hardware accelerators. Nvidia Jetson platforms have also emerged as strong contenders for real-time inference of neural network- based vision algorithms, demonstrating applicability in UAV [30] and space domains [2]. Specifically, we employed the Nvidia Jetson AGX Xavier System on Module, which boasts eight ARM Cortex-A57 processor cores and an integrated GPU, enhancing the acceleration of neural network inference. Fig. 2: Detailed architecture of the U-Net model (a) Input image (b) Ground truth (c) U-Net prediction Fig. 3: Qualitative evaluation of our Float32 Keras lightweight U-Net on a 256x256 image of the validation set B. Workflows Overview We evaluated various workflows to implement our U-Net on CPU, GPU, and FPGA platforms.",
    "source": "2503.08700v1_Real-Time_Semantic_Segmentation_of_Aerial_Images_U.pdf",
    "length": 2003,
    "tokens": 468
  },
  {
    "text": "5. If submodules are found in the input code, provide a combined description of the general functionalities of all submodules. Avoid detailed internal logic descriptions within submodules and use qualifiers like \"maybe\" to prevent specific functional assumptions based on names. Integrate the functionality descriptions of all submodules into overall module functional description. 6. Avoid speculation on specific product models, such as GPU manufacturer and model numbers. 7. Do not mention any detailed internal logic or variable transformations. The description should include answers to the following question with professional words terms in Verilog EDA RTL: Based on the functionality implemented by the provided Verilog code, what is the professional or functional term for this code? Please ensure the reply accurately reflects the specific operation or module described by the code. If the code cannot be described in terms of functionality, such as in cases of interface definitions or simple variable definitions, do not infer based on variable or module names or specification I provided. Instead, format the output to end with \"It does not perform any logical functions. \", refer to the following example for generating the response: Must respond in one complete sentence and do not use bullet points or numbered lists. Try to keep the length of answer under 100 tokens. Format the output to begin with: 'The module ... implements ...'. Here's an example output format for your reference: The module [module name] implements [overall function description]. [Optional: If it cannot be described in terms of functionality, such as in cases of interface definitions or simple variable definitions] It does not perform any logical functions. Filtering Line-level Description Prompt You are an expert in hardware design with a deep understanding of Verilog RTL codes. Now I want to train a large language model for code comment generation tasks specifically focused on Verilog RTL codes. Your task is to evaluate the relevance of the code and its corresponding comment. Please note that the comment should be directly related to the code, and the comment should be explicitly stated in the code I provide. Do not include any speculative information, such as inferring functionality from variable names that the code itself does not explicitly show. If so, generate \" True \", otherwise, generate \" False \". Your answer should strictly be \" True \" or \" False \", no other content is allowed to be generated. Figure 4: Detailed prompts used in the CoT annotation process.",
    "source": "2502.15832v1_DeepRTL_Bridging_Verilog_Understanding_and_Generat.pdf",
    "length": 2576,
    "tokens": 482
  },
  {
    "text": "Journal of Research Practice, 10(1), 2014. Dur E Shahwar Kundi, Song Bian, Ayesha Khalid, Chenghua Wang, MÃ¡ire O Neill, and Weiqiang Liu. Axmm: Area and power efficient approximate modular multiplier for r-lwe cryptosystem. In 2020 IEEE International Symposium on Circuits and Systems (ISCAS), pp. 1 5. IEEE, 2020. Los Alamos National Laboratory. Los alamos national laboratory and sk hynix to demonstrate first-of-a-kind ordered key-value store computational storage device. gov news 0728-storage-device . (Accessed on 11 13 2023). Shiju Li, Kevin Tang, Jin Lim, Chul-Ho Lee, and Jongryool Kim. Computational storage for an energy-efficient deep neural network training system. In European Conference on Parallel Processing, pp. 304 319. Springer, 2023. Weiqiang Liu, Sailong Fan, Ayesha Khalid, Ciara Rafferty, and MÃ¡ire O Neill. Optimized schoolbook polynomial multiplication for compact lattice-based cryptography on fpga. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 27(10):2459 2463, 2019. Corne Lukken and Animesh Trivedi. Past, present and future of computational storage: A survey. arXiv preprint arXiv:2112.09691, 2021. Siwei Ma, Xinfeng Zhang, Chuanmin Jia, Zhenghui Zhao, Shiqi Wang, and Shanshe Wang. Image and video compression with neural networks: A review. IEEE Transactions on Circuits and Systems for Video Technology, 30(6):1683 1698, 2019. Vikram Sharma Mailthody, Zaid Qureshi, Weixin Liang, Ziyan Feng, Simon Garcia De Gonzalo, Youjie Li, Hubertus Franke, Jinjun Xiong, Jian Huang, and Wen-mei Hwu. Deepstore: In-storage acceleration for intelligent queries. In Proceedings of the 52nd Annual IEEE ACM International Symposium on Microarchitecture, pp. 224 238, 2019. Michael Mesnier. Intel labs showcases multi-vendor, computational storage plat- form.",
    "source": "SaLT.pdf",
    "length": 1796,
    "tokens": 485
  },
  {
    "text": "Symp. on High-Performance Computer Architecture (HPCA), 2020, p. 328 341. [9] B. Keller, R. Venkatesan, S. Dai, S. G. Tell, B. Zimmer, C. Sakr, W. J. Dally, C. T. Gray, and B. Khailany, A 95.6-TOPS W deep learning inference accelerator with per-vector scaled 4-bit quantization in 5 nm, IEEE Journal of Solid-State Circuits, vol. 58, no. 4, p. 1129 1141, 2023. [10] S. Lu, M. Wang, S. Liang, J. Lin, and Z. Wang, Hardware accelerator for multi-head attention and position-wise feed-forward in the trans- former, in IEEE Intern. System-on-Chip Conference (SOCC), 2020, pp. 84 89. [11] H. Jang, J. Kim, J.-E. Jo, J. Lee, and J. Kim, Mnnfast: a fast and scalable system architecture for memory-augmented neural networks, in Intern. Symp. on Computer Architecture (ISCA), 2019, p. 250 263. [12] Z. Wang, G. Wang, and G. He, COSA plus: Enhanced co-operative systolic arrays for attention mechanism in transformers, IEEE Trans. on Computer-Aided Design of Integrated Circuits and Systems (TCAD), vol. 44, no. 2, p. 723 736, 2025. [13] T. J. Ham, Y. Lee, S. H. Seo, S. Kim, H. Choi, S. J. Jung, and J. W. Lee, ELSA: Hardware-software co-design for efficient, lightweight self- attention mechanism in neural networks, in Intern. Symp. on Computer Architecture (ISCA), 2021, p. 692 705. [14] Z. Song, C. Qi, Y. Yao, P. Zhou, Y. Zi, N. Wang, and X. Liang, TSAcc: An efficient tempo-spatial similarity aware accelerator for attention acceleration, in ACM IEEE Design Automation Conference, 2024.",
    "source": "2505.14201v1_FLASH-D_FlashAttention_with_Hidden_Softmax_Divisio.pdf",
    "length": 1484,
    "tokens": 473
  },
  {
    "text": "Therefore, the performance of QServe and vLLM outperform Oaken in this range by leveraging the higher parallelizable resources available on GPUs. However, as the se- quence length increases, Oaken-HBM surpasses other baselines, Table 3: Accuracy and effective bits using the Llama2-7B model with varying number of groups and group ratios while keeping the total inner and outer group ratio at 10 . (o m i) 4 90 6 5 4.8 5.526 90 10 5 4.8 5.804 10 90 5 4.8 5.546 4 90 3 3 5 5.6 5.523 2 2 90 6 5 5.6 5.516 2 2 90 3 3 5 5.6 5.516 4 90 3 3 4 4.8 5.572 2 2 90 6 4 4.8 5.531 2 2 90 3 3 4 4.8 5.532 Outlier Bits Wikitext2 Perplexity Effective Bitwidth Group Ratio 1K 2K 4K 8K 16K 32K GPU (vLLM) GPU (QServe) Tender LPU Oaken-LPDDR Oaken-HBM Throughput (token sec) Total Sequence Length 1500 1200 900 600 300 0 Figure 13: Throughput results on Llama2-13B model with a batch size of 16 when increasing total sequence length from 1K to 32K. The ratio of input and output length is set to 1:1. including Oaken-LPDDR, with its high memory bandwidth and KV cache quantization. However, HBM-based systems including QServe and Oaken-HBM cannot handle sequences longer than 16K, making it difficult to complete the entire batch due to insufficient ca- pacity. Oaken-LPDDR, on the other hand, can accommodate longer sequences of up to 32K by mitigating both bandwidth and capacity pressure through KV quantization and large-capacity memory. Real-world benchmark. Figure 14 presents the generation through- put results for Llama2-13B and Mixtral-8x7B models, evaluated us- ing two real-world LLM inference traces. We exclude Oaken-HBM and QServe for Mixtral-8x7B model, as Oaken-HBM s memory can- not accommodate the entire model and QServe lacks support for MoE layers.",
    "source": "2503.18599v2_Oaken_Fast_and_Efficient_LLM_Serving_with_Online-O.pdf",
    "length": 1751,
    "tokens": 499
  },
  {
    "text": "Simulation is performed using Pytorch and the IBM Analog hardware acceleration kit (aihwkit) [17]. The aihwkit is an open-source toolkit integrated within Pytorch to enable the simulation of analog crossbar arrays. It is centered around the concept of analog tile, a building block that captures the computation performed on a crossbar array. The toolkit is designed to use customized unit cell configurations and ad- vanced optimization algorithms like tiki-taka algorithms. The toolkit can simulate analog tiles of different device materials such as ReRAM, PCM ECRAM, etc. We simulate HfOx-based ReRAM devices, as they have many desirable properties, such as non-volatility, energy efficiency, high density, and ability to scale. A detailed insight into the device structure can be found here [27]. We list all relevant hyperparameters used in Table I. III. RESULTS A. Comparison between Analog TL and Digital TL Fig. 2 shows the test error trace for analog and digital TL for a 2-class and 5-class task for the Swin-ViT model. The figure also shows the test error trace when the models under consideration are trained from scratch. For the ViT model, the performance of the analog TL model is better than the results for regular analog training for both fine-tuning tasks. It is also shown that the analog TL model outperforms the digital 100 101 102 of Epoch 100 101 102 Test Error ( ) Model ViT, Class [Beaver, Otter] Dig_TL Dig_Ref Analog_Ref Analog_TL 100 101 102 of Epoch Test Error ( ) Model ViT, Class [Apple, Man, Beetle, Tiger, Otter] Dig_TL Dig_Ref Analog_Ref Analog_TL Figure 2: Training results of the analog and digital Swin- ViT model for TL and regular training (referred to as Ref) performed on the 2-class task (left) and 5-class task (right) using c-TTV2 algorithm . 100 101 102 Weight Transfer Noise [ ] 10 20 30 40 50 Test Error ( ) 2-Class 5-Class Figure 3: Relationship between the inference accuracy and weight transfer noise for Swin-ViT analog TL model for the 2-class and 5-class tasks. model s performance. These observations can be explained by the presence of additional noise in the analog model training process.",
    "source": "2505.11067v1_Assessing_the_Performance_of_Analog_Training_for_T.pdf",
    "length": 2146,
    "tokens": 499
  },
  {
    "text": "The goal is to let the GPU hardware scheduler manage the execution of these two processes such 3 Fig. 2. SM throughput comparison with an increasing number of input tokens demonstrates compute intensity in the prompt processing phase. Fig. 3. Memory throughput doesn t change, but increasing output tokens results in sustained memory usage. Fig. 4. SM and memory throughputs running a batch of 5 inference requests. Throughputs are similar to single inference and are not fully utilized. that there is a possible overlap of prompt and token phases between the two vLLM instances. To maximize memory reuse, we needed to instantiate a single model that could be shared. The vLLM engine was modified to consume this shared model instead of instantiating individual copies on startup. For this experiment, we tested both with multiprocessing (MPx2) and with MPS (MPSx2). MPSx2 is the same as MPx2 except running as an MPS client. 3) Attempt 2: vLLM Scheduler Multiprocessing: It is noted that the first attempt has scalability limitations as the number of processes and max batch size is limited by available GPU memory. As the vLLM scheduler is already capable of efficient memory management and knowing best when swapping is necessary, it would be reasonable to keep it as the singular top-level scheduler that is aware of all request contexts. So, in the following attempt, the goal is to create and execute separate processes within the scheduler explicitly when both a prompt and token phases can be scheduled. 4 IV. EXPERIMENTS EVALUATION The setup used to evaluate these approaches is described in Table I TABLE I EXPERIMENT SETUP CONFIGURATIONS Hugging Face vLLM Model facebook opt-125m facebook opt-125m GPU A10, A100 A10 Input Token Size 512 1024 Output Token Size 20 1024 Batch Size 20 10, 20, 40, 80, 160 A. Profiling LLM Phases Fig. 5. KV-Cache usage ( ) for a range of batch sizes, for the prompt and token phase. In Figure 2 it is observed that increasing the number of input tokens increases SM throughput across the prompt processing kernels, supporting that the prompt phase is compute intensive as the input tokens get processed in parallel.",
    "source": "2505.03763v1_Splitwiser_Efficient_LM_inference_with_constrained.pdf",
    "length": 2156,
    "tokens": 471
  },
  {
    "text": "Considering the dense features, 3D geometry and the visual attributes captured in PC, especially for the media applications like telepresence and virtual visits, pre-processing [21], [44], [61], [84], compressing and storing [14], [16], [19], [47], [48], [74], post-processing and streaming [25], [40], [66], [76], [90] PC using a mobile device, while maintaining a reasonable quality of service (QoS), are fast becoming challenging tasks. Speciï¬cally, PC compression (PCC, or PC encoding) consists of both geometry (e.g., x, y, z coordinates in the 3D space) and attribute (e.g., RGB colors) compression. Our experiments show that PCC is the most expensive computation in a PC processing pipeline that takes 4seconds, especially when deployed in mobile edge devices, and hence, is a major contributor to the performance, video quality, and transmission energy, for the entire PC pipeline. However, it is challenging to design an optimal point cloud 282 2022 55th IEEE ACM International Symposium on Microarchitecture (MICRO) 978-1-6654-6272-3 22 31.00 2022 IEEE DOI 10.1109 MICRO56248.2022.00031 2022 55th IEEE ACM International Symposium on Microarchitecture (MICRO) 978-1-6654-6272-3 22 31.00 2022 IEEE DOI: 10.1109 MICRO56248.2022.00031 Authorized licensed use limited to: Penn State University. Downloaded on August 10,2023 at 18:50:20 UTC from IEEE Xplore. Restrictions apply. compression (PCC) pipeline which is fast (within or close to real-time), accurate (with good quality), and efï¬cient (with high compression ratio). The state-of-the-art PCC pipeline typically utilizes tree structures like Octree [63] or kd-tree [62] for compression, and often, the tree construction becomes a bottleneck due to lack of parallelization. Moreover, the conventional PC typically stores the geometry, while a wide array of applications, especially the ones meant for content consumption, infotainment and gaming, need the attributes to be stored as well, hence making the compres- sion even more complex.",
    "source": "PCcompress.pdf",
    "length": 1999,
    "tokens": 499
  },
  {
    "text": "(DATE), 2021, pp. 402 407. [6] M. Wang, R. Xue, J. Lin, and Z. Wang, Exploring quantization in few-shot learning, in 2020 18th IEEE Int. New Circuits and Systems Conf. (NEWCAS), 2020, pp. 279 282. [7] S. Kim, W. Lee, S. Kim, S. Park, and D. Jeon, An in-memory computing sram macro for memory-augmented neural network, TCAS- II, vol. 69, no. 3, 2022. [8] H. Li, W.-C. Chen, A. Levy, C.-H. Wang, H. Wang, P.-H. Chen, W. Wan, H.-S. P. Wong, and P. Raina, One-shot learning with memory- augmented neural networks using a 64-kbit, 118 gops w rram-based non-volatile associative memory, in 2021 Symp. on VLSI Technology, 2021, pp. 1 2. [9] H. Yang, C. E. Song, W. Xu, B. Khaleghi, U. Mallappa, M. Shah, K. Fan, M. Kang, and T. Rosing, Fsl-hdnn: A 5.7 tops w end-to- end few-shot learning classifier accelerator with feature extraction and hyperdimensional computing, in 2024 IEEE European Solid-State Electronics Research Conf. (ESSERC), 2024, pp. 33 36. [10] J. S. P. Giraldo, S. Lauwereins, K. Badami, and M. Verhelst, Vocell: A 65-nm speech-triggered wake-up soc for 10- Âµ w keyword spotting and speaker verification, IEEE J. of Solid-State Circuits, vol. 55, no. 4, pp. 868 878, 2020. [11] J. S. P. Giraldo, V. Jain, and M. Verhelst, Efficient execution of temporal convolutional networks for embedded keyword spotting, IEEE Trans. on Very Large Scale Integration (VLSI) Systems, vol. 29, no. 12, pp. 2220 2228, 2021.",
    "source": "2505.24852v2_Chameleon_A_MatMul-Free_Temporal_Convolutional_Net.pdf",
    "length": 1415,
    "tokens": 481
  },
  {
    "text": "These architectures represent fundamentally different approaches to AI acceleration, with designs optimized for specific performance char- acteristics rather than general-purpose computation. VII. Future Architectural Trends Our analysis of accelerator architectures and computational challenges highlights key trends that will shape future AI accelerators for tril- lion-parameter models. These trends represent not just incremental improvements but fundamental architectural shifts necessary to address the scaling challenges of next-generation LLMs. A. Heterogeneous Memory Systems Future accelerators will increasingly adopt heterogeneous memory systems that combine high- bandwidth but limited-capacity memory (like HBM) with larger, slightly slower memory tiers. This approach addresses the fundamental challenge of model sizes exceeding practical HBM capacities. The Compute Express Link (CXL) standard is emerging as a key technology for implementing heterogeneous memory systems, enabling coherent memory expansion beyond accelerator-attached HBM. CXL-attached memory pools can provide terabytes of additional capacity with lower bandwidth but acceptable latency for less frequently accessed model parameters. Fig. 9 illustrates the concept of a heterogeneous memory architecture for LLM inference, showing a tiered approach with different memory technologies. Fig. 9. Heterogeneous memory architecture for LLM inference, showing tiered approach with HBM for frequently accessed parameters (attention mechanism, current layer), CXL-attached DRAM for less frequently accessed parameters (other layers), and optional NVMe storage for lowest priority parameters. Key Benefits of Heterogeneous Memory Architecture Cost Efficiency: Expensive HBM used only for critical data paths Scalability: Supports larger models by using tiered storage Performance Optimization: Places parameters based on access frequency Hardware Utilization: Better utilization of available memory resources Flexibility: Adaptable to different model sizes and deployment scenarios Implementation Considerations Memory Management: Software stack must support intelligent parameter placement Prefetching: Predictive loading of parameters from slower to faster tiers CXL Integration: Leverages CXL.mem protocol for coherent memory expansion Quantization: Different precision formats can be used at different tiers Attention to Bandwidth Ratios: Design system to avoid bottlenecks between tiers Our analysis indicates that a well-designed heterogeneous memory system could support models up to 5-10 larger than current HBM- only solutions, with only 15-30 performance degradation for typical inference workloads. This represents a favorable tradeoff for deployment of trillion-parameter models where alternative approaches would require complex distributed inference strategies.",
    "source": "2506.00008v1_AI_Accelerators_for_Large_Language_Model_Inference.pdf",
    "length": 2851,
    "tokens": 491
  },
  {
    "text": "P5 P12 P11 P13 P1 P10 P4 P9 P3 P6 P7 P2 P8 C1 C2 O1 O2 O4 S7 S9 S6 S5 S4 S8 S3 S2 S10 S1 0.0 0.5 1.0 1.5 2.0 2.5 3.0 CPI Baseline L1i L1d L2 caches L1d stride prefetcher ROB Load queue Store queue Load pipes Load-store pipes ALU issue width Floating-point issue width Load-store issue width Commit width Branch predictor Maximum icache fills Fetch buffers Fetch width Decode width Rename width Figure 16: CPI attribution for ARM N1 across all workloads Sample index 0 1 2 3 CPI Figure 17: CPI attributions for all Search3(P9) sample regions ARM N1 is responsible for the performance degradation relative to the baseline. This provides a bird s eye view of the dominant per- formance bottlenecks across the entire corpus of workloads. For instance, all the proprietary programs and half of the SPEC2017 programs are mainly backend bound on ARM N1, with prominent bottlenecks being the Load queue size and ROB size. A few programs such as S4(541.leela_r) (a chess engine using tree search) are frontendbound,withtheTAGEbranchpredictorthemostprominent frontend bottleneck. Cache sizes and L1 prefetching have a large effect on some SPEC2017 benchmarks (e.g., S10(502.gcc_r), S1) but a less pronounced impact on our proprietary workloads, perhaps in indication of our programs being cache-optimized. Foradeeperlook,wecanfurtherzoomintothebehaviorofasingle program.Forexample,Figure17showstheCPIattributionforall2000 sampleregionsofP9,sortedonthex-axisbasedontheirsensitivityto cachesize.AlthoughtheP9barinFigure16showslimitedsensitivity to cache sizes on average, the zoomed-in view in Figure 17 shows high sensitivity to cache size in about 10 of the sampled regions, highlighting the different phase behaviors in the program [39]. 7 Related Work Conventional CPU simulators.",
    "source": "2503.23076v1_Concorde_Fast_and_Accurate_CPU_Performance_Modelin.pdf",
    "length": 1772,
    "tokens": 478
  },
  {
    "text": "Convolutional layers dominate image-processing models, whereas fully connected layers appear frequently in language and recommendation models, each requiring tailored accelerator resources and mappings. A.2.2 Cost Models Accurate and efficient estimation of accelerator performance and resource usage is provided by analytical cost models such as MAESTRO [25] and Timeloop [26]. These tools analyze hardware configurations and mapping strategies to quantify runtime, area, and power consumption, thus enabling informed decisions during accelerator design optimization. B Policy Network Architecture In the experiment, we use a 4-layer multilayer perceptron with ReLU activations after each hidden layer. The input dimension is 512. The hidden layers have widths 4096, and the output layer matches the action space size. The layer dimensions are: 512 4096 4096 4096 parameters for Ï€Î¸(a1, ..., aN; s0). The network outputs conditional probability distributions for the design actions. Specifically, each design parameter pi is modeled using a parametric distribution: Beta distribution: requires 2 parameters (e.g., Î±i, Î²i) Categorical distribution: requires k parameters for k categories (e.g., logits) Let N be the number of design parameters, and let dim(pi) denote the number of parameters required for pi. Then, the total output size of the policy network is: parameters for Ï€Î¸(a1, ..., aN; s0) N X i 1 dim(pi) C Broader Impacts This work proposes a constraint-aware reinforcement learning framework to accelerate simulation- based DSE for DNN accelerator co-design. The primary positive societal impact lies in reducing 12 the computational and time cost of designing efficient AI hardware, which may enable deployment in resource-constrained domains such as edge computing, mobile healthcare, and environmental monitoring. Our method promotes sustainability by optimizing design with fewer simulations, contributing to greener hardware design workflows. However, potential negative societal impacts include the risk of such optimization techniques being used to develop accelerators for ethically concerning applications. Furthermore, design automation could displace roles traditionally held by domain experts. These concerns warrant thoughtful deployment and governance when adopting such techniques. We believe that transparent reporting, open-access implementation, and continued interdisciplinary dialogue are important to mitigate misuse and align technical advancement with societal values. 13",
    "source": "2506.03474v1_CORE_Constraint-Aware_One-Step_Reinforcement_Learn.pdf",
    "length": 2505,
    "tokens": 476
  },
  {
    "text": "The model employs a GNN with a novel functionality-aware message passing mechanism that aggregates information from neighboring nodes while distinguishing between AND and NOT gates through specialized operators. To achieve this, PolarGate [107] introduces an ambipolar embedding space, where each node is mapped to both a positive and a negative embedding to represent the two logical states. It also uses differentiable A Survey of Circuit Foundation Model: Foundation AI Models for VLSI Circuit Design and EDA 25 logical operators, such as OPAND and OPNOT, that are designed to be differentiable and compatible with embedding propagation in the AIG structure. Additionally, the message passing strategy is modified to adhere to Boolean logical behavior, ensuring more accurate functional representation of the circuit. These innovations enable PolarGate [107] to effectively capture the logical operations of circuits and improve the model s ability to process and learn from netlist-based designs. Supervised AIG encoder enhanced for sequential circuits. Beyond focusing on the com- binational logics of AIGs, DeepSeq [109] explores capturing the sequential behavior of AIGs. DeepSeq [109] further advances this by using a directed acyclic GNN, which is optimized for sequential netlists. It incorporates a customized propagation scheme that avoids recursive prop- agation and handles cyclic sequential netlists in a single forward pass. The architecture separates learning into three distinct embedding spaces: structure embedding for circuit connectivity, function embedding for logic computations, and sequential embedding for capturing the temporal behavior between consecutive clock cycles. During pre-training, DeepSeq [109] uses multiple functional pre-training supervisions, including transition probability prediction to model sequential behav- ior, logic probability prediction to capture logic functionality, and pairwise truth-table difference to identify functional similarities among logic gates. These techniques enable DeepSeq [109] to effectively learn both the functional and sequential aspects of sequential AIG circuits. Self-supervised AIG encoder with contrastive learning. In addition to customized supervised pre-training tasks based on circuit properties, another key approach for netlist encoders is leveraging self-supervised learning techniques to learn from unlabeled circuit data and capture the intrinsic information of the circuit. FGNN [110, 111] is a pioneer in adopting self-supervised contrastive learning for AIG netlist encoding.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 2570,
    "tokens": 485
  },
  {
    "text": "This indicates that the INT8 quantization applied for deployment on Ascend 910 effectively preserves the model s capabilities across a diverse range of tasks. 5.4 Ablation Study To understand the individual contributions and effectiveness of key optimization techniques em- ployed in CloudMatrix-Infer, we conduct a series of ablation studies. These studies isolate the impact of our microbatch-based pipeline strategies for both prefill and decode phases, the Multi-Token Prediction (MTP) mechanism, and the EMS-based Context Caching. 5.4.1 Microbatch-based Pipeline This ablation study quantifies the performance impact of the microbatch-based pipeline strategies by comparing system performance with and without these microbatch optimizations. Decode Pipeline. We first evaluate our microbatch-based pipeline for the decode phase, pre- viously detailed in 4.2.3. The ablation compares system performance with and without this microbatch optimization. Figure 20a illustrates the decode throughput across various batch sizes. We observe that enabling the microbatch-based pipeline improves decode throughput by 5.8 , 9.4 , and 6.9 for batch sizes of 64, 96, and 128, respectively. This gain, while beneficial, is rela- tively more modest when compared to potential improvements reported for other platforms (e.g., SGLang [53] cited 35 on NVIDIA H100 clusters). This difference is primarily attributed to the inherently lower MoE dispatch and combine communication overheads on the CloudMatrix384 with its high-performance UB plane (as detailed in Section 5.5.1), compared to NVIDIA GPU clusters typically utilizing RDMA. With smaller MoE communication stalls on the UB plane, the improve- ment ceiling from communication hiding via microbatching is naturally more constrained for the CloudMatrix384. Figure 20b provides a per-layer latency breakdown for decode execution with a batch size of 96. It reveals that although individual microbatch execution latency for stages like Gating, Dispatch, and MoE is marginally increased due to decreased per-stream compute resources (e.g., 44 1K 2K 4K 8K Prompt Length 0 2000 4000 6000 8000 Prefill Throughput (tokens s) With Microbatch Without Microbatch (a) Prefill throughput.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2220,
    "tokens": 462
  },
  {
    "text": "385 399. [11] HU, Y., LIU, Y., AND LIU, Z. A survey on convolutional neural network accelerators: Gpu, fpga and asic. In 2022 14th International Conference on Computer Research and Development (ICCRD) (2022), IEEE, pp. 100 107. [12] JIN, W., LI, Y., XU, H., WANG, Y., JI, S., AGGARWAL, C., AND TANG, J. Adversarial attacks and defenses on graphs. ACM SIGKDD Explorations Newsletter 22, 2 (2021), 19 34. [13] KIM, Y., DALY, R., KIM, J., FALLIN, C., LEE, J. H., LEE, D., WILKERSON, C., LAI, K., AND MUTLU, O. Flipping bits in memory without accessing them: An experimental study of dram disturbance errors. ACM SIGARCH Computer Architecture News 42, 3 (2014), 361 372. [14] KIPF, T. N., AND WELLING, M. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016). [15] KOSE, H. T., NUNEZ-YANEZ, J., PIECHOCKI, R., AND POPE, J. A survey of computationally efficient graph neural networks for reconfig- urable systems. Information 15, 7 (2024), 377. [16] KUMMER, L., MOUSTAFA, S., SCHRITTWIESER, S., GANSTERER, W., AND KRIEGE, N. Attacking graph neural networks with bit flips: Weisfeiler and leman go indifferent. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (2024), pp. 1428 1439. [17] LI, G., HARI, S. K. S., SULLIVAN, M., TSAI, T., PATTABIRAMAN, K., EMER, J., AND KECKLER, S. W. Understanding error propagation in deep learning neural network (dnn) accelerators and applications.",
    "source": "2507.05531v1_Bit-Flip_Fault_Attack_Crushing_Graph_Neural_Networ.pdf",
    "length": 1467,
    "tokens": 491
  },
  {
    "text": "Action networks Ï€ help each node determine the 6 Running Title for Header Figure 3: An abstract illustration of the CoGNNs architecture, where a dedicated MLP is constructed for each prediction target. way information flows in and out, while environment networks Î· update the features of each node according to the state of the node. Therefore, CoGNNs update the feature representation hv of each node v according to the following steps. To begin with, the action networks Ï€ will obtain the action of each node. For a set of actions Î¦ {S, L, B, I}, the action of node v can be drawn from a categorical distribution pv R Î¦ by aggregating the information of its neighbors N(v). pv Ï€ (hv, N(v)) (5) Next, we input the probability vector pv into the Gumbel-softmax Estimator to obtain the action of the node av. We can use environment networks Î· to update the feature representation hv and obtain h v. h v Î· (hv, N) (6) where N is related to the value of av: N {hu u N(v), au S B} , av L S {} , av B I (7) For the environment and action networks, we employ a combination of aggregation models (e.g., MEANGNN, SUMGNN) to iteratively update node feature representations. To construct the graph-level embedding, we replace CoGNNs direct node feature summation with an attention-based mechanism [33], which assigns importance scores tv to each node v based on its relevance to the HLS prediction task. This approach captures the heterogeneous contributions of nodes (e.g., loop headers vs. memory operations) and computes the final graph-level vector hG by weighted summation. hG n X i 1 expMLP1(hi) Pn j 1 expMLP1(hj) ! MLP2 (hi) (8) where n is the total number of nodes in the data graph, and MLP1 and MLP2 are used to map hi and hj to R. In the prediction phase, we set up MLPs for each prediction target. Compared to SOTA works such as [15], this method only requires training one prediction model to predict all targets.",
    "source": "2504.19649v2_Intelligent4DSE_Optimizing_High-Level_Synthesis_De.pdf",
    "length": 1917,
    "tokens": 487
  },
  {
    "text": "This allows us to perform flexible and efficient neighborhood training by localizing the training parameters and freezing the rest of the parameters as much as possible to save cost. To this end, we propose a method for continual adaptation of EoE systems by LLM expert split, merge, grow, and shrink. For example, as shown in Figure 3, we can split a large expert model into smaller domain-specific models and train them on smaller, focused datasets in corresponding domains. After training, we merge them in the final stage to form a reinforced model. Each expert can also grow to absorb new knowledge and shrink to discard obsolete knowledge. Domain-Based Expert Splitting and Merging. We propose a novel and efficient approach for continual learning of EoE. We split a large expert into smaller, domain-specific experts, train them on focused domain- specific datasets, and then merge them back into a reinforced model. This strategy is particularly useful when we need a single model to handle diverse knowledge sources or when we aim to reduce training costs by first training smaller models and then combining them. To create a smaller model for a domain- specific dataset D, we will identify parameters that have the least influence on loss L(D) and remove them from the large expert. The importance of each weight at index i, denoted as IWi, can be approximated by: IWi L(D) LWi 0(D) , where LWi 0(D) is the loss, by setting parameter Wi to zero. Our solution will leverage our previous work on LLM structured pruning [140], unstructured pruning [181], and parameter- efficient fine-tuning [32,33]. We will explore several directions for estimating IWi efficiently such as using a memory-efficient zeroth-order optimizer to estimate gradients using only forward passes [101]. To merge experts into a single one, assuming we have n experts each with parameter Î¸i, i 1, . . . , n, we merge them into one model Î¸m by computing a weighted average of parameters where the weight is each parameter s Fisher information: Î¸m Pn i 1 FiÎ¸i Pn i 1 Fi, where Fi is the Fisher Information for Î¸i. Knowledge Adaptation via Expert Growing and Shrinking. To incorporate new knowledge, we will explore novel algorithms for expert-growing via life-long learning [147].",
    "source": "NSF_LLM_Medium_Proposal.pdf",
    "length": 2258,
    "tokens": 493
  },
  {
    "text": "Larger, Cheaper, but Faster: SSD-SMR Hybrid Storage Boosted by a New SMR-Oriented Cache Framework. In MSST, 2017. [3] Seongjin Lee, Youjip Won, and Sungwoo Hong. Mining-Based File Caching in a Hybrid Storage System. In JISE, 2014. [4] Wes Felter, Anthony Hylick, and John Carter. Reliability-Aware Energy Man- agement for Hybrid Storage Systems. In MSST, 2011. [5] Kai Bu, Meng Wang, Hongshan Nie, Wei Huang, and Bo Li. The Optimization of the Hierarchical Storage System Based on the Hybrid SSD Technology. In ISDEA, 2012. [6] KR Krish, Bharti Wadhwa, M Safdar Iqbal, M Mustafa Rafique, and Ali R Butt. On Efficient Hierarchical Storage for Big Data Processing. In CCGrid, 2016. [7] Lin Lin, Yifeng Zhu, Jianhui Yue, Zhao Cai, and Bruce Segee. Hot Random Off- Loading: A Hybrid Storage System with Dynamic Data Migration. In MASCOTS, 2011. [8] Junpeng Niu, Jun Xu, and Lihua Xie. Hybrid Storage Systems: A Survey of Architectures and Algorithms. In IEEE Access, 2018. [9] Jianzhe Tai, Bo Sheng, Yi Yao, and Ningfang Mi. SLA-Aware Data Migration in a Shared Hybrid Storage Cluster. In CC, 2015. [10] Jiaxin Ou, Jiwu Shu, Youyou Lu, Letian Yi, and Wei Wang. EDM: An Endurance- Aware Data Migration Scheme for Load Balancing in SSD Storage Clusters. In IPDPS, 2014. [11] Yuxia Cheng, Wenzhi Chen, Zonghui Wang, Xinjie Yu, and Yang Xiang. AMC: An Adaptive Multi-Level Cache Algorithm in Hybrid Storage Systems. In CCPE, 2015. [12] Ziliang Zong, Ribel Fares, Brian Romoser, and Joal Wood. FastStor: Data-Mining- Based Multilayer Prefetching for Hybrid Storage Systems. In CC, 2014. [13] Chihiro Matsui, Chao Sun, and Ken Takeuchi.",
    "source": "2503.20507v2_Harmonia_A_Multi-Agent_Reinforcement_Learning_Appr.pdf",
    "length": 1626,
    "tokens": 497
  },
  {
    "text": "Below, we will first discuss the empirical analysis results for KV cache distribution and then describe the three techniques one by one. 4.1 Observations in KV Distribution Existing LLM quantization methods often suggest that dealing with large values has a significant impact on model accuracy [15, 19, 22, 33, 71, 80, 86]. Moreover, other prior works suggest that small values near zero can vanish due to underflow during quantization, leading to larger error [2, 13, 27, 34]. These observations underscore the importance of analyzing the distribution and characteristics of the quantization targets. We examine the value distribution of the KV cache across several LLMs and datasets and derive key insights for designing effective quantization techniques. Observation 1. Figure 6(a) presents the minimum and maximum range of KV cache values for each decoder layer across various LLMs using the Wikitext2 dataset. Notably, the magnitude of keys and values varies across models and among decoder layers within each model. These variations are distinctive properties of each model and decoder layer, driven by differences in their model weights. From this observation, we gain the insight that the quanti- zation factor should be determined separately for each model and its individual decoder layers. ISCA 25, June 21 25, 2025, Tokyo, Japan Kim, Hong, et al.",
    "source": "2503.18599v2_Oaken_Fast_and_Efficient_LLM_Serving_with_Online-O.pdf",
    "length": 1359,
    "tokens": 276
  },
  {
    "text": "340 346, 1962. [7] Y. Blumenfeld, I. Hubara, and D. Soudry, Towards cheaper inference in deep networks with lower bit-width accumulators, arXiv preprint arXiv:2401.14110, 2024. [8] S. Cass, Taking ai to the edge: Google s tpu now comes in a maker- friendly package, IEEE Spectrum, vol. 56, no. 5, pp. 16 17, 2019. [9] F.-C. Cheng, S. H. Unger, and M. Theobald, Self-timed carry-lookahead adders, IEEE Transactions on Computers, vol. 49, no. 7, pp. 659 672, 2000. [10] A. Delmas Lascorz, P. Judd, D. M. Stuart, Z. Poulos, M. Mahmoud, S. Sharify, M. Nikolic, K. Siu, and A. Moshovos, Bit-tactical: A software hardware approach to exploiting value and bit sparsity in neural networks, in Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, 2019, pp. 749 763. [11] C.-Y. Du, C.-F. Tsai, W.-C. Chen, L.-Y. Lin, N.-S. Chang, C.-P. Lin, C.-S. Chen, and C.-H. Yang, A 28nm 11.2 tops w hardware-utilization- aware neural-network accelerator with dynamic dataflow, in 2023 IEEE International Solid-State Circuits Conference (ISSCC). IEEE, 2023, pp. 1 3. [12] A. A. Farooqui and V. G. Oklobdzija, General data-path organization of a mac unit for vlsi implementation of dsp processors, in 1998 IEEE International Symposium on Circuits and Systems (ISCAS), vol. 2. IEEE, 1998, pp. 260 263. [13] A. Feldmann and D. Sanchez, Spatula: A hardware accelerator for sparse matrix factorization, in Proceedings of the 56th Annual IEEE ACM International Symposium on Microarchitecture, 2023, pp. 91 104.",
    "source": "2503.06342v1_Exploring_the_Performance_Improvement_of_Tensor_Pr.pdf",
    "length": 1561,
    "tokens": 486
  },
  {
    "text": "This resilience makes it suited for high-stakes environments where data integrity is essential. Adjusting Ï„ allows balancing decontamination and predictive accuracy. Figure 7 illustrates the relation between the filtering threshold Ï„ and pass rate accuracy in TED s application to VerilogEval. Accuracy of the Verigen-contaminated model decreases gradu- ally with higher Ï„, while the RTLCoder-contaminated model has a sharp decline at low Ï„ values but stabilizes as Ï„ grows. The results mirror those shown in Figure 6, with accuracy decreasing as Ï„ increases, a pattern consistent with TED s successful removal of memorization-based correct inferences. However, two key differences emerge in the VerilogEval evaluation: 1) The baseline functionality accuracy is notably lower than in RTLLM, and 2) The accuracy curve exhibits greater irregularity, reflecting VerilogEval s higher problem 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) Steeper decline when 20. Compared to RTLLM, limited details from VerilogEval make contamination easier to mitigate. on VerilogEval fine-tuned with RTLCoder Functional 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) Smooth decline from initial. Accuracy dropped from 0 to 100. on VerilogEval fine-tuned with RTLCoder 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) Steeper decline when 20. Less contaminated dataset mitigation with lower on VerilogEval fine-tuned with Verigen 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) Smooth decline from initial. With same on same benchmark, accuracy drop patterns show similarity. on VerilogEval fine-tuned with Verigen Fig. 7: Impact of Pass Rate on TED Mitigation Following Contamination Simulation on LLaMA 3.1 by Varying Ï„ for Top Exclusion on VerilogEval. complexity. This increased difficulty suggests that individual inference response exert a more substantial influence on overall pass rates in VerilogEval compared to RTLLM. Consequently, accuracy declines more steeply as Ï„ increases, particularly evident when comparing the fine-tuned with RTLCoder results between the two benchmarks (Figure 6).",
    "source": "2503.13572v3_VeriContaminated_Assessing_LLM-Driven_Verilog_Codi.pdf",
    "length": 2207,
    "tokens": 493
  },
  {
    "text": "To generate such annotations, where appropriate, we plan to use well-established data lineage tools such as Keboola [78] and Octopai [120]. The data lineage information will also help the project to reduce its storage footprint . More specifically, instead of storing all versions of each and every dataset and model we generate, we can only store the most important ones and, for the remaining ones, the data lineage information (a kind metadata ) can be used to re-generate them, if when needed. Collaboration Plan Project Team The proposed project spans design and analysis of LLM and expert models, characterization and evaluation of such models, development of compiler and runtime system support for efficient LLM expert training and inference as well as chiplet selection for LLM expert execution. The project will be managed by the three PIs from Penn State. The specific responsibilities of the PIs and their complementary expertise are explained below: Chitaranjan Das (PI): Das is the PI of the project and will be responsible for the overall coordination and progress as planned in the project schedule. His expertise includes multicore architectures, architectural op- timization of ML kernels, on-chip and chip-to-chip interconnect design, cloud computing, and performance evaluation. He will lead Thrust-3 and also co-lead Thrust-4 with Co-PIs Zhang and Kandemir. Mahmut Taylan Kandemir (Co-PI): Kandemir s expertise includes optimizing compilers, storage sys- tems, HPC, and workload characterization. He will lead Thrust-2 and collaborate with Das and Zhang in Thrust-4. Rui Zhang (Co-PI): Zhang s research expertise includes LLMs, trustworthy human-centered AI, and AI for science. He has an extensive research background and publication record in efficient methods for LLMs such as LLM pruning (NAACL 2024), LLM parameter-efficient finetuning (ACL 2022, EMNLP 2023), long-context LLMs (ACL 2022, NeurIPS 2024), data selection for LLM in-context learning (ICLR 2023). In the context of this project, he will lead Thrust-1 and also co-lead Thrust-4 with Kandemir and Das. All the three PIs will work in close coordination on the individual research topics as well as the overall integration of the project.",
    "source": "NSF_LLM_Medium_Proposal.pdf",
    "length": 2223,
    "tokens": 487
  },
  {
    "text": "These capabilities, combined with up to 6.5 faster runtime compared to NeuroSim V1.4, enable more extensive design space exploration than previous versions. Through our case studies with a variety of device tech- nologies and neural network topologies, we demonstrate how NeuroSim V1.5 can provide insights into CIM design trade- offs: 1) Optimal designs must balance array size, ADC preci- sion, and multi-level cell capability. 2) Network complexity influences sensitivity to device- level variations, particularly in larger networks like ResNet-50 and Swin-T. 3) Charge-domain computation (e.g., based on nvCap) is a promising device for future large-scale ACIM accelera- tion. Looking forward, the emergence of large language models (LLMs) with billions to trillions of parameters presents new challenges beyond on-chip memory capacity. These models will require GB-level high bandwidth memory (HBM) and process-in-memory (PIM) architectures that place compute logic near DRAM dies. While direct LLM and PIM support is beyond NeuroSim V1.5 s current scope, we have initiated efforts toward these capabilities through our work on 3D- stacked DRAM on TPU-like architectures [64], [65]. REFERENCES [1] C. E. Tripp, et al., Measuring the energy consumption and efficiency of deep neural networks: An empirical analysis and design recommenda- tions, arXiv:2403.08151, Mar. 2024. [2] S. Williams, et al., Roofline: An insightful visual performance model for multicore architectures, Commun. ACM, vol. 52, no. 4, pp. 65 76, Apr. 1 2009. [3] Y.-D. Chih, et al., 16.4 An 89 TOPS W and 16.3TOPS mm2 all- digital SRAM-based full-precision compute-in memory macro in 22nm for machine-learning edge applications, 2021 IEEE Interna-tional Solid- State Circuits Conference (ISSCC), vol. 64, pp. 252 254, Feb. 2021. [4] S. Yu, Neuro-inspired computing with emerging non-volatile memories, Proc. IEEE, vol. 106, pp. 260 285, Feb. 2018.",
    "source": "2505.02314v1_NeuroSim_V15_Improved_Software_Backbone_for_Benchm.pdf",
    "length": 1922,
    "tokens": 488
  },
  {
    "text": "MobiSys 16, 2016, pp. 291 304. [3] X. Corbillon, F. De Simone, and G. Simon, 360-Degreee Video Head Movement Dataset, in Proceedings of the 8th ACM on Multimedia Systems Conference, 2017, pp. 199 204. [4] Discovery, Caring for Rhinos: Discovery VR (360 Video). https: www.youtube.com watch?v 7IWp875pCxQ , 2019. [5] Discovery, Elephants on the Brink. v 2bpICIClAIg , 2019. [6] T. El-Ganainy and M. Hefeeda, Streaming Virtual Reality Content, CoRR, vol. abs 1612.08350, 2016. [Online]. Available: abs 1612.08350 [7] Facebook, Facebook 360, , 2019. [8] Facebook Inc., Facebook Oculus, . [9] Google, 360 videos - Google Arts Culture, google.com project 360-videos . [10] Google, More Ways to Watch and Play with AR and VR. https: blog.google products google-vr more-ways-watch-and-play-ar-and-vr . [11] Google, Build Virtual Worlds. , 2019. [12] Google, GVR Android SDK Samples - Video360. googlevr gvr-android-sdk blob master samples sdk-video360 src main java com google vr sdk samples video360 VrVideoActivity.java L257 , 2019. [13] M. Ham, I. Dae, and C. Choi, LPD: Low Power Display Mechanism for Mobile and Wearable Devices, in Proceedings of the USENIX Conference on Usenix Annual Technical Conference (ATC), 2015, pp. 587 598. [14] K. Han, Z. Fang, P. Diefenbaugh, R. Forand, R. R. Iyer, and D. Newell, Using Checksum to Reduce Power Consumption of Display Systems for Low-motion Content, in 2009 IEEE International Conference on Computer Design, 2009, pp. 47 53.",
    "source": "DejaView.pdf",
    "length": 1468,
    "tokens": 453
  },
  {
    "text": "13b, incorpo- rates four groups of buffers that leverage on-chip BRAM resources. These buffers temporarily store input activa- tions, model weights, and output activations before and af- ter quantization. The accelerator supports two configura- tions of PEs: (1) an 8-input integer multiplier-and-adder PE for uniform quantization, where both inputs and out- puts are integers and partial sums are updated using an ac- cumulator, and (2) an 8-input decimal bit-shift-and-adder PE for log2 quantization, with integer inputs, decimal out- puts, and a decimal accumulator for partial sums. In our 4- bit AHCPTQ implementation, the accelerator uses 64 lanes for multiplier and bit-shift PEs. To enable HLUQ, weights and activations must be dynamically allocated to their cor- responding PEs. This allocation logic is embedded in the controller, which indexes the MSBs of activations. When data is read from the IA buffer, weights and activations are automatically distributed to the corresponding PE types. After completing the integer and decimal inner product operations, the output values are transferred to the quantiza- tion processor in the FP32 domain. A small set of quantiza- tion parameter registers loads the scales required for the cur- rent and next layers, switching addresses to provide these scales to the quantization processor. The output activations are then fed into the dequantization unit, where, if HLUQ is applied, the uniform and log2 branches are merged. Subse- quently, the activations pass through the activation functions and quantization units, ensuring the resulting integer values can be directly used by the next layer. Finally, the activa- tions are sent to the output buffer. The dequantization, acti- vation function, and quantization processes are designed in a pipeline to minimize latency, which is critical for HLUQ deployment in practical applications. Weights are grouped offline and reordered based on group indices, while activations are reordered on-the-fly. At the start of layer computation, the accelerator accesses the DRAM to transfer weights and activations to the weight and IA buffers. Simultaneously, the quantization processor loads the quantization parameters required for quantization and dequantization.",
    "source": "2503.03088v2_AHCPTQ_Accurate_and_Hardware-Compatible_Post-Train.pdf",
    "length": 2257,
    "tokens": 481
  },
  {
    "text": "Kraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms Vivek M. Bhasi The Pennsylvania State University Jashwant Raj Gunasekaran The Pennsylvania State University Prashanth Thinakaran The Pennsylvania State University Cyan Subhra Mishra The Pennsylvania State University Mahmut Taylan Kandemir The Pennsylvania State University Chita Das The Pennsylvania State University Abstract The growing popularity of microservices has led to the pro- liferation of online cloud service-based applications, which are typically modelled as Directed Acyclic Graphs (DAGs) comprising of tens to hundreds of microservices. The vast majority of these applications are user-facing, and hence, have stringent SLO requirements. Serverless functions, hav- ing short resource provisioning times and instant scalability, are suitable candidates for developing such latency-critical applications. However, existing serverless providers are un- aware of the workflow characteristics of application DAGs, leading to container over-provisioning in many cases. This is further exacerbated in the case of dynamic DAGs, where the function chain for an application is not known a pri- ori. Motivated by these observations, we propose Kraken, a workflow-aware resource management framework that minimizes the number of containers provisioned for an ap- plication DAG while ensuring SLO-compliance. We design and implement Kraken on OpenFaaS and evaluate it on a multi-node Kubernetes-managed cluster. Our extensive ex- perimental evaluation using DeathStarbench workload suite and real-world traces demonstrates that Kraken spawns up to 76 fewer containers, thereby improving container uti- lization and saving cluster-wide energy by up to 4 and 48 , respectively, when compared to state-of-the art schedulers employed in serverless platforms. CCS Concepts Computer systems organization Cloud Comput- ing; Resource-Management; Scheduling. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee.",
    "source": "kraken.pdf",
    "length": 2474,
    "tokens": 500
  },
  {
    "text": "469 474. [7] S. Kim, A. Gholami, Z. Yao et al., I-BERT: Integer-only BERT Quantization, Jun. 2021, arXiv:2101.01321. [8] G. Islamoglu, M. Scherer, G. Paulin et al., ITA: An Energy-Efficient Attention and Softmax Accelerator for Quantized Transformers, in 2023 IEEE ACM International Symposium on Low Power Electronics and Design (ISLPED), Aug. 2023, pp. 1 6. [9] T. Dao, D. Y. Fu, S. Ermon et al., FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness, Jun. 2022, arXiv:2205.14135. [10] T. Dao, FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning, Jul. 2023, arXiv:2307.08691. [11] J. van der Hoeven and F. Johansson, Fast multiple precision exp(x) with precomputations, in 2024 IEEE 31st Symposium on Computer Arithmetic (ARITH), Jun. 2024. [12] H. Chen, L. Quan, and W. Liu, HGH-CORDIC: A High-Radix Gen- eralized Hyperbolic COordinate Rotation Digital Computer, in 2024 IEEE 31st Symposium on Computer Arithmetic (ARITH), Jun. 2024. [13] Q. Sun, Z. Di, Z. Lv et al., A High Speed SoftMax VLSI Architecture Based on Basic-Split, in 2018 14th IEEE International Conference on Solid-State and Integrated Circuit Technology (ICSICT), Oct. 2018. [14] H. Dong, M. Wang, Y. Luo et al., PLAC: Piecewise Linear Ap- proximation Computation for All Nonlinear Unary Functions, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 28, no. 9, Sep. 2020. [15] N. N. Schraudolph, A Fast, Compact Approximation of the Exponential Function, Neural Computation, vol. 11, no. 4, May 1999.",
    "source": "2504.11227v1_VEXP_A_Low-Cost_RISC-V_ISA_Extension_for_Accelerat.pdf",
    "length": 1540,
    "tokens": 482
  },
  {
    "text": "Experiment 1: Overall Performance of RETENTION To evaluate the overall performance of RETENTION, we compare Random Forest models with tolerance set to {1 , 3 , 5 } against baseline methods. As shown in Fig. 5, RE- TENTION exhibits substantial reductions, achieving 66.25 to 99.96 fewer TCAMs required (equivalent to 2.96 to 2420.81 better space efficiency) in energy-efficient map- ping, and 70.83 to 99.70 reduction (3.43 to 334.48 improvement) in space-efficient mapping. Specifically, setting tolerance to a moderate value of 3 yields 4.35 to 207.12 better space efficiency. These results demonstrate that RE- TENTION effectively minimizes CAM capacity requirement, offering a promising solution for resource-efficient tree-based model acceleration with CAM. C. Experiment 2: Purity Threshold Pruning Noticing the extraordinary performance of RETENTION, we break down the framework and evaluate each component separately. This section analyzes the effectiveness of purity threshold pruning by mapping Random Forest models with naive unified mapping. By setting tolerance to {1 , 3 , 5 }, purity threshold pruning alone achieves considerable im- provements, reducing CAM capacity requirement by 21.04 to 99.93 (1.27 to 1357.12 improvement) compared to unpruned models. The relatively lower improvement on the Letter and Wine datasets stems from the increased number of classes, which makes it more challenging for nodes to achieve high purity. Pruning such nodes may result in sig- nificant accuracy degradation, which the tolerance mecha- nism prevents. Consequently, only a few higher-purity nodes can be pruned, thereby limiting the impact of the purity threshold pruning. We further scale tolerance up to {10 , 15 , 20 } to investigate the correlation between tolerance and different datasets. As shown in the left part of Fig. 6, the performance of purity threshold pruning converges at different levels of tolerance, depending on task complexity.",
    "source": "2506.05994v1_RETENTION_Resource-Efficient_Tree-Based_Ensemble_M.pdf",
    "length": 1954,
    "tokens": 407
  },
  {
    "text": "This approach significantly improves both accuracy and computational efficiency, particularly for large-scale circuit designs, outperforming previous methods in terms of scalability and overall performance. Supervised AIG encoder enhanced with GNN architecture. In addition to DeepGate family, other works (i.e., GAMORA [54], HOGA [106] and PolarGate [107]) explore to customize the GNN architecture and message-passing mechanism to enhance the scalability and performance of AIG encoding, combining with supervised pre-training tasks. In GAMORA [54], netlist AIGs are transformed into a graph representation and processed using a GNN. During pre-training, the GNN is trained with multiple functionally driven tasks, which jointly reason about Boolean function aggregation and structural topology. This enables efficient symbolic reasoning for large- scale Boolean networks. Specifically, GAMORA [54] s model is designed to recognize fundamental functional components within circuits, including identifying adder root and leaf nodes and detecting XOR and MAJ functions. The multi-task learning framework enhances the model s ability to generalize across various functional tasks, leveraging shared representations to improve both accuracy and scalability in processing large AIG-based netlists. In HOGA [106], hop-wise features are precomputed for each design to capture interactions over multiple hops before training. This step is done independently of the graph structure, enabling scalability for distributed training. The AIG format of circuits is processed using a customized GNN with a hop-wise aggregation scheme, which precomputes features based on multiple hops. It also employs gated self-attention to adaptively learn high-order circuit structures. This approach avoids recursive aggregation, which can be computationally expensive for large circuits. The model is then trained using task-specific labels, allowing it to be adapted for downstream tasks. In PolarGate [107], each node in the netlist AIGs represents two logical states: low level (0) and high level (1), which are fundamental for Boolean logic tasks. The model employs a GNN with a novel functionality-aware message passing mechanism that aggregates information from neighboring nodes while distinguishing between AND and NOT gates through specialized operators. To achieve this, PolarGate [107] introduces an ambipolar embedding space, where each node is mapped to both a positive and a negative embedding to represent the two logical states.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 2520,
    "tokens": 489
  },
  {
    "text": "The logic design stage aims to generate a hardware descrip- tion, represented by HDLs such as Verilog and VHDL. This is achieved by either manually programming based on func- tional requirements or utilizing HLS tools based on hardware functionalities described in high-level programming languages such as C, C , or SystemC. The former approach simplifies the design flow through hardware abstraction. For instance, Nurvitadhi et al. [94] propose an automated transaction-to- pipeline transcompilation methodology. The ASSIST frame- work [95] supports RISC architecture design via micro- operation languages but lacks control over pipeline optimiza- tion. TL-Verilog [96] partitions combinational logic through temporal abstraction but exhibits deficiencies in data hazard detection. Languages such as BSV [97] and Koika [98] facili- tate formal verification but enforce single-cycle rule execution without dynamic scheduling. The latter approach generates hardware descriptions from C C . For example, Rokicki et al. [99] generate processor cores from C while requiring manual handling of bypass logic. Josipovi c et al. [100] intro- duce dynamic scheduling to optimize pipeline performance, while Dahlia [101] leverages affine types to ensure predictabil- ity in statically scheduled accelerators. However, these conven- tional methods rely on formal language template conversion, which incurs high learning costs and constrains design spaces. Thus, recent advancements employ AI algorithms for rapid estimation of quality, performance, and timing to enhance HLS efficiency. For example, Zhao et al. [102] utilize linear regression and Artificial Neural Networks (ANNs) to predict routing congestion in HLS. Makrani et al. [103] propose a neural network (NN)-based approach to predict resource utilization and performance on specific field programmable gate arrays (FPGAs), thereby improving the efficiency of DSE. Ferianc et al. [104] employ Gaussian processes for latency estimation to optimize accelerator configuration selection. The circuit design stage, also known as logic synthesis, aims to transform hardware descriptions into gate-level circuits, i.e., netlists. During this stage, Boolean expressions and logical structures are optimized based on specified process libraries to achieve minimal logical expressions and netlists.",
    "source": "2506.05007v1_QiMeng_Fully_Automated_Hardware_and_Software_Desig.pdf",
    "length": 2340,
    "tokens": 473
  },
  {
    "text": "Figure 2 illustrates an overview of the tiling logic. BLOCK_M TILE_SIZE MATRIX_A MATRIX_B MATRIX_C INTER-TILE LOOP BRAM CONTENT INTRA-TILE LOOP PARTIAL SUM PARTIAL SUM TILE Figure 2: Overview of tiled matrix multiplication approach. High-Level Dataflow Mapping Strategies Our design leverages multi-level mapping strategies to optimize both computation and data movement: I. Temporal Mapping: Tiling: We employ a two-level tiling approach (Block Tiling with ğ‘€ğ‘ 256 and Inner Tiling with ğ‘‡ 32) to decompose large matrices into smaller tiles. This allows matrix A to be loaded once into on-chip BRAM, significantly reducing DRAM accesses. Pipelining: The accumulation loop over the K dimension is pipelined (II 1), ensuring continuous processing and high throughput. Data Persistence: Persistent storage of matrix A enhances data reuse across multiple computations. II. Spatial Mapping: Loop Unrolling: Fully unrolling the innermost loops creates a 32 32 array of multipliers-adders for parallel computation. Memory Partitioning: Partitioning local tile arrays into registers enables simultaneous data access, reducing memory access bottlenecks. III. Inter Intra-tile Mapping: Inter-tile: Matrix B is partitioned into 256-column blocks, with each block assigned to a different processing column- tile with corresponding row-tile from Matrix A for parallel execution. Intra-tile: Within each tile, pipelined and fully unrolled loops enable efficient parallel processing of tiles. Together, these mapping strategies optimize the balance between computation and data movement, enabling high throughput on the resource-constrained Xilinx KV260. Recent data-centric approaches, such as those in MAESTRO [2] and the work on DNN dataflows [3], provide an analytical per- spective on how temporal and spatial mapping can be exploited to maximize data reuse. Inspired by these frameworks, our design ex- plicitly separates temporal mapping through techniques like tiling, pipelining, and data persistence from spatial mapping through loop unrolling and memory partitioning. This dual strategy mini- mizes off-chip accesses and maximizes parallelism, thereby achiev- ing high efficiency on an edge FPGA platform.",
    "source": "2503.16731v3_Design_and_Implementation_of_an_FPGA-Based_Hardwar.pdf",
    "length": 2200,
    "tokens": 497
  },
  {
    "text": "Buchberger, B. and Loos, R. Algebraic simplification. In Computer algebra: symbolic and algebraic computation, pp. 11 43. Springer, 1982. Carette, J. Understanding expression simplification. In Pro- ceedings of the 2004 international symposium on Sym- bolic and algebraic computation, pp. 72 79, 2004. Chang, K., Wang, Y., Ren, H., Wang, M., Liang, S., Han, Y., Li, H., and Li, X. Chipgpt: How far are we from natural language hardware design. CoRR, abs 2305.14019, 2023. Chen, D. and Cong, J. Register binding and port assignment for multiplexer optimization. In Proceedings of the 2004 Asia and South Pacific Design Automation Conference, ASP-DAC 04, pp. 68 73. IEEE Press, 2004. ISBN 0780381750. Chu, P. P. RTL hardware design using VHDL: coding for efficiency, portability, and scalability. John Wiley Sons, 2006. Cocke, J. Global common subexpression elimination. In Proceedings of a symposium on Compiler Optimization, pp. 20 24. ACM, 1970. doi: 10.1145 800028.808480. Cooper, K. D., Simpson, L. T., and Vick, C. A. Operator strength reduction. ACM Transactions on Programming Languages and Systems (TOPLAS), 23(5):603 625, 2001. Deepa Tilwani, R. V. and Sheth, A. P. Neurosymbolic ai approach to attribution in large language models. arXiv, 2410.03726, September 2024. doi: 10.48550 arXiv.2410. 03726. Paper under review. Diego Calanzone, S. T. and Vergari, A. Logically consistent language models via neuro-symbolic integration. arXiv, 2409.13724, September 2024. doi: 10.48550 arXiv.2409. 13724.",
    "source": "2504.10369v1_SymRTLO_Enhancing_RTL_Code_Optimization_with_LLMs_.pdf",
    "length": 1504,
    "tokens": 442
  },
  {
    "text": "Open-Source Framework: We will open-source our framework and artifacts to spur further research in ultra-low-bit LoRA fine-tuning. 2 This paper is organized as follows: Section 2 introduces LoRA fine-tuning, quantization for LoRA, and three key limitations of existing quantized LoRA methods. Section 3 presents the LowRA end-to-end workflow, while Section 4 discusses its key design insights and benefits. Sections 5 and 6 detail the mapping threshold learner and the fine-grained precision assignment, respectively. Finally, we describe our evaluation setup, results, and takeaways in Section 7. 2 Background and Motivation Pretrained Weight (T1) A B Intelligently Initialized Low-rank Tensors (T5) Mapping and Threshold Learner (P1) Fine-grained Mappings and Thresholds Per-output-channel Mixed-Precision Downquant Residual (T4) A B Final Low-rank Tensors (T6) f FP32 FP16 BF16 4-bit 2-bit 1-bit Programs Tensors Precision Assigner (P2) (T2) Precs (T3) Output- channel- wise Quantize Kernel (P3) Low-Rank Initializer (P4) Further Finetuning (P5) Output- channel- wise Dequantize Kernel (P5.1) f Out In User Deï¬ned Compression Ratio Figure 1: End-to-end workflow of LowRA. 2.1 Low-Rank Adaptation (LoRA) of LLMs Fine-tuning large language models (LLMs) allows us to adapt pre-trained LLMs to particular tasks or domains [50, 48, 55]. This process usually requires changing all model parameters, which can be prohibitively expensive (in terms of compute and memory) when the number of model parameters increases. Low-Rank Adaptation (LoRA) [14] tackles this by freezing the base weights and introducing a small set of trainable adapter parameters, drastically reducing memory and compute requirements for fine-tuning. 2.2 Quantization for LoRA Fine-Tuning Quantized LoRA fine-tuning further cuts memory usage by quantizing the base model weights without hurting performance.",
    "source": "2502.08141v1_LowRA_Accurate_and_Efficient_LoRA_Fine-Tuning_of_L.pdf",
    "length": 1875,
    "tokens": 470
  },
  {
    "text": "Rong et al. [182] instead partition CNNs into subnetworks via width-wise partitioning, grouping filters into blocks and exchanging filter blocks among subnetworks periodically to ensure equal training across all devices while reducing training costs. Given the various participating devices, their energy-budgets, and their CNN partitions, a heuristic approach is used to optimize the allocation of subnetworks to devices. The devices are ranked by their communication latency, and subnetworks by their number of weights, and matched accordingly. A \"resource configuration search\" then determines the minimum communication latency fitting the energy-budget of each device, taking into account time constraints for FL round completion. Other recent approaches model the FL trade-off as a joint optimization problem to minimize energy-cost while ensuring convergence, considering the various heterogenous devices and their energy budgets. Tran et. al [146] formulate a joint-optimization problem based on the sum of computation and communication energy. However, their approach requires the synchronous upload of weights from all devices, which is unrealistic, and does not provide convergence guarantees. Luo et. al [91] jointly optimize the number of participating devices and local iterations to minimize energy cost. They propose a sampling-based control solution that realizes minimal overhead and provides convergence guarantees. Yang et. al [177] derive energy consumption models of different FL approaches based on their convergence rates, utilizing these to form a novel joint-optimization problem. FL on intermittent energy-harvesting devices poses unique challenges, as devices can only participate when they have sufficient energy available. Given the energy-generation of participating devices is non-uniform, device scheduling (ğ‘–.ğ‘’., when to participate) becomes important. One approach is to let each device participate as soon as it can do so. However, Guler et. al [35] demonstrate that this approach can bias the global FL network towards devices with more frequent energy-availability, resulting in an overall loss in performance. Another approach is to wait until all devices have sufficient energy to participate in the FL round, then use conventional FL sampling. This mitigates bias but requires waiting on the device with the slowest energy generation, which can result in slow convergence. Guler et. al introduce a FL protocol, with convergence guarantees, in which devices decide 14 Millar et al. whether to participate in a given FL round via a stochastic process based on their energy profiles.",
    "source": "2505.12523v1_Energy-Aware_Deep_Learning_on_Resource-Constrained.pdf",
    "length": 2620,
    "tokens": 497
  },
  {
    "text": "This is for enabling users to freely move around in a large area without the need of connecting with a power cable constantly. Hence, the power energy efficiency is critical metrics in many AR use cases so that the battery lifetime can be sufficiently long. With these sensors and compute resources in place, an AR head- set executes a set of software tasks, either entirely or selectively based on the applications requirements [19]. Without loss of gener- ality, a typical AR pipeline [19] is shown in Fig. 1c. At a high level, this AR pipeline has three major stages: â¶Inputs stage first collects the real-time information from all the on-board sensors such as IMU, IR, camera and depth image sensors. With these inputs, â· Perception stage understands the current surrounding environment such as pose estimation for head rotations directions, eye tracking for pupil centers, and scene reconstruction for the current view analysis. Finally, â¸Visual stage combines the physical world with the virtual information (which is generated in real-time) together, and renders the final images (both the physical scene as well as the virtual frame augmented with it) for the user to view. We want to emphasize that, compared to virtual reality (VR), the AR video processing typically incurs additional computational Table 1: Ideal latency requirements [19]. Task Ideal Latency Algo. Pose Estimate 33 ms Kimera [53] Eye Track 33 ms NVGaze [26] Scene Reconstruct 100 ms InfiniTAM [50] Hologram 33 ms GSW [49, 63] tasks and interacts with more hardware resources [61]. Based on our measurements collected from a smartphone [60] running a sim- ple AR application [3], the processing performance can be lower than 0.5 fps, and the battery life can be as short as just 1 hour. This motivates us to investigate which component is the major perfor- mance and energy bottleneck, charging most of the performance- and or energy-taxes from the battery-backed AR headsets. 2.2 Motivation 2.2.1 What is the Major Bottleneck? To identify the major performance bottlenecks in the current AR headsets, we characterized the execution latency of the software pipeline (discussed above in Fig.",
    "source": "HoloAR.pdf",
    "length": 2167,
    "tokens": 464
  },
  {
    "text": "should satisfy the following condition: m(n AF P U Abank) AMax (3) This equation allows us to calculate m to obtain the maximum capacity achievable in a PIM-enabled HBM die using an nP1B PIM configuration. We use the analytical tool CACTI-3DD [60] to estimate area. The area of one HBM bank Abank is 0.83mm2 3 using a 22nm technology node. The area of one HBM die is constrained to 121 mm2 according to prior work [61]. The area of one FPU AF P U is 0.1025 mm2 [23]. Thus, the equation for a 4P1B PIM configuration becomes as follows: m(0.1025 4 0.83) 121 (4) Therefore, the maximum number of memory banks must be smaller than 97. In our design, we use 96 banks per HBM memory unit, i.e., 3 bank groups (BGs) in the 8-High HBM stack, so as to meet the area constraint of one HBM die with a 4P1B PIM configuration, as shown in Figure 5(b). 6.2. Attn-PIM Design To address the varying arithmetic intensity, computation demands, and memory footprint of FC and attention kernels, while ensuring high hardware resource utilization, we propose dedicated Attn-PIM units, separate from the FC-PIM units (as described in Section 6.1). The Attn-PIM units are disaggregated from the high-performance processor through an interconnect. This disaggregated design of Attn-PIM allows us to tackle the growing memory footprint demands of KV caches of LLMs, as we explain next. We find that FC kernels of LLMs have larger computation intensity and result in significantly larger latency than the attention kernels, while attention kernels have larger mem- ory footprint demands. Therefore, given a fixed area budget, FC-PIM requires a configuration with higher computation ca- pability, while the attention kernel does not need as much computation capability. To meet these constraints, we allocate FC-PIM devices with high execution parallelism, i.e., 4 FPUs per DRAM bank (as described in Section 6.1).",
    "source": "2502.15470v2_PAPI_Exploiting_Dynamic_Parallelism_in_Large_Langu.pdf",
    "length": 1887,
    "tokens": 469
  },
  {
    "text": "[12] S. Venkatachalam et al., Realtime person identification via gait analysis using imu sensors on edge devices, in ICONS, 2024, pp. 371 375. [13] C. Kadway et al., Low power low latency cloud cover detection in small satellites using on-board neuromorphic processors, in IJCNN, 2023, pp. 1 8. [14] G. Lenz et al., Low-power ship detection in satellite images using neuromorphic hardware, arXiv preprint arXiv:2406.11319, 2024. [15] D. Silva et al., End-to-end edge neuromorphic object detection system, in AICAS, 2024, pp. 194 198. [16] R. V. W. Putra and M. Shafique, Spikedyn: A framework for energy- efficient spiking neural networks with continual and unsupervised learn- ing capabilities in dynamic environments, in DAC, 2021, p. 1057. [17] BrainChip. Metatf: The akida neuromorphic ml framework. [Online]. Available: [18] R. V. W. Putra and M. Shafique, lpspikecon: Enabling low-precision spiking neural network processing for efficient unsupervised continual learning on autonomous agents, in IJCNN, 2022, pp. 1 8. [19] M. F. Minhas et al., Continual learning with neuromorphic computing: Theories, methods, and applications, arXiv preprint:2410.09218, 2024. [20] R. V. W. Putra and M. Shafique, Q-spinn: A framework for quantizing spiking neural networks, in IJCNN, 2021, pp. 1 8. [21] R. V. W. Putra, M. A. Hanif, and M. Shafique, Respawn: Energy- efficient fault-tolerance for spiking neural networks considering unreli- able memories, in ICCAD, 2021, pp. 1 9. [22] , Softsnn: Low-cost fault tolerance for spiking neural network accelerators under soft errors, in DAC, 2022, pp. 151 156.",
    "source": "2504.00957v2_Enabling_Efficient_Processing_of_Spiking_Neural_Ne.pdf",
    "length": 1599,
    "tokens": 471
  },
  {
    "text": "0.08 0.06 0.04 0.02 0.00 0.02 0.04 0.06 0.08 Correlation Coefficient node_degree_(skip_connect,nor_conv_3x3)_avg_in node_degree_(none,nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect)_in_degree node_degree_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(none,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_in_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3)_out_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_out_degree min_path_len_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3) op_count_(skip_connect) node_degree_(skip_connect)_avg_in node_degree_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3)_avg_in max_op_on_path_(skip_connect,nor_conv_3x3) max_op_on_path_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3) max_op_on_path_(skip_connect,nor_conv_3x3,avg_pool_3x3) node_degree_(nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,none,nor_conv_3x3)_avg_in node_degree_(none,nor_conv_1x1)_out_degree Figure 13: Top 20 features correlated with robustness over 1 Month for Noisy Drift (without HWT). 17 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Correlation Coefficient min_path_len_(skip_connect,nor_conv_3x3) min_path_len_(skip_connect,nor_conv_3x3,nor_conv_1x1) max_op_on_path_(skip_connect,nor_conv_3x3) max_op_on_path_(skip_connect,nor_conv_3x3,nor_conv_1x1) node_degree_(none,nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_avg_in node_degree_(none,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3)_out_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_out_degree node_degree_(none,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_out_degree node_degree_(none,avg_pool_3x3)_out_degree min_path_len_(nor_conv_3x3,nor_conv_1x1) node_degree_(skip_connect,none,nor_conv_1x1,avg_pool_3x3)_avg_in op_count_(nor_conv_3x3) node_degree_(nor_conv_3x3)_avg_in node_degree_(none,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_in_degree Figure 14: Top 20 features correlated with robustness over 1 Day for Analog Drift (with HWT). 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Correlation Coefficient min_path_len_(skip_connect,nor_conv_3x3) min_path_len_(skip_connect,nor_conv_3x3,nor_conv_1x1) max_op_on_path_(skip_connect,nor_conv_3x3) max_op_on_path_(skip_connect,nor_conv_3x3,nor_conv_1x1) node_degree_(none,nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_in_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3)_out_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_out_degree node_degree_(none,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_out_degree node_degree_(none,avg_pool_3x3)_out_degree min_path_len_(nor_conv_3x3,nor_conv_1x1) node_degree_(skip_connect,none,nor_conv_1x1,avg_pool_3x3)_avg_in op_count_(nor_conv_3x3) node_degree_(nor_conv_3x3)_avg_in min_path_len_(nor_conv_3x3) node_degree_(none,avg_pool_3x3)_in_degree Figure 15: Top 20 features correlated with robustness over 1 Month for Analog Drift (with HWT).",
    "source": "2506.18495v1_AnalogNAS-Bench_A_NAS_Benchmark_for_Analog_In-Memo.pdf",
    "length": 3387,
    "tokens": 1534
  },
  {
    "text": "T0{v0, v1} T1{v0, v1} T2{v0, v1} T3{v0, v1} T0{v4, v5} T1{v4, v5} T2{v4, v5} T3{v4, v5} T0{v8, v9} T1{v8, v9} T0{v8, v9} T1{v8, v9} T0{v12, v13} T1{v12, v13} T2{v12, v13} T3{v12, v13} T0{v2, v3} T1{v2, v3} T2{v2, v3} T3{v2, v3} T0{v6, v7} T1{v6, v7} T2{v6, v7} T3{v6, v7} T0{v10, v11} T1{v10, v11} T2{v10, v11} T3{v10, v11} T0{v14, v15} T1{v14, v15} T2{v14, v15} T3{v14, v15} Figure 20: FP32 accumulator register layout - rows 0 and 8, thread 0-3, entries 0-15. T0{v0, v1} T1{v0, v1} T2{v0, v1} T3{v0, v1} T0{v2, v3} T1{v2, v3} T2{v2, v3} T3{v2, v3} T0{v4, v5} T1{v4, v5} T2{v4, v5} T3{v4, v5} T0{v6, v7} T1{v6, v7} T2{v6, v7} T3{v6, v7} T0{v8, v9} T1{v8, v9} T0{v8, v9} T1{v8, v9} T0{v10, v11} T1{v10, v11} T2{v10, v11} T3{v10, v11} T0{v12, v13} T1{v12, v13} T2{v12, v13} T3{v12, v13} T0{v14, v15} T1{v14, v15} T2{v14, v15} T3{v14, v15} Figure 21: Permuted FP32 accumulator register layout - rows 0 and 8, thread 0-3, entries 0-15. A.4 Additional Experiments Table 5 10 show Qwen2.5 (1.5B), Qwen2.5 (3B), and Llama3.2 (3B) fine-tuning results on four datasets with five different random seeds.",
    "source": "2505.11594v1_SageAttention3_Microscaling_FP4_Attention_for_Infe.pdf",
    "length": 1094,
    "tokens": 683
  },
  {
    "text": "In addition, the design of more compact structures to replace the original ones is also common. For instance, employing multi-query attention [15] or grouped-query attention [16] in place of the original multi-head attention in Transformer architecture can reduce key-value heads, resulting in a more streamlined model. Nevertheless, both model compression and the design 3 TABLE I: Literature Review Work Throughput Latency Cache Management Dynamic Decision Uncertainty Request Management Iteration Management Batching Distributed Strategies Orca vLLM Sarathi-Serve [4] Llumnix [5] InfiniGen [6] dLoRA [7] VTC [8] FastServe [9] DistServe [10] MoonCake OURS of compact structures can alter model weights, potentially leading to a decline in accuracy. Instead of optimizing the model size, data parallelism (DP) and model parallelism aim to fully leverage the computational power of the devices. In DP [17], the model weights are replicated across multiple devices, allowing different inference requests to be processed in parallel on different devices. Model parallelism distributes the model weights across several devices to minimize the per- device memory footprint of the model weights. Consequently, each device can operate more efficiently by executing a smaller portion of the task. Several model parallelization methods exist, such as pipeline parallelism (PP) [18], tensor parallelism (TP) [19], sequence parallelism (SP) [20], and context paral- lelism (CP) [21]. Since our scheduling methods are orthogonal to the aforementioned techniques, both model optimization and parallelization strategies can be employed seamlessly in conjunction with our methods to enhance inference efficiency. In addition to general model serving techniques, the opti- mization of LLM inference serving systems primarily involves enhancing the model forward pass. Researchers have improved system efficiency from various perspectives, including kernel fusion, Key-Value (KV) cache management, request manage- ment, iteration management, batching, distributed strategies, etc. Here, we present several classic techniques that are prevalent in LLM inference serving systems. FlashAttention [22] amalgamates the operations of data transfer between hard- ware components within the attention mechanism to expedite operation execution without compromising model accuracy. Speculative decoding [23], [24] employs an auxiliary model to generate a preliminary draft, followed by a verification process executed by the main model.",
    "source": "2502.15763v1_Hybrid_Offline-online_Scheduling_Method_for_Large_.pdf",
    "length": 2510,
    "tokens": 498
  },
  {
    "text": "However, the method of selecting -1, 0, and 1 to determine the summation and subtraction may not be optimal, as there are only a limited number of combinations of -1, 0, and 1, leading to repetitive computations for the corresponding A entries. Furthermore, when increasing computation parallelism by duplicating the selection unit of the adding and subtracting path, the resource consumption of the selection may exceed that of the TL tables themselves. This is because the multiple reading ports of the on-chip distributed RAM unit can support multiple accesses to the TL tables, requiring only additional buffers for addressing. The supportive ablation study will be presented in the next subsection.",
    "source": "2504.16266v2_TeLLMe_An_Energy-Efficient_Ternary_LLM_Accelerator.pdf",
    "length": 703,
    "tokens": 136
  },
  {
    "text": "Îµm \" } v1 v2 v3 L TgJBEOzF IL9ehlIjHxRHYNQY9ELx4xyiOBDZkdGpgwO7uZmSUhGz7BiweN8eoXefNvHGAPClbSaWqO91dQSy4Nq7eQ2Nre2d K7hb39g8Oj4vFJU0eJYthgkYhU O6AaBZfYMNwIbMcKaRgIbAXju7nfmqDSPJPZhqjH9Kh5APOqLHS46RX6RVLbtldgKwTLyMlyFDvFb 6 YglIUrDBNW647mx8VOqDGcCZ4VuojGmbEyH2LFU0hC1ny5 OnZELq TJIFK2pCEL9fdESkOtp2FgO0NqRnrVm4v eZ3EDG78lMs4MSjZctEgEcREZP436XOFzIipJZQpbm8lbEQVZcamU7AheKsvr5PmVdmrlqsPlVLtNosjD2dwDpf gwTXU4B7q0AGQ3iGV3hzhPivDsfy9ack82cwh84nz8Qwo2s latexit v4 N S8NAEJ3Ur1q qh69LBbBU0lEqseiF48V7Ae2oWy2k3bpZhN2N0IJ RdePCji1X jzX jts1BWx8MPN6bYWZekAiujet O4W19Y3NreJ2aWd3b gfHjU0nGqGDZLGLV CahGwSU2DTcCO4lCGgUC28H4dua3n1BpHsHM0nQj hQ8pAzaqz02MNEcxHLvtcvV9yqOwdZJV5OKpCj0S9 9QYxSyOUhgmqdzE NnVBnOBE5LvVRjQtmYDrFrqaQ Raj bXzwlZ1YZkDBWtqQhc X3REYjrSdRYDsjakZ62ZuJ 3nd1ITXfsZlkhqUbLEoTAUxMZm9TwZcITNiYglitbCRtRZmxIZVsCN7y6ukdVH1atXa WlfpPHUYQ TOIVz8OAK6nAHDWgCAwnP8ApvjnZenHfnY9FacPKZY gD5 MHe6OQzw latexit Ï‰1 N S8NAEJ34WetX1aOXxSJ4KkmR6rHoxWMF 4FtKJvtpF262YTdjVBC 4UXD4p49d9489 4bXPQ1gcDj dmJkXJIJr47rfztr6xubWdmGnuLu3f3BYOjpu6ThVDJsFrHq BFSj4BKbhuBnUQhjQKB7WB8O PbT6g0j WDmSToR3QoecgZNVZ67GiuYhlv9ovld2KOwdZJV5OypCj0S9QYxSyOUhgmqdzE NnVBnOBE6LvVRjQtmYDrFrqaQ Raj bXzwl51YZkDBWtqQhc X3REYjrSdRYDsjakZ62ZuJ 3nd1ITXfsZlkhqUbLEoTAUxMZm9TwZcITNiYglitbCRtRZmxIRVtCN7y6ukVa14tUrt rJcv8njKMA pnMEFeHAFdbiDBjSBgYRneIU3RzsvzrvzsWhdc KZE gD5 MHfSeQ0A latexit Ï‰2 Ï‰3 Ï‰4 Ï‰5 Ï‰6 Ï‰7 Ï‰8 Ï‰9 Ï‰H : Ï‰NN Figure 3. Hardware parameterization instances. On the left, we show B spline patches whose interior determine freespace pores amidst a homogeneous dielectric (e.g., SiO2).",
    "source": "2504.20401v1_Nonlinear_Computation_with_Linear_Optics_via_Sourc.pdf",
    "length": 1547,
    "tokens": 1037
  },
  {
    "text": "The Verilog designs are pipelined every 5 adders. II is 1 for all designs shown. Implementation Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] hls4ml DA 76.9 31 (44.1 ns) 12,682 0 19,056 702.2 da4ml 20 (34.0 ns) 12,298 0 13,664 588.6 hls4ml DA 76.6 26 (37.4 ns) 7,392 0 11,462 695.9 da4ml 16 (27.5 ns) 6,944 0 7,419 582.4 hls4ml DA 76.5 25 (35.4 ns) 6,448 0 10,109 707.2 da4ml 17 (23.1 ns) 6,165 0 7,207 735.8 hls4ml DA 76.3 22 (30.2 ns) 5,019 0 7,682 729.4 da4ml 15 (20.8 ns) 4,655 0 5,258 722.0 hls4ml DA 76.0 23 (32.6 ns) 3,602 0 5,693 706.2 da4ml 14 (20.6 ns) 3,127 0 3,587 681.2 hls4ml DA 75.9 21 (24.9 ns) 2,908 0 4,720 844.6 da4ml 13 (19.0 ns) 2,734 0 3,139 685.4 Table 11. Resource utilization and latency of the high-level feature jet tagging network generated with hls4ml with da4ml, and with da4ml directly via Verilog, marked with hls4ml DA and da4ml , respectively. The FPGA part is xcvu13p-flga2577-2-e with 1 GHz target clock frequency. Delay constraint is set to 2 for the da4ml optimized designs for each CMVM operation. The Verilog designs are pipelined every adder. II is 1 for all designs shown. Table 12, respectively. The designs generated with hls4ml are marked with hls4ml DA , and the designs generated directly by da4ml with Verilog are marked with da4ml .",
    "source": "2507.04535v1_da4ml_Distributed_Arithmetic_for_Real-time_Neural_.pdf",
    "length": 1283,
    "tokens": 486
  },
  {
    "text": "1). FT-Bias updates even fewer parameters than FT-Last, but it only reduces the backward FLOPs by 2.4 3.1x compared to FT-All, as it still requires gradients of forward activations dx. InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models A PREPRINT 0 50 100 150 Fine-tuning Time (s) 50 55 60 65 70 75 80 Accuracy ( ) InstantFT (FPGA) (0.36s) InstantFT (6.2s) FT-Last (26.3s) LoRA-Last (26.2s) LoRA-All (140.5s) Figure 1: Fine-tuning time vs. accuracy of InstantFT and baselines (Rotated Fashion-MNIST, 75deg). Table 1: Comparison between InstantFT and baselines (LeNet-5) MNIST SVHN Method Trainable params FLOPs (forward) FLOPs (backward) Memory usage (KB) Trainable params FLOPs (forward) FLOPs (backward) Memory usage (KB) FT-All 61706 0.851M 1.444M 567.8KB 62006 1.321M 1.914M 579.4KB FT-Last 850 0.851M 0.002M 285.8KB 850 1.321M 0.002M 296.1KB FT-Bias 236 0.851M 0.611M 322.0KB 236 1.321M 0.611M 332.3KB LoRA-All 36328 0.923M 0.743M 611.8KB 45480 1.412M 0.762M 695.4KB LoRA-Last 376 0.852M 0.001M 285.4KB 376 1.322M 0.001M 295.8KB InstantFT 10456 0.872M 0.036M 366.2KB 19608 1.360M 0.054M 449.8KB As in Fig. 2 (bottom), we consider two LoRA-based approaches: inserting LoRA adapters into all layers (LoRA-All) or applying LoRA to only the last layer (LoRA-Last).",
    "source": "2506.06505v1_InstantFT_An_FPGA-Based_Runtime_Subsecond_Fine-tun.pdf",
    "length": 1283,
    "tokens": 481
  },
  {
    "text": "To reduce the energy consumption of edge devices, processing near data sources such as sensors is crucial. In general, data sources generate a large amount of time-series data that must be processed in real time. In addition, because processing is performed at the edge devices, they must operate with a low power consumption and be implemented using small hardware resources. Reservoir computing is a machine learning method that uses a reservoir. Input signals are transformed nonlinearly to a higher-dimensional space via a layer called a reservoir with fixed weights, and only the weights of the output layer are learned [10]. This method is considered to be particularly suitable for edge computing because it is capable of processing time-series data with its Authors addresses: Sosei Ikeda, Kyoto University, Kyoto, Japan, Hiromitsu Awano, Kyoto University, Kyoto, Japan, Takashi Sato, Kyoto University, Kyoto, Japan, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. Request permissions from 0000 Association for Computing Machinery. 1539-9087 0000 0-ART000 15.00 ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. arXiv:2504.11981v1 [cs.LG] 16 Apr 2025 000:2 Ikeda et al. recurrent structure that reflects past inputs. In addition, the reservoir layer does not require learning and can therefore be implemented in hardware utilizing a variety of physical phenomena [17]. The greatest obstacle in applying reservoir computing to real-world time-series classification tasks is the transformation of features obtained via the reservoir [7]. In general, the number of features obtained by the reservoir varies with the length of the input data series, thereby preventing the structure of the output layer from being fixed.",
    "source": "2504.11981v1_Hardware-Friendly_Delayed-Feedback_Reservoir_for_M.pdf",
    "length": 2243,
    "tokens": 462
  },
  {
    "text": "The GAP8 [30], part of GreenWaves Technologies Green- Waves Application Processor series, features an 8-core RISC- V cluster and 22.65-GOPS hardware convolution engine for neural network acceleration at 8 or 16-bit precision. The platform has 512 KB of L2 RAM, up to 8 MB of L3 SRAM, and 20MB flash storage, enabling it to store and run larger, more complex models or mixture-of-experts (MoE) architec- tures. The GAP series of ğœ‡NPUs have also been the subject of several recent works, again mainly centered on model optimization [26, 31, 32]; no platform benchmark exists yet. The Himax HX6538 WE2 (or HX-WE2) [33] is a more powerful ğœ‡NPU platform from Taiwan-based semiconductor manufacturer, Himax Technologies. This platform features a Corstone-300 set up, with Cortex M55 CPU and Ethos U55 NPU, delivering up to 512 GOPS. The platform also features 512KB TCM, 2MB SRAM, and 16MB flash, suitable for large or more complex models, but at an increased power draw. NXP s MCXN947 [34] is part of the MCX N94x line of MCUs, featuring dual Cortex-M33 CPUs and NXP s eiQ Neu- tron NPU. The MCXN947 is designed for lower-power appli- cations, with 8-bit neural acceleration of only 4.8 GOPS. The platform features 512 KB RAM and 2 MB flash storage. Our benchmark also includes MCUs without neural hard- ware for comparison, to quantify any efficiencies gained from specialized NPU architectures. The STM32H7A3ZI [35] is a high-performance MCU based on the Cortex M7, with 2 MB of flash and 1.4 MB of SRAM. Manufactured by Swiss-based ST Microelectronics, it is frequently used with on-board NNs [16, 36]. The ESP32s3 MCU [37] features dual-core Tensilica LX6 processors, 512 KB of SRAM, 2MB PSRAM, and 8MB flash.",
    "source": "2503.22567v2_Benchmarking_Ultra-Low-Power_Î¼NPUs.pdf",
    "length": 1708,
    "tokens": 477
  },
  {
    "text": "The algorithm makes sure that no data is shared with a centralized agent (like the cloud, which performs the training action), yet the learner is able to learn by combining multiple pre-learnt models. We extended this design idea by constructing a random forest model as a combination of multiple decision trees from different learners (different deployments). To preserve the data privacy of the participat- ing deployments, the cloud learns by ensembling randomly sampled decision trees from each of the deployments, instead of learning a random forest from the data shared by each of them. Random sampling of the decision trees also augments the model by minimizing the data induced bias of each of the models. The random sampling method is data agnostic, and hence ensures minimal data induced bias. Random sampling of decision trees, albeit a na Ä±ve way, has been empirically shown to work well in preserving model characteristics and providing accurate predictions. III. EXPERIMENTAL EVALUATION AND RESULTS In this section, we describe our evaluation methodology and evaluate both privacy-preserving and data-sharing partitioning strategies compared with a traditional random-forest approach. We apply our techniques on Appliance energy prediction data [3], and also provide a case analysis of material surface roughness prediction using real world grinding-machine sensor data. We analyze the accuracy-latency trade offs of each strategy and show their benefits in different scenarios. A.",
    "source": "EdgeClourRF.pdf",
    "length": 1495,
    "tokens": 285
  },
  {
    "text": "6 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 6. Implementation and Evaluation 6.1. Discussions and Limitations 7. Conclusion This should finish at 8 pages. Impact Statement Authors are required to include a statement of the potential broader impact of their work, including its ethical aspects and future societal consequences. This statement should be in an unnumbered section at the end of the paper (co- located with Acknowledgments the two may appear in either order, but both must be before References), and does not count toward the paper page limit. In many cases, where the ethical impacts and expected societal implications are those that are well established when advancing the field of Machine Learning, substantial discussion is not required, and a simple statement such as the following will suffice: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. The above statement can be used verbatim in such cases, but we encourage authors to think about whether there is content which does warrant further discussion, as this statement will be apparent if the paper is later flagged for ethics review. References A. Guidelines for Hyperparameter Selection and Bounds on Reward Parameters The parameters Î³, Î´, and Î· govern the reward structure of the proposed framework, influencing whether sensors participate consistently, over-participate and waste energy, or abstain altogether. This appendix provides a systematic approach to selecting these parameters, including formal bounds, practical heuristics, and an algorithmic procedure to explore suitable values. Conceptual Role of Parameters The scalar Î³ 0 represents the reward scaling for correct participation. If Î³ is too low, sensors will not have suffi- cient incentive to expend energy on high-SNR captures. If Î³ is too high, sensors may waste energy attempting difficult inferences. The parameter Î´ 0 penalizes incorrect in- ferences, discouraging reckless submissions of low-quality data. The parameter Î· 0 penalizes non-participation, ensuring that sensors do not remain idle indefinitely. As discussed, maintaining Î· Î´ encourages sensors to at least attempt participation rather than always remain offline.",
    "source": "ParticipationGamesICML25.pdf",
    "length": 2499,
    "tokens": 492
  },
  {
    "text": "Beam searching presents an obvious trade-off between computational resources utilization for an inference and prediction accuracy. Combined with a large context window, this is a very powerful technique which we found to be more pronounced when a model was not already near perfect accuracy: in Figure 5, we show an increase going up to 99.39 with the use of beam search for assembly transpilation. We found diminishing returns for using more than 4 beams on accuracy. Finally, from an efficiency perspective, we show that aggressive quantization does not severely impact our transpilers accuracy. Going from FP32 down to INT4 substantially reduces the transpilers inference footprint, with a minimal (less than 12 4 ) impact on model prediction accuracy. This shows the potential of designing small enough models for deployment on edge devices, which we would envision the GG transpiler to be used for CISC-to-RISC translations in practice. Transpilation Error Analysis. We provide a de- tailedanalysisoffunctionallyequivalentpredictions produced by our model that deviate syntactically from the ground truth. Such cases reveal the model s ability to generalize instruction patterns while main- taining semantic correctness, a desirable trait in low- level code generation where multiple implementa- tions can achieve the same functional outcome.",
    "source": "2506.14606v1_Guaranteed_Guess_A_Language_Modeling_Approach_for_.pdf",
    "length": 1347,
    "tokens": 262
  },
  {
    "text": "Yao Lu, Shang Liu, Qijun Zhang, and Zhiyao Xie. Rtllm: An open-source benchmark for design rtl generation with large language model, 2023. Shang Liu, Yao Lu, Wenji Fang, Mengming Li, and Zhiyao Xie. Openllm-rtl: Open dataset and benchmark for llm-aided design rtl generation. In Proceedings of the 2025 IEEE ACM International Conference on Computer-Aided Design (ICCAD). IEEE, 2025b. URL https: arxiv.org abs 2503.15112. Nathaniel Pinckney, Christopher Batten, Mingjie Liu, Haoxing Ren, and Brucek Khailany. Revisiting verilogeval: A year of improvements in large-language models for hardware code generation. ACM Trans. Des. Autom. Electron. Syst., February 2025. ISSN 1084-4309. doi: 10.1145 3718088. URL Just Accepted. Ahmed Allam and Mohamed Shalan. Rtl-repo: A benchmark for evaluating llms on large-scale rtl design projects, 2024. Zixi Zhang, Greg Chadwick, Hugo McNally, Yiren Zhao, and Robert Mullins. Llm4dv: Using large language models for hardware test stimuli generation. arXiv:2310.04535, October 2023. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL forum?id VTF8yNQM66. Anisha Agarwal, Aaron Chan, Shubham Chandel, Jinu Jang, Shaun Miller, Roshanak Zilouchian Moghaddam, Yevhen Mohylevskyy, Neel Sundaresan, and Michele Tufano. Copilot evaluation harness: Evaluating LLM-guided software programming, 2024. URL 2402.14261. CocoTB. Cocotb: Coroutine-based cosimulation testbench for vhdl and systemverilog, 2025. URL Version 1.9.2.",
    "source": "2506.14074v1_Comprehensive_Verilog_Design_Problems_A_Next-Gener.pdf",
    "length": 1649,
    "tokens": 491
  },
  {
    "text": "To the author s knowledge, no edge GPU has been explicitly designed for space-grade reliability. The need for ionizing- radiation tolerance to ensure mission reliability is more readily addressed by the accessibility of radiation-tolerant FPGAs [119]. AMD s KCU105 kit, for example, is a commercial equivalent of the space-grade Kintex Ultrascale XQRKU060 Manuscript submitted to ACM 14 CÃ©dric LÃ©onard, Dirk Stober, and Martin Schulz used in the VIDEO project covered by Neris et al. [86]. In summary, while GPUs especially edge GPUs offer strong computational performance and power efficiency trade-offs for ML tasks, FPGAs remain the preferred choice in power- and space-constrained environments due to their energy efficiency and radiation tolerance. FPGAs vs. ASICs (TPUs VPUs). Application-Specific Integrated Circuits (ASICs) are fully custom-designed chips optimized for specific applications, potentially offering unmatched performance and energy efficiency. Yet, their design is a complex, lengthy, and expensive process no study has compared an FPGA-based design against a suitable custom ASIC. Many studies list reconfigurability as a key advantage of FPGAs over ASICs [76, 99, 117, 130, 133, 140]; in defense and space for example, re-programmability enhances mission versatility by enabling the \"deploy and program\" paradigm11 [48, 71]. Consequently, FPGAs are increasingly replacing traditional ASICs due to their faster time-to-market, cost-efficiency in small-batch applications, and competitive performance [98]. Despite these advantages, ASICs remain the optimal choice for ultra-low-power applications, as the generic nature of FPGAs cannot match the efficiency of fully customized hardware [4]. Tensor Processing Units (TPUs) are commercially available ASICs optimized for Deep Learning, primarily used in data centers [60]. While the Google Coral TPUs [39] target edge devices, no study in this survey directly compared FPGA designs to a TPU. Li et al. [75] only compare their Zynq 7000 design against Google cloud TPU s theoretical peak performance. Vision Processing Units (VPUs), like Intel s Movidius Myriad series, focus on low-power CNN inference.",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 2174,
    "tokens": 486
  },
  {
    "text": "7, pp. 1306 1318, 2021. [48] M. Yasin et al., Provably-secure logic locking: From theory to practice, in Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, ser. CCS 17. New York, NY, USA: Association for Computing Machinery, 2017, p. 1601 1618. Available: [49] M. Nabeel et al., Cophee: Co-processor for partially homomorphic en- crypted execution, in 2019 IEEE International Symposium on Hardware Oriented Security and Trust (HOST), 2019, pp. 131 140. [50] N. Limaye et al., Antidote: Protecting debug against outsourced test entities, IEEE Transactions on Emerging Topics in Computing, vol. 10, no. 3, pp. 1507 1518, 2022. [51] M. Nabeel et al., Cofhee: A co-processor for fully homomorphic encryption execution, in 2023 Design, Automation Test in Europe Conference Exhibition (DATE), 2023, pp. 1 2. [52] M. Nabeel et al., Exploring constrained-modulus modular multipliers for improved area, power and flexibility, in IFIP IEEE International Conference on Very Large Scale Integration-System on a Chip, 2023, pp. 93 108. [53] D. Soni et al., Quantifying the overheads of modular multiplication, in 2023 IEEE ACM International Symposium on Low Power Electronics and Design (ISLPED), 2023, pp. 1 6. [54] D. Soni et al., Design space exploration of modular multipliers for asic fhe accelerators, in 2023 24th International Symposium on Quality Electronic Design (ISQED), 2023, pp. 1 8. [55] S. Icarus, Icarus Verilog, [56] R. Maertens et al., Discovering and exploring cases of educational source code plagiarism with dolos, SoftwareX, vol. 26, p. 101755, 2024. APPENDIX A SUPPLEMENTARY MATERIALS Figure 9 shows some examples for prompting for quality assessment. Figure 10 shows the quality of code generation for MIP base across FT and inference parameters, for prompts Phuman IP . Overall, quality is notably lower than for prompts PGPT IP (Fig. 3).",
    "source": "2503.13116v4_VeriLeaky_Navigating_IP_Protection_vs_Utility_in_F.pdf",
    "length": 1887,
    "tokens": 498
  },
  {
    "text": "1(a)). However, the occurrence of entire zero columns of height n with uniformly distributed zero elements and sparsity s is described by the Bernoulli process n n sn(1 s)n n sn and is consequently very low for large n. Therefore, we introduce the compressed sparse block (CSB) format, which allows merging multiple non-zero columns with high sparsity into one. Similarly to the two-stage bitmap format, the CSB format uses one array to store all non-zero elements of a matrix, additionally for each non-zero element FlexiSAGA 5 LU 0,0 SU 0,n LU 0,1 Controller PE 0,0 PE 0,1 Main Memory SU 1,n SU m,n SU m,0 SU m,1 LU 0,n PE 0,n LU 1,0 PE 1,0 PE 1,1 LU m,0 PE 1,n PE m,0 PE m,1 PE m,n DecU (a) GEMM tiles dense weight stationary (dWS) dense output stationary (dOS) dense input stationary (dIS) movement of the streamed matrix tile movement of the the partial sums a d g h b e i c f A D G E H B I C F weight matrix input matrix c f i h b e g a d A D G H B E I C F A D G E H B I C F a d g h b e i c f c f i h b e g a d A B C E F D I G H (b) Fig. 2. (a) Block diagram of the FlexiSAGA architecture. (b) Visualization of the three dense tiled GEMM dataflows supported by the FlexiSAGA architecture. the column index is stored. The row index is implicitly encoded in the order of the column index array. For each column starting from the first, we use greedy search to find matching columns to merge with. Columns match if the position of the non-zero elements of one column matches the position of the zero elements in the other column, while it is allowed that zero elements of one column can match zero elements of the other column. Columns only containing zeros are not merged and are skipped entirely. The resulting number of columns after the merge is stored together with the non-zero elements and the column index array. Fig. 1(c) shows an example of the CSB format.",
    "source": "2506.01566v1_FlexiSAGA_A_Flexible_Systolic_Array_GEMM_Accelerat.pdf",
    "length": 1869,
    "tokens": 501
  },
  {
    "text": "Host (Python) PYNQ Overlay PS: ARM Linux AXI PL: FPGA ACC DDR Memory data control r w config status data result Figure 4: Diagram of the PYQ-based interface on the KV260. On the software side, we created a PYNQ overlay in Figure 4 to manage buffers and FPGA invocation. The host (Python) con- figures the accelerator via the PYNQ overlay and the Processing System (PS), as depicted in the diagram of the PYNQ-based inter- face on the KV260 where the AXI interface provides bidirectional communication (Config Status, Result) between the PS and the FPGA Accelerator, and DDR memory holds buffers for A, B, and C. Specifically, the host allocates contiguous buffers for A, B, and C (using pynq.allocate), sets up the accelerator registers (point- ing to physical addresses of buffers and dimensions N, K, M), and toggles AP_START [10]. We wrapped this in a Python call_fpga() function that optionally retains A between calls. This allowed in- tegration with a quantized DistilBERT: we replaced the PyTorch linear layers for Q, K, V in attention with calls to the FPGA (us- ing custom PyTorch extensions to invoke call_fpga()), feeding int8 quantized weights and inputs. In doing so, we ensured con- sistent scaling by using symmetric quantization with a fixed scale factor and zero-point for the weights and activations so that the int8 results could be dequantized to match the FP32 baseline. We verified that the FPGA outputs matched CPU computation exactly for small test matrices and remained within quantization error for end-to-end model outputs.",
    "source": "2503.16731v3_Design_and_Implementation_of_an_FPGA-Based_Hardwar.pdf",
    "length": 1550,
    "tokens": 379
  },
  {
    "text": "Holdings, White Paper: 360-Degree Video Rendering. blog posts white-paper-360-degree-video-rendering , 2019. [23] J. Huang, Z. Chen, D. Ceylan, and H. Jin, 6-DOF VR Videos with a Single 360-camera, 2017 IEEE Virtual Reality (VR), pp. 37 44, 2017. [24] A. Inc., Rendering Omni-directional Stereo Content. https: developers.google.com vr jump rendering-ods-content.pdf , 2019. [25] N. INSTRUMENTS, Peak Signal-to-Noise Ratio as an Image Quality Metric. signal-to-noise-ratio-as-an-image-quality-metric.html , 2019. [26] B. C. Kim and C. E. Rhee, Compression Efï¬ciency Evaluation for Virtual Reality Videos by Projection Scheme, IEIE Transactions on Smart Processing Computing, pp. 102 108, 2017. [27] S. M. LaValle, The Geometry of Virtual Worlds. edu vr vrch3.pdf , 2019. [28] Y. Leng, C.-C. Chen, Q. Sun, J. Huang, and Y. Zhu, Energy-efï¬cient Video Processing for Virtual Reality, in Proceedings of the International Symposium on Computer Architecture (ISCA), 2019, pp. 91 103. [29] Y. Li and W. Gao, DeltaVR: Achieving High-Performance Mobile VR Dynamics through Pixel Reuse, in 2019 18th ACM IEEE International Conference on Information Processing in Sensor Networks (IPSN), 2019, pp. 13 24. [30] L. Liu, R. Zhong, W. Zhang, Y. Liu, J. Zhang, L. Zhang, and M. Gruteser, Cutting the Cord: Designing a High-quality Untethered VR System with Low Latency Remote Rendering, in Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services, ser. MobiSys 18, 2018, pp. 68 80.",
    "source": "DejaView.pdf",
    "length": 1510,
    "tokens": 454
  },
  {
    "text": "AVX is a set of SIMD (Single Instruction, Multiple Data) instructions extending the x86 architecture, enabling parallel operations on data vectors. It uses special AVX registers to store arguments and outputs for these operations. This work focuses on AVX-512, which operates on 512-bit registers. For example, the instruction mm512 loadu si512 can load 512 bits of data from main memory into an AVX register. To perform a dot product, you can load elements from two vectors into AVX registers and use the mm512 dpbf16 ps instruction. This operation multiplies 32 pairs of 16-bit elements of the two registers, adds the results of each two consecutive elements together to form 16 32-bit elements which are accumulated in a third register. OUT IN W 0 1 2 3 ... 31 30 0 1 2 3 30 31 0 1 2 3 4 5 6 7 60616263 ... 0 1 2 3 4 5 6 7 60616263 0 0 ... 63 63 ... INT8 Mode BF16 Mode ... 0 0 ... 31 31 Figure 4. AMX Matrix Multiplication: AMX tiles support BF16 (blue) and INT8 (red) values. First two tiles contain the input matrix tiles, which are then multiplied and accumulated to the third tile. Building on the success of AVX, Intel introduced AMX in their Sapphire Rapids processor series. Unlike AVX, which focuses on vectorized operations, AMX provides specialized hardware acceleration for matrix multiplication. AMX introduces a set of tiled registers, which are two- dimensional and significantly larger than AVX registers (as shown in Figure 4). Each tile consists of up to 16 rows, with each row containing up to 512 bits, and there are eight tile registers in each AMX unit. These tiles support operations to zero all elements, load data from memory, store data into memory, and perform matrix multiplication. Matrix multiplication in AMX involves multiplying two tiles and accumulating the result in a third tile. These operations support two data formats: BF16 (Bfloat16): Each row can store up to 512 16 32 elements. Results are stored in FP32. INT8 (8-bit integers): Each row can store up to 512 8 64 elements. Results are stored in INT32.",
    "source": "2502.12444v1_SparAMX_Accelerating_Compressed_LLMs_Token_Generat.pdf",
    "length": 2047,
    "tokens": 459
  },
  {
    "text": "Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. (2022). Palm: Scaling language modeling with pathways. Cohen, J., Kaur, S., Li, Y., Kolter, J. Z., and Talwalkar, A. (2021). Gradient descent on neural networks typically occurs at the edge of stability. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.",
    "source": "2506.20752v1_Characterization_and_Mitigation_of_Training_Instab.pdf",
    "length": 1144,
    "tokens": 433
  },
  {
    "text": "The quality constraint refers to the desired classification accuracy defined by the user. To elaborate, XAI- Gen first initializes an array R of accurate multipliers, which will be replaced with the most suitable approximate multipliers for each layer in the subsequent steps (Line 1). Then, XAI- Gen estimates the neuron conductance of each neuron in a pre-trained model Î¸ (Lines 4-5) and aggregates the neuron conductance in each layer to find the layer importance (Line 7). The layers are identified in order of their highest to Finding neuron conductance using XAI tools c Forward pass Backward pass Input 0 Input 1 Input 2 Class 0 Class 1 Identifying critical and non-critical neurons Forward pass Backward pass c1 c2 c3 c4 c1 c2 c3 c4 c2 c1, c4 c3 Aggregating neuron conductance layer-wise for layer importance c1 c2 c3 c4 z c1 c2, z c3 c4 Layer l Layer l Skipping neuron connections and applying approximate multipliers m1 m2 m3 m4 e(m1) e(m2), pth 1 Layer l Layer l Fig. 3: XAI-guided approximate computing: Skipping the non-critical neurons and applying approximate multipliers layer-wise by identifying the neuron conductance and layer importance using the XAI tools Approximate Multipliers Analytical Model of HW Accelerator Pre-trained DNNs Estimate Neuron Importance Estimate Layer Importance Energy Analysis of whole AxDNN Accuracy Analysis Accuracy- Energy Tradeoffs Evaluation HW Accelerator RTL Description Energy for a Single Operation Synopsys Design Compiler Hardware Parameters Dataset Approximation Skip Non- critical Neurons XAI-Guided AxDNN Generation XAI Tools AxDNN Fig. 4: XAI-Gen: Proposed methodology for generating AxDNNs using XAI techniques lowest importance (Line 9). While traversing through the layers in descending order of importance, XAI-Gen assigns the least approximate multipliers m l to the most important layer and updates R with those approximate multipliers (Line 11). The degree of approximation needed for a layer l is determined based on the threshold tc on the layer importance. The approximate multipliers are selected from a library of ap- proximate multipliers, such as Evoapprox8b [15].",
    "source": "2503.16583v1_Explainable_AI-Guided_Efficient_Approximate_DNN_Ge.pdf",
    "length": 2139,
    "tokens": 493
  },
  {
    "text": "We configured the firewall to allow NFS services and created mount points on the client nodes. The NFS shares were mounted manually for testing and configured in etc fstab for persistence after reboots. This setup ensures secure and reliable file sharing across the cluster. [25] We utilized FreeIPA for centralized user management and synchronization across all cluster nodes. FreeIPA provided seamless integration of user authentication and access control, ensuring consistent user accounts and credentials throughout the system. This streamlined administrative tasks, enhanced security, and simplified user management in the cluster environment. Figure 3: Slurm Command sinfo for listing cluster User management in HPC clusters requires a balance between security and ease of administration. While NIS is widely used in many HPC environments due to its simplicity, it is considered less secure than modern alternatives [26, 27]. FreeIPA, which integrates LDAP and Kerberos, offers enhanced security and centralized authentication but is more complex to install and configure [28]. Our cluster utilizes FreeIPA with LDAP to provide a robust identity management 4 A PREPRINT - MARCH 17, 2025 system, ensuring secure user authentication and streamlined access control. However, for clusters prioritizing ease of use over security, NIS remains a simpler alternative despite its vulnerabilities. Slurm is an open-source workload manager designed for efficient resource allocation, job scheduling, and queue management in high-performance computing (HPC) clusters. To set up Slurm in our cluster, we installed the necessary packages on all nodes and configured the slurm.conf file to match our cluster s specifications. The configuration file was distributed to all nodes, and appropriate permissions were set for Slurm directories. On the controller node, the Slurm controller daemon ( slurmctld ) was enabled and started, while the Slurm node daemon ( slurmd ) was activated on all worker nodes. The installation was verified using commands like sinfo for cluster information and srun for test jobs. This setup ensures efficient job scheduling and resource management, optimizing the performance of the cluster. [23]. We configured the master node to support computations but chose not to use it for this purpose. Utilizing the master node for processing tasks would add significant load to it, as it is already responsible for managing multiple tasks, including job scheduling, resource allocation, and acting as the login node for multiple users. Prioritizing these critical functions ensures the stability and efficiency of the cluster.",
    "source": "2503.11246v1_Cost-effective_Deep_Learning_Infrastructure_with_N.pdf",
    "length": 2638,
    "tokens": 498
  },
  {
    "text": "11.2 Production before standard cell libraries are available Traditional semiconductor designs follow digital design flows that require mature standard cell libraries and associated synthesis capabilities - components typically unavailable until 9-12 months after a new process node is defined. SHAPE circumvents this constraint by employing a radically simplified SLD design consisting almost exclusively of highly replicated, minimalist processing elements (PEs). These PEs are deliberately architected to be sufficiently simple for manual design by experienced circuit engineers, eliminating dependencies on automated synthesis and standard cell libraries while still leveraging the performance benefits of cutting-edge process technology. SHAPE's multi-die architecture could provide another critical 20 advantage: the BID and HILT are implemented in in production, well-characterized process nodes with established design tools and IP blocks. This approach allows the BID and HILT development and validation to proceed in parallel with - and be completed ahead of - the SLD's availability. When the advanced process node becomes production-ready, only the SLD requires fabrication using the new technology, while the fully-validated BID and HILT designs can already be production-ready. 11.3 Production before SRAM, PLL, I O, analog, mixed signal, bond pads, TSVs, and IP blocks are available By strategically partitioning functionality between the dies, SHAPE eliminates the need to implement and qualify complex components in the advanced node: high-precision PLLs, I O structures, SRAM arrays, analog mixed-signal circuits, bond pads, and TSVs. These components typically require multiple design iterations and extensive characterization in any new process node, often becoming critical path elements for commercial deployment. 11.4 Reduced design and verification cycles The simplified SLD design dramatically reduces design and verification cycles. Rather than synthesizing and validating millions of unique logic paths across a complex SoC, engineers need only optimize a single PE containing a few hundred transistors, replicate it across the die, and add a small amount of full custom inter-array logic. This focused approach accelerates time-to-silicon compared to conventional flows, with verification complexity reduced by several orders of magnitude. 11.5 Reduced mask calculation Further time savings occur during mask preparation.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2449,
    "tokens": 449
  },
  {
    "text": ". . . . . . . . . . . . . . . . . . 12 3.4.1 CANN for Ascend NPUs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.4.2 Infrastructure Software for Cloud Deployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.5 Suitability Analysis for DeepSeek Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 3.5.1 DeepSeek Models and Their Deployment on NVIDIA H800. . . . . . . . . . . . . . . . . . 15 3.5.2 Architectural Synergy between CloudMatrix384 and DeepSeek Models. . . . . . . . 16 4 DeepSeek Serving on Huawei CloudMatrix384 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 4.1 Overview: A Peer-to-Peer Serving Architecture with PDC Disaggregation. . . . . . . . . . 17 4.2 Tightly-Coupled Decode with Large-scale Expert Parallelism. . . . . . . . . . . . . . . . . . . . . 20 4.2.1 Fused Communication Operators for LEP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 4.2.2 MLA Optimization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 4.2.3 Microbatch-Based Decode Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 1247,
    "tokens": 502
  },
  {
    "text": "Subsequently, the neural network must be manually implemented in C with calls to the cuDNN library to execute operations on the GPU. The neural network is then cross-compiled for an ARM target using g and NVCC (Nvidia CUDA Compiler), resulting in an executable that is deployed on the Nvidia Jetson AGX Xavier, which operates a Linux-based system with the cuDNN library installed. 2) Quantitative Evaluation: During the evaluation, we en- countered challenges, particularly due to the lack of a cuDNN implementation for the transposed convolution in the up- sampling path of the U-Net, as well as for the nearest neighbor upsampling operation. A feasible solution could have been to implement these layers in a custom CUDA program; however, due to limited time and inadequate support on the Nvidia forum, this approach was not viable. We successfully imple- mented the down-sampling path and the middle convolution of the U-Net using cuDNN. The implementation s accuracy was Fig. 5: GPU workflow from Keras TensorFlow training to Nvidia Jetson AGX Xavier inference with cuDNN validated by comparing the intermediate tensor outputs from the middle convolution produced by cuDNN with those from TensorFlow, finding them equivalent within an absolute toler- ance of 1e 8. Thus, we conclude that the cuDNN workflow is unlikely to alter the evaluation metrics significantly. Table IV presents the evaluation metrics measured on the validation set for the implemented down-sampling path and middle convolution of the U-Net using cuDNN. We estimated the full U-Net implementation performance by considering that the down-sampling path and middle convolutions com- prise 31.6 of the MAC operations, and we scaled the measured latency accordingly to estimate the total latency. Similarly, since these components represent 60.7 of the parameters and intermediate feature maps, we adjusted the memory footprint to estimate the total memory usage. These estimates should be interpreted with caution.",
    "source": "2503.08700v1_Real-Time_Semantic_Segmentation_of_Aerial_Images_U.pdf",
    "length": 1987,
    "tokens": 412
  },
  {
    "text": "Proceedings of the IEEE 107(8), 1655 1674 (2019) [126] Gill, S.S., Golec, M., Hu, J., Xu, M., Du, J., Wu, H., Walia, G.K., Murugesan, S.S., Ali, B., Kumar, M., et al. : Edge ai: A taxonomy, systematic review and future directions. Cluster Computing 28(1), 1 53 (2025) [127] Wu, X., Zhang, Y., Shi, M., Li, P., Li, R., Xiong, N.N. : An adaptive feder- ated learning scheme with differential privacy preserving. Future Generation Computer Systems 127, 362 372 (2022) [128] Chen, S., Yu, D., Zou, Y., Yu, J., Cheng, X.: Decentralized wireless federated learning with differential privacy. IEEE Transactions on Industrial Informatics 18(9), 6273 6282 (2022) [129] El Ouadrhiri, A., Abdelhadi, A.: Differential privacy for deep and federated learning: A survey. IEEE access 10, 22359 22380 (2022) [130] Wang, K., Hu, Z., Ai, Q., Liu, Q., Chen, M., Liu, K., Cong, Y.: Membership infer- ence attack with multi-grade service models in edge intelligence. IEEE Network 35(1), 184 189 (2021) [131] Bai, L., Hu, H., Ye, Q., Li, H., Wang, L., Xu, J.: Membership inference attacks and defenses in federated learning: A survey. ACM Computing Surveys 57(4), 1 35 (2024) [132] Yuan, X., He, P., Zhu, Q., Li, X.: Adversarial examples: Attacks and defenses for deep learning. IEEE transactions on neural networks and learning systems 30(9), 2805 2824 (2019) [133] Wang, H., Sayadi, H., Dinakarrao, S.M.P., Sasan, A., Rafatirad, S., Homayoun, H.: Enabling micro ai for securing edge devices at hardware level.",
    "source": "2505.08793v1_Onboard_Optimization_and_Learning_A_Survey.pdf",
    "length": 1489,
    "tokens": 475
  },
  {
    "text": "0 50 100 Walking Climbing Cycling Running Jogging Jumping Accuracy Chest Left Ankle Right Wrist Majority Voting Fig. 2: Accuracy of the individual DNNs and with a majority voting ensemble for different activities. To design these DNNs, we leverage the work in [11], [14] and further apply state of the art optimizations given in [3], [15] to make the DNN more suitable for energy-scarce applications. Fig. 2 gives the accuracy of these DNNs on MHEALTH dataset [12], [13]. A detailed description of the setup is explained in Section IV. In this section, we provide an overview of our proposed solution. A. Preamble to Origin Human activity has temporal continuity, i.e. most activ- ities last for some duration (in the range of hundreds of milliseconds to seconds). Therefore, there is an opportunity to skip some intermediate inferences over the period of an activity in order to increase harvesting duration and the prob- ability that an initiated inference will complete. So long as the number of skipped inferences is modest, there will still likely be samples processed before an activity ï¬nishes. This can be extended further adopting a round-robin activation schedule to both increase harvesting periods per initiated in- ference on each node while increasing the odds that at least some node is attempting an inference at any given time. Chest No Op Right Wrist No Op Left Ankle No Op Chest No Op No Op Right Wrist No Op No Op Left Ankle No Op No Op Chest No Op No Op No Op Right Wrist No Op No Op No Op Left Ankle No Op No Op No Op Chest Right Wrist Left Ankle RR3 RR6 RR9 RR12 Fig. 3: Different ï¬‚avors of (extended) round-robin scheduling and their execution ï¬‚ow. Each policy is named after the num- ber of nodes the cycle has, i.e. RR3 has 3 nodes with no no-ops and RR6 has 3 nodes with 3 no-ops. Even using a round robin execution, we observe that only 28 of the inferences are completed (shown in Fig. 1b). Therefore, we induce a delay (no-op cycles in Fig.",
    "source": "Origin.pdf",
    "length": 1970,
    "tokens": 466
  },
  {
    "text": "TheinputbatchnumberBNinisdeterminedbythepower budgetbecausePload Pbudgetshouldalwaysbesatisï¬ed. Thelatency modelfordataloadforoneconvolution operationisLatload Bitsinput BWld.ThetermLatload representsthelatencytoloadthedatarequiredbythe convolutionoperationsforone-cycle MACoperationsfora full-sizeReRAM.ThetermBWlddenotesthebandwidthof eachloadoperation.ThemodelsofPstoreandLatstorecan bederivedinasimilarfashion. 2) ComputationonReRAMs:Pcompisthedominantand mostcomplicatedpartwheretheanaloganddigitalsignals aremixed.Theenergyofone-cycle MACoperationsforan activationsizeofm nandactualduplicationaG,Pcomp tile dividesintothefollowingparts:1)EDAC denotestheenergy consumedforconvertingthedigitalinputsignaltotheanalog signalinabit-serialfashion;2)EMAC denotestheenergy forperforming MACoperationsonReRAMs;and3)EADC consistsofthreepartsasshowninFigure5:3i)EBL denoting theenergyforactivatingbitlines;3ii)ESA Ref denotingthe energyforsensingandamplifyingthe MACresultsignaland thenreferencinganalogsignalstodigitalsignals;and,3iii) ES A denotingtheenergyofShift Addparttocomposethe ï¬naloutput. IntheResiRCAdesign,thetimeforperformingone-cycle of MACoperationsononeReRAMtileisï¬xedasLatcomp Tcomp,andisindependentoftheactivationsize.Therefore,we canbuildthepowermodelforthecomputationpartinterms ofatileasshowninEquation1. Ecomp Ecomp tile Latcomp (EDAC EMAC EADC) Tcomp (1) Thepowerofeachpartistakentobelineartothetiling factorsofm ornortheactualparallelismgranularityaG.",
    "source": "ResiRCA.pdf",
    "length": 1470,
    "tokens": 457
  },
  {
    "text": "Computation includes the computation energy in floating point multiplication units (FPUs) of the processing core. As shown in Figure 7(a), most of the energy in PIM execution is consumed by DRAM Access, which accounts for 96.7 of the total energy consumption1 1This energy consumption breakdown is very different from that the HBM- PIM paper [30] reported. The key difference is that the HBM-PIM paper [30] reports only the energy consumption breakdown of data movement, while we report the energy consumption breakdown of both data movement and computation. 6 0 100 200 300 400 500 1 4 16 64 Power (W) DRAM data reuse level 4P1B 2P1B 1P1B Power budget 116 0 20 40 60 80 100 DRAM Access Transfer Computation 0 20 40 60 80 100 DRAM Access Transfer Computation (a) (b) (c) Figure 7: (a) Energy breakdown of PIM for executing the FC kernel with no DRAM data reuse. (b) Energy breakdown of PIM for executing the FC kernel when one DRAM access (i.e., an activated DRAM row) is used 64 times for computation (i.e., data reuse level 64). (c) Power consumption of PIM architecture with different data reuse levels and different numbers of FPUs per bank. larger than the Q vector (activation data). Based on the above analysis, accessing data from DRAM once and reusing it for multiple computations can significantly re- duce energy consumption. If data can be accessed from DRAM once and used for multiple computations, the total energy consumption of PIM execution can be reduced significantly. Figure 7(b) shows the energy breakdown of PIM when data is fetched once from DRAM and then reused for 64 FC kernel computations. The energy consumption of DRAM Access reduces to 33.1 of the overall energy consumption. This ap- proach gives us a new opportunity to enhance the parallel computation throughput of near-bank PIM. By lowering the energy cost of DRAM access, we gain additional energy budget for the PIM cores. As described in Section 2, parallelism techniques (batching and speculative decoding) enable data reuse in LLM decoding, which enables parallel PIM execution by allowing the reduc- tion of the DRAM Access component.",
    "source": "2502.15470v2_PAPI_Exploiting_Dynamic_Parallelism_in_Large_Langu.pdf",
    "length": 2125,
    "tokens": 482
  },
  {
    "text": "optimizations : 1. remove unnecessary code 2. simplify arithmetic and propagate constants to simplify expressions 3. merge instructions 4. merge high -level operations 5. reorder operations or blocks of operations 6. move cpu -based computation to the accelerator 7. add or subtract a matrix using the bias 8. hoist redundant operations out of loops 9. substitute operations with equivalent operations that are faster 10. pipeline operations to better overlap computation and data movement 11. eliminate data dependencies and fence operations 12. minimize data movement 13. minimize loop overhead 14. other methods not listed here Figure 14: The optimization menu for TinyMPC code optimization. 18 Here is an example of increasing scratchpad tile size for the Y dimension of a 512 x512 (X x Z) matrix A and 512 x512 (Z x Y) matrix B multiplication. Original code: uint32_t b_offset 16 16 4 8 sizeof(int8_t); for (int_fast32_t y 0; y 8; y ) { uint32_t b_base_y 64 y; Load B matrix slice for (int_fast32_t zo 0; zo 8; zo ) { uint32_t b_zo_offset 4 16 zo; Number of columns per zo iteration for (int_fast32_t z 0; z 4; z ) { uint32_t b_index ((zo 4 z) ((16 4) 16)) 16; Divide number of elements by 16 since scratchpad is row -indexed mvin3( B[b_zo_offset 16 z][ b_base_y], b_offset b_index , 16 4, 16); }} for (int_fast32_t x 0; x 32; x ) { uint32_t res 1 31; uint32_t a_base_x 16 x; Load A matrix slice for (int_fast32_t zo 0; zo 8; zo ) { uint32_t a_index (zo (16 4) 16) 16; mvin2( A[a_base_x ][64 zo], a_index , 16 4, 16); } Computation for (int_fast32_t zo 0; zo 8; zo ) { uint32_t a_index (zo (16 4) 16) 16; for (int_fast32_t z 0; z 4; z ) { uint32_t preload_flag (zo 0 z 0) ?",
    "source": "2505.18574v2_Autocomp_LLM-Driven_Code_Optimization_for_Tensor_A.pdf",
    "length": 1678,
    "tokens": 508
  },
  {
    "text": "CapBand: Battery-free Successive Capacitance Sensing Wristband for Hand Gesture Recognition. In Proceedings of the 16th ACM Conference on Embedded Networked Sensor Systems (Shenzhen, China) (SenSys 18). Association for Computing Machinery, New York, NY, USA, 54 67. doi:10.1145 3274783.3274854 [148] Arya Tschand, Arun Tejusve Raghunath Rajan, Sachin Idgunji, Anirban Ghosh, Jeremy Holleman, Csaba Kiraly, Pawan Ambalkar, Ritika Borkar, Ramesh Chukka, Trevor Cockrell, Oliver Curtis, Grigori Fursin, Miro Hodak, Hiwot Kassa, Anton Lokhmotov, Dejan Miskovic, Yuechao Pan, Manu Prasad Manmathan, Liz Raymond, Tom St. John, Arjun Suresh, Rowan Taubitz, Sean Zhan, Scott Wasson, David Kanter, and Vijay Janapa Reddi. 2025. MLPerf Power: Benchmarking the Energy Efficiency of Machine Learning Systems from Microwatts to Megawatts for Sustainable AI. arXiv:2410.12032 [cs.AR] [149] X. Tu, A. Mallik, D. Chen, K. Han, O. Altintas, H. Wang, and J. Xie. 2023. Unveiling Energy Efficiency in Deep Learning: Measurement, Prediction, and Scoring Across Edge Devices. In 2023 IEEE ACM Symposium on Edge Computing (SEC). IEEE Computer Society, Los Alamitos, CA, USA, 80 93. doi:10.1145 3583740.3628442 [150] Ultralytics. 2021. YOLOv5: A state-of-the-art real-time object detection system. Accessed: insert date here. [151] uTensor. 2024. uTensor. [152] Shubham Vaishnav, Maria Efthymiou, and Sindri MagnÃºsson. 2023. Energy-Efficient and Adaptive Gradient Sparsification for Federated Learning. In ICC 2023 - IEEE International Conference on Communications.",
    "source": "2505.12523v1_Energy-Aware_Deep_Learning_on_Resource-Constrained.pdf",
    "length": 1542,
    "tokens": 486
  },
  {
    "text": "8. (a) Actual MAC output with 2b FeFET current domain CIM [43] under 50mV Vth variation; (b) Actual MAC output with 2b FeFET charge domain CIM [43] under 50mV Vth variation; (c) Actual MAC output of RRAM CIM tape-out [41]; (d) Actual MAC output with 1b nvCap charge domain CIM [18], [33] under 50mV Vth variation; (e) Inference accuracy of various neural network algorithms across CIM macro A-D. using either simulation data or silicon measurements. By providing a standardized methodology for capturing MAC output statistics, the framework enables direct comparison of different technologies and compute circuits under realistic operating conditions. C. Runtime Evaluation and Simulator Comparison We first compare NeuroSim V1.5 against existing open- source ACIM simulation frameworks to highlight key capabili- ties and performance improvements (Table IV). CrossSim [37] provides detailed device-level accuracy simulation but faces significant runtime overhead when modeling noise effects. AIHWKit [63] specializes in noise-aware training but offers limited support for general noise modeling and architectural exploration. While NeuroSim V1.4 [54] includes PPA analysis and has some noise modeling, it was constrained by slower inference speeds and limited network support. The runtime is primarily influenced by the precision param- eters that determine computation complexity through Equation TABLE IV COMPARISON OF DNN ACIM SIMULATORS DNN CIM Simulator CrossSim [37] AIHWKit [63] NeuroSim V1.4 [54] NeuroSim V1.5 Supported devices PCM, RRAM, EcRAM, DRAM RRAM, PCM, Flash SRAM, RRAM, FeFET SRAM, RRAM, FeFET, nvCap Supported network types Linear, Conv Linear, Conv, Recurrent, Transformer Linear, Conv, Recurrent Linear, Conv, Recurrent, Transformer Custom Compute Circuit Support No No No Yes Bit Slicing Support Yes No Yes Yes PPA Support No No Yes Yes PPA evaluation excluded. 3: DAC precision affects the number of input cycles, cell bit- precision (MLC) determines the number of weight compo- nents, and noise modeling adds statistical sampling overhead.",
    "source": "2505.02314v1_NeuroSim_V15_Improved_Software_Backbone_for_Benchm.pdf",
    "length": 2065,
    "tokens": 481
  },
  {
    "text": "35, pp. 16 344 16 359, 2022. [13] T. Dao, Flashattention-2: Faster attention with better parallelism and work partitioning, arXiv preprint arXiv:2307.08691, 2023. [14] M. N. Rabe and C. Staats, Self-attention does not need O(n2) memory, arXiv preprint arXiv:2112.05682, 2021. [15] K. Alexandridis, 2025. [16] A. Wang et al., Glue: A multi-task benchmark and analysis platform for natural language understanding, arXiv preprint arXiv:1804.07461, 2018. [17] B. Keller et al., A 95.6-TOPS W deep learning inference accelerator with per-vector scaled 4-bit quantization in 5 nm, IEEE Journal of Solid-State Circuits, vol. 58, no. 4, p. 1129 1141, 2023. [18] S. Lu et al., Hardware accelerator for multi-head attention and position- wise feed-forward in the transformer, in IEEE Intern. System-on-Chip Conference (SOCC), 2020, pp. 84 89. [19] S. Sridharan et al., X-former: In-memory acceleration of transformers, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 31, no. 8, pp. 1223 1233, 2023. [20] T. J. Ham et al., ELSA: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks, in Intern. Symp. on Computer Architecture (ISCA), 2021, p. 692 705. [21] Z. Song et al., TSAcc: An efficient tempo-spatial similarity aware accelerator for attention acceleration, in ACM IEEE Design Automation Conference, 2024. [22] A. Marchisio et al., Swifttron: An efficient hardware accelerator for quantized transformers, in Intern. Joint Conference on Neural Networks (IJCNN), 2023, pp. 1 9. [23] M. Milakov and N. Gimelshein, Online normalizer calculation for softmax, arXiv preprint arXiv:1805.02867, 2018.",
    "source": "2505.14314v2_Low-Cost_FlashAttention_with_Fused_Exponential_and.pdf",
    "length": 1655,
    "tokens": 496
  },
  {
    "text": "The DeepRTL2 models, however, achieve the best performance among all open-source mod- els, with results comparable to GPT-4o. The per- formance improvement from base models to Deep- RTL2 highlights the effectiveness of our dataset construction process and training strategy. Further- more, DeepRTL2 outperforms the original Deep- RTL models, likely due to the incorporation of additional open-source datasets, aside from data sourced from GitHub, and the inclusion of more diverse problem formulations that enhance Deep- RTL2 s generalization ability. Given that Deep- RTL2 is a multi-task model and the generation benchmark may overlap with the training data used by OpenAI s models, these results highlight Deep- RTL2 s impressive performance for this task. Table 3 presents the results for RTL code un- derstanding. Since the CodeV-CodeLlama model outputs random messages for this task, we ex- clude it from the comparison. The results show that DeepRTL2 models significantly outperform all other models, including the previous state-of- the-art DeepRTL models, underscoring its strong capabilities in RTL code understanding. Notably, DeepRTL2 surpasses GPT-4o by a substantial mar- gin, despite the fact that its training data is anno- tated using GPT-4o. The main reason is that during benchmark testing, all models, including GPT-4o, are required to generate high-level functional de- scriptions directly from RTL code. As shown in Appendix A, CoT-based annotations are more ac- curate than direct annotations. This enhanced anno- tation quality contributes to DeepRTL2 s superior performance in RTL code understanding. 5.2 Embedding-Based Tasks Since none of the existing models are specifi- cally designed for RTL embedding-based tasks, the baselines used for the generation-based tasks, e.g., CodeV series and DeepRTL models, perform poorly in this setting. These models show near- zero performance, with an F1 score close to 0 on the natural language code search task and an aver- age precision of approximately 0.5 on the func- tionality equivalence checking task. Therefore, we select state-of-the-art general-purpose embed- ding models as baselines for comparison.",
    "source": "2506.15697v1_DeepRTL2_A_Versatile_Model_for_RTL-Related_Tasks.pdf",
    "length": 2177,
    "tokens": 497
  },
  {
    "text": "[5] V. Mrazek et al., Alwann: Automatic layer-wise approximation of deep neural network accelerators without retraining, in ICCAD, 2019, pp. 1 8. [6] M. Pinos et al., Evolutionary neural architecture search supporting approximate multipliers, in EuroGP. Springer, 2021, pp. 82 97. [7] M. Sabih et al., Utilizing explainable ai for quantization and pruning of deep neural networks, arXiv preprint arXiv:2008.09072, 2020. [8] T. Speith et al., A review of taxonomies of explainable artificial int.elligence (xai) methods, in FAccT, 2022, pp. 2239 2250. [9] S. . Yeom et al., Pruning by explaining: A novel criterion for deep neural network pruning, Pattern Recognition, vol. 115, p. 107899, 2021. [10] A. Colucci et al., Towards transient fault mitigation techniques opti- mized for compressed neural networks, in DSN-S. IEEE, 2023, pp. 211 213. [11] J. Gu et al., Capsule network is not more robust than convolutional network, in CVPR, 2021, pp. 14 309 14 317. [12] N. Jouppi et al., Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings, in Pro- ceedings of the 50th Annual International Symposium on Computer Architecture, 2023, pp. 1 14. [13] Y. LeCun et al., Mnist handwritten digit database, ATT Labs [Online]. Available: vol. 2, 2010. [14] A. Krizhevsky et al., Learning multiple layers of features from tiny images, 2009. [15] V. Mrazek et al., Evoapprox8b: Library of approximate adders and mul- tipliers for circuit design and benchmarking of approximation methods, in DATE, 2017, pp. 258 261. [16] A. Siddique et al., Exploring fault-energy trade-offs in approximate dnn hardware accelerators, in ISQED. IEEE, 2021, pp. 343 348.",
    "source": "2503.16583v1_Explainable_AI-Guided_Efficient_Approximate_DNN_Ge.pdf",
    "length": 1691,
    "tokens": 485
  },
  {
    "text": "As ğ‘€contains only fixed-point integers, the algorithm first normalizes it by applying bit-shifts across the rows and columns such that no row column has all entries even except for zeros. The resultant scaling factors are recorded and will be applied to the input output vectors. Manuscript submitted to ACM 6 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu Table 1. List of parameters used in the proposed da4ml algorithm Parameter Description ğ‘‘ğ‘–ğ‘› The number of input elements. ğ‘‘ğ‘œğ‘¢ğ‘¡ The number of output elements. ğ‘€[ğ‘‘ğ‘–ğ‘›,ğ‘‘ğ‘œğ‘¢ğ‘¡] The input constant matrix containing fixed-point numbers. ğ‘‘ğ‘ Delay constraint, maximum number of extra adder depth permitted. ğ‘ğ‘–ğ‘›ğ‘¡ğ‘–ğ‘›[ğ‘‘ğ‘–ğ‘›] An array representing the quantized intervals of the input vector. ğ‘™,â„,ğ›¿ The low, high, and step size that uniquely defines a quantized interval. ğ‘‘ğ‘’ğ‘ğ‘¡â„ğ‘–ğ‘›ğ‘¡[ğ‘‘ğ‘–ğ‘›] The adder depth associated with each element of the input vector. ğ‘€, ğ¼,ğ‘† The width, integer bits (include sign bit if presents), and sign bit of a fixed-point numbers. ğ‘  The bit-shift applied to an operand in a two-term subexpression. ğ‘ğ‘¤ğ‘€ The bitwidth of the constant matrix ğ‘€. First Stage ğ‘£ğ‘– A vector corresponding to the i-th column of the constant matrix ğ‘€. ğ‘£ğ‘– A vertex corresponds to the column vector ğ‘£ğ‘–. ğ‘£0 The root vertex in the graph, associated with the zero vector ( 0). ğ‘€1, ğ‘€2 The two submatrices such that ğ‘€ ğ‘€1ğ‘€2.",
    "source": "2507.04535v1_da4ml_Distributed_Arithmetic_for_Real-time_Neural_.pdf",
    "length": 1375,
    "tokens": 496
  },
  {
    "text": "Substantial performance gains are possible by reusing historical KV cache from earlier requests. This is Serving Large Language Models on Huawei CloudMatrix384 35 especially valuable in scenarios involving recurring prefixes, such as multi-turn conversations, few-shot prompting, and repeated system instructions. Within our architecture, Context Caching refers to a dedicated mechanism for storing and efficiently retrieving these historical KV caches. Context Caching is implemented by EMS [26], a service on Huawei Cloud. EMS leverages the UB- driven disaggregated memory pool ( 4.4.1) to create a shared, distributed repository for historical KV caches. These caches are organized into paged blocks (e.g., 128 512 tokens per block) based on model characteristics and UB transfer efficiency. All NPUs in the serving cluster can access or contribute to this cache via EMS APIs. Indexing, Deduplication, and Retrieval. EMS provides a specialized Context Caching SDK (i.e., API layer) to the upper-level LLM serving framework for storing and retrieving historical KV cache blocks. Internally, this EMS SDK utilizes the APIs of the MP SDK ( 4.4.1) to interact with the underlying distributed DRAM and tiered storage. Each KV cache block is associated with a unique hash key derived from its token sequence and augmented with a prefix hash to enable content-addressable indexing. This allows for fast lookups and deduplication: identical KV blocks are stored once and reused across requests. The portion of the disaggregated memory pool allocated to Context Caching is subject to capacity constraints. When nearing these limits, the MP Server ( 4.4.1) triggers eviction of colder KV cache blocks from DRAM to the EVS-backed SSD tier. If SSD capacity is also constrained, data is removed entirely based on LRU-style policies. This eviction process ensures fair and efficient resource sharing between context and model caches within the unified pool. Interaction with PDC Disaggregation. EMS tightly integrates with the disaggregated prefill and decode pipeline: Prefill Reuse and Store: Upon receiving a new request, the prefill engine queries EMS with a hash of the input prefix to identify reusable KV cache blocks. If found, these blocks are fetched via the UB plane and loaded directly into NPU memory, bypassing redundant computation.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2336,
    "tokens": 488
  },
  {
    "text": "[17] FX16 - BERT FPGA - - - - No fine-tuning Kim et al. [18] FX8 FX16 71.2e-12 4.77e-122 - 28 3.12 2.52 7100 249002 22.82 52.462 24.96 20 ,2 - Xia et al. [19] FP16 FP32-FX3 - BERT FPGA - - - - Fine-tuning Yu et al. [20] INT32 FP16 FP32 - RoBERTa, MobileBERT 7 1.5 0.74 0.624 1009 498 11344 0.06 0.02 0.044 - No fine-tuning Wang et al. [21] INT8 FP32 - DeiT, Swin, BERT 28 1 - - - No fine-tuning Liu et al. [22] INT8-FP5 - GPT-2 16 1.25 800 0.2 - Training Our BF16 1.62e-9 GPT-2, ViT 12 1 9686 7.16 0.456 No fine-tuning Results are reported only for standalone designs (all synthesis results except for [18]). For our design, we present the frequency of the full cluster and the post-layout area. Denotes peak throughput, which may differ from average throughput. 1 The precision of the design is adjustable. The first value corresponds to the lowest precision setting, while the second value represents the highest precision setting (P 3) evaluated in the referenced paper. 2 The accelerator supports two input precisions: FX8 (first) and FX16 (second). For FX16, the reported results correspond to the version with a parallelization factor of 8. 3 Internal computations are performed in fixed-point format, with input and output values converted from and to floating-point format. 4 Values are reported for INT32, FP16, and FP32, respectively. 5 Internal computations are performed in floating-point format, with input and output values converted from and to INT8. 6 For our design, the reported area corresponds to the EXP unit per core, while the power and throughput are averaged over the entire Softmax operation per core. Our approach employs BF16 precision and achieves a mean squared error (MSE) of 1.62e 9, which is comparable to fixed-point approximations by Zhu et al.",
    "source": "2504.11227v1_VEXP_A_Low-Cost_RISC-V_ISA_Extension_for_Accelerat.pdf",
    "length": 1779,
    "tokens": 500
  },
  {
    "text": "This takes around 5 hours to run. Fig. 7 shows that Autocomp significantly outperforms even extensively hand-optimized code (Exo Opt) by a geomean of 1.4 , Exo Unoptimized code (the starting point of Autocomp s search) by 2.9 , and Gemmini s software library by 5.6 . Autocomp is consistent: its generated code always achieves at least 85 of the hardware FSM s utilization (and 91 on average). Autocomp especially outperforms prior implementations thanks to extensive exploration of software pipelining and double-buffering, which allows better overlapping of data movement and computation, for example by double-buffering both the scratchpad and accumulator. In many cases, Autocomp s exploration also leads to different tiling and loop ordering choices than hand-optimized code, reducing data movement. We qualitatively analyze Autocomp-generated GEMM code in detail in Appendix C. 7 512x512x512 12544x256x64 12544x64x256 3136x512x128 3136x128x512 784x1024x256 0 1 2 3 4 5 6 7 Speedup 1.0 1.0 1.0 1.0 1.0 1.0 2.6 1.9 1.7 1.9 1.9 1.8 5.2 4.2 2.5 4.5 4.3 4.0 5.2 5.6 5.5 5.7 5.5 5.8 6.0 6.5 5.9 6.3 5.9 6.2 Gemmini SW lib Exo Unopt Exo Opt Autocomp Hardware FSM Figure 7: Speedup for GEMM benchmarks. 4x3x56x64x64 4x3x28x128x128 4x3x14x256x256 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 Speedup 1.0 1.0 1.0 0.9 0.9 0.9 2.3 2.6 2.7 2.7 2.6 2.7 3.5 3.2 3.3 Figure 8: Speedup for convolu- tion benchmarks. 4.4 Convolution We also optimize convolution benchmarks from ResNet-50 via the same process.",
    "source": "2505.18574v2_Autocomp_LLM-Driven_Code_Optimization_for_Tensor_A.pdf",
    "length": 1484,
    "tokens": 493
  },
  {
    "text": "In Proceedings of the 2024 ACM IEEE International Symposium on Machine Learning for CAD, MLCAD 24, New York, NY, USA, 2024. Association for Computing Machinery. [48] Yao Lu, Shang Liu, Qijun Zhang, and Zhiyao Xie. Rtllm: An open- source benchmark for design rtl generation with large language model. In 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC), pages 722 727, 2024. APPENDIX A. Multi-level Retrieval Figure 9 demonstrates two real-world repository-level ex- amples used for multi-level retrieval, which are CVA6 and Opentitan, respectively. The highlighted part demonstrates the retrieval query. (Issue CVA6-2732) [BUG] Cross-privilege TLB leakage through SLS Our microarchitectural fuzzer has found that CVA6 is susceptible to SLS (straight-line speculation [1]) and thus allows leakage through the TLB across privileges. Since speculatively issued loads and stores from a higher privilege access the TLB, their addresses can be recovered from a lower privilege. Thus, privileged code that (architecturally) does not leak any sensitive data through its control flow or memory operations, leaks transiently to an unprivileged attacker. We provide a snippet from the generated test case bellow:....... (Issue Opentitan-26355) [sram_ctrl,rtl] Remove macro timing assumptions Some parts of the sram_ctrl design (e.g., the readback feature) make assumptions about the timing of the underlying SRAM macro (e.g., a read always comes back at the next cycle). We should identify those assumptions and rewrite the design such that the controller can handle different SRAM macros. Fig. 9. Two real-world issues posted in repository-level projects.",
    "source": "2505.15701v1_HDLxGraph_Bridging_Large_Language_Models_and_HDL_R.pdf",
    "length": 1668,
    "tokens": 393
  },
  {
    "text": "1804 1816, Nov 2017. [22] A. Colin, E. Ruppel, and B. Lucia, A reconï¬gurable energy storage architecture for energy-harvesting devices, in Proceedings of the Twenty- Third International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2018, Williamsburg, VA, USA, March 24-28, 2018, pp. 767 781, 2018. [23] X. Sheng, C. Wang, Y. Liu, H. G. Lee, N. Chang, and H. Yang, A high- efï¬ciency dual-channel photovoltaic power system for nonvolatile sensor nodes, in 2014 IEEE Non-Volatile Memory Systems and Applications Symposium (NVMSA), pp. 1 2, Aug 2014. [24] X. Sun, S. Yin, X. Peng, R. Liu, J. Seo, and S. Yu, XNOR-RRAM: A scalable and parallel resistive synaptic architecture for binary neural networks, in 2018 Design, Automation Test in Europe Conference Exhibition (DATE), pp. 1423 1428, 2018. [25] A. K. Mishra and D. Marr, WRPN apprentice: Methods for training and inference using low-precision numerics, CoRR, vol. abs 1803.00227, Apr 2018. [26] S. Han, H. Mao, and W. J. Dally, Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding, CoRR, vol. abs 1510.00149, 2015. [27] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan, Deep learning with limited numerical precision, in Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37, ICML 15, pp. 1737 1746, 2015. [28] S. Jain, S. Venkataramani, V. Srinivasan, J. Choi, P. Chuang, and L. Chang, Compensated-dnn: Energy efï¬cient low-precision deep neural networks by compensating quantization errors, in 2018 55th ACM ESDA IEEE Design Automation Conference (DAC), pp. 1 6, 2018.",
    "source": "ResiRCA.pdf",
    "length": 1683,
    "tokens": 503
  },
  {
    "text": "D N formats bfloat16 E4M3 E5M2 E4M3 E5M2 bfloat16 bfloat16 bfloat16 E4M3 E5M2 87.35 2e 17 1.1522 -0.027 -0.027 -0.027 -0.012 46.99 1.1084 0.002 0.007 0.007 0.012 26.897 1.1011 0.004 0.001 0.004 0.009 16.06 1.0956 -0.001 0.014 0.004 0.009 9.92 1.0971 0.003 0.003 0.008 0.013 6.30 1.0950 0.0 -0.005 -0.01 0.01 4.10 1.1042 0.001 -0.006 0.006 0.006 2.73 1.1255 -0.001 -0.004 0.004 0.019 191.02 4.37e 17 1.030 0.005 0.0 0.010 0.01 102.78 1.0464 -0.016 0.036 -0.011 -0.021 58.81 0.9898 0.005 0.005 0.005 0.015 35.14 0.9806 -0.001 0.004 0.004 0.009 21.70 0.9765 0.003 0.003 0.003 0.013 13.78 0.9717 0.003 0.003 0.003 0.008 8.97 0.9732 0.002 0.002 0.002 0.012 5.97 2.3174 0.303 0.843 2.763 1.237 4.05 0.9839 0.001 0.006 0.006 0.006 2.80 0.9949 0.0 0.0 0.0 0.005 128.62 9.56e 17 0.9198 0.0 0.0 0.005 0.015 76.84 0.9052 0.0 0.005 0.005 0.015 47.46 0.8969 0.002 0.003 0.003 0.008 30.14 0.8894 0.001 0.001 0.006 0.011 19.62 0.8846 0.0 0.005 0.005 0.01 13.05 0.8879 0.002 0.002 0.002 0.012 8.86 0.8849 0.0 0.005 0.005 0.005 6.13 0.8882 0.002 0.002 0.002 0.007 4.31 0.8933 0.002 0.002 0.002 0.007 3.08 0.8961 0.004 0.004 0.004 0.009 2.24 0.9059 -0.001 0.004 0.064 0.004 168.03 2.09e 18 0.8546 0.0 0.005 0.005 0.015 103.78 0.8430 0.002 0.002 0.187 0.012 65.91 0.8335 0.001 0.001 0.001 0.011 42.896 0.8258 -0.001 0.004 0.004 0.009 28.54 0.8242 0.001 0.001 0.001 0.011 19.37 0.8200 0.0 0.0 0.0 0.005 13.399 0.8197 0.0 0.0 0.0 0.005 9.428 0.8187 0.001 0.001 0.001 0.006 6.74 0.8192 0.001 0.001 0.001 0.006 4.89 0.8215 0.003 0.006 0.003 0.003 2.02 0.8327 0.002 0.002 0.002 0.002 Table 4: Validation loss table, with separate columns for various weight and activation precisions. For the last 2 columns, we quantize only the forward pass. The second column indicates the total FLOP count used for those values of tokens-to-parameter ratios (D N).",
    "source": "2506.20752v1_Characterization_and_Mitigation_of_Training_Instab.pdf",
    "length": 1826,
    "tokens": 893
  },
  {
    "text": "[75] H. Ren, G. F. Kokai, W. J. Turner, and T. Ku, ParaGraph: Layout Parasitics and Device Parameter Prediction Using Graph Neural Networks, Proceedings of the IEEE ACM Design Automation Conference, pp. 1 6, Nov. 2020. [76] K. Zhu, H. Chen, W. J. Turner, G. F. Kokai, P. Wei, D. Z. Pan, and H. Ren, TAG: Learning Circuit Spatial Embedding from Layouts, Proceedings of the IEEE ACM International Conference on Computer-Aided Design, pp. 1 9, Oct. 2022. [77] Rongjian Liang, Zhiyao Xie, Jinwook Jung, Vishnavi Chauha, Yiran Chen, Jiang Hu, Hua Xiang, and Gi-Joon Nam, Routing-free crosstalk prediction, Proceedings Emerging ML-AI Techniques for Analog and RF EDA , of the International Conference on Computer-Aided Design, Dec. 2020. [78] V. A. Chhabria, W. Jiang, A. B. Kahng, and S. S. Sapatnekar, From Global Route to Detailed Route: ML for Fast and Accurate Wire Parasitics and Timing Prediction, Proceedings of the ACM IEEE Workshop on Machine Learning for CAD, pp. 7 14, Sep. 2022. [79] P. Shrestha and I. Savidis, Graph Representation Learning for Parasitic Impedance Prediction of the Interconnect, Proceedings of the IEEE International Symposium on Circuits and Systems, pp. 1 5, May 2023. [80] T. Nguyen, B. Shi, H. Ma, E. Li, X. Chen, A. C. Cangellaris, and J. E. Schutt- Aine, Comparative study of surrogate modeling methods for signal integrity and microwave circuit applications, IEEE Transactions on Components, Packaging and Manufacturing Technology, Vol. 11, pp. 1369 1379, Sep. 2021. [81] P. Shrestha, A. Aversa, S. Phatharodom, and I. Savidis, EDA-schema: A graph datamodel schema and open dataset for digital design automation, Proceedings of the ACM Great Lakes Symposium on VLSI, pp.",
    "source": "2506.00007v1_Emerging_ML-AI_Techniques_for_Analog_and_RF_EDA.pdf",
    "length": 1703,
    "tokens": 502
  },
  {
    "text": "While companies have access to the internals of their system, it is hard to argue that they are aware of their own designs either due to third-party IPs, mobility of engineers, and silos isolating their engineering teams from each other. 1 arXiv:2502.14307v1 [cs.CR] 20 Feb 2025 IPs are orphaned with little superficial information surviving after only a few years of breaking institutional memory. These factors combined pose a great danger for ÂµArch security. The primary goal of the proposed work is to answer the fol- lowing question: Can we use AI to automatically find brand- new vulnerabilities? In practical terms, can we build an AI agent that can discover the next Meltdown or Spectre vulner- abilities? Currently, there are intense efforts in the cyberse- curity research community to deploy AI tools to scan Open Source Software (OSS) for known vulnerabilities, e.g. for detection in ÂµArch we have [5, 12, 16, 56, 58, 59] and for patching [13,17,40,49,60,62] and [51]. We take on a more challenging problem and investigate how we can build an AI Agent that constantly searches the target platform for brand new ÂµArch vulnerabilities. In a way, such an ability would bring true scalability and a tipping point since, if granted, we could surpass human abilities by creating as many AI Agents as we want by just throwing more cycles at the problem. In the hands of software hardware ven- dors, such a tool would allow us to address vulnerabilities early on before the software advances deeper in the deploy- ment pipeline. What is missing is the know-how to put such a system together i.e. a tool that can constantly analyze a hardware software stack under popular configurations, iden- tify and report found vulnerabilities, articulating cause and effect and severity of the vulnerability. In this work, we take inspiration from cybersecurity researchers on how they came up with new vulnerabilities: Randomization. There is a healthy dose of manual or au- tomated trial and error in discovering new vulnerabilities. In ÂµArch security fuzzing has become an indispensable tool to test randomized attack vectors and thereby identify or generate improved versions of vulnerabilities. For in- stance, Oleksenko et al. [38] developed SpecFuzz to test for speculative execution vulnerabilities.",
    "source": "2502.14307v1_Î¼RL_Discovering_Transient_Execution_Vulnerabilitie.pdf",
    "length": 2299,
    "tokens": 482
  },
  {
    "text": "In this experiment, we partition the dataset into two subsets comprising 10 applications. As shown in Table 4, six applications (aes, bfs, fft, gemm, md, nw) are used for training, while the remaining applications are reserved as unseen applications for inference. As shown in Table 5, it can be observed that CoGNNs(Î²,Î±) achieves the smallest prediction errors for FF, CP, POWER, DSP, and BRAM compared to baseline models, with values of 5.98 , 7.54 , and 10.01 in terms of MAPE, 0.5339 and 0.1033 in terms of MAE, respectively. For LUT prediction, PNA-R exhibits the lowest error at 6.40, while CoGNNs(Î²,Î±) closely follows with 6.82, demonstrating a marginal difference. Notably, in POWER prediction, CoGNNs(Î²,Î±) reduces the error to 10.01 , outperforming PowerGear, a model specifically designed for power prediction, which achieves 11.61 . The performance of HGP SAGE GF on unseen applications underscores that complex model architectures or mere structural stacking cannot enhance performance in HLS or implementation prediction tasks. In summary, the task-adaptive CoGNN demonstrates exceptional performance in predicting post-place-and-route resource usage and exhibits strong generalization capabilities. In general, as can be seen from Section 5.2 and Section 5.3, CoGNNs(Î², Î±) achieves the best prediction performance on these two datasets. Facts have proven that in the post-HLS QoR prediction and the post-implementation QoR prediction, complex model structures like PNA-R may not yield excellent prediction results. The key lies in improving the model structure according to the task objectives and extracting high-dimensional features from the CDFG, which can lead to better prediction performance. 5.4 Evaluation of Design Space Exploration In the DSE experiment, we applied our proposed LLMMH framework to explore five unseen applications (atax, doitgen, gemm-p, heat-3d, and mvt) that were excluded from the training phase, utilizing the CoGNNs(Î², Î±) model trained in Section 5.3 as the evaluator. For each application, the design configurations encompass all possible combinations of the pragma values and the C C code.",
    "source": "2504.19649v2_Intelligent4DSE_Optimizing_High-Level_Synthesis_De.pdf",
    "length": 2137,
    "tokens": 497
  },
  {
    "text": "We used the inertial measurement unit data of MHEALTH [10] dataset as the sensor data, which is pre-processed and compressed at the Adafruit compute node and then sent over Bluetooth low-energy to the host. We used Circuit Python [12] and Mu Editor [51] to imple- ment the compression and the communication algorithms in the Adafruit board, and TensorFlow lite [1] to deploy the DNN inference at the host. We evaluate the efficiency, both in terms of compression ratio, energy consumption and accuracy preservation, of the recoverable clustering and recoverable importance sampling algorithms against three other popular methods: 1) sending raw data without com- pression; 2) compression using DCT; 3) Compression using DWT. We measure the energy consumption and inference accuracy over 1000 iterations to provide an average fair es- timate. As depicted in Figure 10, Seeker out performs both DCT and DWT in compression ratio, and the recovery fea- ture of Seeker helps preserving inference accuracy close to the original raw data. 5.2 Seeker for Activity Recognition Human Activity Recognition (HAR) using body area net- work is becoming mainstream on most of the warble devices. Moreover, the pervasive nature of HAR along with ample opportunities to harvest energy, makes HAR on body area network quite interesting. Therefore, as a case study, we simulate an entirely EH body area network using all the components of Seeker; specifically, to leverage intermittent computing using EH only, we simulate HAR on the hardware described in Section 4.2. This includes three different sen- sors located at left ankle, right arm, and chest. Each sensor has (i) sensing element a.k.a Inertial Measurement Unit that collects acceleration data), (ii) two DNN Re-RAM crossbar (16bit 12bit) built using XB-SIM [21], (iii) two coreset com- putation engines synthesized using Design Compiler [16], (iv) an energy harvester unit which is modeled after real- world energy harvester trace data obtained from the works by Qiu et al. [56] and Geissdoerfer et al.",
    "source": "Seeker.pdf",
    "length": 2044,
    "tokens": 462
  },
  {
    "text": "This provides full control over all quantisation pa- rameters, enabling mixed-precision quantisation for the LSTM layer an aspect that, to our knowledge, is not available using the hls4ml flow. Additionally, our approach effectively leverages existing FINN compiler transformations and introduces few new ones to generate a computation graph that is entirely free of floating- point operations. This enables primary computations within the graph to rely solely on integer arithmetic to achieve significant resource savings during the hardware generation phase, allowing seamless deployment on resource-constrained FPGAs, as demonstrated in our integration case study. In contrast, hls4ml retains floating point operators in the compute graph, and has primarily targeted LSTM deployments on large FPGA fabrics. The use of fixed-point datatypes with fractional TABLE V: The resource consumption of the Q-ConvLSTM accelerator on the ZCU104 FPGA. Q-ConvLSTM LUTs FFs LUTRAM BRAMs DSPs Conv Network 65091 36398 4596 96 252 LSTM Network 49439 22601 6631 31.5 13 Overall 101489 58999 11227 127.5 265 49.6 12.8 11 40.8 15.2 bits in hls4ml is effective in certain cases at the expense of increased resource consumption. Also, to reduce the II, hls4ml proposes a non-static imple- mentation of LSTM layers, where multiple LSTM compute units are instantiated to match the input sequence length. This design allows computed states to be passed efficiently to subsequent stages, enabling new inputs to be processed every clock cycle and achieving an II of 1. However, this approach results in high resource utilisation, which may limit scalability for complex models with a large number of intermediate states. While our approach does not consider this implementation scheme at present, we recognize its potential and see value in exploring its integration with our resource-efficient design methodology. Finally, both frameworks share the broader goal of providing a parameterisable hardware layer to streamline LSTM deploy- ment. However, while our approach prioritises optimisation for constrained FPGA devices by balancing performance with efficient resource utilisation, the hls4ml approach prioritises latency minimisation at the expense of high resource usage.",
    "source": "2506.20810v1_FINN-GL_Generalized_Mixed-Precision_Extensions_for.pdf",
    "length": 2254,
    "tokens": 478
  },
  {
    "text": "Advancing AI-assisted Hardware Design with Hierarchical Decentralized Training and Personalized Inference-Time Optimization Hao (Mark) Chen , Zehuan Zhang , Wanru Zhao , Nicholas Lane , Hongxiang Fan Imperial College London, London, UK University of Cambridge, Cambridge, UK {hc1620, zehuan.zhang22, w.luk, {wz341, Abstract Recent years have witnessed a significant increase in the adoption of AI techniques to enhance electronic design automation. In particular, the emergence of Large Language Models (LLMs) has sparked significant interest in LLM-assisted hardware design generation, spanning applications from classi- cal digital circuits to quantum computing. Despite substantial progress in this direction, the quality of LLM-generated hard- ware design still cannot meet the requirements for practical deployment. In this work, we identify three critical challenges hindering the development of LLM-assisted hardware design generation: 1) limited data availability, 2) varied data quality, 3) inadequate inference-time efficiency. To address these fundamen- tal challenges, this paper introduces a two-stage framework for AI-assisted hardware design by exploring decentralized training and personalized inference. In the first stage, we propose to harness private domain design sources through a hierarchical decentralized training mechanism that addresses data-sharing constraints. To mitigate the impact of low-quality data, we identify optimization opportunities in hardware generation tasks, using user-defined metrics for model aggregation. The second stage focuses on client personalization to enhance both speed and quality. We introduce a new metric, Trueput, to ana- lyze LLM-assisted hardware generation efficiency. To optimize Trueput, we implement personalized inference-time acceleration and customized sampling strategies. Evaluating both classical and quantum benchmarks, our experimental results demonstrate that the proposed two-stage framework can significantly improve the model capability for hardware design generation. As orthogonal enhancements to existing methods, our framework can achieve 33 50 semantic accuracy improvement and 2.3 times speedup, depending on the difficulty of the generation tasks. Both the code and benchmarks will be released publicly to foster further development in this field. I. INTRODUCTION Recent advancements in Large Language Models (LLMs) have demonstrated their great potential in automated software programming [16] and debugging [17]. This impressive capa- bility has sparked significant research and industrial interest in leveraging LLMs to automate hardware design for both classical and quantum domains.",
    "source": "2506.00002v1_Advancing_AI-assisted_Hardware_Design_with_Hierarc.pdf",
    "length": 2673,
    "tokens": 497
  },
  {
    "text": "8(d.1) and Fig. 8(d.2), respectively. Latency is stable around 1.5ms across 9 iterations of experiments (see M), which leads to more than 650KPS throughput (see N ). Such low latency and high throughput come from the efficient few-shot learning that utilizes relatively small number of samples for new classes. The same reason also leads the on-chip learning to incur low power and energy consumption. Power consumption is about 7 TABLE I SUMMARY OF COMPARISON BETWEEN OUR NEUROMORPHIC PLATFORM (AKIDA) WITH EXISTING CONVENTIONAL AI SOLUTIONS FOR OBJECT DETECTION USING YOLOV2; BASED ON OUR RESULTS AND DATA FROM STATE-OF-THE-ART [28] [30]. Desktop CPU Desktop GPU Embedded CPU Embedded GPU FPGA Our Akida Neuromorphic Platform Intel i7-6700HQ Nvidia GTX 960M ARM Cortex-A57 Nvidia Jetson TX2 ZedBoard ZCU102 Virtex-7 XC7V690t Performance [FPS] 78.2 219.7 0.23 7.8 1.02 40.81 302.3 6 Power [W] 29.88 46.67 4 5.8 1.2 4.5 11.35 0.078 Efficiency [FPS W] 2.62 4.71 0.06 1.34 0.85 9.06 26.63 76.92 41mW (see O ), and energy consumption is about 62ÂµJ (see P ), as shown in Fig. 8(d.3) and Fig. 8(d.4), respectively. E. Further Discussion It is important to compare neuromorphic-based solutions against the state-of-the-art ANN-based solutions, which typi- cally employ conventional hardware platforms, such as CPUs, GPUs, and specialized accelerators (e.g., FPGA or ASIC). To ensure a fair comparison, we select object recognition as the application and YOLOv2 as the network, while considering performance efficiency (FPS W) as the comparison metric. Summary of the comparison is provided in Table I, and it clearly shows that our Akida-based neuromorphic solution achieves the highest performance efficiency.",
    "source": "2504.00957v2_Enabling_Efficient_Processing_of_Spiking_Neural_Ne.pdf",
    "length": 1704,
    "tokens": 476
  },
  {
    "text": "Additionally, the MTE ISA seamlessly interacts with the existing vector ISAs, which makes MTE able to leverage vector architecture registers for matrix computations and vector instructions for element-wise operations. MTE incurs minimal implementation overhead since it only requires six additional operations and a 64-bit Control Status Register (CSR) to keep its state. Specifically, MTE can i) vectorize GEMMs across the three GEMM loops over M, N, and K; ii) fully exploit the capacity of the vector register file for storing matrices in uniform and mixed- precision scenarios; and iii) decouple the matrix tile geometry from the underlying microarchitecture. By leveraging these three properties, MTE achieves significant performance gains with respect to state-of-the-art vector [11], [13], [14] and matrix [5], [7] ISAs for a heterogeneous set of GEMM workloads belonging to relevant convolution and transformer models. Specifically, this paper makes the following contributions: It identifies the main performance bottlenecks of state- of-the-art matrix ISAs. These bottlenecks come from poor utilization of the matrix storage space, and also the restricted number of registers these ISAs expose to the compiler. It proposes MTE, the first geometry-agnostic ISA decoupled TABLE I THE BLAS GEMM ROUTINE ARGUMENTS Parameter Description A, B, C Pointer to the operand matrices M, N, K Matrix multiplication size LD{A, B, C} Matrices leading dimension offset TRANS{A, B, C} Matrices memory layout Î±,Î² Scaling factors from the microarchitecture, enabling code portability across implementations without programmer intervention. The paper also describes two microarchitectures supporting MTE, the first one is based on a lean extension of a long vector processor and the second one leverages a systolic array. It evaluates the performance of MTE considering 75 convo- lution workloads belonging to deep neural networks [15], [16], [17], [18], [19] and 18 transformer workloads obtained from natural language processing models [20], [21] and recommen- dation systems [22], [23], [24].",
    "source": "2507.03522v1_A_Flexible_Instruction_Set_Architecture_for_Effici.pdf",
    "length": 2085,
    "tokens": 470
  }
]