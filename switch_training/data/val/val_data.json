[
  {
    "text": "[32] D. Przewlocka-Rus, S. S. Sarwar, H. E. Sumbul, Y. Li, and B. De Salvo, Power-of-two quantization for low bitwidth and hardware compliant neural networks , 2022. arXiv:2203.05025. [33] S. Ioffe and C. Szegedy, Batch normalization: Accelerating deep network training by reducing internal covariate shift , 2015. arXiv:1502.03167. [34] P. Goyal, Accurate, large minibatch sgd: Training imagenet in 1 hour , 2017. arXiv:1706.02677. [35] K. He, X. Zhang, S. Ren, and J. Sun, Delving deep into rectifiers: Sur- passing human-level performance on imagenet classification, in 2015 IEEE Int. Conf. on Computer Vision (ICCV), 2015, pp. 1026 1034. [36] A. Pappalardo. Xilinx brevitas. (2023). [37] R. Krishnamoorthi, Quantizing deep convolutional networks for effi- cient inference: A whitepaper , 2018. arXiv:1806.08342. [38] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko, Quantization and training of neural networks for efficient integer-arithmetic-only inference, in Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Jun. 2018. [39] M. Nagel, M. Fournarakis, R. A. Amjad, Y. Bondarenko, M. van Baalen, and T. Blankevoort, A white paper on neural network quan- tization , 2021. arXiv:2106.08295. [40] O. Vinyals, C. Blundell, T. Lillicrap, k. kavukcuoglu koray, and D. Wierstra, Matching networks for one shot learning, in Advances in Neural Information Processing Systems, vol. 29, 2016.",
    "source": "2505.24852v2_Chameleon_A_MatMul-Free_Temporal_Convolutional_Net.pdf",
    "length": 1451,
    "tokens": 486
  },
  {
    "text": "Datasheets were converted from PDF to Markdown using the TextIn API (v2.1) to preserve tables. The framework utilized Qwen-3-8b for generation and Qwen-text-embedding-v2 for embeddings, employing a hybrid retrieval strategy with a keyword weight of 0.3, a similarity threshold of 0.2, and retrieval of the top-8 results. 4.2 Validation of Parameter Extraction and Analysis The proposed framework was evaluated against the baselines across all test sets, demonstrating superior performance in both TABLE I STATISTICAL DISTRIBUTION OF DATASET SAMPLES Device Type DigiKey Samples Mouser Samples Total Samples Equal Dist. Market Dist. Equal Dist. Market Dist. Diode 6 5 6 5 22 BJT 6 4 6 4 20 MOSFET 6 3 6 3 18 JFET 6 4 6 4 20 Total 24 16 24 16 80 Fig. 7. Illustration of Test Set Construction Method Three independent test sets 100 test questions 100 test questions 100 test questions Total of 80 chip datasheets (24 16 24 16) 20 questions per device type (4 question types ? 5 instances) Complete Dataset (Targeting 4 device types: Diode BJT MOSFET JFET) Randomly select 10 questions Randomly select 10 questions (per device type) TABLE II PARAMETER COMPARISON EXPERIMENT RESULTS Method Parameter Search Quality Metrics Resource Consumption Performance Metrics EM F1 EC Response Time s API Call Frequency API Token Usage Irrelevant Information Ratio BM25 Regex 0.41 0.47 0.44 7 - - 0.42 RAG 0.68 0.75 0.76 23 4.4 3845 0.24 CoT 0.72 0.87 0.84 29 5.2 4035 0.21 D2S 0.86 0.92 0.96 20 4.2 2493 0.04 Fig. 8. Comparative Analysis of Parameter Extraction Methods REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) 7 quality and efficiency, as detailed in TABLE II and illustrated in Fig. 8.",
    "source": "2502.16540v2_D2S-FLOW_Automated_Parameter_Extraction_from_Datas.pdf",
    "length": 1713,
    "tokens": 463
  },
  {
    "text": "For simplicity in installation and cluster configuration, we only set up the root user with same password and did not create additional user accounts at this stage. [20] The next step was to configure static IP addresses, which are essential for inter-server communication needed for tasks such as SLURM, MPI, and data transfer. To achieve this, we used \"nmtui\", a Network Manager tool, to assign static IP addresses to the nodes, as illustrated in the figure above. To streamline communication within the cluster, we configured the etc hostname file on all nodes. The host file maps IP addresses to hostnames, allowing us to use simple names instead of IP addresses for communication. We assigned 10.80.0.100 to the master node and sequential IP addresses (e.g., 10.80.0.101 , 10.80.0.102 , 10.80.0.103 ) to the compute nodes c1 , c2 , and c3 . Additionally, we updated the etc hostname file on each node to define its hostname, ensuring consistent identification across the cluster. This configuration simplifies node communication and is essential for seamless cluster operations, including MPI, SLURM, and other distributed tasks. We installed the SSH package on all nodes to enable secure communication within the cluster. To streamline operations, we configured passwordless SSH, allowing quick and seamless login between nodes without requiring repeated password entry. This setup simplifies administrative tasks and facilitates efficient execution of distributed processes across the cluster. [21] All logins to the cluster are routed through the master node, which serves as the central point for user access and management. The master node is equipped with all necessary software for package management, resource scheduling, and deployment, ensuring efficient coordination and operation of the cluster. This centralized setup simplifies user management and resource allocation across the cluster. To streamline software deployment across multiple nodes in the cluster, we utilize pdsh, a parallel shell tool that enables simultaneous execution of commands. First, pdsh is installed on the head node using dnf install -y pdsh, ensuring SSH access is configured for passwordless authentication across all compute nodes. With pdsh, software packages can be installed in parallel, significantly reducing deployment time. For instance, executing pdsh -w node[1-4] sudo dnf install -y package-name installs the required software on all designated nodes simultaneously.",
    "source": "2503.11246v1_Cost-effective_Deep_Learning_Infrastructure_with_N.pdf",
    "length": 2472,
    "tokens": 497
  },
  {
    "text": "The architecture is designed without relying on attention-based mechanisms, instead using lightweight operations such as mean aggregation and linear transformation. This design ensures computational efficiency and scalability, enabling fast and robust extraction of full-graph semantics from complex and heterogeneous AIG structures. Next, the concatenated feature vectors pass through a linear mapping layer that re-weights and integrates multi-scale information, reduces dimensionality, and preserves key signals. The resulting node features are then used in the TTDP loss computation. The process is defined as: J. ACM, Vol. 1, No. 2, Article 3. Publication date: May 2025. 3:10 Qiyun Zhao ‚Ñédense ùë£ ùëäfuse concat ‚Ñé(1) ùë£,‚Ñé(2) ùë£, . . . ,‚Ñé(ùêø) ùë£ ùëèfuse (13) where ‚Ñédense ùë£ denotes the node features after dense concatenation. The dense concatenation aggre- gates the output features from all ùêølayers alternating between GraphSAGE and GINConv into a higher-dimensional feature vector. ‚Ñé(ùëô) ùë£ refers to the output node features at the ùëô-th layer, and ùëèdenotes the bias term, which helps adjust the distribution of the fused features. The resulting output features are utilized for computations in the TTDP task or other similar downstream tasks. Finally, these fused features enter an MLP-based read-out layer, which applies nonlinear trans- formations, LayerNorm, Dropout, and a Sigmoid activation to map outputs to the probability range (0,1), yielding the SPP results. The read-out MLP is formally defined as: ‚Ñé(ùëô 1) ùë£ dropout ùúé Norm ùëäùëô ‚Ñé(ùëô) ùë£ ùëèùëô , ùëô {0, . . .",
    "source": "2506.06787v1_FuncGNN_Learning_Functional_Semantics_of_Logic_Cir.pdf",
    "length": 1558,
    "tokens": 448
  },
  {
    "text": "The GAP8 demonstrates the highest end-to-end latency across all models - averaging 17 slower than the MAX78000, despite having similar compute capacity (22.65 GOPs vs. 30 GOPs on the MAX78000). However, again, the GAP8 s large flash and RAM size make it more suitable for deploying large models or MoE architectures 4.2.4 CPU Post-Processing Post-processing operations, while often overlooked in benchmarking studies, can contribute to end-to-end latency and overall efficiency. We find CPU processing overhead is generally low across most of the eval- uated platforms, in comparison to other execution stages, but is non-negligible for YoloV1 s NMS on certain platforms. For instance, the MAX78000 with RISC-V CPU active takes 3.82 ms in post-processing for YoloV1, compared to 2.62 ms spent in actual inference. This outlines the importance of minimizing CPU-dependent post-processing, and highlights a key design consideration with our benchmark; by ensur- ing all models are fully NPU-compatible across the various platforms, we aim to enable a fair comparison of end-to-end latency, avoiding bottlenecks or penalties caused by unsup- ported operators falling back to CPU execution. However, in real-world use, developers would build models that are optimized for a given target platform, making it necessary to consider the range of supported operators (which is quite limited on certain NPUs), and accuracy or performance trade- offs that might arise from using other, more compute-capable platforms, with more complex or unmodified models. Millar et al.",
    "source": "2503.22567v2_Benchmarking_Ultra-Low-Power_ŒºNPUs.pdf",
    "length": 1560,
    "tokens": 347
  },
  {
    "text": "The duty cycle is set to 0.3. Canonical formulation (CF) Pure-text adjacency-matrix-based formulation (PM) Explore a tabular formulation Input numbers as float to transformer Duty cycle options 0.1 0.3 0.5 0.7 0.9 sep voltage conversion ratio 0.85973 sep efficiency 0.93273 sep Embedding (E) E Linear layer (L) L L Float-input adjacency-matrix-based formulation (FM) E E LaMAGIC LaMAGIC2 LM input: 0.1 0.3 0.5 0.7 0.9 0.85973 0.93273 sep VIN VOUT GND Sa Sa C L sep LM output: duty_0.3 sep VIN no_edge no_edge no_edge edge_1 no_edge no_edge no_edge VOUT no_edge no_edge no_edge no_edge no_edge edge1 edge_1 GND no_edge no_edge no_edge no_edge edge_1 edge_1 no_edge Sa edge_1 no_edge no_edge no_edge edge_2 edge_2 no_edge Sa no_edge no_edge edge_1 edge_2 no_edge edge_1 edge_2 C no_edge edge_1 edge_2 no_edge edge_2 no_edge edge_1 L no_edge edge_1 no_edge no_edge edge_2 edge_1 sep Succinct float-input adjacency-matrix-based formulation (SFM) LM input: 0.1 0.3 0.5 0.7 0.9 0.85973 0.93273 sep VIN VOUT GND Sa 0 Sa 1 C 2 L 3 sep LM output: duty_0.1 sep VIN Sa 0 , VOUT C 2 L 3 , GND Sa 1 C 2 , Sa 0 Sa 1 L 3 sep Succinct float-input canonical formulation with identifier (SFCI) Shrink redundant words in natural language Figure 3. The circuit formulations proposed by LaMAGIC (Chang et al., 2024) and our work LaMAGIC2. Tokens enclosed within and denote those added to the tokenizer s dictionary, enabling distinct embeddings for each token. nodes of the same component type.",
    "source": "2506.10235v1_LaMAGIC2_Advanced_Circuit_Formulations_for_Languag.pdf",
    "length": 1473,
    "tokens": 493
  },
  {
    "text": "To address potential local temperature non-uniformities across the WSSCB, the system includes a 3D-printed JETSTREAM manifold made of titanium powder fused via laser melting. This manifold is specifically designed to incorporate individually optimized nozzles to jet 2-PIC coolant evenly to each TRIMERA stack. By jetting a carefully metered flow of 2-PIC coolant to each chip location, the JETSTREAM manifold ensures effectively identical coolant velocities and pressure drops to each TRIMERA stack, irrespective of their position on the WSSCB. As a result, heat removal remains consistent from die to die, avoiding the common problem of some chips receiving less coolant flow, or chips located at trailing edges of coolant flows receiving coolant already heated by chips closer to the coolant inlet, or of some chips being in thermal hot spots. The uniform distribution of 2-PIC coolant by jets tuned by individual static 3D printed baffles bolsters the ability to operate each SLD at the high power densities described in this disclosure, without compromising reliability or performance due to uneven cooling. 24.3 Heat transfer Table 17 shows various aspects of the ZettaLith JETSTREAM cooling system. The back-side of the SOTA wafer is patterned with an array of deep channels defining heat-sink fins in silicon. The fins are etched to within approximately 25 Œºm of the CMOS layer to minimize temperature difference through the silicon. The 2-PIC coolant is individually jetted directly into the heat- sink silicon fin arrays etched into each of the 172 SLDs. This could provide an optimal and consistent temperature and mass flow for every SLD. In comparison, most current systems flow coolant over a larger area, where chips nearer the coolant inlet receive fresh coolant, while chips closer to the exit receive coolant already heated by prior chips. This results in hot-spots in the design, which ZettaLith eliminates. The HBM4 stacks generate comparatively little heat and are cooled by minor 2-PIC coolant flow patterns of each nozzle. A precision 3D-printed JETSTREAM manifold manages the flow of 2-PIC coolant to and from all 172 WSSCB locations for TRIMERA stacks and CPUs. The JETSTREAM manifold is manufactured using additive manufacturing of metal (e.g.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2269,
    "tokens": 494
  },
  {
    "text": "Notable efforts in this area include VeriGen, MEV-LLM, and AutoVCoder. VeriGen [33] fine-tunes CodeGen (2B, 6B, 16B) on a Verilog dataset collected from GitHub repositories and textbooks. It employs supervised fine-tuning with problem-code pairs, validating functional correctness using a benchmark problem set and problems from HDLBits tutorials [31]. MEV-LLM [25] trains on 31,104 source code files from GitHub, labeled by GPT-3.5, to fine-tune CodeGen (2B, 6B, 16B) and GEMMA (2B, 7B) models. This approach yields an improve- ment of up to 23.9 in the metric [6] over VeriGen. AutoVCoder [10] also uses Verilog code from GitHub, fil- tering high-quality samples with ChatGPT-3.5. It applies a two-stage fine-tuning process to improve generalization, with final evaluation conducted on a real-world benchmark. Beyond direct fine-tuning, reinforcement learning approaches like Golden Code Feedback is used to refine models iteratively using user feedback [40]. Similarly, multi-modal techniques such as VGV [41] integrate circuit diagrams with textual data during training, allowing models to understand spatial and parallel aspects of circuit design. 2.2 Benchmarks for LLM-Generated Software The research community has recognized the need for standardized benchmarks to rigorously evaluate LLM-generated code in terms of design correctness. Most existing benchmarks for code generation are tailored for software development rather than hardware design. For instance, HumanEval [6] and MBPP (Mostly Basic Python Problems) [20] are widely used to evaluate LLMs for software code generation. HumanEval consists of 164 Python programming tasks, each with a function signature and a set of test cases to validate correct- ness. While relatively small in scope, this benchmark is carefully designed, making it well-suited for quick functional correctness evaluations of Python-based code generation. Several extensions have expanded its coverage, including HumanEval [20], which increases the number of test cases by 80 times, and HumanEvalPack [24], which extends the benchmark to six programming languages.",
    "source": "2503.08823v2_ResBench_Benchmarking_LLM-Generated_FPGA_Designs_w.pdf",
    "length": 2105,
    "tokens": 478
  },
  {
    "text": "See Figure 15 for the raw distribution. System Nodes GPUs Median p95 Commercial VM (V100) 1 8 3.1x 11.4x Supercomputer (A100) 8 32 1.09x 1.32x 4 2.2 Kernel launch overhead. Comet FasterMoE Megatron DeepEP 0 20 40 60 80 100 Average SM Util ( ) 42.31 9.67 59.11 13.55 Forward Pass E 64 k 2 2 A100s is better (a) GPU SM Utilization across baselines MoE Iteration [6.557 ms] FlashDMoE DeepEP (b) Kernel Launch overhead (CUDA API row) Figure 5: 5a shows GPU utilization averaged across 100 MoE forward passes on 2 NVLinked A100s with 300 GB s unidrectional bandwidth. Despite the high-bandwidth interconnect, we observe up to 90 idle time, which we attribute to kernel launch gaps and non-overlapping communication. We compare the kernel launch overheads between FlashDMoE and existing baselines. Table 1 shows the number of kernel launches during a single forward pass: FlashDMoE launches exactly one persistent kernel, while the baselines launch up to 550 short-lived kernels to perform the same computation. Figure 5 provides a visual comparison using CUDA API traces captured by NSight Systems, illustrating the difference between FlashDMoE and DeepEP. DeepEP exhibits numerous small CUDA API calls, with frequent stalls between individual operators, leading to increased GPU idle time (Figure 5a). In contrast, FlashDMoE maintains high GPU utilization by avoiding launch overhead and synchronization gaps achieving 93.17 GPU utilization compared to 14 for DeepEP. See 4 for experimental results and A for a discussion of related work. 3 Fused MoE Kernel Design Block n-1 FusedGate Packet Encode and Dispatch Subscriber Block n-2 Block 1 Block 0 ... ...",
    "source": "2506.04667v2_FlashDMoE_Fast_Distributed_MoE_in_a_Single_Kernel.pdf",
    "length": 1652,
    "tokens": 397
  },
  {
    "text": "Subsequently, we analyze several key aspects of performance and efficacy: this includes the overall system performance ( 5.2); the inference accuracy achieved with our INT8 quantization scheme ( 5.3); an ablation study that Serving Large Language Models on Huawei CloudMatrix384 39 investigates the specific contributions of different optimization techniques employed ( 5.4); and finally, a look at the performance of critical underlying operators ( 5.5). 5.1 Experimental Setup Our evaluation is conducted on a Huawei CloudMatrix384 supernode, provided by the ModelArts Lite (Cluster Mode) service in Huawei Cloud. Specifically, we utilize a configuration comprising 256 Ascend 910 NPUs and their associated host Kunpeng CPUs from a single CloudMatrix384. The serving system consists of an LLM inference engine optimized by Huawei and SiliconFlow2 together, deployed with the requisite Huawei CANN software packages. The elastic memory service (EMS) in Huawei Cloud, providing distributed caching capabilities as detailed in 4.4, is pre-deployed across the allocated compute nodes. The entire deployment adheres to our proposed peer-to-peer serving architecture with PDC disaggregation ( 4.1), with the following specific configurations for each logical cluster: Decode Cluster: We deploy a single decode instance utilizing 160 Ascend 910 NPUs (across 20 compute nodes, yielding 320 NPU dies). This instance employs an expert parallelism degree of 320 (EP320) for the sparse MoE layers. For other components like MLA and dense FFN layers, a data parallelism degree of 320 (DP320) is used across the NPU dies. Within these 320 EP ranks, we deploy one expert instance per rank. The expert configuration comprises 32 copies of the shared expert, 256 distinct router experts, and an additional 32 redundant router experts to facilitate expert parallelism load balancing (EPLB). Prefill Cluster: The prefill cluster consists of 6 prefill instances, each allocated 16 Ascend 910 NPUs (two compute nodes per instance, yielding 32 NPU dies). In total, the prefill cluster uses 96 NPUs. Each prefill instance employs an expert parallelism degree of 32 (EP32) for sparse MoE layers. MLA components within prefill instances utilize a hybrid parallelism strategy detailed in 4.3.1.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2270,
    "tokens": 471
  },
  {
    "text": "With the PI scheme, as shown in Fig. 8d and Fig. 8c, the latency is further decreased by 13 (V1P) and 19 (V2P), whereas the energy saving is increased by 12 and 22 . The improvement brought by PI for YOLOv4-tiny model is more signiÔ¨Åcant as shown in Fig. 9c and Fig.9d, where the latency is further decreased by 36 (V1P) and 35 (V2P), while the energy saving is increased by 41 and 40 , indicating that our PI technique is quite effective. 6) Tradeoffs between Accuracy and Energy Consumption: So far in our evaluation, we wanted to minimize the accuracy impact (see Table III). However, as an alternate design princi- ple, one may want to relax this accuracy constraint and thus save more energy. We used Pytorch [46] to proÔ¨Åle the accuracy behavior of two videos picked from VIRAT [33] dataset, and show that our proposal can adaptively support such alternate design choices. More speciÔ¨Åcally, the accuracy-loss, energy- savings pairs are plotted in Fig. 10. From Fig. 10(a), one can observe that, for V1, the energy saving is about 56 with only 0.07 accuracy drop; however, if the application is willing tolerant to live with 1.3 accuracy drop, then 21 more energy can be saved (amounting to 77 energy reduction compared to the baseline). A similar trend can also be observed in V2, as shown in Fig. 10(b). These results indicate that our proposal is Ô¨Çexible adaptive with different design preferences. D. Comparison against Prior Work As discussed in Sec. II, DeepCache [8] also exploits the sim- ilarity between continuous frames at runtime, and decreases the computation via memory copies. However, it misses the opportunities of frame-level data reuse, and hence needs to perform inference for each and every frame. Thus, as shown 1082 Authorized licensed use limited to: Penn State University. Downloaded on June 29,2023 at 18:36:04 UTC from IEEE Xplore. Restrictions apply. TABLE IV: Comparison against Potluck and MCDNN mAP Latency Energy MCDNN 36.9 33 35 Potluck 51.6 63 61 This Work 50.3 35 34 in Fig.",
    "source": "PCframeSim.pdf",
    "length": 2012,
    "tokens": 498
  },
  {
    "text": "We can observe that FASTLIBRA achieves the lowest queue, LoRA cold-start, and KV cold-start latency in all scenarios. This means that FASTLIBRA has the highest HBM utilization efficiency. For in-depth analysis, we sample the average HBM utiliza- tion of FASTLIBRA and baselines across different scenarios, shown in Figure 13a. FASTLIBRA improves HBM utiliza- tion by 1.2X and 2.6X over vLLM and S-LoRA, respectively, due to its dynamic swapping of LoRAs and KV caches in a unified caching pool. In contrast, S-LoRA wastes HBM by not retaining history KV caches, while vLLM s static HBM partition makes the HBM for LoRAs or KVs under-utilized under dynamic loads. These factors also contribute to lower queue and cold-start latency for FASTLIBRA, as shown in Figure 12. 0 200 400 600 800 1000 1200 1400 1600 1800 0 20 40 60 80 100 HBM Usage History KV caches vLLM S-LoRA FastLibra (d)FastLibra achieve higher usage (b)FastLibra achieve higher usage 0 200 400 600 800 1000 1200 1400 1600 1800 0 5 10 15 20 25 HBM Usage LoRAs (a)FastLibra prefetch all LoRAs 0 200 400 600 800 1000 1200 1400 1600 1800 0 20 40 60 80 100 HBM Usage Running KV caches (c) FastLibra Fully use HBM Up to 612 0 200 400 600 800 1000 1200 1400 1600 1800 Times(s) 100 200 300 400 500 TTFT(ms) TTFT Up to 586 Up to 6752 Figure 14: The HBM allocation over time of FASTLIBRA and baselines in different application scenarios. We also compare the average KV cache and LoRA hit rates of FASTLIBRA and baselines across different scenarios, as shown in Figure 13b. FASTLIBRA increases the cache hit rate by 1.3X and 3.2X compared to vLLM and S-LoRA, re- spectively. This is because FASTLIBRA maintains the usage dependencies between LoRAs and KV caches to eliminate invalid KV caches which enhances the HBM utilization effi- ciency.",
    "source": "2505.03756v1_Improving_the_Serving_Performance_of_Multi-LoRA_La.pdf",
    "length": 1794,
    "tokens": 483
  },
  {
    "text": "New York, NY, USA: Association for Computing Machinery, 2023, p. 1002 1015. [Online]. Available: 1145 3613424.3614249 [29] H. Lu, L. Chang, C. Li, Z. Zhu, S. Lu, Y. Liu, and M. Zhang, Distilling bit-level sparsity parallelism for general purpose deep learning accelera- tion, in MICRO-54: 54th Annual IEEE ACM International Symposium on Microarchitecture, 2021, pp. 963 976. [30] W. Lu, G. Yan, J. Li, S. Gong, Y. Han, and X. Li, Flexflow: A flexible dataflow accelerator architecture for convolutional neural networks, in 2017 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE, 2017, pp. 553 564. [31] S. Markidis, S. W. Der Chien, E. Laure, I. B. Peng, and J. S. Vetter, Nvidia tensor core programmability, performance precision, in 2018 IEEE international parallel and distributed processing symposium workshops (IPDPSW). IEEE, 2018, pp. 522 531. [32] L. Mei, P. Houshmand, V. Jain, S. Giraldo, and M. Verhelst, Zigzag: Enlarging joint architecture-mapping design space exploration for dnn accelerators, IEEE Transactions on Computers, vol. 70, no. 8, pp. 1160 1174, 2021. [33] T. Norrie, N. Patil, D. H. Yoon, G. Kurian, S. Li, J. Laudon, C. Young, N. Jouppi, and D. Patterson, The design process for google s training chips: Tpuv2 and tpuv3, IEEE Micro, vol. 41, no. 2, pp. 56 63, 2021. [34] Y. Pan, J. Yu, A. Lukefahr, R. Das, and S. Mahlke, Bitset: Bit-serial early termination for computation reduction in convolutional neural networks, ACM Transactions on Embedded Computing Systems, vol. 22, no. 5s, pp. 1 24, 2023.",
    "source": "2503.06342v1_Exploring_the_Performance_Improvement_of_Tensor_Pr.pdf",
    "length": 1559,
    "tokens": 496
  },
  {
    "text": "Ieee, 2005. [62] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Brad- bury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in neural information processing systems, pages 8026 8037, 2019. [63] Heyang Qin, Syed Zawad, Yanqi Zhou, Lei Yang, Dongfang Zhao, and Feng Yan. Swift machine learning model serving scheduling: a region based reinforcement learning approach. In Proceedings of the Inter- national Conference for High Performance Computing, Networking, Storage and Analysis, pages 1 23, 2019. [64] Xueheng Qiu, Le Zhang, Ye Ren, Ponnuthurai N Suganthan, and Gehan Amaratunga. Ensemble deep learning for regression and time series forecasting. In 2014 IEEE symposium on computational intelligence in ensemble learning (CIEL), pages 1 6. IEEE, 2014. [65] Atul Rahman, Jongeun Lee, and Kiyoung Choi. EfÔ¨Åcient fpga acceler- ation of convolutional neural networks using logical-3d compute array. In 2016 Design, Automation Test in Europe Conference Exhibition (DATE), pages 1393 1398. IEEE, 2016. [66] Sara Rosenthal, Noura Farra, and Preslav Nakov. SemEval-2017 task 4: Sentiment analysis in Twitter. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 502 518, Vancouver, Canada, August 2017. Association for Computational Linguistics. [67] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. [68] Prateek Sharma, David Irwin, and Prashant Shenoy. Portfolio-driven resource management for transient cloud servers. Proc. ACM Meas. Anal. Comput. Syst., 1(1), June 2017.",
    "source": "cocktail.pdf",
    "length": 1772,
    "tokens": 472
  },
  {
    "text": "The framework s efficiency and adaptability make it particularly valuable as the field moves toward more complex network architectures requiring extensive accuracy and performance optimization. TABLE VI INFERENCE RUNTIME (SECONDS IMAGE) COMPARISON OF RESNET50 ON IMAGENET (8B ACTIVATION, 8B WEIGHT) BETWEEN CROSSSIM, AIHWKIT, AND NEUROSIM V1.5 DAC Precision MLC Noise CrossSim [37] AIHWKit [63] NeuroSim V1.5 8b 8b None 0.3 0.0025 0.0085 8b 1b None 1.1 0.033 1b 8b None 1 0.04 1b 1b None 5.8 0.24 8b 8b Device Noise 9.8 0.003 0.0085 8b 1b Device Noise 200 0.033 1b 8b Device Noise 45 0.041 1b 1b Device Noise 1220 0.24 8b 8b Output noise 0.0025 0.0093 8b 1b Output noise 0.039 1b 8b Output noise 0.047 1b 1b Output noise 0.29,0.61 128 128 array size 0.29: same noise on each MAC output, 0.61: individual noise on each MAC output V. CONCLUSION The growing complexity of AI models places increasing demands on CIM accelerator design, requiring tools that can efficiently evaluate both accuracy and hardware implemen- tation trade-offs. This work presents NeuroSim V1.5, which combines accuracy analysis with power, performance, and area estimation to support systematic design space exploration of CIM architectures. The framework s integration with TensorRT enables evalu- ation of pre-trained networks without modification, while our flexible noise modeling methodology supports both detailed device-level analysis and efficient MAC-level noise injection, validated against published SPICE simulations and silicon measurements. These capabilities, combined with up to 6.5 faster runtime compared to NeuroSim V1.4, enable more extensive design space exploration than previous versions. Through our case studies with a variety of device tech- nologies and neural network topologies, we demonstrate how NeuroSim V1.5 can provide insights into CIM design trade- offs: 1) Optimal designs must balance array size, ADC preci- sion, and multi-level cell capability.",
    "source": "2505.02314v1_NeuroSim_V15_Improved_Software_Backbone_for_Benchm.pdf",
    "length": 1957,
    "tokens": 497
  },
  {
    "text": "Inductive repre- sentation learning on large graphs, 2018. [40] Ahmed Allam and Mohamed Shalan. Rtl-repo: A benchmark for evaluating llms on large-scale rtl design projects. arXiv, 2024. [41] Sonnet Anthropic. Model card addendum: Claude 3.5 haiku and upgraded claude 3.5 sonnet. [42] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-coder technical report. arXiv, 2024. [43] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, and et al. The llama 3 herd of models. arXiv, 2024. [44] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333 389, 2009. [45] OpenRISC. mor1kx. 2022. [46] Chin-Yew Lin. ROUGE: A package for automatic evaluation of sum- maries. In Text Summarization Branches Out, pages 74 81, Barcelona, Spain, July 2004. Association for Computational Linguistics. [47] Andre Nakkab, Sai Qian Zhang, Ramesh Karri, and Siddharth Garg. Rome was not built in a single step: Hierarchical prompting for llm- based chip design. In Proceedings of the 2024 ACM IEEE International Symposium on Machine Learning for CAD, MLCAD 24, New York, NY, USA, 2024. Association for Computing Machinery.",
    "source": "2505.15701v1_HDLxGraph_Bridging_Large_Language_Models_and_HDL_R.pdf",
    "length": 1567,
    "tokens": 484
  },
  {
    "text": "For generic Transformer-based models without MoE layers, many works [41 49] have provided insights and techniques to partition and schedule computation and communication operations, aimed at finer-grained overlapping. To address the challenges posed by AllToAll communication and expert parallelism in MoE training, Tutel [50] and FasterMoE [13] overlap AllToAll with expert computation. Lancet [51] additionally enables both non-MoE computation in forward pass and weight gradient computation in backward pass to be overlapped with AllToAll. Despite overlapping, the performance of these approaches is limited in practice due to blocking synchronous collective communication with barriers. In contrast, FlashD- MoE fundamentally eliminates these inefficiencies with asynchronous, device-initiated data transfers overlapped with tiled computation all within a single kernel. FlashDMoE further differentiates itself from SOTA works like COMET [11] and DeepEP [1], which also use this form of kernel-initiated communication but at a coarse-grained granularity and without complete kernel fusion.",
    "source": "2506.04667v2_FlashDMoE_Fast_Distributed_MoE_in_a_Single_Kernel.pdf",
    "length": 1093,
    "tokens": 219
  },
  {
    "text": "Sparsity ( ) Training Dataset Monkey K Monkey L SNDR (dB) R2 Score SNDR (dB) R2 Score 0 Individual 23.72 2.02 0.85 0.14 28.40 2.25 0.95 0.17 Combined 24.26 2.01 0.87 0.13 29.46 2.33 0.96 0.14 25 Individual 24.33 2.21 0.87 0.10 28.63 2.34 0.95 0.15 Combined 24.35 2.07 0.87 0.11 29.52 2.17 0.96 0.13 50 Individual 24.18 2.21 0.87 0.13 28.48 2.29 0.95 0.14 Combined 24.30 2.03 0.87 0.15 29.55 2.24 0.96 0.14 75 Individual 24.37 2.19 0.87 0.10 28.49 2.28 0.95 0.15 Combined 24.47 1.99 0.88 0.13 29.49 2.22 0.96 0.15 (a) MobileNetV1-CAE(0.25x) Sparsity ( ) Training Dataset Monkey K Monkey L SNDR (dB) R2 Score SNDR (dB) R2 Score 0 Individual 22.72 2.35 0.81 0.14 27.71 2.47 0.94 0.17 Combined 22.65 2.24 0.81 0.14 27.32 2.35 0.93 0.17 25 Individual 22.78 2.38 0.82 0.16 27.55 2.42 0.94 0.15 Combined 22.56 2.17 0.81 0.16 27.33 2.39 0.93 0.17 50 Individual 22.70 2.21 0.81 0.19 27.55 2.52 0.94 0.15 Combined 22.53 2.15 0.81 0.16 27.35 2.45 0.93 0.15 75 Individual 22.61 2.21 0.81 0.13 27.43 2.41 0.94 0.13 Combined 22.28 2.03 0.79 0.17 27.08 2.43 0.93 0.20 (b) DS-CAE1 Training set of either Monkey K or Monkey L, evaluation is done on the test set of the same monkey. The training set of both Monkey K and L are combined together. TABLE V: Comparison of the proposed neural signal compression scheme with existing works.",
    "source": "2504.06996v1_Neural_Signal_Compression_using_RAMAN_tinyML_Accel.pdf",
    "length": 1317,
    "tokens": 538
  },
  {
    "text": "1041 1044. [79] A. Samajdar, Y. Zhu, P. Whatmough, M. Mattina, and T. Kr- ishna, Scale-sim: Systolic cnn accelerator simulator, arXiv preprint arXiv:1811.02883, 2018. [80] A. Sarma, S. Singh, H. Jiang, A. Pattnaik, A. K. Mishra, V. Narayanan, M. T. Kandemir, and C. R. Das, Exploiting activation based gradient output sparsity to accelerate backpropagation in cnns, arXiv preprint arXiv:2109.07710, 2021. [81] A. Sarma, S. Singh, H. Jiang, A. Pattnaik, A. K. Mishra, V. Narayanan, M. T. Kandemir, and C. R. Das, Exploiting activation based gradient output sparsity to accelerate backpropagation in cnns, arXiv preprint arXiv:2109.07710, 2021. [82] scale.com, Data labeling: The authoritative guide, data-labeling- for-computer-vision, (Accessed on 11 21 2022). [83] A. W. Services, Aws outposts rack pricing, outposts rack pricing , (Accessed on 11 21 2022). [84] K. Seyerlehner, G. Widmer, and P. Knees, Frame level audio similarity-a codebook approach, in Proc. of the 11th Int. Conf. on Digital Audio Effects (DAFx-08), 2008, p. 31. [85] Shadi Noghabi, Landon Cox, Sharad Agarwal, Ganesh Anantha- narayanan, The emerging landscape of edge-computing, in ACM SIGMOBILE GetMobile, 2020. [86] Si Young Jang, Yoonhyung Lee, Byoungheon Shin, Dongman Lee, Dionisio Vendrell Jacinto , Application-aware iot camera virtualization for video analytics edge computing, in ACM IEEE SEC, 2018.",
    "source": "Usas.pdf",
    "length": 1382,
    "tokens": 466
  },
  {
    "text": "In the mixed-precision quantization and PIM configuration search phase, training begins with a floating-point (FP) model to achieve high accuracy. An SGD optimizer is used with an initial learning rate of 0.1, training for 200 epochs, with a step learning rate sched- uler reducing the rate by a factor of 5 at epochs 60, 120, and 160. The FP weights are then used to retrain the model with variable mixed-precision quantization, employing the Adam optimizer [17] for quantization-aware training (QAT). Due to the sensitivity of QAT to learning rate, a low initial learning rate of 0.0008 is set, which is further divided by 5 every 40 epochs using a step learning rate scheduler. Table 2: Performance comparison against different PIM- oriented NAS methods on CIFAR-10 dataset. Method PIM Latency EDP Search Accuracy (ùëöùë†) (ùëöùêΩ ùëöùë†) time (h) NACIM [14] 73.9 - 1.55 59 UAE [30] 83 - - 154 NAS4RRAM [31] 84.4 - - 289 Gibbon [28] (acc opt.) 89.2 3.44 1.67 6 Gibbon [28] (edp opt.) 83.4 1.99 0.26 6 CrossNAS (ùë§ùëéùëêùëê 0.99) 91.27 1.35 0.28 6 CrossNAS (ùë§ùëéùëêùëê 0.8) 88.09 0.577 0.073 5 Table 3: Performance comparison against various well- known deep learning models on CIFAR-10 Dataset.",
    "source": "2505.22868v1_CrossNAS_A_Cross-Layer_Neural_Architecture_Search_.pdf",
    "length": 1172,
    "tokens": 374
  },
  {
    "text": "Fig. 11. Batch inference elapsed time comparison between default single process (SP), MPx2, and MPSx2. For the MP scenarios, this captures the average batch inference time between the two processes. C. Multiprocessing MPS with vLLMs vLLM Multiprocessing From Figure 10, we observe a 1.42x speedup completing 160 inference requests comparing a single instance of vLLM (SP) and MPSx2 (two vLLM engines, each processing 80 inference requests). At lower batches, the benefit of MPS throughput is less noticeable as there is an initialization cost to starting multiple processes. Interestingly, we observe that MPx2 without leveraging MPS consistently performs worse than single vLLM. Possibly the throughput gains of multiprocessing are overshadowed by any GPU context switching overhead required switching between the two processes. A trade-off between throughput and latency is expected and observed when comparing the elapsed time of a batch of inferences in a single process. As shown in Figure 11, the time taken to process the same batch of inferences in a single process is consistently higher whenever multiprocessing is enabled, regardless of whether MPS is enabled. vLLM Scheduler Multiprocessing While implemented to the extent that inference requests could be serviced success- fully, the initial approach was flawed due to the significant overheads and complications from spawning processes on- demand. PyTorch with CUDA tensors necessitate spawning processes instead of forking, and this not only induces heavy 6 initialization cost but also restricts the objects that can be parent-defined as they must be picklable. For prototyping, this required us to remove swapping and CUDA graphs usage as both involved objects that could not be pickled. We discuss improved approaches in the following section. V. CONCLUSION FUTURE CONSIDERATIONS Overall, we demonstrate that using multiprocessing alone compared to sequential inference can lead to a 17.6 reduc- tion in latency when running the Huggingface pipeline on the A100. We also observe a 1.1 improvement in steady- state throughput when running 4 processes in parallel using splitwiser, compared to sequential inference. Enabling MPS shows an 18.2 latency reduction compared to sequential inference when running the Huggingface pipeline on the A10.",
    "source": "2505.03763v1_Splitwiser_Efficient_LM_inference_with_constrained.pdf",
    "length": 2309,
    "tokens": 481
  },
  {
    "text": "[15] highlight a performance comparison between FPGA and GPU inferences of binarized neural networks, revealing a trade-off between throughput and energy efficiency. In the embedded domain, Dimitrovski et al. [7] review neural network architectures for aerial imagery, primarily focusing on image classification accuracy while neglecting real-time inference capabilities. Wang et al. [31] and Wu et al. [32] propose new neural network architectures for real-time semantic segmentation of aerial images, yet their deployment on embedded hardware remains unexplored. Moreover, exist- ing research often limits its focus to single COTS platforms and toolchains for real-time inference [18], [30]. The literature exhibits significant limitations, predominantly focusing on image classification networks, which are less relevant for earth observation via UAVs and satellites. Fur- thermore, the research largely relies on benchmark datasets (e.g., ImageNet) and often restricts its experimental scope to single COTS platforms and toolchains. Studies encompassing multiple hardware targets or compilers are typically classified as surveys rather than experimental research. In contrast, our research stands out due to its comprehensive approach in several key areas: A focus on semantic segmentation, an essential task for analyzing imagery from UAVs and satellites, diverging from the common focus on image classification. The adoption of a U-Net architecture for image segmen- tation, which includes both down-sampling (encoder) and up-sampling (decoder) paths, contrasting with the solely down-sampling nature of image classification networks. This approach exposes unique challenges in certain work- flows that previous studies have not addressed. The utilization of the Inria Aerial Image Labeling Dataset for real-world applications, moving away from the con- ventional use of benchmark datasets like ImageNet. A comprehensive evaluation involving multiple work- flows and hardware targets, providing a holistic view of their performance and limitations. III. EMBEDDABLE U-NET-BASED SEMANTIC SEGMENTATION OF AERIAL IMAGES A. Semantic Segmentation of Aerial Images Our research is situated within the context of earth ob- servation, focusing primarily on two application domains: satellites and UAVs.",
    "source": "2503.08700v1_Real-Time_Semantic_Segmentation_of_Aerial_Images_U.pdf",
    "length": 2299,
    "tokens": 460
  },
  {
    "text": "Rapidchiplet: A toolchain for rapid design space exploration of chiplet architectures, 2023. [69] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79 87, 1991. [70] Rishabh Jain, Vivek M Bhasi, Adwait Jog, Anand Sivasubramaniam, Mahmut Taylan Kandemir, and Chita R Das. Pushing the performance envelope of dnn-based recommendation systems inference on gpus. In To be presented in proceedings of the 57th Annual IEEE ACM International Symposium on Microarchitecture, pages 62 76, 2024. [71] Rishabh Jain, Scott Cheng, Vishwas Kalagi, Vrushabh Sanghavi, Samvit Kaul, Meena Arunachalam, Kiwan Maeng, Adwait Jog, Anand Sivasubramaniam, Mahmut Taylan Kandemir, and Chita R. Das. Optimizing cpu performance for recommendation systems at-scale. In Proceedings of the 50th Annual International Symposium on Computer Architecture, pages 1 15, 2023. [72] Natalie Enright Jerger, Ajaykumar Kannan, Zimo Li, and Gabriel H Loh. Noc architectures for silicon interposer systems: Why pay for more wires when you can get them (from your interposer) for free? In 2014 47th Annual IEEE ACM International Symposium on Microarchitecture, pages 458 470. IEEE, 2014. [73] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L√©lio Re- nard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Tim- oth√©e Lacroix, and William El Sayed. Mistral 7b, 2023.",
    "source": "NSF_LLM_Medium_Proposal.pdf",
    "length": 1581,
    "tokens": 457
  },
  {
    "text": "Pixel values: The pixel contents values (denoted as F in Fig. 3) matter only during data transfer (from the input 360 frame to the framebuffer) in the projection mapping stage, after the coordinate mappings (P in Fig. 3) are gener- ated. Potentially, content-based optimizations (e.g., content cache [63]) can beneÔ¨Åt the data transfer; however, they are not attractive candidates to leverage compute reuse, which is the major power-hungry stage (as shown in Fig. 2a). In 246 TABLE II: Video workloads. No. Video Type (Cam movement focus of attention direction) Frame Rate (fps) FramesBit Rate (kbps) V1 Rhinos [4] Stationary cam, no focus direction 30 3280 13462 V2 Timelapse [56] Stationary cam, fast-moving objects, no focus direction 30 2730 15581 V3 Rollercoaster [35] Fast-moving cam hooked in front of a rollercoaster, uni-direction focus 29.97 6194 16075 V4 Paris [51] Stationary cam, smooth scene cuts, no focus direction 59.94 14629 14268 V5 Elephants [5] Stationary cam, uni-direction focus 30 5510 16522 this work, we are focusing on reusing computation results rather than reducing the content maintenance transfer, and hence do not consider that optimization. Video meta-information: This contains the additional infor- mation, such as frame rates, video semantics types, etc., about the video inputs. This feature can only be used as an add-on, along with other inputs to further improve compute reuse scope. For example, if the 360 video frame rate increases from the typical 30 fps to 60 fps, then one can potentially leverage this enhanced compute frequency in conjunction with the head orientation, to further expand the compute reuse window. Note however that, this meta- information is not on the data-dependence chain, and we do not consider it for memoization. To summarize, among the above discussed features, we identify head orientation as the only suitable memoization candidate for boosting the compute reuse scope.",
    "source": "DejaView.pdf",
    "length": 1942,
    "tokens": 449
  },
  {
    "text": "5.1 TinyNLP Benchmark Results On the TinyNLP benchmark, where models are evaluated based on Accuracy, EmbBERT-Q demon- strates the best performance compared to the other models, achieving an Average Accuracy of 88.17 , as shown in Table 5. Notably, EmbBERT-Q outperforms BERT-Tiny, which requires around 25 more memory but only achieves the second-highest average Accuracy of 86.99 . Interestingly, the only models in the 2 MB range that offer comparable results were the Embedder and Embedder Conv configurations. Despite their seemingly simplistic design, these models perform well on the TinyNLP tasks, which generally feature shorter and less complex phrases with respect to the GLUE benchmark. For these tasks, the pretraining applied to the other models had a relatively limited impact, as shown in A. These results highlight the Embedder models ability to handle lightweight tasks effectively. The 2 MB down-scaled versions of the BERT and MAMBA models, on the other hand, score lower on Average Accuracy with respect to Embedder models, indicating that these models may be less suitable for environments with stringent memory budgets. This suggests that the overall architecture of EmbBERT-Q, with highly optimized embedding and attention structures, is particularly well-suited for the TinyNLP classification tasks in memory-constrained scenarios, with respect to down-scaled versions of standard models. 5.2 GLUE Benchmark Results On the GLUE benchmark, models were evaluated using various metrics, including Matthews Correla- tion Coefficient (MCC), F1 score, Spearman s Correlation Coefficient (SCC), and Accuracy, depending on the nature of each task. EmbBERT-Q once again emerges as the top-performing model within the 2 MB memory range, achieving an average score of 62.81. It demonstrates competitive performance across multiple tasks, excelling particularly in datasets such as WNLI (87.32 F1 score) and STSB (49.25 F1 score). Detailed results for the score metrics are presented in Table 6, along with the average computed across all datasets (in the last column), while complete results can be found in A.",
    "source": "2502.10001v1_EmbBERT-Q_Breaking_Memory_Barriers_in_Embedded_NLP.pdf",
    "length": 2124,
    "tokens": 458
  },
  {
    "text": "Among them, VerilogEval, HDLEval, and PyHDL-Eval are widely recognized. VerilogEval [21] is a widely adopted benchmark for evaluat- ing LLMs in Verilog code generation [1, 10, 43]. It consists of 156 problems taken from the HDLBits tutorial website [31], covering a range of Verilog tasks from combinational circuits to finite state machines. The framework automates functional correctness testing by comparing the simulation outputs of generated designs against predefined golden so- lutions. HDLEval [17] follows a language-agnostic approach. In par- ticular, this benchmark allows the same set of problems, formulated in plain English, to be evaluated across different HDLs. The benchmark consists of 100 problems systemati- cally categorized into combinational and pipelined designs, covering fundamental hardware components such as logic gates, arithmetic operations, and pipelined processing units. A prominent feature of this benchmark is the use of formal verification instead of unit tests. This feature ensures that the generated HDL code is functionally correct and maintains logical equivalence with reference implementations. PyHDL-Eval [4] is a framework for evaluating LLMs on specification-to-Register Transfer Level (RTL) tasks within Python-embedded domain-specific languages (DSLs). It in- cludes 168 problems across 19 subclasses, covering combi- national logic and sequential logic. The evaluation process involves executing the generated code in Python-embedded HDLs (e.g., PyMTL3, PyRTL) and measuring functional cor- rectness based on pass rates. 2.4 Challenges in LLM Benchmarking for FPGA Design Despite advancements in LLM-driven Verilog generation, existing models primarily focus on producing syntactically and functionally correct HDL but fail to address critical hardware constraints essen- tial for FPGA deployment, such as resource efficiency, timing con- straints, and power consumption. Unlike ASIC design, FPGA-based development demands careful consideration of resource usage, in- cluding lookup tables (LUTs), flip-flops (FFs), block RAM (BRAM), and digital signal processing (DSP) blocks.",
    "source": "2503.08823v2_ResBench_Benchmarking_LLM-Generated_FPGA_Designs_w.pdf",
    "length": 2127,
    "tokens": 465
  },
  {
    "text": "These results highlight NetTAG s strong accuracy and robustness for circuit-level tasks. TABLE V: Performance comparison on Task4: overall circuit power area prediction. Target Metric EDA Tool GNN NetTAG R MAPE R MAPE R MAPE ( ) ( ) ( ) Area w o opt 0.99 5 0.99 5 0.99 4 w opt 0.95 34 0.95 18 0.96 11 Power w o opt 0.99 34 0.99 12 0.99 8 w opt 0.73 38 0.76 19 0.86 12 Post-layout labels are collected in two scenarios: without considering optimization (denoted as w o opt ) and with optimization as illustrated in [7] (denoted as w opt ). We adapt the GNN model from [7] for netlist power and area prediction, as it was originally designed for the layout-stage power prediction. C. Comparision with pre-trained netlist encoders In addition to the task-specific methods, we also compare Net- TAG with pre-trained netlist encoders. Since they are all limited to combinational AIG circuits, we evaluate them only on Task 1, using an AIG-format dataset. As shown in Fig. 5, NetTAG outperforms SOTA AIG-based encoders, including FGNN [29] and DeepGate3 [27], achieving the highest average performance across all metrics. Notably, NetTAG achieves superior accuracy on the AIG dataset compared with more diverse gate datasets in TABLE III for Task 1, underscoring its adaptability. We also evaluate the standalone ExprLLM component of NetTAG, which performs well using symbolic expressions, highlighting the power of gate semantic understanding. These results emphasize NetTAG s advantage in com- bining semantic and structural information through the TAG format. 88 90 88 86 90 92 90 89 96 96 96 95 97 98 97 97 80 90 100 Acc. Prec. Recall F1 Acc. Prec. Recall F1 Acc. Prec. Recall F1 Acc. Prec. Recall F1 FGNN [28] DeepGate3 [27] ExprLLM only (ours) NetTAG (ours) Classif. Metrics ( ) Fig. 5: Comparision with pre-trained AIG encoders on AIG dataset.",
    "source": "2504.09260v1_NetTAG_A_Multimodal_RTL-and-Layout-Aligned_Netlist.pdf",
    "length": 1844,
    "tokens": 473
  },
  {
    "text": "3Note that this synaptic sparsity does not yield memory savings because sparse encoding of a 1024 1024 ternary matrix takes 7.5 more memory than dense encoding. However, zero weights do allow for energy savings by skipping calculations. 2 Published at ICLR 2025 Workshop (SCOPE) of the GRU (Cho et al., 2014), inspired by the HGRN (Qin et al., 2023; 2024), modified to use only additions and element-wise products. This is achieved by employing BitLinear layers for all linear transformations within the MLGRU cell. Formally, we denote ht Rd as the hidden state at time t and xt Rd as the input at time t. The dynamics of the MLGRU layer at time step t are given by: ft œÉ (BitLinear(xt; Wf, gf, œµ)) , ct œÑ (BitLinear(xt; Wc, gc, œµ)) , ht ft ht 1 (1 ft) ct, gt RMSNorm (BitLinear(xt; Wg, gg, œµ); gg , œµ) , o t gt œÑ(ht), ot BitLinear(o t; Wo, go, œµ) where Wc, Wf, Wo, Wg Rd d are ternary weights, ft, gt, ct, o t are intermediate activations at time step t, ht is the hidden state, and ot is the final output at time step t. The initial hidden state h0 is set to zero. Similarly to the HGRN (Qin et al., 2023), the MLGRU also employs the cummax operation to bias the forget gate values in deeper layers closer to 1, which we omit for brevity. 3 MODEL ADAPTATION FOR LOIHI 2 Quantization of weights and activations As a first step, we quantize the original half-precision model from Zhu et al. (2024). Loihi 2 supports 8-bit weights and 24-bit activations.",
    "source": "2503.18002v2_Neuromorphic_Principles_for_Efficient_Large_Langua.pdf",
    "length": 1453,
    "tokens": 434
  },
  {
    "text": "3. Complexity Analysis: The heuristic has a time complexity of O(N log N) due to sorting tasks based on P eff i , which is acceptable for real-time applications. Complexity Analysis of DynInfer: The time complexity of the scheduling algorithm is O(N log N) due to sorting tasks, and the space complexity is O(N) for storing task parameters. Compared to classical inference, DynInfer introduces additional overhead for scheduling and task fusion, but this is offset by the gains in reliability and efficiency under intermittent power. Handling Extremely Low or Sporadic Energy Levels: In environments with extremely low or sporadic energy levels where consistent dropout and quantization adjustments may not be feasible, NExUME handles this by: 1. Implementing a minimum viable model configuration that operates at the lowest acceptable energy consumption, achieved by maximizing dropout rates and using the lowest quantization bit-widths. 2. Prioritizing essential tasks and deferring non-critical computations. 3. Employing predictive energy harvesting models to anticipate energy availability and adjust computations proactively. In extreme cases, the system can enter into a low-power standby mode and resume operation when sufficient energy is available. These strategies ensure that the system remains operational and provides degraded but acceptable performance under severe energy constraints. Novelty in Energy-Aware Scheduling: While energy-aware scheduling is not novel in itself, our contribution lies in adapting scheduling algorithms specifically for intermittent power environments. Existing scheduling algorithms typically assume stable energy availability and do not account for the atomicity constraints imposed by intermittent power supply. Our scheduling approach uniquely integrates: 1. Real-time energy availability into scheduling decisions. 2. Task fusion to minimize checkpointing overhead, which is critical in intermittent environments. 3. Dynamic adjustment of computational tasks based on both energy and task criticality. These innovations enable efficient and reliable DNN inference under intermittent power conditions, differentiating our work from existing energy-aware schedulers. Rationale Behind Method Design: The overall method design of NExUME is motivated by the need to enable DNNs to function reliably in environments with intermittent and unpredictable energy supply. By integrating energy variability into both training and inference, we allow the DNN to adapt its computational load dynamically, ensuring that critical tasks are completed within energy constraints. This holistic approach addresses the limitations of existing methods that treat training and inference separately or do not account for real-time energy fluctuations.",
    "source": "NexUME.pdf",
    "length": 2776,
    "tokens": 494
  },
  {
    "text": "This convergent behaviour suggests that more complex similarity calculations provide no additional benefit when working with these features. While binary pattern matching achieves lower accuracy compared to the student model with softmax, its computa- tional simplicity and minimal memory requirements make it an attractive option for extremely resource-constrained edge applications where the 11 accuracy trade-off is acceptable in exchange for significant energy savings and hardware sim- plification. C. Multiple template analysis The investigation of using multiple templates per class reveals slight improvements in classification accuracy, as shown in Table II. This analysis explores configurations using Fig. 7. Per-class accuracy of optimised student model using feature-based pattern matching classifier. one, two, and three templates per class to capture intra-class variations more effectively. Number of templates Accuracy ( ) 1 70.91 2 71.64 3 71.60 TABLE II COMPARISON OF ACCURACY WHEN USING VARYING NUMBER OF TEMPLATES. The increasing the number of templates from one to two improved accuracy from 70.91 to 71.64 , a gain of 0.73 . However, further increasing to three templates per class showed a slight decrease to 71.60 , suggesting diminishing returns from additional templates. This pattern suggests that binary quantisation, with its reduction of features, creates rel- atively distinct and well-separated class representations where a single template can already capture the essential pattern for each class. The minimal improvement with a second template and decrease with a third indicates that these features lack the granularity to meaningfully distinguish additional intra- class patterns, essentially leading to redundant or potentially conflicting templates. These consistently small improvements indicate that in- creasing the number of templates may not be an effective strategy for enhancing classification performance. The limited gains must be weighed against the significant increase in memory requirements and computational complexity associ- ated with storing and comparing multiple templates per class. This suggests that simpler configurations with fewer templates might offer a better balance between resource utilisation and classification performance. The results indicate that focusing on optimising the quality of a single template per class might be more beneficial than increasing the number of templates, particularly given the minimal returns observed with additional templates. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL., NO., FEBRUARY 2025 11 D. Energy efficiency estimates For the back-end classifier, each template matching opera- tion consumes 185fJ per cell.",
    "source": "2502.10089v1_A_Hybrid_Edge_Classifier_Combining_TinyML-Optimise.pdf",
    "length": 2728,
    "tokens": 491
  },
  {
    "text": "(2024b) propose mapping the 1 arXiv:2502.15832v1 [cs.AR] 20 Feb 2025 Published as a conference paper at ICLR 2025 Verilog Abstract Syntax Tree (AST) directly to natural language, but this approach is limited to line- level translation and produces descriptions that lack high-level semantics. To further bridge this gap, Zhang et al. (2024) introduce the MG-Verilog dataset with multi-level descriptions alongside corre- sponding code samples, but its small size and reliance on LLaMA2-70B-Chat for annotations raise quality concerns. Such poor alignment between natural language and Verilog code can degrade the generation performance, leading to generation of non-synthesizable or non-functional Verilog code. To address these challenges, we introduce DeepRTL, a unified representation model that bridges Ver- ilog understanding and generation. Achieving this requires an extensive collection of high-quality, hardware-specific datasets, which are scarce in the open-source community. To this end, we have meticulously curated a comprehensive Verilog dataset that ensures strong alignment between Verilog code and natural language across multiple levels. This dataset includes both open-source and propri- etary Verilog design data. For the open-source data, we adopt the chain-of-thought (CoT) approach and use GPT-4, the most advanced model available, to generate precise natural language descriptions of the Verilog code. Human evaluations have verified that these annotations are approximately 90 accurate, underscoring the dataset s high quality and reliability for training. For the proprietary data, we engage a team of professional hardware designers to provide detailed annotations, which capture intricate design features and further boost the dataset s quality. This comprehensive dataset enables us to develop DeepRTL capable of both understanding and generating Verilog code. By integrating high-quality annotations, the model enhances efficiency and accuracy in various design tasks. We are the first to integrate the task of Verilog understanding into our model, addressing a signifi- cant gap left by previous works that focus exclusively on Verilog generation. These earlier efforts lack benchmarks to evaluate LLMs understanding capabilities of Verilog code, prompting us to introduce the first benchmark for Verilog understanding.",
    "source": "2502.15832v1_DeepRTL_Bridging_Verilog_Understanding_and_Generat.pdf",
    "length": 2351,
    "tokens": 475
  },
  {
    "text": "MGS: Markov Greedy Sums for Accurate Low-Bitwidth Floating-Point Accumulation Vikas Natesh Harvard University Cambridge, MA, USA H.T. Kung Harvard University Cambridge, MA, USA David Kong Harvard University Cambridge, MA, USA Abstract We offer a novel approach, MGS (Markov Greedy Sums), to im- prove the accuracy of low-bitwidth floating-point dot products in neural network computations. In conventional 32-bit floating- point summation, adding values with different exponents may lead to loss of precision in the mantissa of the smaller term, which is right-shifted to align with the larger term s exponent. Such shift- ing (a.k.a. swamping ) is a significant source of numerical errors in accumulation when implementing low-bitwidth dot products (e.g., 8-bit floating point) as the mantissa has a small number of bits. We avoid most swamping errors by arranging the terms in dot product summation based on their exponents and summing the mantissas without overflowing the low-bitwidth accumulator. We design, analyze, and implement the algorithm to minimize 8-bit floating point error at inference time for several neural networks. In contrast to traditional sequential summation, our method has signif- icantly lowered numerical errors, achieving classification accuracy on par with high-precision floating-point baselines for multiple image classification tasks. Our dMAC hardware units can reduce power consumption by up to 34.1 relative to conventional MAC units. 1 Introduction Quantization has become a ubiquitous optimization for compress- ing deep neural networks (DNNs) on both low-power edge devices [20, 29, 35, 37] as well as large-scale training and inference sys- tems made up of many GPUs [34]. Low-power devices for tinyML typically have small local memories [5] and often lack support for efficient floating-point computation [3, 37]. Hence, integer quanti- zation is, by default, necessary on such systems, and most tinyML models are quantized to 8 bits or less. Meanwhile, large generative AI workloads push GPU-based training and inference clusters to the limits of available memory, bandwidth, and computation power.",
    "source": "2504.09072v1_MGS_Markov_Greedy_Sums_for_Accurate_Low-Bitwidth_F.pdf",
    "length": 2141,
    "tokens": 467
  },
  {
    "text": "URL acl-main.449 . Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. GQA: Training generalized multi-query transformer models from multi-head check- points. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https: openreview.net forum?id hmOwOZWzYE. Alazemi, F., AziziMazreah, A., Bose, B., and Chen, L. Routerless network-on-chip. In 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), pp. 492 503, 2018. doi: 10.1109 HPCA.2018. 00049. Allam, A. and Shalan, M. Rtl-repo: A benchmark for evaluating llms on large-scale rtl design projects, 2024. Allamanis, M., Panthaplackel, S., and Yin, P. Unsupervised evaluation of code llms with round-trip correctness, 2024. URL An, C., Zhang, J., Zhong, M., Li, L., Gong, S., Luo, Y., Xu, J., and Kong, L. Why does the effective context length of 9 ML For Hardware Design Interpretability: Challenges and Opportunities llms fall short?, 2024. URL abs 2410.18745. Anthropic. The claude 3 model family: Opus, son- net, haiku, 2024. URL 2303.08774. Bairi, R., Sonwane, A., Kanade, A., C., V. D., Iyer, A., Parthasarathy, S., Rajamani, S., Ashok, B., and Shet, S. Codeplan: Repository-level coding using llms and planning. Proc. ACM Softw. Eng., 1(FSE), July 2024. doi: 10.1145 3643757. URL 1145 3643757. Bajwa, I. and Choudhary, M. A rule based system for speech language context understanding. 23, 12 2006.",
    "source": "2504.08852v1_ML_For_Hardware_Design_Interpretability_Challenges.pdf",
    "length": 1448,
    "tokens": 472
  },
  {
    "text": "User: 2 System: SS8050.pdf (EVVO) has been selected. Subsequent queries will be based on this document for precise retrieval. User: Search for the emitter-base breakdown voltage, DC current gain, and output capacitance of SS8050 System: Relevant parameter information has been found in the SS8050- D.pdf technical document, as follows: 1.BVEBO (Emitter-Base Breakdown Voltage) Minimum (Min): 6V Test Conditions: IE 100ŒºA, IC 0 2.hFE (DC Current Gain) 2.1 hFE1 (Low Current Condition) Minimum (Min): 45 Test Conditions: VCE 1V, IC 5mA 2.2 hFE2 (Medium Current Condition) Minimum (Min): 85 Maximum (Max): 300 Test Conditions: VCE 1V, IC 100mA 2.3 hFE3 (High Current Condition) Minimum (Min): 40 Test Conditions: VCE 1V, IC 800mA 3.Cob (Output Capacitance) Typical (Typ): 9.0 pF Test Conditions: VCB 10V, IE 0, f 1MHz User: Continue searching for the fT parameter System: Continue searching in SS8050.pdf (EVVO) until the user specifies the next document. Context Memory: Query the DC current gain (hFE) of SS8050 in document SS8050-D.pdf Large Model Prediction of Parameter-Relevant Chapter: hFE, as a core electrical parameter of transistors, is typically listed in the \"Electrical Characteristics\" chapter in tabular form, including typical value ranges (e.g., minimum, typical, maximum) and test conditions. Additionally, transistor datasheets often include an hFE classification table, segmented by grades (e.g., hFE1, hFE2), providing detailed ranges for core parameters. { Parameter1 : hFE Primary Chapter : Electrical Characteristics , Supplementary Chapters : [ hFE Classification Table ] } S1 Optimize Retrieval and Prioritize Answer Organization S2 DC Current Gain (hFE) Search Procedure: Implicit Knowledge Mining of Document Layout for Parameters redict the most likely chapter for parameter user_input} of chip model {chip_model} based on ommon datasheet structures, and output the result in he specified format: Primary Chapter\": A string indicating the most likely main hapter for the parameter.",
    "source": "2502.16540v2_D2S-FLOW_Automated_Parameter_Extraction_from_Datas.pdf",
    "length": 2008,
    "tokens": 501
  },
  {
    "text": "This multi-granularity representa- tion enables our database to store both the code structure and the hardware behavior of a single HDL file, thereby facilitating a more comprehensive graph abstraction of the HDL, as shown in Figure 3. 2) Meta-data generation. After the parsing of graph data, we generate embeddings for nodes (both MODULE and module incrementer(input wire clk, input wire rst, input wire [3:0] in_data, output wire [3:0] out_data); reg [3:0] data_reg; always (posedge clk or posedge rst) begin if (rst) data_reg 4'b0000; else data_reg in_data; end assign out_data data_reg 1; endmodule module top_module(input wire clk, input wire rst, input wire [3:0] sw, output wire [3:0] led); wire [3:0] inc_result; incrementer u_incrementer (.clk(clk), .rst(rst), .in_data(sw), .out_data(inc_result)); reg[3:0] led_reg; always (posedge clk or posedge rst) begin if (rst) led_reg 4'b0000; else led_reg inc_result; end assign led led_reg; endmodule (1) source code (2) nodes edges example nodes MODULE: \"incrementer\", \"top_module\" Block: \"always...\", \"assign...\" Signal: \"clk\",\"rst\", \"in_data\",\"out_data\"... Temp: \"Branch_in_data_4'd0\" example edges meta-data of an BLOCK node type: \"Always\" code_embedding: \"0.08,0.028,-0.09 ,0.003,...\" code: \"always (posedge clk or posedge rst) begin .......\" AST DFG AST DFG CONTAINS: (\"top_module\")- (\"always_led_reg\") INITIATES: (\"top_module\")- (\"u_incrementer\") FLOWS_TO: (\"inc_result\")- (\"led_reg\") COND: (\"rst\")- (\"Branch_incrementer.in_ data_4'd0\") (3) visualization FLOWS_TO FLOWS_TO FALSE FLOWS_TO F FLOWS_TO FLOWS_TO C F TRUE FLOWS_TO FLOW CONTAINS CONTAINS CONTAINS CONTAINS CONTAINS CONTAINS CONTAINS CONTAINS CONTAI CONTAINS CONTAINS CONTAINS CONTAINS CONTAINS INS CONTAINS CONTAINS CONTAINS CONTAINS CONTAINS CONTAINS CONTAINS FLOWS_TO FLOWS_TO FALSE FLOWS FLOWS_TO FLOWS_TO C FLOWS_TO TRUE CONTAINS CONTAINS C CONTAI CONTAINS CONTAINS CONTAINS CONTAINS CONTAINS CONTAINS CONTAINS CONTAINS CONTAINS CONTAINS CONTAINS in_data data_reg rst out_data Branch_ 4'd0_gra Plus_gra 'd1_grap top_mo input wire clk, i wire [3:0] inc_r u_incre reg [3:0] led_re always (pos assign led led_ led inc_result clk sw led_reg rst Branch_ 4'd0_gra increme input wire clk, i reg [3:0] data_r always (pos assign out_data d clk Module Block Signal Temp Fig.",
    "source": "2505.15701v1_HDLxGraph_Bridging_Large_Language_Models_and_HDL_R.pdf",
    "length": 2289,
    "tokens": 781
  },
  {
    "text": "1203 1216, 2008. [Online]. Available: [20] T. Oxley, N. Opie, S. John, and et al., Minimally invasive endovascular stent-electrode array for high-fidelity, chronic recordings of cortical neural activity, Nature Biotechnology, vol. 34, pp. 320 327, 2016. [21] E. Musk, An Integrated Brain-Machine Interface Platform With Thousands of Channels, J Med Internet Res, vol. 21, no. 10, p. e16194, Oct 2019. [Online]. Available: [22] N. A. Steinmetz, C. Aydin, and et al., Neuropixels 2.0: A Miniaturized High-Density Probe for Stable, Long-Term Brain Recordings, Science (New York, N.Y.), vol. 372, no. 6539, p. eabf4588, Apr 2021, copyright 2021 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. [Online]. Available: [23] A. Wang, Z. Jin, C. Song, and W. Xu, Adaptive compressed sensing architecture in wireless brain-computer interface, in Proceedings of the 52nd Annual Design Automation Conference, ser. DAC 15. New York, NY, USA: Association for Computing Machinery, 2015. [Online]. Available: [24] R. R. Shrivastwa, V. Pudi, C. Duo, R. So, A. Chattopadhyay, and G. Cun- tai, A Brain Computer Interface Framework Based on Compressive Sensing and Deep Learning, IEEE Consumer Electronics Magazine, vol. 9, no. 3, pp. 90 96, 2020. [25] M. Shoaran, M. H. Kamal, C. Pollo, P. Vandergheynst, and A. Schmid, Compact Low-Power Cortical Recording Architecture for Compressive Multichannel Data Acquisition, IEEE Transactions on Biomedical Circuits and Systems, vol. 8, no. 6, pp. 857 870, 2014.",
    "source": "2504.06996v1_Neural_Signal_Compression_using_RAMAN_tinyML_Accel.pdf",
    "length": 1586,
    "tokens": 459
  },
  {
    "text": "Input: Design space ds, design space size S, pragma possible values Vp, graph code code, optimal solution stagnation threshold Nost, LLM temperature decay rate vd, LLM temperature t, population size N, initial temperature Ti, stop iteration temperature Ts, cooling rate rcs; Output: Pareto optimal solution set Pest; 1: nt Ti, nost 0; 2: P, Fitness establish prompt to instruct LLM to initialize N solutions and evaluate each solution in P; 3: while nt Ts do 4: Ptemp, fitness, nost establish LLM prompt to instruct LLM to generate neighbor solutions Pnew, evaluate each solution in Pnew and select the top solutions into Ptemp; 5: Cost Fitnessavg fitnessavg; 6: if Cost then 7: P Ptemp, Fitness fitness with Prob 1 exp Cost nt ; 8: else 9: P Ptemp, Fitness fitness; 10: end if 11: if nost Nost, t 0 then 12: t t vd ; 13: end if 14: nt nt 1 rcs ; 15: end while 16: Pest the best solutions in P; 17: return Pest LLMs to generate an initial population, which is evaluated by a pre-trained model to obtain Fitness (line 4). The process then enters an iterative phase: prompts are dynamically rebuilt using current P and Fitness to guide LLMs in generating new solutions Pnew. The exploration engine employs the pre-trained predictive models to assess the Fitness of each new solution in the current population and selects top configurations, updating both the current population P and returning the fitness of population Fitness (line 8). The fitness is primarily composed of two key metrics: estimated latency derived from critical path analysis and resource utilization metrics including LUT, DSP, FF and BRAM usage. The temperature parameter in LLMs governs the stochasticity of text generation during sampling: elevated temperature values induce higher randomness in output generation, while reduced temperature settings promote deterministic responses. This mechanism directly mirrors the exploration-exploitation trade-off inherent in DSE optimization processes. To better balance the exploration-exploitation trade-off, we implement an adaptive temperature mechanism specifically through dynamic updating of LLMs temperature parameters (lines 9-11) [34]. This process 9 Running Title for Header Algorithm 3 LLMACO algorithm.",
    "source": "2504.19649v2_Intelligent4DSE_Optimizing_High-Level_Synthesis_De.pdf",
    "length": 2228,
    "tokens": 484
  },
  {
    "text": "Figure 20b provides a per-layer latency breakdown for decode execution with a batch size of 96. It reveals that although individual microbatch execution latency for stages like Gating, Dispatch, and MoE is marginally increased due to decreased per-stream compute resources (e.g., 44 1K 2K 4K 8K Prompt Length 0 2000 4000 6000 8000 Prefill Throughput (tokens s) With Microbatch Without Microbatch (a) Prefill throughput. 0 5 10 15 20 25 30 35 40 45 50 55 Scaled Mean Time (ms) per Layer Combine MoE Dispatch Gating Attn-2 (Post FA) Attn-1 (FA) Attn-0 (Pre FA) Overall With Microbatch (Microbatch 0) With Microbatch (Microbatch 1) Without Microbatch With Microbatch Without Microbatch (b) Prefill latency breakdown (4K prompt length). Fig. 21. Prefill throughput and per-layer latency breakdown with and without the microbatch-based pipeline. All experiments are executed with a batch containing 16K total tokens per NPU. In (b), the Overall with Microbatch indicates the per-layer latency after overlapping Microbatch 0 and Microbatch 1. AICs from 24 to 16), the microbatch-based pipeline significantly benefits overall performance. This is achieved by effectively overlapping the attention path (Stream 0) and MoE path (Stream 1) for different microbatches, leading to an approximate 10 reduction in overall per-layer latency and a corresponding considerable enhancement in end-to-end decode throughput. Prefill Pipeline. Next, we examine the impact of our proposed microbatch-based prefill pipeline, detailed in Section 4.3.2. Figure 21a shows the prefill throughput under various prompt lengths, com- paring performance with and without this pipeline. We observe that enabling the microbatch-based pipeline significantly improves prefill throughput by 23 to 31 across the tested configurations. Moreover, prefill throughput decreases as prompt lengths increase. This trend occurs because the per-token execution latency of attention computation increases with prompt length. Figure 21b provides a corresponding per-layer latency breakdown for request execution with a 4K prompt length. The data reveals that the overall execution latency per layer is reduced by approximately 24 when the microbatch pipeline is active.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2220,
    "tokens": 471
  },
  {
    "text": "We set the SLO at 1000ms. We compare these metrics for Kraken against the container provisioning policies of Archipelago [44], Fifer [32] and Xanadu [27], which we will, henceforth, refer to as Arch, Fifer and Xanadu, respectively. Additionally, we compare Kraken against policies with (a) statically assigned function probabilities (SProb) and (b) func- tion probabilities that dynamically adapt to changing invoca- tion patterns (DProb). These policies use all the components of Kraken except Commonality and Connectivity. 5.3 Large Scale Simulation To evaluate the effectiveness of Kraken in large-scale sys- tems, we built a high fidelity, multi-threaded simulator in Python using container cold start latencies and function execution times profiled from our real-system counterpart. It simulates the working of DDAs running on a serverless framework that are subjected to both real-world (Twitter and Wiki) and synthetic (Poisson-based) traces. We have validated its correctness by correlating various metrics of interest generated from experiments run on the real system with scaled-down versions of the same traces (average ar- rival rate of 100rps). Therefore, the simulator allows us to evaluate our model for a larger setup, where we mimic an 11k core cluster which can handle up to 7000 requests (70 more than the real system). Additionally, it helps compare the resource footprint of Kraken against a clairvoyant policy (Oracle) that has 100 load prediction accuracy. 6 Analysis of Results This section presents experimental results for single ap- plications run in isolation for all schemes on the real system and simulation platform. We have also verified that Kraken (as well as the other schemes) yield similar results (within 2 ) when multiple applications are run concurrently. 6.1 Real System Results 6.1.1 Containers Spawned:Figure 8 depicts the function- wise breakdown of the number of containers provisioned across all policies for individual applications. This repre- sents ùëÅùê∂ùëë ùë°(Section 3) for all possible depths, ùëë. It can be ob- served that, existing policies, namely, Arch, Fifer and Xanadu spawn, respectively, 2.41x, 76 and 30 more containers than Kraken, on average, across all applications.",
    "source": "kraken.pdf",
    "length": 2223,
    "tokens": 488
  },
  {
    "text": "D. Adaptive Compute for Mixed Precision To improve computing efficiency and save on-chip memory usage, NSFlow supports mixed precisions ranging from FP16 8 to INT8 4 in different components of the workload specified by user at frontend. DAG employs compute units adaptive to various precisions. In NVSA for example, NN and Symbolic operations are quantized to INT8 and INT4 respectively, thus the multipliers in AdArray and the SIMD support both precisions with sufficient leverage of DSP units [30]. Low-precision additions are handled by LUT for fast outcome. E. Efficient Custom SIMD Unit NSFlow incorporates a custom SIMD unit to efficiently perform vector reductions, element-wise operations, etc., with fluid data transfer between the output of the NSFlow array and the input SRAM for successive executions. It comprises multiple processing elements (PEs), each equipped with compact logic circuits (i.e., sum, mult div, exp log tanh, norm, softmax, etc.) to handle vector operations or optimized sparse computations on mixed level of quantized data. V. NSFLOW FRONTEND: DATAFLOW ARCHITECTURE GENERATION In the frontend, we implement a Dataflow Architecture Generator (DAG) that first builds a dataflow graph based on operation trace ex- tracted from the workload, then generates an optimal (or sub-optimal) dataflow architecture design, defined by a design configuration file for instantiating hardware modules, and a host code for the CPU to schedule accelerator kernels. This section first identifies the NSAI dataflow challenges (Sec. V-A). Then we illustrates the process to generate operation graph and subsequently dataflow graph (Sec. V-B). Finally, we discuss how DAG searches for an optimal architectural design and mapping based on the dataflow graph (Sec. V-C). A. NSAI Dataflow Challenges We identify three main NSAI dataflow challenges (Fig. 4(b)). First, the sequential execution and frequent interactions of neural and symbolic components results in increased latency and low system throughput. Second, the heterogeneous neural and symbolic kernels lead to low compute array utilization and efficiency of ML accelerators. Third, heavy memory transactions exhibited in both components can cause large communication latency, which is even more challenging in FPGA deployment.",
    "source": "2504.19323v2_NSFlow_An_End-to-End_FPGA_Framework_with_Scalable_.pdf",
    "length": 2295,
    "tokens": 480
  },
  {
    "text": "For each skewness, the best preditor has the minimum total latency across strategies and accuracies. 4 Results In this section, we discuss how to choose the best predictor for overall system performance under different system configurations, workload sizes and token distribution skewnesses. Our goal is to identify when to use Distribution-Only Prediction, which reduces FFN computation load imbalance without communication savings, and when to use Token-to-Expert Prediction, which additionally reduces communication latency at the cost of higher predictor complexity. Additionally, for Token-to- Expert Prediction, we want to find the optimal prediction accuracy to minimize overall runtime. Figure 6 shows the time-to-first-token latency for a single layer of Mixtral 8 7B prefill workload, simulated by LLMCompass. To match the experimental setup described in Section 3.2, we set the batch size to 1 and the sequence length to 512. The simulation includes all architectural components of Mixtral, such as a 4K sliding window, Grouped Query Attention, and SwiGLU activation. For system configurations, we modeled four A100s, either connected via NVLink 3.0 [22] (high-end interconnect with 2 TB s bandwidth) or PCIe 4.0 [24] (low-end interconnect with 32GB s bandwidth). Figures 6a and 6c show baseline latencies without any prediction strategy with NVLink and PCIe interconnect respectively. While FFN and Attention latencies remain constant across configurations, PCIe s limited bandwidth makes communication latency a dominant bottleneck. 8 Figure 6b and 6d show the impact of applying prediction strategies under varying skewness and accuracy. For Token-to-Expert Prediction, we use the fitted curve in Figure 4 to model the overhead for each prediction accuracy. For skewness numbers without a matching dataset, we interpolated the overhead from the skewnesses that we measured in Section 3.2. Within each skewness group, the leftmost bar shows latency without prediction, same as those in Figure 6a and 6c. The second bar shows latency under Distribution-Only Prediction. This strategy reduces FFN compute time, though communication time remains unchanged. Since distribution is estimated offline, there is no prediction overhead. As skewness increases, distribution estimation becomes less accurate, slightly diminishing its benefits. The remaining stacked bars (shown as curves) show latencies for Token-to-Expert Prediction across multiple accuracy levels.",
    "source": "2506.07366v1_MoE-GPS_Guidlines_for_Prediction_Strategy_for_Dyna.pdf",
    "length": 2469,
    "tokens": 499
  },
  {
    "text": "Weights are statistically analyzed and partitioned into smaller blocks. For each block, a distinct, tolerable clipping range is established. This range can be determined by optimizing a scaling factor ùõºfor clipping (e.g., ùëäclip_max ùõº max(ùëäblock) and ùëäclip_min ùõº min(ùëäblock)) that minimizes the quantization error for that specific block, for instance, by solving: arg min ùõº Block(ùëã;ùëä) Block(ùëã;ùëÑ(ùëä;ùõº)) (4) Here, ùëÑ(ùëä;ùõº) represents quantizing the weights ùëäwithin the block using the clipping factor ùõº. Concurrently, lightweight error compensation terms are strategically inserted into the inference computation graph. These terms aim to counteract or partially correct the systematic errors introduced by quantization at different points in the model, thereby mitigating the cumulative impact of quantization noise on the final model output. A significant advantage of this method is that it requires no modifications to the original model training process and does not depend on additional fine-tuning stages, facilitating rapid deployment and iteration. In concert, these five strategies form a robust and hierarchical INT8 quantization framework that enables high-performance inference for massive models like DeepSeek-V3 R1 on Ascend hardware, carefully balancing computational efficiency with the preservation of model accuracy. 5 Evaluations In this section, we present a comprehensive performance evaluation for our proposed serving system CloudMatrix-Infer, previously detailed in 4, when deployed on the CloudMatrix384. We begin by outlining the common experimental setup used for our evaluation ( 5.1). Subsequently, we analyze several key aspects of performance and efficacy: this includes the overall system performance ( 5.2); the inference accuracy achieved with our INT8 quantization scheme ( 5.3); an ablation study that Serving Large Language Models on Huawei CloudMatrix384 39 investigates the specific contributions of different optimization techniques employed ( 5.4); and finally, a look at the performance of critical underlying operators ( 5.5). 5.1 Experimental Setup Our evaluation is conducted on a Huawei CloudMatrix384 supernode, provided by the ModelArts Lite (Cluster Mode) service in Huawei Cloud.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2225,
    "tokens": 489
  },
  {
    "text": "Our method intentionally isolates architectural efficiency, but further ex- perimentation could explore the impact of varying CPU fre- quencies on end-to-end latency and power consumption. Other hardware parameters are largely standardized by default. Further work could investigate their impact on per- formance; the number of active PEs, memory layout con- figuration, and other hardware-specific optimizations are variably configurable across the ùúáNPU platforms and can influence overall efficiency. 3.2 Models Table 2 details the various CNN-based models used in our benchmark, covering image classification, object recognition, and signal reconstruction applications. We provide more detail on each model below. CIFAR10-NAS: the optimal CNN model generated by neural architecture search (NAS) for the CIFAR-10 dataset, combin- ing diverse convolutional units, pooling layers, and unique connectivity patterns. The model was generated using the Once-for-All (OFA) NAS framework, a weight-sharing-based framework that decouples search and training by construct- ing a supernet model from which various hardware-specific subnet models can be derived [39]. This is our largest model, with 74.3 Million MACs (MMACs) and 36.4 Million FLOPs (MFLOPs). Trained on the CIFAR-10 dataset, with 3x32x32 input size and 10-class output. ResidualNet: a CNN framework built around residual func- tions, helping to mitigate gradient vanishing. ResidualNet has 37.7 MMACs and 18.5 MFLOPs. Trained on the CIFAR- 100 dataset, with 3x32x32 input size and 100-class output. SimpleNet: a simpler CNN framework composed of a basic stack of convolutional and pooling layers, without complex branches or residual functions. Despite its basic architecture, SimpleNet often outperforms more complex models includ- ing certain versions of ResidualNet [40]. SimpleNet has 38.0 MMACs and 18.5 MFLOPs. Trained on the CIFAR-100 dataset, with 3x32x32 input size and 100-class output. Autoencoder: a symmetric encoder-decoder model. This model is our simplest, with just 0.5 MMACs and 0.2 MFLOPs.",
    "source": "2503.22567v2_Benchmarking_Ultra-Low-Power_ŒºNPUs.pdf",
    "length": 2065,
    "tokens": 498
  },
  {
    "text": "However, in SAM s de- coder, the LayerNorm placement differs significantly from ViT s image encoder, making efficient reparameterization infeasible. This fundamental difference motivated the in- troduction of CAG as a dedicated PTQ solution for SAM with high hardware efficiency. We perform PTQ on DeiT-T, DeiT-S, and DeiT-B, comparing AHCPTQ against static PTQ methods (FQ- ViT [23], PTQ4ViT [41], APQ-ViT [4], RepQ-ViT [21]) and optimization-based PTQ methods (BRECQ [17], QDrop [39], PD-Quant [24], I S-ViT [47]). As shown in Table 4, AHCPTQ achieves the highest classification accu- racy in W4A4 quantization, surpassing all competing meth- ods. E. Parameter Sensitivity Analysis To initialize ÀÜb, s1, and s2 for subsequent optimization in HLUQ, we perform a grid search over two parameters, Œ± and Œ≤. We extend the search to Œ± {0.1, 0.2, . . . , 0.9} and Œ≤ { 1 8, 1 4, . . . , 7 8} on SAM-B with YOLOX. The resulting W4A4 W5A5 W6A6 mAP scores of 13.1 32.0 35.3 align with the reported values as shown in Table 5, indicating that quantization performance is empirically insensitive to these parameters and a limited subset suffices. Table 5. Parameter sensitivity analysis on SAM-B with YOLOX. Method W4A4 W5A6 W6A6 Default 13.4 31.8 35.4 Extended Search 13.1 32.0 35.3 F. Generalizing CAG to Weight Grouping To ensure a fair comparison with existing baselines, we ap- ply per-channel quantization to model weights. However, CAG can also be extended to weight quantization, poten- tially broadening its applicability. To evaluate this, we ap- ply CAG with 32 groups to all model weights on SAM- B with YOLOX.",
    "source": "2503.03088v2_AHCPTQ_Accurate_and_Hardware-Compatible_Post-Train.pdf",
    "length": 1611,
    "tokens": 475
  },
  {
    "text": "Salient Store: Enabling Smart Storage for Continuous Learning Edge Servers Cyan Subhra Mishra, Deeksha Chaudhary, Jack Sampson, Mahmut Taylan Knademir, Chita Das The Pennsylvania State University {cyan, dmc6955, jms1257, mtk2, Abstract As continuous learning based video analytics continue to evolve, the role of effi- cient edge servers in efficiently managing vast and dynamic datasets is becoming increasingly crucial. Unlike their compute architecture, storage and archival system for these edge servers has often been under-emphasized. This is unfortunate as they contribute significantly to the data management and data movement, especially in a emerging complute landscape where date storage and data protection has be- come one of the key concerns. To mitigate this, we propose Salient Store that specifically focuses on the integration of Computational Storage Devices (CSDs) into edge servers to enhance data processing and management, particularly in continuous learning scenarios, prevalent in fields such as autonomous driving and urban mobility. Our research, gos beyond the compute domain, and identifies the gaps in current storage system designs. We proposes a framework that aligns more closely with the growing data demands. We present a detailed analysis of data movement challenges within the archival workflows and demonstrate how the strategic integration of CSDs can significantly optimize data compression, encryption, as well as other data management tasks, to improve overall system performance. By leveraging the parallel processing capabilities of FPGAs and the high internal bandwidth of SSDs, Salient Store reduces the communication latency and data volume by 6.2 and 6.1 , respectively. This paper pro- vides a comprehensive overview of the potential of CSDs to revolutionize storage, making them not just data repositories but active participants in the computational process.",
    "source": "SaLT.pdf",
    "length": 1909,
    "tokens": 365
  },
  {
    "text": "Note that, within the LBC algorithm, certain components, specifically polynomial multiplications, exhibit similarities to operations performed in CNNs. These similarities open up the possibility of reusing hardware designed for CNN operations to accelerate LBC computations. The most common algorithms used fod the said operations are High-Speed Schoolbook Polynomial Multiplication (HSPM) and Pipelined Systolic Dimension Modular Multiplier (SDMM), and we propose to accelrate the same using the FPGAs in the CSDs. The data locality due to the SSD and the high throughput due to the FPGA could facilitate swift polynomial processing and refined modular multiplications using DSP slices, thereby accelerating the encryption process. Designing HSPM Accelerator on CSD FPGA: The HSPM hardware is characterized by its fully parallelized design, incorporating 128 Multiply-Accumulate (MAC) units for handling polynomials of degree n 256. Each MAC unit within the HSPM architecture is capable of conducting two parallel modular multiplications. This is achieved through the use of a single Digital Signal Processing (DSP) block that operates on signed data representation. Consequently, these units are referred to as Signed Double Modular Multiplication (SDMM) units. The HSPM accelerator s architecture, as illustrated in Fig. 3a, comprises three pipelined stages. These stages include the data loading phase, the modular polynomial multiplication via the SDMM unit, and the final accumulation registers. Data is input into the HSPM in a serial-to-parallel conversion process, while the outputs, after undergoing addition operations, are retrieved serially through an address signal. Central to the Ring-Learning with Errors (R-LWE) based Public Key Encryption (PKE) is the equation d a b c. In this context, the operand a can represent polynomials such as p, a, or c1 from Algorithm 1, while b corresponds to e1, r2, and c to e2, e3, c2. During the data loading phase, the 256 coefficients of polynomial b are input serially into a 6-bit shift register. Simultaneously, the first coefficient a0 of polynomial a is fed in parallel to all 128 SDMM units.",
    "source": "SaLT.pdf",
    "length": 2150,
    "tokens": 486
  },
  {
    "text": "MCP is rapidly gaining acceptance, and there are already thousands of MCP servers. Frequently accessed MCP server instances may be hosted directly on ZettaLith for very high bandwidth and low latency access. 19.3 PCIe 6.0 links The 16 CPU chips in ZettaLith provide 16 PCIe 6.0 links from the CPUs to SSD storage, external servers, and the Internet. Each PCIe 6.0 link has 16 lanes of 8 GB s for a total bandwidth of 2 TB s (16 Tb s). During typical transformer inference, this bandwidth is unused. High bandwidth is required to load parameters when rapidly switching to transformers which are not loaded into HBM, and to load large user contexts which are not stored on ZettaLith. Since ZettaLith has enough HBM for 20 trillion parameters (5 trillion in low cost system), it can hold multiple different trillion parameter LLMs in memory simultaneously, thereby not normally requiring any PCIe 6.0 bandwidth to switch between transformers. 20 Fault Tolerance The practical implementation of ZettaLith capabilities requires careful consideration of module yield management through redundant routing, power distribution uniformity across the wafer, thermal gradient minimization, signal integrity maintenance across UCIe interfaces, known good die-attach to the WSSCB, and system-level fault tolerance and graceful degradation. These considerations are addressed through the redundant routing structures, distributed power delivery architecture, silicon spring thermal isolation, JETSTREAM cooling, and fault tolerance mechanisms. ZettaLith incorporates multi-level fault tolerance. At the passive WSSCB level, all wires are individually fault tolerant, requiring highly coincidental defects on separate wiring layers to cause a single fault. The UCIE 2.0 and HBM4 interfaces have wire-level fault tolerance. Defective stacks, or stacks without power as their PSU is defective, are automatically mapped out of the compute array during power-up self-test. Within an SLD, spare CASCADE array columns are included to dynamically replace any faulty columns via the CREST mechanism. At the system level, if an entire TRIMERA stack is found to be defective, the design employs a fail-in-place strategy that maps out the faulty module without disrupting overall operation. There are two levels in which a TRIMERA stack can be mapped out of the array at bootup, the TRIMERA stack will automatically be mapped out if the BID doesn t boot.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2427,
    "tokens": 496
  },
  {
    "text": "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436, 2019. Bu Jin, Xinyu Liu, Yupeng Zheng, Pengfei Li, Hao Zhao, Tong Zhang, Yuhang Zheng, Guyue Zhou, and Jingjing Liu. Adapt: Action-aware driving caption transformer. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 7554 7561. IEEE, 2023. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R emi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092 1097, 2022. 11 Published as a conference paper at ICLR 2025 Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74 81, 2004. Mingjie Liu, Teodor-Dumitru Ene, Robert Kirby, Chris Cheng, Nathaniel Pinckney, Rongjian Liang, Jonah Alben, Himyanshu Anand, Sanmitra Banerjee, Ismet Bayraktaroglu, et al. Chipnemo: Domain-adapted llms for chip design. arXiv preprint arXiv:2311.00176, 2023a. Mingjie Liu, Nathaniel Pinckney, Brucek Khailany, and Haoxing Ren. Verilogeval: Evaluating large language models for verilog code generation. In 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD), pp. 1 8. IEEE, 2023b. Shang Liu, Wenji Fang, Yao Lu, Jing Wang, Qijun Zhang, Hongce Zhang, and Zhiyao Xie. Rtlcoder: Fully open-source and efficient llm-assisted rtl code generation technique. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2024.",
    "source": "2502.15832v1_DeepRTL_Bridging_Verilog_Understanding_and_Generat.pdf",
    "length": 1624,
    "tokens": 489
  },
  {
    "text": "{1, 4, 8} 4 DecodeWidth The number of instructions the decode unit can process simultaneously. {1, 2, 3, 4, 5, 6} 1 RobEntry The number of reorder buffer entries. {32, 64, 96, 128, 120} 32 FetchBufferEntry The number of entries in the instruction fetch buffer. {8, 16, 24, 32, 35, 40} 16 Some example constraints for the BOOM processor are listed in Table III. TABLE III EXAMPLE CONSTRAINTS OF BOOM DESIGN SPECIFICATION Classification Descriptions Inequalities FetchWidth DecodeWidth FetchBufferEntry FetchWidth Conditional if icache nWays [64, 128], then nSets [2, 4] { if dcache nWays [16, 32], then nSets [2, 4]; or if dcache nWays [128, 256], then nSets [4, 8] } Divisibility RobEntry DecodeWidth FetchBufferEntry DecodeWidth To formulate these three constraint-checking functions, we define that a negative output from either function signifies unmet constraints, while a non-negative output indicates that the constraints are satisfied. For the inequality constraint- checking function, denoted as PCinequality(x), all inequality constraints are generalized into the following format: PCinequality(x) kaxa kbxb t (1) where ka, kb, and t are real-valued parameters that are automatically assigned based on inequality constraints derived from the processor s specifications. In addition to inequality constraints, ASPO also supports non-linear conditional constraints. Their relationships can be categorized as either conjunctive or disjunctive. A conjunctive structure consists of multiple conditional constraints and is satisfied only when all constraints are met. In contrast, a disjunctive structure is satisfied if at least one of its conditional constraints is true. With minimal human effort, all the conditional constraints in the processor s specification can be systematically organized into a hierarchical logical structure comprising conjunctive and disjunctive components. At the top level, a conjunctive constraint combines all the underlying conditional constraint structures.",
    "source": "2506.06817v1_ASPO_Constraint-Aware_Bayesian_Optimization_for_FP.pdf",
    "length": 1995,
    "tokens": 445
  },
  {
    "text": "For simplicity, we only include the index coordinates when describing a write. Assume that there exist at least two writes w1(ps1, pt1, i1), w2(ps2, pt2, i2) with pt1 pt2 and valid destination coordinates i1, i2, where i1 i2 lexicographically and both are unpacked below. i1 (p1, r1, b1, e1, c1), i1 (p2, r2, b2, e2, c2) Note that for the message staging state, even though i1 i2 the resultant memory segments reside in different physical buffers resident in ps1 and ps2 respectively. Therefore, for this state, there are no conflicts as intra-process writes always have distinct cj coordinates, where j {0, C 1}. For inter-process transfers, we have two cases. Case 1: ps1 ps2 Here, w1 and w2 are identical operations. This contradicts the definition of a conflict, which requires that ps1 ps2. In practice, such repeat writes never even occur. Case 2: ps1 ps2 To ensure validity for i1 and i2, it is the case that p1 ps1 and p2 ps2. However, this implies that i1 i2 yielding a contradiction as desired.",
    "source": "2506.04667v2_FlashDMoE_Fast_Distributed_MoE_in_a_Single_Kernel.pdf",
    "length": 1004,
    "tokens": 277
  },
  {
    "text": "The sense amplifiers voltage threshold can be arbitrarily set depending on the intrinsic RRAM-CMOS cell dynamics that define the charging rate of the matchline. In Fig. 4(b), a schematic representation of the 3T1R TXL- ACAM pixel is shown [27]. The 3T1R ACAM cell consists of a single 1-transistor-1-resistor (1T1R) memory cell (serial connection of RM1 RRAM and MN1 nMOS devices) and two additional transistors (MP 1 pMOS and MN2 nMOS) for matchlines evaluation. The input voltage controls the resistance of the MN1 transistor, and thus the voltage divider formed by the RRAM devices and the MN1. The additional nMOS and pMOS devices (MN1 and MP 1, respectively) are connected to a dual matchline configuration (MLLOW and MLHIGH). The two matchlines MLHIGH and MLLOW represent the high and low bounds of the matching window, respectively. Due to the complementary behaviour between nMOS and pMOS, the nMOS transistor is responsible for discharging MLLOW when the input voltage is below the IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL., NO., FEBRUARY 2025 8 RM1 RM2 MP1 MN1 MP2 MN2 MP3 MP4 R1 R2 PC BLB BL ML MN5 IN BLB BL VD IN PG MLLOW MLHIGH RM1 MN1 MN2 MP1 (a) (b) Fig. 4. RRAM-CMOS -based Template piXeL (TXL) ACAM cell schematics. There are many version of the TXL-ACAM technologies with each version comprised of a specific set of trade-offs. In (a) a 6T4R charging design is showcased aimed at ML applications with increased sparsity [19] In (b), a 3T1R precharging design is shown that is aimed at applications that has strict area specifications as well as differentiability as trait for the final stage classification [27]. lower bound while the pMOS transistor is responsible for discharging MLHIGH when the input voltage exceeds the higher bound. Contrary to Fig. 4(a), the 3T1R design follows a precharging and evaluating operation cycle instead of a discharging initialisation and evaluation.",
    "source": "2502.10089v1_A_Hybrid_Edge_Classifier_Combining_TinyML-Optimise.pdf",
    "length": 1920,
    "tokens": 497
  },
  {
    "text": "For relevance in comparison to prior work [27, 83] we chose image inference as our ensemble workload. While ensembling is applicable in other classiÔ¨Åcation workloads like product recommendations [24,53], text classiÔ¨Åcation [71] etc, the observations drawn are generic and applicable to other applications. 2.3.1 Ensembling Compared to Single Models To analyze the accuracy offered by ensemble models, we con- duct an experiment using 10000 images from ImageNet [29] test dataset, on a C5.xlarge [8] instances in AWS EC2 [5]. For a given baseline model, we combine all models whose latency is lower than that of the baseline, and call it full- ensemble. We perform ensembling on the predictions using a simple majority voting policy. The latency numbers for the baseline models and the corresponding ensemble models along with the size of the ensemble are shown in Table 3. In majority voting, every model votes for a prediction for each input, and the Ô¨Ånal output prediction is the one that receives more than half of the votes. Figure 3a, shows the accuracy comparison of the baseline (single) and static ensemble (ex- plained in Section 3) compared to the full-ensemble. It is evident that full-ensemble can achieve up to 1.65 better accuracy than single models. Besides accuracy again, ensembling can also achieve lower latency. The latency of the ensemble is calculated as the time between start and end of the longest running model.As shown in Table 3, in the case of NASLarge, the ensemble latency is 2 lower (151ms) than the baseline latency (311ms). Even a 10ms reduction in latency is of signiÔ¨Åcant importance to the providers [35]. We observe a similar trend of higher en- semble accuracy for other four baseline models with a latency reduction of up to 1.3 . Thus, depending on the model sub- set used in the ensemble, it achieves better accuracy than the baseline at lower latencies. Note that in our example model-set, the beneÔ¨Åts of ensembling will diminish for lower NASLarge IRV2 Xception DNet121 NASMob 0.5 1.0 1.5 Accuracy Loss( ) Static Single (a) Accuracy loss compared to full- ensemble.",
    "source": "cocktail.pdf",
    "length": 2108,
    "tokens": 484
  },
  {
    "text": "Tiansheng Zhou: Data curation, Visualization. Bingtao Ma: Writing review and editing. Yaqi Wang: Conceptualiza- tion, Writing review and editing. Jianxiang Zhao: Writing review and editing. Zhi Li: Visualization. Yugui Lin: Validation. Pietro Li o: Writ- ing review and editing. Shuai Wang: Resources, Supervision, Writing review and editing. Data Availability Data will be made available on request. References [1] H. Chen, M. Liu, X. Tang, K. Zhu, N. Sun, D. Z. Pan, Challenges and Opportunities toward Fully Automated Analog Layout Design, Journal of Semiconductors 41 (11) (2020) 111407. doi:10.1088 1674-4926 41 11 111407. URL [2] A. Samad, M. Qadir, I. Nawaz, M. A. Islam, M. Aleem, A comprehensive survey of link prediction techniques for social network., EAI Endorsed Trans. Ind. Networks Intell. Syst. 7 (23) (2020) e3. [3] D. Liben-Nowell, J. Kleinberg, The link prediction problem for social networks, in: Proceedings of the twelfth international conference on Information and knowledge management, 2003, pp. 556 559. [4] P. R. Genssler, L. Alrahis, O. Sinanoglu, H. Amrouch, HDCircuit: Brain-Inspired HyperDimensional Computing for Circuit Recognition, in: 2024 Design, Automation amp; Test in Europe Conference amp; 17 Exhibition (DATE), IEEE, Valencia, Spain, 2024, pp. 1 2. doi:10. 23919 DATE58400.2024.10546587. URL [5] L. Ge, K. K. Parhi, Classification using hyperdimensional computing: A review, IEEE Circuits and Systems Magazine 20 (2) (2020) 30 47. doi:10.1109 mcas.2020.2988388.",
    "source": "2504.10240v2_GNN-ACLP_Graph_Neural_Networks_based_Analog_Circui.pdf",
    "length": 1501,
    "tokens": 441
  },
  {
    "text": "[17] K. Ma, Y. Zheng, S. Li, K. Swaminathan, X. Li, Y. Liu, J. Sampson, Y. Xie, and V. Narayanan, Architecture exploration for ambient energy harvesting nonvolatile processors, in 2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA), pp. 526 537, 2015. [18] K. Ma, X. Li, M. T. Kandemir, J. Sampson, V. Narayanan, J. Li, T. Wu, Z. Wang, Y. Liu, and Y. Xie, NEOFog: Nonvolatility-exploiting optimizations for fog computing, in Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems ASPLOS, pp. 782 796, 2018. [19] M. Zhao, K. Qiu, Y. Xie, J. Hu, and C. J. Xue, Redesigning software and systems for non-volatile processors on self-powered devices, in 2016 IFIP IEEE International Conference on Very Large Scale Integration (VLSI-SoC), pp. 1 6, Sep. 2016. [20] L. Ni, Z. Liu, H. Yu, and R. V. Joshi, An energy-efÔ¨Åcient digital ReRAM-crossbar-based cnn with bitwise parallelism, IEEE Journal on Exploratory Solid-State Computational Devices and Circuits, vol. 3, pp. 37 46, Dec 2017. [21] M. Zhao, C. Fu, Z. Li, Q. Li, M. Xie, Y. Liu, J. Hu, Z. Jia, and C. J. Xue, Stack-size sensitive on-chip memory backup for self-powered nonvolatile processors, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), vol. 36, pp. 1804 1816, Nov 2017. [22] A. Colin, E. Ruppel, and B. Lucia, A reconÔ¨Ågurable energy storage architecture for energy-harvesting devices, in Proceedings of the Twenty- Third International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS 2018, Williamsburg, VA, USA, March 24-28, 2018, pp.",
    "source": "ResiRCA.pdf",
    "length": 1683,
    "tokens": 499
  },
  {
    "text": "1, 2024, pp. 437 445. [10] M. Akyash and H. Mardani Kamali, Simeval: Investigating the similarity obstacle in llm-based hardware code generation, in Proceedings of the 30th Asia and South Pacific Design Automation Conference, ser. ASPDAC 25. New York, NY, USA: Association for Computing Machinery, 2025, p. 1002 1007. [Online]. Available: [11] M. Liu, N. Pinckney, B. Khailany, and H. Ren, Verilogeval: Evaluating large language models for verilog code generation, in 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD). IEEE, 2023, pp. 1 8. [12] M. Chen et al., Evaluating large language models trained on code, 2021. [Online]. Available: [13] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, Codegen: An open large language model for code with multi-turn program synthesis, arXiv preprint arXiv:2203.13474, 2022. [14] Y. Li et al., Competition-level code generation with alphacode, Science, vol. 378, no. 6624, p. 1092 1097, Dec. 2022. [Online]. Available: [15] A. Vijayakumar, M. Cogswell, R. Selvaraju, Q. Sun, S. Lee, D. Crandall, and D. Batra, Diverse beam search for improved description of complex scenes, Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, Apr. 2018. [Online]. Available: [16] A. Fan, M. Lewis, and Y. Dauphin, Hierarchical neural story generation, in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), I. Gurevych and Y. Miyao, Eds. Melbourne, Australia: Association for Computational Linguistics, Jul. 2018, pp. 889 898. [Online].",
    "source": "2507.02226v1_DecoRTL_A_Run-time_Decoding_Framework_for_RTL_Code.pdf",
    "length": 1603,
    "tokens": 487
  },
  {
    "text": "Inference accuracy of VGG8 and ResNet18 under different stuck-at- fault rates. 11 0 20 40 60 80 100 0 20 40 60 80 100 Mean Actual MAC Output Ideal MAC Output CIM A CIM B 89.7 75.6 80.0 80.1 CIM C CIM D 0 20 40 60 80 100 0 20 40 60 80 100 Mean Actual MAC Output Ideal MAC Output (a) (b) (c) (d) VGG8 CIFAR-10 ResNet18 CIFAR-100 ResNet50 ImageNet Swin_t ImageNet 0 20 40 60 80 100 Device Computing Mode Activated Rows ADC Precision Average Noise Std Max. Noise Std CIM A[43] 2b FeFET Current 32 7b 0.75 1.1 CIM B[43] 2b FeFET Charge 32 7b 0.24 0.57 CIM C[41] 1b RRAM Current 8 3b 0.28 0.54 CIM D[18, 33] 1b nvCap Charge 32 5b 0.08 0.1 Accuracy ( ) Software Accuracy (e) 2b FeFET Current-based CIM 0 2 4 6 8 0 2 4 6 8 Mean Actual MAC Output Ideal MAC Output 2b FeFET Charge-based CIM 2b FeFET Current-based CIM 1b RRAM Current-based CIM 0 8 16 24 32 0 8 16 24 32 Mean Actual MAC Output Ideal MAC Output 2b FeFET Charge-based CIM 2b FeFET Current-based CIM 1b nvCap Charge-based CIM (b) (d) Fig. 8. (a) Actual MAC output with 2b FeFET current domain CIM [43] under 50mV Vth variation; (b) Actual MAC output with 2b FeFET charge domain CIM [43] under 50mV Vth variation; (c) Actual MAC output of RRAM CIM tape-out [41]; (d) Actual MAC output with 1b nvCap charge domain CIM [18], [33] under 50mV Vth variation; (e) Inference accuracy of various neural network algorithms across CIM macro A-D. using either simulation data or silicon measurements.",
    "source": "2505.02314v1_NeuroSim_V15_Improved_Software_Backbone_for_Benchm.pdf",
    "length": 1441,
    "tokens": 480
  },
  {
    "text": "57, no. 6, p. 102342, Nov. 2020. [35] Y. Qi, J. Zhang, W. Xu, et al., \"Salient context-based semantic matching for information retrieval,\" EURASIP Journal on Advances in Signal Processing, vol. 2020, no. 1, pp. 1 17, Dec. 2020. [36] Y. Djenouri, A. Belhadi, D. Djenouri, et al., \"Cluster-based information retrieval using pattern mining,\" Applied Intelligence, vol. 51, no. 4, pp. 1888 1903, Apr. 2021. [37] S. Jain, K. R. Seeja, and R. Jindal, \"A fuzzy ontology framework in information retrieval using semantic query expansion,\" International Journal of Information Management Data Insights, vol. 1, no. 1, p. 100009, Apr. 2021. [38] S. Yao, D. Yu, J. Zhao, et al., \"Tree of thoughts: Deliberate problem solving with large language models,\" Advances in Neural Information Processing Systems, vol. 36, to be published, 2024. REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) 10 APPENDIX A: DATASET CONSTRUCTION AND STATISTICS FOR TECHNICAL MANUALS This appendix details the construction process, sampling strategy, and question design for the dataset used in this study. The dataset comprises 80 datasheets covering four electronic device types: diodes, bipolar junction transistors (BJTs), metal- oxide-semiconductor field-effect transistors (MOSFETs), and junction field-effect transistors (JFETs). Sourced from DigiKey and Mouser (40 each), these documents ensure diversity and authority. A stratified sampling method was employed to balance device type representation and market distribution authenticity. Sampling consists of two parts: uniform allocation (60 , 48 datasheets) and market allocation (40 , 32 datasheets). Uniform allocation assigns 12 datasheets per device type (6 from DigiKey, 6 from Mouser), ensuring equal representation.",
    "source": "2502.16540v2_D2S-FLOW_Automated_Parameter_Extraction_from_Datas.pdf",
    "length": 1781,
    "tokens": 460
  },
  {
    "text": "Introducing DBRX: A New State-of-the-Art Open LLM. Accessed: 2025-04-28. [20] Bin Gao, Zhuomin He, Puru Sharma, Qingxuan Kang, Djordje Jevdjic, Junbo Deng, Xingkun Yang, Zhou Yu, and Pengfei Zuo. 2024. Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention. In Proceedings of the 2024 USENIX Annual Technical Conference (USENIX ATC 24). USENIX Association, 111 126. [21] Google DeepMind. 2025. Gemini 2.5: Our Most Intelligent AI Model. gemini-model-thinking-updates-march-2025 . Accessed: 2025-04-28. [22] Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Weixun Wang, Hui Huang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, Zhuoran Lin, and Xue Peng. 2024. Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models. arXiv preprint arXiv:2411.07140 (Nov 2024). [23] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring Massive Multitask Language Understanding. arXiv preprint arXiv:2009.03300 (2020). 2009.03300 [24] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring Mathematical Problem Solving With the MATH Dataset. arXiv preprint arXiv:2103.03874 (Mar 2021). [25] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. 2023. C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 1543,
    "tokens": 489
  },
  {
    "text": "Model Key Novelty Training Dataset, [Size] Fine-tuned Model Multi-modal HW Efficiency VeriGen [15] Fine-tuning on Dataset collected from GitHub and Textbooks Open-source, GitHub and Textbooks, [not listed] CodeGen-16B No None RTLCoder [10] GPT-3.5-based Code-Instruction Pair Synthesis Open-source, Synthesized, [27K] Mistral-7B DeepSeek-Coder-6.7b No None BetterV [16] Applying Controllable Text Generation w Discriminators for Engineering Optimization Closed-source, From internet, [not listed] CodeLlama-7B DeepSeek-Coder-6.7b-Instruct Code Qwen1.5-7B No Area Improvement OriGen [8] Code-to-code Augmentation, Self-reflection for Fixing Open-source, [222K] DeepSeek-Coder-7B No Iterative Functional Correctness Check AutoVCoder [17] Domain-specific RAG with Two-round LLM fine-tuning for Constructive Prompting Collected from Github, [not listed] Codellama-7B DeepSeek-Coder-6.7B CodeQwen1.5-7B No (Text and Retrieval ) RAG-based Optimization CodeV [25] Multi-Level Summarization for Verilog Generation Close-source Github, [165K] Codellama-7B DeepSeek-Coder-6.7B CodeQwen1.5-7B No Code Generation Improvement CraftRTL [18] Correct-by-construction data Synthetic and GitHub, [80.1K] Codellama-7B DeepSeek-Coder-6.7B Starcoder2-15B No Fine-tuning Correction RTL Structural-based Optimization (Graph Embedding for Instruction Tuning) Open-source, [200K] Codellama-7B Yes (Graph and Text) Structural Optimization, Area and Delay Improvement : This RAG is to identify the piece of data (RTL code). It has nothing to do with cross-modality understanding. the hierarchical structure of designs, (ii) comprehending data and control flow, and (iii) addressing the intrinsic concurrency of hardware designs.",
    "source": "2505.13479v1_RTL_Graph-enhanced_LLM_for_RTL_Code_Generation.pdf",
    "length": 1701,
    "tokens": 509
  },
  {
    "text": "Performance bench- marks were conducted using ModelSim-2022.3, with the sys- tem running at 1 GHz. In Figure 6a, our final implementation (SW EXP HW Optim) achieve up to 162.7 speedup over the baseline, while software-only optimizations show minimal gains due to the exponential operation bottleneck. The Schraudolph method in software offers some acceleration but is far outperformed by hardware by a factor of 19.6 . Figure 6b demonstrates the negligible impact of MAX and NORM on total latency, with software achieving only a 1.1 speedup, compared to 61.6 for combined hardware and software optimizations. Finally, Figure 6c shows energy reductions of up to 74.3 . FlashAttention-2: We evaluated the FlashAttention-2 ker- nel on one Snitch cluster with a head dimension of 64 (GPT2 configuration). The results, shown in the second row of Figure 6, highlight several improvements. In Figure 6d, our implementation achieves up to 8.2 increase in throughput over the baseline. Figure 6e illustrates that Softmax dominates the latency in the baseline, while its contribution is reduced to 6 in the optimized version. Moreover, the energy efficiency of FlashAttention-2 improves up to 4.1 with the optimized Softmax as shown in Figure 6f. D. Scalability Analysis The Snitch cluster is designed to scale into a multi-cluster architecture, silicon-proven in Occamy [27]. As shown in Figure 7, a group of C compute clusters is connected by a 64-bit crossbar for fast synchronization and a 512-bit AXI crossbar for high-bandwidth inter-cluster access. Further scaling is achieved by linking G groups through a group- level AXI crossbar, enabling inter-group communication. Each group also interfaces with eight HBM channels through a wide crossbar, ensuring high-bandwidth access to main memory. We benchmark runtime and energy metrics against [5] on GPT-2 Small, GPT-3 XL, ViT-Base, and ViT-Huge models. All models are evaluated non-autoregressively on a 16-cluster version of the Occamy system [27], with sequence lengths of 2048 for GPT models and 197 for ViT models.",
    "source": "2504.11227v1_VEXP_A_Low-Cost_RISC-V_ISA_Extension_for_Accelerat.pdf",
    "length": 2064,
    "tokens": 471
  },
  {
    "text": "Design Manual Verilog synthesis Lorecast Speedup Verilog coding Logic Layout Total Prompt writing LLM code generation ML prediction Total Manual time CPU time End-to-end time Incremental change time (synthesis param update only) right shifter 1489 215 37 1741 270 5 0.0017 275 5.5X 50.4X 6.3X 148 103 X adder bcd 1120 223 43 1386 420 5 0.0013 425 2.7X 53.2X 3.3X 205 103 X signal generator 1990 220 38 2248 439 6 0.0164 445 4.5X 43.0X 5.1X 16 103 X accu 1129 223 55 1407 250 8 0.023 258 4.5X 34.8X 5.5X 12 103 X LIFObuffer 1640 223 44 1907 560 9 0.0021 569 2.9X 29.7X 3.4X 127 103 X multiply 1780 217 40 2037 287 5 0.0243 292 6.2X 51.4X 7.0X 11 103 X asyn fifo 5320 225 41 5586 1030 21 0.0259 1051 5.2X 12.7X 5.3X 10 103 X sobel filter 2820 235 51 3106 660 26 0.022 686 4.3X 11.0X 4.5X 13 103 X matmul22 2459 224 41 2724 490 9 0.0285 499 5.0X 29.4X 5.5X 9 103 X mux256to1v 739 221 49 1009 320 3 0.0014 323 2.3X 90X 3.1X 193 103 X pe 32bit 1650 231 52 1933 469 5 0.0292 474 3.5X 56.7X 4.1X 10 103 X huffmandecode 2200 270 56 2526 550 32 0.0089 582 4X 10.2X 4.3X 37 103 X izigzag 1819 265 49 2133 480 38 0.0132 518 3.8X 8.3X 4.1X 24 103 X pe 64bit 1588 252 69 1909 512 5 0.013 517 3.1X 64.2X 3.7X 25 103 X fft 16bit 3600 343 94 4037 990 52 0.0247 1060 3.6X 8.4X 3.8X 17 103 X Average 2090 239 51 2379 515 15 0.0157 530 4X 19.3X 4.6X 57 103 X The time unit is s. Power(mW) TNS(ns) (a) (b) Fig. 9. (a): post-layout TNS and power from Verilog code generated by LLMs and ground truth.",
    "source": "2503.11662v2_Lorecast_Layout-Aware_Performance_and_Power_Foreca.pdf",
    "length": 1478,
    "tokens": 598
  },
  {
    "text": "Still, Autocomp is able to improve over the original code by a notable margin even without the ISA, given that all its other features are still enabled (see Table 3). This is because we inherently provide an example of accelerator ISA code at each step via the current code, so the model is able to infer some properties of the accelerator ISA. In addition, many of the nested loop optimizations for the GEMM and convolution workloads are well-understood transformations that operate on the loops, addresses, and indexes in the code, which resemble general-purpose programming, rather than using accelerator-specific constructs such as the configuration instructions. However, full Auto- comp performance is not matched as the proportion of functionally correct responses is lower, and instruction-level optimizations cannot easily be identified. For example, the first-compute handling optimization in Appendix C.1 and the negative-scaled bias loading in Appendix C.2 would not have been identified without knowledge of the ISA. Overall, we find that the accelerator ISA is an important part of Autocomp s prompts. A.2 Optimization Menu We ablate the optimization menu by removing the menu in the planning prompt, and instead simply asking the model to select one optimization and generate a plan. From this experiment, we find that adding domain knowledge and optimization diversity via the optimization menu is essential to Autocomp. As shown in Table 3, optimization performance completely deteriorates without the optimization menu. Qualitatively, without the optimization menu, we find that the models tend to repeat similar optimizations, with significantly less diversity and relevance in the generated optimization plans. A.3 Optimization Menu Dropout Dropout for optimization menu options is a key contribution of this for increasing the diversity of generated optimization plans. Table 3 shows that menu dropout has a significant effect on performance. Qualitatively, we find that without dropout, models tend to be biased towards a limited set of menu options, a limitation which can be resolved via menu dropout. A.4 Hardware Performance Feedback As discussed in Sec. 3.2, during plan generation, we include the latency, scratchpad utilization, and accumulator utilization of the original code. Table 3 shows that this component is helpful, 15 but in some cases its effects may be limited. This is because the options listed in the optimization menu already capture some of the metrics measured in our performance feedback, for example the menu options which suggest using larger tile sizes.",
    "source": "2505.18574v2_Autocomp_LLM-Driven_Code_Optimization_for_Tensor_A.pdf",
    "length": 2604,
    "tokens": 497
  },
  {
    "text": "3, pp. 33 39, 2024. [4] P. Chi, S. Li, C. Xu, T. Zhang, J. Zhao, Y. Liu, Y. Wang, and Y. Xie, PRIME: A novel processing-in-memory architecture for neural network computation in ReRAM-based main memory, ACM SIGARCH Computer Architecture News, vol. 44, no. 3, pp. 27 39, 2016. [5] Y. Zhao, J. Yang, B. Li, X. Cheng, X. Ye, X. Wang, X. Jia, Z. Wang, Y. Zhang, and W. Zhao, NAND-SPIN-based processing-in-MRAM ar- chitecture for convolutional neural network acceleration, Science China Information Sciences (SCIS), vol. 66, no. 4, p. 142401, 2023. [6] C. Duan, J. Yang, X. He, Y. Qi, Y. Wang, Y. Wang, Z. He, B. Yan, X. Wang, X. Jia et al., DDC-PIM: Efficient algorithm architecture co-design for doubling data capacity of SRAM-based processing-in- memory, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), vol. 43, no. 3, pp. 906 918, 2023. [7] A. Shafiee, A. Nag, N. Muralimanohar, R. Balasubramonian, J. P. Stra- chan, M. Hu, R. S. Williams, and V. Srikumar, ISAAC: A convolutional neural network accelerator with in-situ analog arithmetic in crossbars, ACM SIGARCH Computer Architecture News, vol. 44, no. 3, pp. 14 26, 2016.",
    "source": "2505.01107v1_CIMFlow_An_Integrated_Framework_for_Systematic_Des.pdf",
    "length": 1160,
    "tokens": 399
  },
  {
    "text": "This validates the effectiveness of architecture aware training, tokenizer extension, and longer context modeling in capturing fine-grained register and memory se- mantics. For -O2 optimized code, accuracy drops to 45.12 (ARMv8) and 50.30 (ARMv5), exposing the fragility of current LLMs under aggressive compiler transformations. This suggests that while our model learns to generalize well under minimal Error Type Files with Errors after Guess Input output out of context window LongDiv, Regex-Parser, RLE-Compress, FFT-Int, Blake2B, Anagram, C-Interp, Totient, Banner, Lz Compress, Satomi, Rho-Factory Duplicate function error Frac-Calc, Minspan Stack memory error Boyer-Moore-Search, Topo-Sort, Audio-Codec, Weekday, Simple-Grep, Max-Subseq, Priority-Queue, Dhrys- tone, Cipher, AVL-Tree, QSort-Demo, Vectors-3D, Pascal Missing function error Fuzzy-Match, Tiny-NN, Kadane, Audio- Codec, Frac-Calc, Kepler, Dhrystone, Cipher, Graph-Tests, Quaternions, AVL-Tree, K-Means, QSort-Demo, Vectors-3D Labels referred but not defined Fuzzy-Match, Life, AVL-Tree, K-Means Register mislabel error Bloom-Filter, Topo-Sort, Weekday, Knights-Tour, Simple-Grep, Max- Subseq, Mersenne, Audio-Codec, K-Means, QSort-Demo, Vectors-3D, Pascal, Minspan Incorrect immediate value Kadane Table 4: Failed files on BringupBench. Errors after the Guess stage are largely around dataflow reasoning. File names are grouped by error type. optimization, it struggles with control data flow reordering and register coalescing introduced by -O2 passes. Addressing this challenge may require incorporating optimization-invariant representa- tions, such as symbolic traces or control data-flow graphs, or extending the training set with more aggressively optimized samples.A detailed error analysis can be found in Appendix A.1. RISC-v64.",
    "source": "2506.14606v1_Guaranteed_Guess_A_Language_Modeling_Approach_for_.pdf",
    "length": 1808,
    "tokens": 488
  },
  {
    "text": "Same performance at lower precision: LowRA enables fine-tuning with 0.98 bits (QLoRA) 0.76 bits (LoftQ) fewer per parameter (on average) without performance loss. For example, 2.5-bit LowRA on WikiText-2 (LLaMA-2-7B) matches 4-bit QLoRA; 1.9-bit LowRA on Open-Assistant (LLaMA-2-7B) matches 2.5-bit LoftQ. 7.4 Fine-Tuning LLMs with Ultra-Low Bits We further explore: (1) Fine-tuning larger LLMs, and (2) fine-tuning under ultra-low-bit precision. Results for LLaMA-30B at 1.15 and 1.25 bits are in Table 3. LowRA is the first method to enable accurate LoRA fine-tuning at ultra-low-bit levels: 1.75 bits on LLaMA-2-7B11, LLaMA-2-13B, and BART-large, or 1.15 bits on LLaMA-30B. 7.5 Memory Implications Following the analysis methodology in QLoRA [9], we evaluate the memory footprint of LowRA at different quantization precisions. Full visualizations are in Appendix C. Our results show that LowRA significantly reduces memory usage for both fine-tuning and inference, making ultra-low-bit LoRA practical on resource-constrained hardware. For inference, reducing precision from 4-bit to 2-bit leads to 40 lower memory usage on LLaMA-2-13B and 30 on LLaMA-2-7B (Figures 8). Compressing LLaMA-30B to 1.15 or 1.25 bits achieves even greater savings, reducing the memory footprint by 50 (Figure 9). For fine-tuning, LowRA also achieves substantial reductions. Moving from 4-bit to 2-bit precision cuts memory consumption by 30 on LLaMA-2-13B and 25 on LLaMA-2-7B (Figures 7). On LLaMA-30B, reducing precision to 1.15 or 1.25 bits per parameter leads to an estimated 45 reduction in fine-tuning memory usage, making it feasible to train larger models under stricter memory constraints.",
    "source": "2502.08141v1_LowRA_Accurate_and_Efficient_LoRA_Fine-Tuning_of_L.pdf",
    "length": 1679,
    "tokens": 490
  },
  {
    "text": "49 6.1.3 Physical Disaggregation and Pooling of CPUs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 6.2 Future Serving System Enhancements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 6.2.1 Component-Level Disaggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 6.2.2 Hybrid and Adaptive Deployment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 4 1 Introduction The landscape of large language models (LLMs) has undergone a dramatic transformation in recent years, driven by several defining trends: the exponential growth of parameter scales, the widespread adoption of mixture-of-experts (MoE) architectures, and the substantial extension of context lengths [33]. Modern LLMs such as DeepSeek-R1 [13], LLaMA-4 [39], and Qwen-3 [49] routinely scale to hundreds of billions or even trillions of parameters, placing unprecedented demands on compute power and memory capabilities. MoE models introduce structural sparsity by selectively activating a small subset of experts per token, enabling greater efficiency at scale while introducing new system-level challenges in expert routing and synchronization [18, 30, 34].",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 1539,
    "tokens": 490
  },
  {
    "text": "Under 10 Trace Type Request Type System Size TTFT Throughput Throughput Energy Code Generation Regular Prefill-Decode Small Continuous Disaggregated (Medium) Chunked (High) Disaggregated Large Continuous (Medium) Disaggregated (High) Disaggregated (Medium) Chunked (High) Disaggregated RAG Small Continuous Chunked Disaggregated Large Continuous Chunked Disaggregated Disaggregated Memory Cache Retrieval Small Continuous Disaggregated Disaggregated Large Continuous (Medium) Disaggregated (High) Disaggregated (Medium) Chunked (High) Disaggregated Conv (Chatbots) Regular Prefill-Decode Small Disaggregated Disaggregated Disaggregated Large Disaggregated Disaggregated Disaggregated RAG Small Continuous Chunked Disaggregated Disaggregated Large Continuous Chunked Disaggregated Disaggregated Memory Cache Retrieval Small Continuous Chunked Disaggregated Disaggregated Large Disaggregated Disaggregated Disaggregated Reasoning (Scaling output w parallel thoughts) Small Continuous Continuous Disaggregated (Low) Continuous (Medium) Large Continuous Disaggregated (Low) Continuous (Medium) Disaggregated (Low) Continuous (Medium) TABLE III: √ì Batching Strategy Recommendation based on input trace, inference pipeline, and serving system size. Small serving setup with a single platform (4xTP2) and large serving setup simulating a rack (32xTP2) for serving LLama3-70B. For cases with multiple recommendations, Low Medium High refers to per client incoming request rate (e.g. For optimizing throughput in code generation with regular prefill decode, disaggregated batching is recommended at medium request rates,i.e.3-4 req s, and chunked batch is recommended at high request rate, i.e. 5-6 req s. Last three columns indicate the optimization objective, minimizing TTFT and maximizing throughput, throughput energy.",
    "source": "2504.09775v3_Understanding_and_Optimizing_Multi-Stage_AI_Infere.pdf",
    "length": 1814,
    "tokens": 436
  },
  {
    "text": "6 We modulate this value to the actual task count computed 7 dispatch signals received from peer GPUs 8 taskBound GetTaskBound() 9 while AtomicLoad(interrupt) False do 10 dispatch flags 11 do in parallel 12 Visit dispatch flags 13 Atomically retrieve signal 14 if Signal is set and flag is not visited then 15 Mark visited 16 SelfCorrectTaskBound(taskBound, Signal) 17 Enforce memory consistency before consuming packet 18 Decode packet into a set of GEMM0 task descriptors using X 19 Write task descriptors to tQ 20 Notify Scheduler of decoded tasks 21 end if 22 end 23 Advance flags by number of dispatch flags length 24 Atomically retrieve signal 25 combine signals 26 do in parallel 27 Visit combine flags: one per tile 28 if Signal is set and flag is not visited then 29 Mark visited 30 Enforce memory consistency before consuming packet 31 Decode packet into a set of combine task descriptors using Tœï, Gœï, O 32 Write task descriptors to tQ 33 Notify Scheduler of decoded tasks 34 end if 35 end 36 end while 37 end 25 F Multi-Node Evaluation F.1 Setup In this experiment, we seek to evaluate FlashDMoE in the multi-node setting. We use 4 nodes, where each node comprises 4 A100 GPUs fully interconnected via NVLink. Across nodes, each GPU uses a single NIC providing 25 GB s of bandwidth. We set the number of experts to be 16 and assign each GPU to host only one, so the number of local experts is 1. Note that we define MIV formally as follows: MIV Tokens Experts local_experts precision hidden_size 2 nrg where nrg is the number of remote peers and the multiplicative factor of 2 accounts for communication rounds (dispatch and combine). nrg 12 for this experiment. F.2 Results 128 256 512 1024 2048 4096 8192 Tokens 1.2 1.3 1.4 1.5 1.6 1.7 Latency (ms) Network Failure Region Latency vs Number of Tokens Latency (ms) Network Failure Threshold (MIV 12 MB) 0.75 1.50 3.00 6.00 12.00 24.00 48.00 Maximal Incast Volume (MB) Figure 17: Multi-node Latency evaluation. Embbeding dimension is 1024 and FFN intermediate size is 4096.",
    "source": "2506.04667v2_FlashDMoE_Fast_Distributed_MoE_in_a_Single_Kernel.pdf",
    "length": 2034,
    "tokens": 486
  },
  {
    "text": "Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. Request permissions from SoCC 21, November 1 4, 2021, Seattle, WA, USA 2021 Association for Computing Machinery. ACM ISBN 978-1-4503-8638-8 21 11... 15.00 Keywords serverless, resource-management, scheduling, queuing ACM Reference Format: Vivek M. Bhasi, Jashwant Raj Gunasekaran, Prashanth Thinakaran, Cyan Subhra Mishra, Mahmut Taylan Kandemir, and Chita Das. 2021. Kraken : Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms. In ACM Symposium on Cloud Computing (SoCC 21), November 1 4, 2021, Seattle, WA, USA. ACM, New York, NY, USA, 15 pages. 3486992 1 Introduction Cloud applications are embracing microservices as a pre- mier application model, owing to their advantages in terms of simplified development and ease of scalability [29, 40]. Many of these real-world services often comprise of tens or even hundreds of loosely-coupled microservices [42] (e.g. Ex- pedia [15] and Airbnb [2]). Typically, these online service ap- plications are user-facing and hence, are administered under strict Service Level Objectives (SLOs) [47, 48] and response latency requirements. Therefore, choosing the underlying resources (virtual machines or containers) from a plethora of public cloud resource offerings [31, 33, 37, 41, 45, 50] becomes crucial due to their characteristics (such as provisioning la- tency) that determine the response latency. Serverless com- puting (FaaS) has recently emerged as a first-class platform to deploy latency-critical user facing applications as it miti- gates resource management overheads for developers while simultaneously offering instantaneous scalability. However, deploying complex microservice-based applications on FaaS has unique challenges owing to its design limitations.",
    "source": "kraken.pdf",
    "length": 1916,
    "tokens": 445
  },
  {
    "text": "(b) packet-specific encoding precision and (c) frequency-aware reindexing to further optimize the DRAM bandwidth. depicted in Fig. 4b, employing packet-specific bit-precision allows low-bit encoded values to be packed together more effectively, thereby reducing the number of cycles required for transmission. The encoding precision for each packet is determined by the maximum encoded value in the respective packet. Additional mode bits are now used to determine the bit-precision of each packet (for example 3-bits for packet A and 2-bits for packets E , G ). Packets with mode 0 and mode 1 use 3-bits and 2-bits to represent the encoded values, respectively. The mode bits will be used by the WILU module to unpack the grouped encoded val- ues. Packet-specific encoding allows packing more data per packet thereby improving the DRAM bandwidth efficiency. 5.3 Frequency-aware Re-indexing As illustrated in Fig. 4c, frequently occurring chunk IDs in the encoded W matrix (e.g., chunk ID 3) may necessi- tate higher precision, which can limit the efficiency of bit packing. In frequency-aware re-indexing, the chunk IDs are re-assigned to each unique chunk based on their frequency of occurrence i.e., chunk IDs appearing more frequently are assigned lower chunk ID. For instance, in the example pre- sented in Fig. 4c, chunk IDs [0, 1, 2, 3, 4] with frequencies [2, 2, 1, 6, 5] are re-assigned new chunk IDs [2, 3, 4, 0, 1]. This approach increases the proportion of low-precision chunk IDs in the encoded W matrix, resulting in efficient bit packing and thereby reducing transfer cycles. The modified encoded W and the reindexed unique matrix are transferred from the DRAM for processing.",
    "source": "2503.11663v1_MEADOW_Memory-efficient_Dataflow_and_Data_Packing_.pdf",
    "length": 1691,
    "tokens": 389
  },
  {
    "text": "The running sum-of-exponents ‚Ñìi is implicitly embedded in the computation of each weight (see Eq. (3)), so it does not need to be explicitly computed. Additionally, the maximum value is no longer necessary, as numerical stability is maintained by ensuring that the attention score difference remains within the active region of [-6,11], where the sigmoid function can be effectively computed. Third, the output computation module in the FLASH-D hardware requires one vector adder, one subtractor, and one multiplier, compared to the two multipliers and one adder found in FlashAttention2. This hardware simplification, where one multiplier is replaced by a subtractor, is made possible by the new weighted definition of the output recursion (line 9 of Alg. 3), which can be equivalently written as follows: oi oi 1(1 wi) vi wi oi 1 ( vi oi 1) wi (12) B. Non-linear function evaluation in FLASH-D The non-linear function evaluations in FLASH-D involve the sigmoid and natural logarithm functions, both of which are well-researched and have various hardware implementations available. Without focusing on any specific simplifications, this work implements both functions using standard piece-wise linear (PWL) approximations. This design choice is well justified for both functions. The sigmoid function has a well-defined structure and output range, Fig. 3. A parallel hardware architecture for FLASH-D kernel for multiple preloaded query vectors. making it a natural fit for the PWL approach. Furthermore, in FLASH-D the input dynamic range is constrained to [-6, 11], so no computation is required outside this range. The natural logarithm function is used solely to compute the natural logarithm of the previous weight, i.e., ln wi 1, which has a narrow input dynamic range of (0,1). As a result, there is no need to compute a generic logarithm; instead, we require one that consistently returns a negative result that follows the value of the previous weight. Again, the PWL approach proves to be a suitable choice for this case. In both cases, we approximated the functions using 8 line segments. The coefficients of each segment are produced via pwlf a Python-based PWL-fit optimization library [29].",
    "source": "2505.14201v1_FLASH-D_FlashAttention_with_Hidden_Softmax_Divisio.pdf",
    "length": 2205,
    "tokens": 499
  },
  {
    "text": "Multiplierless Design of Linear DSP Transforms. In VLSI-SoC: Advanced Research for Systems on Chip, Salvador Mir, Chi-Ying Tsui, Ricardo Reis, and Oliver C. S. Choy (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 73 93. [4] Levent Aksoy, Paulo Flores, and Jos√© Monteiro. 2015. A Novel Method for the Approximation of Multiplierless Constant Matrix Vector Multiplication. In 2015 IEEE 13th International Conference on Embedded and Ubiquitous Computing. 98 105. [5] Marta Andronic and George A. Constantinides. 2025. NeuraLUT-Assemble: Hardware-Aware Assembling of Sub-Neural Networks for Efficient LUT Inference. In 2025 IEEE 33rd Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM). 208 216. https: doi.org 10.1109 FCCM62733.2025.00077 [6] Algirdas Avizienis. 1961. Signed-Digit Numbe Representations for Fast Parallel Arithmetic. IRE Transactions on Electronic Computers EC-10, 3 (1961), 389 400. [7] Alan Tendler Leibel Bacellar, Zachary Susskind, Mauricio Breternitz Jr, Eugene John, Lizy Kurian John, Priscila Machado Vieira Lima, and Felipe M.G. Fran√ßa. 2024. Differentiable Weightless Neural Networks. In Proceedings of the 41st International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 235), Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (Eds.). PMLR, 2277 2295. Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 23 [8] D. Benyamin, W. Luk, and J. Villasenor. 1999. Optimizing FPGA-based vector product designs. In Seventh Annual IEEE Symposium on Field- Programmable Custom Computing Machines (Cat. No.PR00375). 188 197. [9] N. Boullis and A. Tisserand. 2005.",
    "source": "2507.04535v1_da4ml_Distributed_Arithmetic_for_Real-time_Neural_.pdf",
    "length": 1767,
    "tokens": 496
  },
  {
    "text": "S up and S lo, corresponding to the upper and lower PDLs, respectively. Each bit in these codes is processed using two elementary delay units: one long (high-latency net) and one short (low-latency net). The delay path selects either the longer or shorter delay unit via a multiplexer: a bit of S up S lo equal to 0 1 inserts the longer shorter delay unit. Once all input binary codes are valid and ready for conver- sion, a start signal propagates from the left to the right end of all PDLs. The delay incurred by each PDL is inversely proportional to the Hamming weight of its corresponding binary code. In other words, a binary code with a higher popcount reaches the end earlier than one with a lower popcount. The arrival times at the right ends of the two PDLs are captured by an arbiter, implementing an argmax operation. Specifically, in the case of TMs, the input binary code for a PDL is derived from the outputs of all clauses belonging to a particular class. As shown in Fig. 1 (a), half of the clauses vote for the class, while the other half vote against it. To handle this polarity within a single PDL, an input bit from a clause supporting the class (positive clause) selects the longer shorter delay unit if it is 0 1 , whereas for a clause opposing the class (negative clause), the selection is reversed: a 1 0 input inserts the longer shorter delay unit. 2) PDL: Time-domain popcount is implemented using PDLs consisting of cascaded delay elements, each realized with a single LUT, similar to [12] [15]. However, our design places a strong emphasis on ensuring structurally symmetric PDLs and physically identical delay elements. Across all PDLs, the start signal triggered by a rising or falling transition is synchronized using D flip-flops (FFs) running at the maximum clock frequency. This synchroniza- tion is essential, as the input transition may be distributed across a large number of PDLs, which otherwise leads to uneven signal propagation. The potential skew caused by fan- out is mitigated by allowing the transition to propagate only at the clock edge, which is uniformly distributed to all FFs through clock tree synthesis.",
    "source": "2505.02181v1_Efficient_FPGA_Implementation_of_Time-Domain_Popco.pdf",
    "length": 2157,
    "tokens": 475
  },
  {
    "text": "A limited range, typically caused by parasitic capacitances reducing the voltage swing of the ADC s capacitor array, poses a challenge in many applications. Here, we deliberately exploit this effect by keeping the sampling capacitors ùê∂ùëß ùüô connected during the digitiza- tion phase (Figure 3 A). Segmenting the IMC matrix into groups with a binary scaling enables granular control over switches ùëÜùëß 2. This allows to disconnect parts of the 4 IMC sampling capacitors after charge sharing, inducing control over the ratio ùê∂ADC ùê∂IMC and thus the ADC s dynamic range (Figure 3 B). Therefore, the circuits can be ideally matched to the layer-specific slope of ùúéùëß (Figure 3 C). For a constant bias on ùëß, we can rely on the ADC s capacitive digital-to-analog converter (DAC) to generate an offset on the sampled potential: During the sampling phase, the capacitor array is pre-set to a 6 b offset be- fore then starting the successive approximation with the initial configuration (s[5:0] 0b100000). This allows shifting of the ADC s transfer characteristics by half of the dynamic range towards both positive and negative voltages (Figure 3 C). 3.1.3 State update through charge sharing The state update itself is, again, implemented through charge sharing. Within a column, each synapse con- tributes one sampling capacitor to represent the previ- ous hidden state ‚Ñéùë° 1 represented as ùëâ‚Ñé on the total capacitance ùê∂‚Ñé, while the other capacitor is used to calculate ‚Ñé according to the IMC scheme introduced be- fore, now present as ùëâ ‚Ñé on the merged capacitance ùê∂ ‚Ñé. Updating ‚Ñé simply involves mixing the charge between ùê∂‚Ñé and ùê∂ ‚Ñé with a weighting determined by ùëß, the 6 b digital representation of ùúéùëß(ùëâùëß). For that purpose, the circuits, again, rely on a segmented IMC matrix and thus granular control over ùëÜ‚Ñé 2 .",
    "source": "2505.08599v1_MINIMALIST_switched-capacitor_circuits_for_efficie.pdf",
    "length": 1805,
    "tokens": 486
  },
  {
    "text": "[19] Shailja Thakur, Baleegh Ahmad, Zhenxing Fan, Hammond Pearce, Benjamin Tan, Ramesh Karri, Brendan Dolan-Gavitt, and Siddharth Garg. Benchmarking large language models for automated verilog rtl code generation. In 2023 Design, Automation Test in Europe Conference Exhibition (DATE), pages 1 6. IEEE, 2023. [20] Shang Liu, Wenji Fang, Yao Lu, Qijun Zhang, Hongce Zhang, and Zhiyao Xie. Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source dataset and lightweight solution. In 2024 IEEE LLM Aided Design Workshop (LAD), pages 1 5. IEEE, 2024. [21] Hasan Genc, Seah Kim, Alon Amid, Ameer Haj-Ali, Vighnesh Iyer, Pranav Prakash, Jerry Zhao, Daniel Grubb, Harrison Liew, Howard Mao, Albert Ou, Colin Schmidt, Samuel Steffl, John Wright, Ion Stoica, Jonathan Ragan-Kelley, Krste Asanovic, Borivoje Nikolic, and Yakun Sophia Shao. Gemmini: Enabling systematic deep-learning ar- chitecture evaluation via full-stack integration. In 2021 58th ACM IEEE Design Automation Conference (DAC), pages 769 774, 2021. [22] Deepak Vungarala, Mahmoud Nazzal, Mehrdad Morsali, Chao Zhang, Arnob Ghosh, Abdallah Khreishah, and Shaahin Angizi. Sa-ds: A dataset for large language model-driven ai accelerator design generation, 2024. [23] Yonggan Fu, Yongan Zhang, Zhongzhi Yu, Sixu Li, Zhifan Ye, Chaojian Li, Cheng Wan, and Yingyan Celine Lin. Gpt4aigchip: Towards next-generation ai accelerator design automation via large language models. In 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD), pages 1 9, 2023. [24] Yun-Da Tsai, Mingjie Liu, and Haoxing Ren. Rtlfixer: Automatically fixing rtl syntax errors with large language models.",
    "source": "2505.15701v1_HDLxGraph_Bridging_Large_Language_Models_and_HDL_R.pdf",
    "length": 1664,
    "tokens": 494
  },
  {
    "text": "arXiv preprint arXiv:2501.11839, 2025. [25] Mingjie Liu, Walker J. Turner, George F. Kokai, Brucek Khailany, David Z. Pan, and Haoxing Ren. Parasitic-aware analog circuit sizing with graph neural networks and bayesian optimization. In 2021 Design, Automation Test in Europe Conference Exhibition (DATE), pages 1372 1377, 2021. [26] Tonmoy Dhar, Kishor Kunal, Yaguang Li, Meghna Madhusudan, Jitesh Poojary, Arvind K Sharma, Wenbin Xu, Steven M Burns, Ramesh Harjani, Jiang Hu, et al. Align: A system for automating analog layout. IEEE Design Test, 38(2):8 18, 2020. [27] Bingyang Liu, Haoyi Zhang, Xiaohan Gao, Zichen Kong, Xiyuan Tang, Yibo Lin, Runsheng Wang, and Ru Huang. Layoutcopilot: An llm-powered multi-agent collaborative framework for interactive analog layout design. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2025. [28] Antonio J Lopez Martin. Cadence design environment. New Mexico State University, Tutorial paper, 35, 2002. [29] Sorin Voinigescu. High-frequency integrated circuits. Cambridge University Press, 2013. [30] Behzad Razavi. RF microelectronics, volume 2. Prentice hall New York, 2012. [31] Thomas H. Lee. The Design of CMOS Radio-Frequency Integrated Circuits. Cambridge University Press, 2nd edition, 2004. [32] John R. Long and Michael A. Copeland. The design of low-noise cmos rf amplifiers. IEEE Journal of Solid-State Circuits, 32(2):292 302, 1997. [33] Ali M. Niknejad. mm-Wave Silicon Technology: 60 GHz and Beyond. Springer, 2008. [34] Xiaohua Fan, Heng Zhang, and Edgar S√Ånchez-Sinencio. A noise reduction and linearity improvement technique for a differential cascode lna. IEEE Journal of Solid-State Circuits, 43(3):588 599, 2008.",
    "source": "2505.21923v1_FALCON_An_ML_Framework_for_Fully_Automated_Layout-.pdf",
    "length": 1706,
    "tokens": 488
  },
  {
    "text": "Although one would expect that at least 5 7 12 bits are required to prevent overflow, the average accumulator bitwidth required varies between 7 and 10 bits. 4.1 Estimating the Probability of Integer Overflow We consider ùëè-bit quantized neural network dot products ùëç √çùëò ùëñ 1 ùë§ùëñùë•ùëñwith weights and activations in the range [ 2ùëè 1, 2ùëè 1]. Weight and input activation vectors ùë§and ùë•are truncated, zero- centered i.i.d normal distributions ùëÅ(ùúáùë§ 0, ùúéùë§) and ùëÅ(ùúáùë• 0, ùúéùë•), respectively. Input activations may also be half-normal dis- tributions due to ReLU operations in the previous layer. The partial products ùëùùëñ ùë§ùëñùë•ùëñare i.i.d product-normal distributions with ùúáùëù 0 and ùúé2ùëù (ùúé2ùë§ ùúá2ùë§)(ùúé2ùë• ùúá2ùë•) ùúá2ùë§ùúá2ùë• ùúé2ùë§ùúé2ùë•. The sum- ming of partial products can be represented by the random variable ùëç √çùëò ùëñ 1 ùëùùëñ. By the central limit theorem (CLT), for large enough ùëò, ùëç ùëÅ(0,ùëò ùúéùë§ùúéùë•).",
    "source": "2504.09072v1_MGS_Markov_Greedy_Sums_for_Accurate_Low-Bitwidth_F.pdf",
    "length": 859,
    "tokens": 398
  },
  {
    "text": "[12] P. Guo and W. Hu, Potluck: Cross-application approximate deduplica- tion for computation-intensive mobile applications, p. 271 284, 2018. [13] Z. Lai, Y. C. Hu, Y. Cui, L. Sun, and N. Dai, Furion: Engineering High-Quality Immersive Virtual Reality on Today s Mobile Devices, in Proceedings of the Annual International Conference on Mobile Computing and Networking (MobiCom), 2017, p. 409 421. [14] L. Gong, C. Wang, X. Li, H. Chen, and X. Zhou, A Power-EfÔ¨Åcient and High Performance FPGA Accelerator for Convolutional Neural Net- works: Work-in-Progress, in Proceedings of the Twelfth IEEE ACM I- FIP International Conference on Hardware Software Codesign and System Synthesis Companion, 2017. [15] J. Wang, J. Lin, and Z. Wang, EfÔ¨Åcient Hardware Architectures for Deep Convolutional Neural Network, IEEE Transactions on Circuits and Systems I: Regular Papers, pp. 1941 1953, 2017. [16] Aashish Chaubey, Downsampling and Upsampling of Images - Demys- tifying the Theory, shorturl.at rCMPU , 2020. [17] L. Liu, H. Li, and M. Gruteser, Edge Assisted Real-Time Object Detection for Mobile Augmented Reality, in Proceedings of the An- nual International Conference on Mobile Computing and Networking (MobiCom), 2019. [18] H. Jiang, A. Sarma, J. Ryoo, J. B. Kotra, M. Arunachalam, C. R. Das, and M. T. Kandemir, A learning-guided hierarchical approach for biomedical image segmentation, in 2018 31st IEEE International System-on-Chip Conference (SOCC), 2018, pp. 227 232. [19] H. Jiang, A. Sarma, M. Fan, J. Ryoo, M. Arunachalam, S. Naveen, and M. T. Kandemir, Morphable convolutional neural network for biomedical image segmentation, in 2021 Design, Automation Test in Europe Conference Exhibition (DATE), 2021, pp. 1522 1525.",
    "source": "PCframeSim.pdf",
    "length": 1727,
    "tokens": 499
  },
  {
    "text": "FASTLIBRA increases the cache hit rate by 1.3X and 3.2X compared to vLLM and S-LoRA, re- spectively. This is because FASTLIBRA maintains the usage dependencies between LoRAs and KV caches to eliminate invalid KV caches which enhances the HBM utilization effi- ciency. Its efficient swapping strategy also prefetches appro- priate KV caches and LoRAs into HBM. S-LoRA has the lowest hit rate because it does not reuse history KV caches. As a result, FASTLIBRA achieves lower queue and cold-start latency for both LoRA and KV caches in Figure 12. 6.5 Investigating HBM Allocation Over Time In this subsection, we compare HBM allocation between FASTLIBRA and baselines to show the effectiveness of FASTLIBRA s cache management. We take the example of using Llama-13B model, LoAR number of 100, and average sending rate of 1.6 for the chatbot scenario. Other scenarios have similar results. Figure 14 shows the HBM allocations for history KV caches, LoRAs, and running KV caches under FASTLIBRA and baselines. From 0s to 400s shown in (a), FASTLIBRA proactively fetches all LoRAs into HBM based on the cost model to eliminate the cold-start overhead of LoRAs under low HBM pressure. In contrast, vLLM and S-LoRA load LoRAs on- demand, leading to higher TTFT in this period. From 400s to 900s shown in (b), as the query sending rate increases, FASTLIBRA swaps out some LoRAs and retains the most history KV caches in HBM due to the unified caching pool. In contrast, vLLM s static HBM partition retains fewer history KVs while S-LoRA directly discards them, leading to poorer KV cache reuse and higher TTFT.",
    "source": "2505.03756v1_Improving_the_Serving_Performance_of_Multi-LoRA_La.pdf",
    "length": 1602,
    "tokens": 413
  },
  {
    "text": "For G2, targeting low power and a higher load capacitor, unity-gain bandwidth and phase margin constraints were re- laxed to account for trade-offs with the load. The agent achieved a 60 success rate within 25 iterations. For G3, it prioritized a higher bandwidth and power relative to G1 and G2. The transistor sizes and bias voltages belonging to a successful attempt of each configuration group are shown in Table II. To further verify the applicability of the optimized circuit, we conducted various tests, including DC, transient and parametric variations on circuit G1-5. These variations include transistor sizes and bias voltages. The results from these tests are shown in Fig. 6, which confirm design robustness and suitability for practical use under varying conditions. IV. DISCUSSION This work found that the proposed LLM-based agent can effectively propose circuit optimization strategies based on 25 0 25 50 75 Gain (dB) (a) 50 75 100 125 150 UGBW (dB) (b) 0 25 50 75 100 Phase Margin ( ) (c) 0.000 0.005 0.010 0.015 Power (W) (d) 0.0 0.5 1.0 1.5 Output Voltage (V) (e) 0.000 0.005 0.010 0.015 Offset (V) (f) 0 5 10 15 20 25 Iterations 50 75 100 125 150 CMRR (dB) (g) 0 5 10 15 20 25 Iterations 40 35 30 25 20 15 THD (dB) (h) Attempt 1 Attempt 2 Attempt 3 Attempt 4 Attempt 5 Target Range Fig. 5: Optimization results for the opamp. (a) Gain (b) Unity-Gain Bandwidth (c) Phase Margin (d) Power (e) Input Offset (f) Output Voltage Range (g) CMRR (h) THD. (b) and (e) are tested under unity gain configuration, others are tested open loop. (a), (b), (c), (d) and (g) are tested at Vin,cm 0.9 V, (e), (f), (h) are tested across the Vin from 0 to 1.8 V. Those already in the target range initially or after a few iterations may still fluctuate or even get worse due to the optimization process prioritizing other performance metrics.",
    "source": "2504.11497v1_LLM-based_AI_Agent_for_Sizing_of_Analog_and_Mixed_.pdf",
    "length": 1843,
    "tokens": 495
  },
  {
    "text": "Together, these enhancements eliminate the frequent pipeline breaks caused by CPU-NPU coordination in na√Øve MTP implementations. As the NPU executes one compute graph, the CPU concurrently schedules the next, enabling sustained parallelism and continuous NPU execution. This achieves a seamless flow of operations on the NPU, maximizing its utilization and fully realizing the potential latency benefits of MTP. 4.3 Resource-Efficient Prefill with Hybrid Parallelism and Microbatching The prefill phase, responsible for processing the input prompt to generate the initial KV cache, significantly impacts time-to-first-token (TTFT) and system throughput. Given its typically compute- intensive nature, achieving high NPU utilization during prefill is paramount. However, this phase often faces challenges such as load imbalances due to heterogeneous input sequence lengths and communication overheads, particularly in complex architectures like MoE models. To address these issues and maximize efficiency on the CloudMatrix384, we propose three key optimizations in CloudMatrix-Infer. First, we introduce a staged hybrid parallelism scheme for MLA computation that overcomes the inherent inefficiencies of conventional data parallelism ( 4.3.1). Second, we present a microbatch-based prefill pipeline that exploits the heterogeneous compute and communication units of the Ascend 910 NPU to maximize latency overlap and reduce contention ( 4.3.2). Finally, we present the transfer optimizations between prefill and decode phases to minimize the interference to decoding ( 4.3.3). 4.3.1 Hybrid Parallelism for MLA Computation Prefill in LLMs presents a significant computational bottleneck. Although CloudMatrix384 offers substantial compute power and high-bandwidth interconnects, we observe that the pure data parallelism (DP) for MLA computation, as originally used in DeepSeek s GPU deployment ( 3.5.1), Serving Large Language Models on Huawei CloudMatrix384 29 Rank 0: Rank 1: Rank 2: Rank 3: layer_input down_proj q_up_proj, kv_up_proj, FA o_proj Stage 1 (SP) Stage 2 (TP) Stage 3 (SP) Input 0: Input 1: Input 2: Input 3: Fig. 17. Illustrative data flow of the staged hybrid parallelism (SP-TP-SP) for MLA computation in prefill. leads to suboptimal load balancing and resource utilization on Ascend NPUs.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2308,
    "tokens": 498
  },
  {
    "text": "148 175, 2016. [20] Q. Xiao and Y. Liang, Towards agile DNN accelerator design using incremental synthesis on fpgas, in Proceedings of the 2022 ACM SIGDA International Symposium on Field-Programmable Gate Arrays, ser. FPGA 22. New York, NY, USA: Association for Computing Machinery, 2022, p. 42 48. [Online]. Available: [21] D. Li, S. Yao, Y.-H. Liu, S. Wang, and X.-H. Sun, Efficient design space exploration via statistical sampling and AdaBoost learning, in 2016 53nd ACM EDAC IEEE Design Automation Conference (DAC), 2016, pp. 1 6. [22] E. H. Lee, V. Perrone, C. Archambeau, and M. Seeger, Cost-aware Bayesian optimization, 2020. [Online]. Available: [23] S. Gal-On and M. Levy, Exploring coremark a benchmark maximizing simplicity and efficacy, The Embedded Microprocessor Benchmark Consortium, 2012. [24] R. P. Weicker, Dhrystone: a synthetic systems programming benchmark, Commun. ACM, vol. 27, no. 10, p. 1013 1030, Oct. 1984. [Online]. Available: [25] T. Newsome, riscv-software-src riscv-tests risc-v, May 2019, [Online; accessed January 17, 2025]. [26] K. Asanovi c, R. AviÀázienis, J. Bachrach, S. Beamer, D. Biancolin, C. Celio, H. Cook, P. Dabbelt, J. Hauser, A. Izraelevitz, S. Karandikar, B. Keller, D. Kim, J. Koenig, Y. Lee, E. Love, M. Maas, A. Magyar, H. Mao, M. Moreto, A. Ou, D. Patterson, B. Richards, C. Schmidt, S. Twigg, H. Vo, and A. Waterman, The Rocket Chip Generator, EECS Department, University of California, Berkeley, Tech. Rep. UCB EECS- 2016-17, April 2016, if used for research, please cite Rocket Chip by the technical report.",
    "source": "2506.06817v1_ASPO_Constraint-Aware_Bayesian_Optimization_for_FP.pdf",
    "length": 1562,
    "tokens": 501
  },
  {
    "text": "IV-B), resulting in 32 scenarios in total. Results. Figures 6 and 7 show leakage for two separate fam- ilies of Mlocked(IP) base related to m1IP base and m4IP base, respectively. Leakage is reduced notably thanks to locking, namely on average by 9.58 pt related to m1IP base and by 9.87 pt related to m4IP base, respectively. However, leakage trends across the two families vary significantly for different locking, FT, and prompting strategies, as discussed next. For the family Mlocked(IP) base related to m4IP base, we find that L50 const for FT w o-k (i.e., correct key values are not provided during FT) and L100 const for FT w k provide similar ranges for overall worst reductions, namely 3.18 pt and 4.87 pt, respectively. L50 all and L100 all , both for FT w o-k, provide similar ranges for overall best reductions of 13.13 pt and 13.84 pt, respectively. For prompting strategies, we find that leakage increases with more information on locking provided, albeit without consistent trends. Locking of all components is confirmed as least leaky across all prompting strategies, with some variations across locking scales and FT strategies. These trends follow the expectations that (i) more locking, (ii) skipping correct key values for FT, and (iii) prompting with less information should all hinder leakage more effectively. For the family Mlocked(IP) base related to m1IP base, we find that L50 all provides the worst reduction of 6.29 pt, whereas L50 const provides the best reduction of 13.27 pt, in both cases irre- spective of the FT strategies (w k vs w o-k). Furthermore, 4Model m4IP base is the 4th best in MIP base for quality, but the first with more varied FT parameters: m4IP base arises from e 2, lr 1e 5, t 0.8, whereas the top-3 models all arise from e 3, lr 1e 5.",
    "source": "2503.13116v4_VeriLeaky_Navigating_IP_Protection_vs_Utility_in_F.pdf",
    "length": 1787,
    "tokens": 434
  },
  {
    "text": "Pruning: The evaluator first prunes the pre-trained model along multiple dimensions, handling the text and vision encoders separately. We adopt pruning strategies from prior work Lin et al. (2024) to reduce the Feed Forward Network dimension, Number of Attention Heads, and Number of Layers based on block 5 Figure 5 Correlation between the proxy accuracy, fine-tuned and evaluated on MS COCO and the final model accuracy evaluated across 41 datasets. importance. Additionally, the Embedding Dimension is pruned using techniques from HAT Wang et al. (2020). Each transformer layer is pruned uniformly. Appendix A provides further details on pruning strategies and observations. Fine-tuning for Accuracy Proxy: Directly evaluating pruned models without training leads to excessively low accuracies. However, fully training each model in the search space is computationally prohibitive, as CLIP training can take hours or even days Xu et al. (2024). To efficiently approximate accuracy while preserving ranking consistency, we fine-tune each pruned model on the MS COCO dataset Lin et al. (2015) (400K samples). As shown in Figure 5, fine-tuning maintains a high Spearman s rank correlation coefficient (0.98) with the final post-pruning training accuracy, ensuring reliable accuracy ranking. We report the mean top-1 recall accuracy on MS COCO as the accuracy proxy for each model. 3.3 Hardware Estimator The Hardware Estimator module calculates the total carbon footprint (the sum of embodied and operational) and inference latency for a given model and hardware design. Our framework integrates existing libraries to estimate operator latency, energy, and hardware area Wu et al. (2019); Olyaiy et al. (2023). To quantify carbon cost, we use ACT Gupta et al. (2022) for embodied carbon estimation and Electricity Maps Maps (2025) for operational carbon per inference. The operational carbon is then scaled over the hardware s lifetime. A detailed overview of the tool-chain integration is provided in Appendix B. 3.4 Multi-Objective Optimization The Multi-Objective Optimizer iteratively explores both model and hardware search spaces, taking the ML model and hardware template as inputs. It utilizes the Ax platform Meta Platforms (2024) and BoTorch Ba- landat et al.",
    "source": "2505.01386v2_Carbon_Aware_Transformers_Through_Joint_Model-Hard.pdf",
    "length": 2269,
    "tokens": 487
  },
  {
    "text": "It is evident that the LSTM and DeepAREst have lowest RMSE value. DeepARest is 10 better than LSTM model. Since the primary contribution in Cocktail is to provide high accuracy and low latency predictions at cheaper cost, appli- cation developers can adapt the prediction algorithm to their needs or even plug-in their own prediction models. C System Overheads We characterize the system-level overheads incurred due to the design choices in Cocktail. The mongodb database is a centralized server, which resides on the head-node. We measure the overall average latency incurred due to all reads writes in the database, which is well within 1.5ms. The DeepARest prediction model which is not in the critical decision-making path runs as a background process incurring 2.2 ms latency on average. The weighted majority voting takes 0.5ms and the model selection policy takes 0.7ms. The time taken to spawn new VM takes about 60s to 100s de- pending on the size of the VM instance. The time taken to choose models from the model-cache is less than 1ms. The end-to-end response time to send the image to a worker VM and get the prediction back, was dominated by about 300ms (at maximum) of payload transfer time. D Instance conÔ¨Åguration and Pricing Instance vCPUs Memory Price C5a.xlarge 4 8 GiB 0.154 C5a.2xlarge 8 16 GiB 0.308 C5a.4xlarge 16 32 GiB 0.616 C5a.8xlarge 32 64 GiB 1.232 Table 7: ConÔ¨Åguration and Pricing for EC2 C5 instances. E CIFAR-100 and BERT Models Table 8 shows the different models available for image predic- tion, that are pretrained on Keras using CIFAR-100 dataset.",
    "source": "cocktail.pdf",
    "length": 1586,
    "tokens": 399
  },
  {
    "text": "4 Model ARMv5 ARMv8 ARMv8 HumanEval HumanEval HumanEval HumanEval BringupBench BringupBench -O0 -O2 -O0 -O2 -O0 -O2 GPT-4o (OpenAI, 2024) 8.48 3.64 10.3 4.24 1.54 0 Qwen2.5-Coder-1.5B (Hui et al., 2024a) 0 0 0 0 0 0 Qwen2.5-Coder-3B (Hui et al., 2024a) 0.61 0 0 0 0 0 StarCoder2-3B (Lozhkov et al., 2024) 0 0 0 0 0 0 Deepseek-R1-1.5B (Guo et al., 2025) 0 0 0 0 0 0 Deepseek-R1-Qwen-7B (Guo et al., 2025) 0 0 0 0 0 0 GG-Deepseek-1.3B 79.25 12.80 75.15 10.3 3.08 0 GG-0.5B 90.85 23.03 86.06 25.45 27.69 3.08 GG-1.5B 93.71 50.30 99.39 45.12 49.23 15.38 Table 3: Models trained with our method outperform baselines across all benchmarks, at all optimization levels. 4 Experiments and Evaluation In this section, we describe our experimental setup, training methodology, evaluation benchmarks, and the metrics used to assess the accuracy and robustness of our CISC-to-RISC transpiler. 4.1 Setup We leveraged LLaMa-Factory (Zheng et al., 2024), DeepSpeed Zero3 (Rasley et al., 2020), liger ker- nels (Hsu et al., 2024), and FlashAttention2 (Dao, 2023) for efficient training and memory optimiza- tion. We also used caching to enhance inference speed and disabled sampling to ensure deterministic outputs. We used vLLM (Zheng et al., 2023) to deployourmodelandachieveathroughputof36xre- questspersecondat32.7ktokenscontextwindowon a single A100 40GB GPU.",
    "source": "2506.14606v1_Guaranteed_Guess_A_Language_Modeling_Approach_for_.pdf",
    "length": 1347,
    "tokens": 476
  },
  {
    "text": "Symp. Comput. Archit., 2023, pp. 1-13. [10] Black-Forest-Labs. Flux.1, 2024. [Online]. Available: [11] Open Compute Project, OCP Microscaling Formats (MX) Specifica- tion, 2023. [Online]. Available: [12] LIU, Jun, et al., FlightVGM: Efficient Video Generation Model In- ference with Online Sparsification and Hybrid Precision on FPGAs, in Proc. 2025 ACM SIGDA Int. Symp. on Field Program. Gate Arrays, 2025, pp. 2-13. [13] Yang, Xinhao, et al., PARO: Hardware-Software Co-design with Pattern-aware Reorder-based Attention Quantization in Video Generation Models, in Des. Automat. Conf., 2025. [14] Zhao, Tianchen, et al., Mixdq: Memory-efficient few-step text-to-image diffusion models with metric-decoupled mixed precision quantization, in Eur. Conf. Comput. Vis., 2024, pp. 285-302. [15] NVIDIA, NVIDIA Blackwell Architecture , 2024. [Online]. Available: architecture",
    "source": "2504.08398v1_MixDiT_Accelerating_Image_Diffusion_Transformer_In.pdf",
    "length": 869,
    "tokens": 254
  },
  {
    "text": "Besides, due to the autoregressive nature of LLM inference, the execution latencies of parallelizable parts on the two devices could be varying and imbalanced, causing idle bubbles on the device where the workload is finished earlier. To address those challenges, we propose L3, a heterogeneous LLM inference system that integrates GPUs with scalable DIMM- PIM. On the hardware side, L3 proposes the fine-grained re-layout and DIMM-tailored KV cache mapping methods to eliminate the two mismatches. Specifically, we propose a zero-latency in-flight re-layout method, which is seamlessly integrated in conventional DIMM-based burst transfer without DDR protocol violations [1, 2]. Moreover, two distinct mapping methods for KV cache are proposed to achieve both communication and computation efficiency, while co-designed configurable processing units (PU) at both rank level and bank level are leveraged to enable MHA kernel fusion with bubble-free pipelined execution. L3 system maximizes the resource utilizations on both GPU and DIMM-PIM devices. First, L3 applies dependency-aware communication- computation overlap, a software-hardware co-design to hide the cross-device communication overheads. It involves three techniques, which enable concurrent communication and computation on the DIMM-PIM side, achieve load balancing of data transfer and com- putation across ranks, and remove most data communication from the critical path of the inference process. Second, L3 scheduler ap- plies adaptive chunk-partitioned batch interleaving to maximize parallel execution on both GPUs and DIMM-PIM with minimum idle bubbles. Specifically, to eliminate the idle bubbles caused by the computation dependency of inference operations, L3 sched- uler coordinates requests with two sub-batches. It enables two key features, fine-grained tunability and performance predictability, to align the parallel execution of the two sub-batches. This fur- ther minimizes the idle bubbles caused by unbalanced cross-device parallel execution.",
    "source": "2504.17584v1_L3_DIMM-PIM_Integrated_Architecture_and_Coordinati.pdf",
    "length": 2024,
    "tokens": 425
  },
  {
    "text": "For example with four heads, GAT assigns four different weight vectors to the neighbor features when aggregating and concatenates the weighted features together. Intuitively, this may correspond to multiple combinations of driving wires or buffers from different directions on the mesh. To compute the final node features hfinal we utilize the JK connections which are shown in Figure 4 with the red dashed line. Generally, the features from each layer hk, hk 1, ...hl where l are the number of layers are passed to the final layer where they are either concatenated, max-pooled or passed to a Bi-directional LSTM. In our case, we use the max-pooling where it learns to select the most informative layer for each feature coordinate. Concatenation is the most straightforward but it only optimizes the weights to combine the features in a way that works best for the overall dataset which can cause underfitting when the graphs are too complex. On the other end, LSTM is more complex and can cause overfitting. Of these aggregation schemes, JK connections with max pooling shows better performance overall in deep GNNs [14] like ours. The JK connections serve a different purpose compared to the auxiliary connections we added to our graphs. They provide learnable combination of features in each neighbor- hood of the RC mesh whereas the aux connections model two physical aspects of a clock mesh: multiple buffer contention and multiple sink drivers which are statically chosen. IV. METHODOLOGY A. Training Data Generation We analyzed some of the open-source benchmark designs with OpenROAD [19] and implemented them using the default flow in the Nangate45 technology. We used the design statistics of area, number of sinks, and sink density to generate various synthetic designs for clock synthesis as shown in Table II. Clock meshes were generated for these synthetic designs using both uniform buffering [7] and non-uniform buffering [10].",
    "source": "2507.05681v1_GATMesh_Clock_Mesh_Timing_Analysis_using_Graph_Neu.pdf",
    "length": 1943,
    "tokens": 397
  },
  {
    "text": "IEEE, 2020. [17] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. Deepfix: Fixing common c language errors by deep learning. In Proceedings of the aaai conference on artificial intelligence, volume 31, 2017. [18] Intel. Microarchitectural data sampling, 2021. Accessed: 2025-01-22. [19] Intel Corporation. Intel 64 and ia-32 architectures soft- ware developer s manual: Combined volumes 1, 2a, 2b, 2c, 2d, 3a, 3b, 3c, 3d, and 4, 2023. Accessed: 2024-11- 15. [20] Gorka Irazoqui, Thomas Eisenbarth, and Berk Sunar. S a: A shared cache attack that works across cores and defies vm sandboxing and its application to aes. In 2015 IEEE Symposium on Security and Privacy, pages 591 604. IEEE, 2015. [21] Saad Islam, Ahmad Moghimi, Ida Bruhns, Moritz Krebbel, Berk Gulmezoglu, Thomas Eisenbarth, and Berk Sunar. SPOILER: Speculative load hazards boost rowhammer and cache attacks. In 28th USENIX Secu- rity Symposium (USENIX Security 19), pages 621 637, Santa Clara, CA, August 2019. USENIX Association. 14 [22] Patrick Jattke, Victor Van Der Veen, Pietro Frigo, Stijn Gunter, and Kaveh Razavi. Blacksmith: Scalable rowhammering in the frequency domain. In 2022 IEEE Symposium on Security and Privacy (SP), pages 716 734. IEEE, 2022. [23] Paul Kocher, Jann Horn, Anders Fogh, , Daniel Genkin, Daniel Gruss, Werner Haas, Mike Hamburg, Moritz Lipp, Stefan Mangard, Thomas Prescher, Michael Schwarz, and Yuval Yarom. Spectre attacks: Exploit- ing speculative execution. In 40th IEEE Symposium on Security and Privacy (S P 19), 2019.",
    "source": "2502.14307v1_ŒºRL_Discovering_Transient_Execution_Vulnerabilitie.pdf",
    "length": 1533,
    "tokens": 454
  },
  {
    "text": "These representations are related through a complex sinuisoidal term ejœât where the frequency œâ in units of [radians s] is implicit (but assumed fixed across all quantities) and j denotes the imaginary unit, as is standard in electrical engineering. E : R3 T 7 R3 The time-harmonic electric field. If the complex electric field is denoted E : R3 7 C3, then E(x, t) Re(ejœâtE(x)). Units [V m]. H : R3 T 7 R3 The time-harmonic magnetic field. If the complex magnetic field is denoted H : R3 7 C3, then H(x, t) Re(ejœâtH(x)). Units [A m]. D : R3 T 7 R3 The time-harmonic electric flux density. If the complex electric flux density is denoted D : R3 7 C3, then D(x, t) Re(ejœâtD(x)). Units [C m2]. B : R3 T 7 R3 The time-harmonic magnetic flux density. If the complex magnetic flux density is denoted B : R3 7 C3, then B(x, t) Re(ejœâtB(x)). Units [W m2]. J : R3 T 7 R3 The time-harmonic electric current density. If the complex electric current density is denoted B : R3 7 C3, then B(x, t) Re(ejœâtB(x)). Units [W m2]. Q : R3 T 7 R The time-harmonic electric charge density. If the complex electric charge density is denoted Q : R3 7 C, then Q(x, t) Re(ejœâtQ(x)). Units [C m3]. To denote the material parameters which appear in the constitutive relations, we use the following scalar functions of position. œµ : R3 7 C The scalar, complex-valued electric permittivity conductivity. The real part Re(œµ) is the electric permittivity and the imaginary part Im(œµ) œÉ is the electric conductivity. ¬µ : R3 7 R The scalar, real-valued magnetic permeability.",
    "source": "2504.20401v1_Nonlinear_Computation_with_Linear_Optics_via_Sourc.pdf",
    "length": 1540,
    "tokens": 479
  },
  {
    "text": "151 164, 2021. [51] B. Liu, H. Cai, Z. Wang, Y. Sun, Z. Shen, W. Zhu, Y. Li, Y. Gong, W. Ge, J. Yang, and L. Shi, A 22nm, 10.8 uw 15.1 uw dual computing modes high power-performance-area efficiency domained background noise aware keyword- spotting processor, IEEE Trans. on Circuits and Systems I: Regular Papers, vol. 67, no. 12, pp. 4733 4746, 2020. [52] F. Tan, W.-H. Yu, J. Lin, K.-F. Un, R. P. Martins, and P.-I. Mak, A 1.8 far, 2 ms decision latency, 1.73 nj decision keywords- spotting (kws) chip incorporating transfer-computing speaker verifica- tion, hybrid-if-domain computing and scalable 5t-sram, IEEE J. of Solid-State Circuits, vol. 60, no. 3, pp. 1103 1112, 2025. Douwe den Blanken (Graduate Student Member, IEEE) received the M.Sc. degree (with honors) in embedded systems from the Delft University of Technology (TU Delft), Delft, The Netherlands, in 2023, where he is currently pursuing the Ph.D. de- gree, under the supervision of Prof. C. Frenkel. His current research interests include efficient learning algorithms and their implementation in silicon, as well as the quantization and acceleration of modern DNNs. Charlotte Frenkel (Member, IEEE) received the M.Sc. degree (summa cum laude) in Electromechan- ical Engineering and the Ph.D. degree in Engineer- ing Science from Universit e catholique de Louvain (UCLouvain), Louvain-la-Neuve, Belgium in 2015 and 2020, respectively. In February 2020, she joined the Institute of Neuroinformatics, UZH and ETH Zurich, Switzerland, as a postdoctoral researcher. She is an Assistant Professor at Delft University of Technology, Delft, The Netherlands, since July 2022, and holds a Visiting Faculty Researcher position with Google since October 2024.",
    "source": "2505.24852v2_Chameleon_A_MatMul-Free_Temporal_Convolutional_Net.pdf",
    "length": 1717,
    "tokens": 481
  },
  {
    "text": "2023. PaLM 2 Technical Report. arXiv preprint arXiv:2305.10403 (2023). [5] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Pashmina Cameron, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. 2024. QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs. arXiv preprint arXiv:2404.00456 (2024). [6] Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long- Document Transformer. [7] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2019. PIQA: Reasoning about Physical Commonsense in Natural Language. arXiv preprint arXiv:1911.11641 (2019). [8] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. arXiv preprint arXiv:2005.14165 (2020). [9] S. Chang, Y. Li, M. Sun, R. Shi, H. H. So, X. Qian, Y. Wang, and X. Lin. 2021. Mix and Match: A Novel FPGA-Centric Deep Neural Network Quantization Framework. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA).",
    "source": "2503.18599v2_Oaken_Fast_and_Efficient_LLM_Serving_with_Online-O.pdf",
    "length": 1435,
    "tokens": 474
  },
  {
    "text": "The TPU s success providing better latency and energy- efficiency for DNN inference than general CPUs GPUs was a seminal deployment of systolic arrays in AI hardware. Given the rapid development of Transformer-focused hardware, comprehensive reviews have emerged. (Kachris, 2025) provides a recent survey of hardware accelerators for LLM transformers, with an emphasis on systolic-array-based designs and other specialized architectures. 12.2 ZettaLith: Very Large Arrays ZettaLith extends the performance advantages of systolic arrays through: 1. Specialization for FP4 multiply with FP8 accumulation; 2. SHAPE ultra-dense simple PEs; 3. CASCADE column-oriented architecture; 4. TRIMERA chip stack optimization; 5. CREST fault tolerance; and 6. WSSCB integration. ZettaLith implements 156 TRIMERA chip-stacks each with 384 CASCADE arrays of 524,288 PEs for a total of 31,406,948,352 simultaneously operating PEs in an all-silicon domain. 22 13 CASCADE ZettaLith implements CASCADE (Column- Array Systolic Computation with Accumulation During Execution) for matrix multiplication through a large scale column- oriented array architecture. This approach differs significantly from traditional systolic array implementations, optimizing for on-chip computation without inter-chip partial sum transfers, while enabling the CREST real-time redundancy system. Though organizationally distinct, the design maintains mathematical equivalence to conventional systolic multiplication while eliminating partial sum transfers and activation fill skew and while offering superior fault tolerance for large arrays. 13.1 Final summation of CASCADE arrays Figure 9 shows a block diagram of the end of the CASCADE arrays. The last two rows of the 24,576 rows of the CASCADE arrays are shown for context. The previous array column segment latches, CREST multiplexers, CASCADE array adders, and current array column segment latches of the last CASCADE array are also shown. There is one output sum HILT memory for each of the 8,208 (8,192 plus 16 spares) columns of the CASCADE arrays on the TRIMERA stacks.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2089,
    "tokens": 465
  },
  {
    "text": "Index Terms Continual learning (CL), digital accelerator, few-shot learning (FSL), keyword spotting (KWS), matrix multiplication-free (MatMul-Free), sequential data, system-on- chip (SoC), temporal convolutional network (TCN). I. INTRODUCTION T HE rise of edge computing has driven demand for deploying deep learning models on resource-constrained devices [1], forming the backbone of the Internet-of-Things (IoT) ecosystem [2]. However, most edge artificial intelligence (AI) devices focus on inference [2], relying on pre-trained models that cannot be adapted post-deployment. This lack of adaptability limits robustness to emerging features and data distribution shifts [2], making smart sensors unreliable over time [3]. Fig. 1(a) shows that while cloud-hosted retraining offers a workaround where new labeled data is uploaded from an edge device to a central server for model updates, this approach is cloud-connectivity-dependent and does not fit latency-constrained scenarios, while leading to privacy concerns [4]. On the other hand, training a model from scratch on-device incurs high energy penalties and still requires the availability of a large labeled dataset, which is not realistic at This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.",
    "source": "2505.24852v2_Chameleon_A_MatMul-Free_Temporal_Convolutional_Net.pdf",
    "length": 1366,
    "tokens": 292
  },
  {
    "text": "Over the past five years, several frameworks have been developed to streamline the prototyping and deployment of ML models, including con- volutional neural networks (CNNs) and unconventional feed- forward architectures (such as, ResNets, Inception nets and U- Nets), by leveraging quantisation techniques to optimise their Vitis, Vivado, PYNQ and combinations thereof are trademarks of Advanced Micro Devices, Inc. Matmul Conv Feedback connections Feed Forward Neural Networks Recurrent Neural Networks Act Matmul Conv Act Matmul Conv Act Matmul Conv Act Concat Matmul Conv Act Inputs Outputs Matmul Conv Act Matmul Conv Act Matmul Conv Act Matmul Conv Act Inputs Matmul Conv Act Matmul Conv Act Concat : ElMul ElAdd State Updates Computed States Fig. 1: The figure contrasts the information flows of con- ventional feed-forward vs. recurrent neural networks. While in feed-forward networks, data flows acyclically from input to output, recurrent neural networks utilise the feedback of previously computed state to track sequential dependencies. hardware performance [2], [3]. Despite these advancements, a significant gap remains in the deployment of recurrent neural networks (RNNs), such as long short-term memory (LSTM) networks and gated recurrent units (GRUs). RNNs are ideally suited for time-series data processing, of- ten outperforming feed-forward networks in extracting mean- ingful features and achieving higher predictive accuracy [4]. Unlike feed-forward architectures, RNNs compute hidden states from a sequence of inputs using recurrent or feedback connections, making their computational graphs inherently sequential. Figure 1 contrasts the information flows in feed- forward networks compared to RNNs. This capability makes them particularly well-suited for time-series applications, such as energy consumption forecasting in power grids and stock price prediction in high-frequency trading (HFT). Such appli- cations often demand an energy-efficient, low-latency acceler- ation of LSTM models on platforms such as FPGAs, where custom operators, precisions and sparsity can be exploited during the design and implementation phase. However, the recurrent dataflow limits the ability of existing frameworks to support efficient RNN deployments.",
    "source": "2506.20810v1_FINN-GL_Generalized_Mixed-Precision_Extensions_for.pdf",
    "length": 2263,
    "tokens": 474
  },
  {
    "text": "In Kraken, containers are provisioned in advance by the Proactive Weighted Scaler (PWS) 2 to serve these incoming requests by avoiding cold starts. To achieve this, the PWS 2 first fetches relevant system metrics (using a monitoring tool 3 and orchestrator logs). These metrics, in addition to a developer-provided DAG Descriptor 4 , are then used by the Weight Estimation module 2a of PWS 2 to assign weights to functions on the basis of their invocation probabil- ities. Commonality and Connectivity (parameters in 2a ) are additional parameters used in weight estimation to account for critical and common functions. Additionally, a Load Pre- dictor module 2b makes use of the system metrics to predict 1Kraken is a legendary sea monster with tentacles akin to multiple paths chains in a Serverless DAG. Containers Request Queue Function 1 Function 2 Function n . . . REPLICA TRACKER LOAD MONITOR OVERLOAD DETECTOR FUNCTION IDLER PROACTIVE WEIGHTED SCALER REACTIVE SCALER WEIGHT ESTIMATOR LOAD PREDICTOR Dev-Provided DAG Descriptor Scrape Metrics APPLICATIONS DECISION SCALE 2a 2 7 7a 7b 1 3 2b 4 5 3a 3b 6 PROBABILITY CONNECTIVITY COMMONALITY KRAKEN Figure 6: High-level View of Kraken Architecture incoming load and uses this in conjunction with the calcu- lated function weights to determine the number of function containers to be spawned by the underlying resource orches- trator 6 . However, only a fraction of these containers are actually spawned, as determined by the function s batch size. The batch size denotes the number of requests per function each container can simultaneously serve without exceed- ing the SLO. In order to effectively handle mis-predictions in load, Kraken also employs a Reactive Scaler (RS) 7 that consists of two major components. First, is an Overload De- tector 7a that keeps track of request overloading at functions by monitoring queuing delays at containers. Subsequently, it triggers container scaling 6 by calculating the additional containers needed to mitigate the delay. Second, a Function Idler component 7b evicts containers from memory 6 when an excess is detected.",
    "source": "kraken.pdf",
    "length": 2118,
    "tokens": 478
  },
  {
    "text": "Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply. W1 W4 W7 W2 W5 W8 W3 W6 W9 A3 A2 A1 A5 A4 0 A7 0 0 W1A1 W4 W7 W2 W5 W8 W3 W6 W9 A4 A3 A2 A6 A5 A4 A8 A7 0 C3 C2 C1 C5 C4 B6 C7 B9 B8 D1 C3 C2 C6 C5 C4 C8 C7 B9 t 0 t 1 t 7 t 6 W1 W4 W7 W8 W5 Placing I-Txn-Var in the SA Grid Tile Level Acclerator Level Global Scratchpad Banked 1 - 4 T12 T13 T14 T15 T8 T9 T10 T11 T4 T5 T6 T7 T0 T1 T2 T3 Weights Output Stream Banked 1 - 4 Input Stream Input Stream Input Stream Input Stream Banked 1 - 4 T12 T13 T14 T15 T8 T9 T10 T11 T4 T5 T6 T7 T0 T1 T2 T3 Weights Output Stream Banked 1 - 1 Input Stream Input Stream Input Stream Input Stream Global Scratchpad Banked 1 - 1 Banked 1 - 1 Power Level -1 Power Level -2 Power Level -3 Tile n W1 W4 W7 X2 W5 W8 X3 X6 X9 A3 A2 A1 A5 A4 0 A7 0 0 X1 X4 X7 X8 X5 W1 W4 W7 W2 W5 W8 W3 W6 W8 A3 A2 A1 A5 A4 0 A7 0 0 W1 W4 W7 W8 W5 Tile 2 W1 W4 W7 V2 W5 W8 V3 V6 V9 A3 A2 A1 A5 A4 0 A7 0 0 V1 V4 V7 V8 V5 Tile 1 A3 A2 A1 I-Txn-00 A6 A5 A4 I-Txn-01 A9 A8 A7 I-Txn-02 Tile 0 W1 W4 W7 U2 W5 W8 U3 U6 U9 A3 A2 A1 A5 A4 0 A7 0 0 U1 U4 U7 U8 U5 Input Stream Global Scratchpad Banked 1 - 16 Banked 1 - 16 T12 T13 T14 T15 T8 T9 T10 T11 T4 T5 T6 T7 T0 T1 T2 T3 Weights Output Stream Banked 1 - 16 Input Stream Input Stream Input Stream Input Stream C1 C2 C3 C4 A5 A6 C7 A8 A9 B1 B2 B3 B4 A5 A6 B7 A8 A9 A1 A2 A3 A4 A5 A6 A7 A8 A9 W1 W2 W3 W4 W5 W6 W7 W8 W9 Weight Stationary Convolution Toy Example PE Level Fig.",
    "source": "Usas.pdf",
    "length": 1459,
    "tokens": 669
  },
  {
    "text": "Association for Computing Machinery. ISBN 9781450348928. doi: 10.1145 3079856.3080246. Norman P. Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing, Brian Towles, Cliff Young, Xiang Zhou, Zongwei Zhou, and David Patterson. Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings, 2023. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. 01.08361. Hyoukjun Kwon, Prasanth Chatarasi, Michael Pellauer, Angshuman Parashar, Vivek Sarkar, and Tushar Krishna. Understanding Reuse, Performance, and Hardware Cost of DNN Dataflow: A Data-Centric Approach. In Proceedings of the 52nd Annual IEEE ACM International Symposium on Microarchitecture, MICRO 52, page 754 768, New York, NY, USA, 2019. Association for Computing Machinery. Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700, 2019. LAION-AI. Clip benchmark, 2022. Baolin Li, Yankai Jiang, Vijay Gadepally, and Devesh Tiwari. Sprout: Green generative AI with carbon-efficient LLM inference. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 21799 21813, Miami, Florida, USA, 13 November 2024. Association for Computational Linguistics. doi: 10.18653 v1 2024.emnlp-main.1215. https: aclanthology.org 2024.emnlp-main.1215 .",
    "source": "2505.01386v2_Carbon_Aware_Transformers_Through_Joint_Model-Hard.pdf",
    "length": 1645,
    "tokens": 473
  },
  {
    "text": "Align: Open-source analog layout automation from the ground up. In Proceedings of the 56th Annual Design Automation Conference 2019, pp. 1 4, 2019. Yao Lai, Sungyoung Lee, Guojin Chen, Souradip Poddar, Mengkang Hu, David Z Pan, and Ping Luo. Analogcoder: Analog circuit design via training-free code generation. arXiv preprint arXiv:2405.14918, 2024. Mingjie Liu, Teodor-Dumitru Ene, Robert Kirby, Chris Cheng, Nathaniel Pinckney, Rongjian Liang, Jonah Alben, Himyanshu Anand, Sanmitra Banerjee, Ismet Bayraktaroglu, et al. Chipnemo: Domain-adapted llms for chip design. arXiv preprint arXiv:2311.00176, 2023a. Mingjie Liu, Nathaniel Pinckney, Brucek Khailany, and Haoxing Ren. Verilogeval: Evaluating large language models for verilog code generation. In 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD), pp. 1 8. IEEE, 2023b. Jialin Lu, Liangbo Lei, Jiangli Huang, Fan Yang, Li Shang, and Xuan Zeng. Automatic op-amp gen- eration from specification to layout. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2023. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language under- standing by generative pre-training. OpenAI blog, 2018. Behzad Razavi. Design of Analog CMOS Integrated Circuits. McGraw-Hill, Inc., USA, 1 edition, 2000. ISBN 0072380322. Behzad Razavi and Razavi Behzad. RF microelectronics, volume 2. Prentice hall New York, 2012. Zhuofu Tao, Yichen Shi, Yiru Huo, Rui Ye, Zonghang Li, Li Huang, Chen Wu, Na Bai, Zhiping Yu, Ting-Jung Lin, et al. Amsnet: Netlist dataset for ams circuits.",
    "source": "2503.00205v1_AnalogGenie_A_Generative_Engine_for_Automatic_Disc.pdf",
    "length": 1590,
    "tokens": 485
  },
  {
    "text": "FuncGNN: Learning Functional Semantics of Logic Circuits with Graph Neural Networks 3:13 defined in Equation 15: ùê¥ùë£ùëî.ùê∏ùëüùëüùëúùëüùëÜùëÉùëÉ 1 ùëÅ ùë£ V ùë¶ùë£ ÀÜùë¶ùë£ (15) where ùëÅis the total number of nodes, ùëâdenotes the set of all nodes in the AIG, ùë¶ùë£represents the simulated signal probability for node ùë£, and ÀÜùë¶ùë£is the predicted signal probability. For the TTDP task, the MAE measures the difference between the predicted and actual truth- table distances for sampled node pairs, capturing functional equivalence across AIGs with varying AND-to-NOT gate ratios. The computation involves calculating the normalized Hamming distance between truth tables and the distance between embedding vectors, followed by zero-normalization to align scales, as specified in Equations 16 to 19: ùê∑ùëá (ùëñ,ùëó) ùêªùëéùëöùëöùëñùëõùëîùê∑ùëñùë†ùë°ùëéùëõùëêùëí(ùëáùëñ,ùëáùëó) ùëôùëíùëõùëîùë°‚Ñé(ùëáùëñ) , (ùëñ, ùëó) N (16) ùê∑ùëç (ùëñ,ùëó) 1 ùê∂ùëúùë†ùëñùëõùëíùëÜùëñùëöùëñùëôùëéùëüùëñùë°ùë¶(ùëßùëñ,ùëßùëó) (17) ùê∑ùëá (ùëñ,ùëó) ùëçùëíùëüùëúùëÅùëúùëüùëö(ùê∑ùëá (ùëñ,ùëó)), ùê∑ùëç (ùëñ,ùëó) ùëçùëíùëüùëúùëÅùëúùëüùëö(ùê∑ùëç (ùëñ,ùëó)) (18) where ùëÅis the set of sampled node pairs, ùëáùëñand ùëáùëóare the truth-table vectors for nodes ùëñ and ùëó, ùêªùëéùëöùëöùëñùëõùëîùê∑ùëñùë†ùë°ùëéùëõùëêùëí(ùëáùëñ,ùëáùëó) computes the number of differing bits between ùëáùëñand ùëáùëó, and ùëôùëíùëõùëîùë°‚Ñé(ùëáùëñ) is the length of the truth table.",
    "source": "2506.06787v1_FuncGNN_Learning_Functional_Semantics_of_Logic_Cir.pdf",
    "length": 1142,
    "tokens": 724
  },
  {
    "text": "Since the byte leakage is a more direct signal of the vulnera- bility, we assign a higher weight to it in the reward function. If an exception is detected at this stage, the environment resets to a safe state, logs the exception. Only the byte leakage part of the reward is set as zero, yet the bad speculation part is calculated as usual. 6 Experiments Experiment Setup. We run the experiments on two systems with different ¬µArch . The first system has an Intel Core i9 7900X CPU with a Skylake-X ¬µArch . The OS running on the system is Ubuntu 22.04.5 LTS with the Linux v6.5.0 44-generic. We use glib v2.72.4, nasm v2.15.05, and gcc v11.4.0 for compiling and testing the generated assembly files; PyTorch v2.2.1, Stable Baselines3 v2.2.1 and Gymna- sium v0.29.1 for custom RL environment and training the RL agent. The RL agent training and the local inference for the text embedding model are done on the GPU clusters with an NVIDIA TITAN Xp, GeForce GTX TITAN X, and two GeForce GTX 1080Ti. The overview of the experiment setup used in the first system is illustrated in Figure 3. The second system has an Intel i9-14900K CPU with a Raptor Lake ¬µArch , Ubuntu 24.04.1 LTS with Linux v6.8.0- 51-generic, glib v2.80.0, nasm v2.16.01, gcc v13.3.0. Py- torch v2.4.1 cu121, Stable Baselines3 v2.3.2, and Gymna- sium v0.29.1 are used for the same purposes as the first sys- tem. The RL agent training is done on NVIDIA GeForce RTX 4090 GPU.",
    "source": "2502.14307v1_ŒºRL_Discovering_Transient_Execution_Vulnerabilitie.pdf",
    "length": 1438,
    "tokens": 410
  },
  {
    "text": "Additionally, InTAR handles reconfiguration at the task level, enabling rapid and convenient development with high-level tools like HLS. This makes InTAR well-suited for FPGAs to adapt quickly to new DNN applications. Lastly, InTAR s reconfiguration scope is not limited to specific computations. Table II compares InTAR with prior DNN accelerator works. Fig. 2. Left: Architecture template of INTAR. Compute cores (CC) compute linear operations (e.g., GEMM, ConvNet), and SFUs compute non-linear operations (e.g., softmax, GeLU). Each CC contains a scratchpad memory, a reconfigurable MAC unit array, a reduction unit, and a data movement control unit. Dashed lines indicate the candidates for connection between CCs and SFUs. Right: example architectures for each execution mode within the template, with n, m 2. For a schedule, we will compose the architectures of the required modes to keep only the necessary interconnects and logic. TABLE II COMPARISON BETWEEN INTAR AND OTHER DNN ACCELERATION WORKS. THE MAJOR REASON FOR HAVING IDLE COMPUTE RESOURCES IS DESCRIBED IN THE PARENTHESES FOR EACH WORK. Prior works Platform Contribution Type Accelerator Inter-task Idle Compute Intermediate When Reconfig Reconfiguration Model-spec. Category Schedule Resource Exist? Data Movement Determined Design Scope Design Opt. Allo [6] FPGA Domain-Spec. Lang. Dataflow Yes (dataflow execution) On-chip only - - FQ-BERT [10], DFX [23] FPGA Accelerator Sequential No On- Off-chip - - SSR [13] FPGA Accelerator Hybrid Yes (PE specialization) On-chip only - - FPCA [18], OverGen [20] FPGA Architecture Reconfigurable Yes (single DNN) On- Off-chip After circuit generation General FEATHER [30] FPGA Accelerator Reconfigurable No On- Off-chip After circuit generation Reduction Network SET [17] ASIC Scheduler Reconfigurable Yes (sub-opt. sched.) On- Off-chip N A Tiled Accelerator INTAR FPGA Design Paradigm Reconfigurable No On-chip only Circuit design time General A.",
    "source": "2502.08807v2_InTAR_Inter-Task_Auto-Reconfigurable_Accelerator_D.pdf",
    "length": 1956,
    "tokens": 500
  },
  {
    "text": "Finally, both frameworks share the broader goal of providing a parameterisable hardware layer to streamline LSTM deploy- ment. However, while our approach prioritises optimisation for constrained FPGA devices by balancing performance with efficient resource utilisation, the hls4ml approach prioritises latency minimisation at the expense of high resource usage. V. CONCLUSION AND FUTURE WORK In this paper, we extend the FINN framework by developing support for the generalized deployment of recurrent neural networks (using LSTMs as a working example) on FPGAs. We create a complete end-to-end framework and introduce the Scan operator from ONNX to enable modelling of mixed- precision quantised LSTM layers within FINN. Additionally, we design and modify FINN compiler transformations to optimise its compute graph and efficiently map operations to hardware blocks available in the FINN backend, while sup- porting arbitrary integer datatypes, dimensions and sequence lengths. We further present a case study for stock prediction using a model architecture that uses quantised LSTM layers with convolutional and dense layers (quantised ConvLSTM model). The quantised ConvLSTM model compiled to an accelerator IP core that can be deployed on FPGAs using our extended FINN flow. We show that our flow can generate a resource-efficient hardware accelerator IP of the model that meets the performance requirements even when mapped on an edge-class hybrid FPGA (Zynq Ultrascale XCZU7EV), and offers competitive inference performance when evaluated using the FI-2010 dataset for mid-price stock prediction. In the future, we aim to explore optimisations to reduce the latency introduced by the sequential layers in the LSTM flow, taking inspiration from the hls4ml approach (using non- static implementations for LSTM layers), and to extend the deployment pipeline to support other RNN models such as gated recurrent units (GRUs). VI. ACKNOWLEDGEMENTS I would like to thank Alessandro Pappalardo and Dr. Ya- man Umuroglu for their valuable insights and the engaging brainstorming sessions throughout the course of this project. REFERENCES [1] Z. Zhang, S. Zohren, and S. Roberts, Deeplob: Deep convolutional neural networks for limit order books, IEEE Transactions on Signal Processing, vol. 67, no. 11, pp.",
    "source": "2506.20810v1_FINN-GL_Generalized_Mixed-Precision_Extensions_for.pdf",
    "length": 2304,
    "tokens": 499
  },
  {
    "text": "The expected outcome is 100 correct classification. The results show that our equivalence checker incorrectly flags 2.7 of golden code as wrong, while the LLM(DeepSeek-V3)-generated testbench misclassifies 7.6 of cases. Our approach reduces error rate by approximately 64.5 compared with DeepSeek-V3, demonstrating significantly fewer false negatives. Fuzzing test for sequential circuits. Second, we perform a fuzzing test on sequential circuits by instructing DeepSeek-V3 to inject subtle errors into the golden code. The goal is to measure how effectively each testbench detects these mistakes. Our testbench identifies 65 of injected errors, whereas the DeepSeek-V3-generated testbench only detects 40 . This highlights a 62.5 relative improvement in error detection for our approach compared to DeepSeek-V3, which indicates that our method also has fewer false positives. 4 Related Work 4.1 Large Language Models for Reasoning The OpenAI-o1 [25] series is the first closed-source model to be trained with large-scale reinforcement learning to perform reasoning through CoT. Inspired by its powerful and effective reinforcement learning training paradigm, QwQ [34], DeepSeek-R1 [3], and Kimi k1.5 [32] have all adopted and improved upon its approach, achieving promising results. Limited by computational resources, open-source communities have actively explored low-cost approaches to replicate o1-like reasoning models. Some efforts have focused on distilling the powerful closed-source reasoning models [8, 22, 23, 33, 41]. while some open-source efforts have also explored training reasoning models using reinforcement learning [7, 10, 18, 20, 21, 26, 38, 44]. The main difference between CodeV-R1-7B and the aforementioned reasoning models lies in its focus on hardware description language code generation, which poses unique challenges due to verification 9 difficulty and limited data quality. In contrast, prior works primarily specialised in domains such as mathematics, which benefit from easily verifiable numerical outputs and rich open-source datasets. 4.2 Large Language Models for Verilog Code Generation With the development of large language models for code generation, specialised code generation models for hardware description languages also receive widespread attention.",
    "source": "2505.24183v2_CodeV-R1_Reasoning-Enhanced_Verilog_Generation.pdf",
    "length": 2296,
    "tokens": 471
  },
  {
    "text": "To analyze the benefits of using invocation frequency, we designed a probability-based policy that employs weighted container scaling. For the purposes of this experiment, we base our function weights only on invocation frequencies that are periodically calculated at the beginning of each scaling window. Figure 2 depicts the number of containers provisioned per function for three container provisioning policies subject to a Poisson arrival trace (ùúá 25 requests per second (rps)) for three applications. The static provision- ing policy is representative of current platforms [50] which spawn containers for functions in a workflow-agnostic fash- ion. Xanadu [27] represents the policy that scales containers only along the Most Likely Path (MLP), which is the request s expected path. If the request takes a different path, Xanadu provisions containers along the path actually taken, in a reactive fashion, and scales down the containers it provi- sioned along the MLP. Consequently, Xanadu, when subject to moderate heavy load, over-provisions containers by 32 compared to the Probability-based policy (from Figure 2) as a result of being locked into provisioning containers for the MLP until it is able to recalculate it. Our probability-based policy, on the other hand, provisions containers for func- tions along every possible path in proportion to their assigned weights. Note that variability in application usage patterns can lead to changes in function probabilities within each DDA, which the policy will have to account for. Challenge 2: Adaptive Container Provisioning. While probability-based container provisioning can significantly reduce the number of containers, the presence of container cold-starts leads to SLO violations (requests not meeting their expected response latency). This is because cold starts can take up a significant proportion of a function s response time (up to 10s of seconds [13, 14]). A significant amount of research [18, 22, 24, 38, 39, 43, 52] has been focused to- wards reducing cold-start overheads (in particular, proactive container provisioning [3, 32, 44, 46]). However, in the case of DDAs, DBPs make it unclear as to how many containers should be provisioned in advance for the functions along each path in the DAG. We identify two interlinked factors, in the context of DDAs, that need to be accounted for when making container scaling decisions.",
    "source": "kraken.pdf",
    "length": 2403,
    "tokens": 491
  },
  {
    "text": "These numbers determine the size of the action space for the RL agent. In the Raptor Lake system, 3996 instructions that be- long to 72 sets were left after filtering. The largest set has 468 instructions, and the maximum number of operands per instruction is 7. For the agent training, we enable all available kernel mitiga- tions against CPU vulnerabilities. Cumulatively, we collected 27 days worth of data from Skylake-X and 20 days worth of data from Raptor Lake. In the longest runs, we trained an RL agent for 4.5 days and 10.5 days for Skylake-X and Raptor Lake, respectively. In total, the cost for embedding model inference through API calls was 2.46 USD. During the RL agent training, we observe that the average reward of each episode increases over time, as shown in Fig- ure 4. The steady increase in reward indicates that the RL agent was able to successfully explore the x86 instruction Figure 4: The increase of the average reward per episode during the 10 days of agent training in Raptor Lake. An episode corresponds to the largest instruction sequence the agent can generate from scratch. The darker line shows the running mean. Figure 5: The increase of the average length of generated assembly sequences during the 10 days of agent training in Raptor Lake. The darker line shows the running mean. set while exploiting the knowledge we provide through the reward signal. At every episode, the agent starts building an instruction sequence from scratch and keeps adding a new instruction until the instruction limit is reached or the sequence gives an exception with the last added instruction. Over time, we observe that the average length of an instruction sequence dur- ing an episode increases during the agent training, as shown in Figure 5. Increasing average length over time indicates the agent was able to learn to select combinations of instructions to avoid exceptions. In our experiments, the maximum se- quence length was chosen as 10 assembly instructions, after which the agent starts building a new sequence. 7 Discovered Transient Execution Vulnerabili- ties When the agent builds an instruction sequence that results in observable transient execution, it is saved for manual analysis with performance counter and data leakage information. Fig- 8 Figure 6: Visualization generated instruction sequences across time, with higher byte leakage marked with lighter colors. Note that, the discovered transient execution mechanisms stand out as it has higher reward compared to the remaining ones.",
    "source": "2502.14307v1_ŒºRL_Discovering_Transient_Execution_Vulnerabilitie.pdf",
    "length": 2528,
    "tokens": 491
  },
  {
    "text": "To balance model ac- curacy and hardware efficiency, N:M semi-structured pruning [17], [18], where N out of every M elements are pruned, is commonly adopted in prevalent LLMs [16], [40]. For example, 3 SparseGPT [17] effectively prunes GPT-family models [3] to achieve 2:4 and 4:8 sparsity in a one-shot manner without any retraining. Moreover, Wanda [18] leverages the product of weights and input activations to achieve 2:4 semi-structured pruning, demonstrating improved perplexity in certain cases, such as Llama-2-13B [4]. B. Quantization for LLMs Quantization is a pivotal compression technique that con- verts floating-point values into discrete integers, thus enhanc- ing LLMs efficiency at the operator level. It is typically categorized into two approaches: quantization-aware train- ing (QAT) and post-training quantization (PTQ). QAT [41] [43] typically achieves higher quantization accuracy by fine- tuning the entire model using full training data, leading to substantial computational costs. In contrast, PTQ [20], [44] relies on only a small dataset for calibration, making it a more feasible solution for LLM quantization. For example, GPTQ [44] introduces a one-shot weight quantization method using approximate second-order information, enabling the fast quantization of weights within GPT OPT models to 3 or 4-bit with negligible accuracy degradation. To facilitate both weight and activation quantization, SmoothQuant [20] employs equivalent mathematical transformations to shift the quantization complexity from activations to weights, allowing both to be quantized to 8-bit. More recently, QServe [19] integrates innovative progressive group quantization, smooth attention, and activation-aware channel reordering to achieve more aggressive quantization in a W4A8KV4 configuration (4-bit weights, 8-bit activations, and 4-bit KV cache). While these approaches demonstrate promising results in compressing LLMs while preserving performance, the resid- ual computational and memory demands remain impractical for deployment on resource-constrained edge devices. This highlights the need for more aggressive model compression methods that combine orthogonal techniques, such as quanti- zation and pruning, to produce even more compact LLMs.",
    "source": "2505.03745v1_AccLLM_Accelerating_Long-Context_LLM_Inference_Via.pdf",
    "length": 2260,
    "tokens": 500
  },
  {
    "text": "Define the energy budget Eb for a single quanta and for the entire inference. Initialize the loop iteration parameters l. Compute the activations a and apply the dropout mask: mi œÉ(zi) adropout i ai mi Compute the loss L(Y, ÀÜY). Calculate the gradients of the loss with respect to the weights and dropout mask parameters: L Wij , L zi For each layer L and loop i within the layer, estimate the energy Ei required for the current quanta size li: Ei DynAgent.estimateEnergy(L, i, li) If Ei Eb, fuse tasks to reduce the overhead: FuseTasks(L, i, li, Eb) Update Ei after task fusion: Ei DynAgent.estimateEnergy(L, i, li) Update the dropout mask parameters z based on the gradients: zi zi Œ∑ L zi Perform the backward pass to update the network weights, considering the dropout mask: W W Œ∑ L W m 20 where Œ∑ is the learning rate and denotes element-wise multiplication. Inference with Learning Sparse Masks Dropout and QuantaTask Optimization: Check the available energy using DynAgent. If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget. Otherwise, maintain or reduce the dropout rate to improve accuracy. Perform the forward pass with the updated dropout mask to obtain the output Y. This approach ensures that the network is robust to varying energy conditions by incorporating dynamic dropout with learnable mask parameters, along with the QuantaTask optimization to handle energy constraints. D.5 Neuron Shapley Value Dropout with QuantaTask Optimization Neuron Shapley Value Dropout applies the concept of Shapley values from game theory (Aas et al., 2021) to assess neuron importance for dropout, combined with the QuantaTask optimization to handle energy constraints in intermittent systems. Mathematical Formulation: The Shapley value œïi of neuron i is a measure of its contribution to the overall network performance. It is calculated by considering all possible subsets of neurons and computing the marginal contribution of neuron i to the network s output: œïi 1 N ! X S N\\{i} S ! ( N S 1)!",
    "source": "NexUME.pdf",
    "length": 2067,
    "tokens": 486
  },
  {
    "text": "We will make our code and dataset public on Github in the future. 7 ACKNOWLEDGEMENTS This work was partially supported by NSF Award 2416375. We also thank Professor Lei Li of Carnegie Mellon University for valuable discussions on graph generation. REFERENCES Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale- man, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Phillip E Allen and Douglas R Holberg. CMOS analog circuit design. Elsevier, 2011. Norman Biggs, E Keith Lloyd, and Robin J Wilson. Graph Theory, 1736-1936. Oxford University Press, 1986. Jason Blocklove, Siddharth Garg, Ramesh Karri, and Hammond Pearce. Chip-chat: Challenges and opportunities in conversational hardware design. In 2023 ACM IEEE 5th Workshop on Machine Learning for CAD (MLCAD), pp. 1 6. IEEE, 2023. Hans Camenzind. Designing analog chips. Virtualbookworm Publishing, 2005. Weidong Cao, Mouhacine Benosman, Xuan Zhang, and Rui Ma. Domain knowledge-infused deep learning for automated analog radio-frequency circuit parameter optimization. In Proceedings of the 59th ACM IEEE Design Automation Conference, pp. 1015 1020, 2022. Weidong Cao, Jian Gao, Tianrui Ma, Rui Ma, Mouhacine Benosman, and Xuan Zhang. Rose-opt: Robust and efficient analog circuit parameter optimization with knowledge-infused reinforcement learning. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2024. Chen-Chia Chang, Yikang Shan, Shaoze Fan, Jing Li, Shun Zhang, Ningyuan Cao, Yiran Chen, and Xin Zhang. Lamagic: Language-model-based topology generation for analog integrated circuits. arXiv preprint arXiv:2407.18269, 2024.",
    "source": "2503.00205v1_AnalogGenie_A_Generative_Engine_for_Automatic_Disc.pdf",
    "length": 1737,
    "tokens": 465
  },
  {
    "text": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. arXiv:2211.10438v7. Yu, X., Dai, F.F., Irwin, J.D., Jaeger, R.C. (2008). A 12 GHz 1.9 W Direct Digital Synthesizer MMIC Implemented in 0.18 Œºm SiGe BiCMOS Technology. IEEE Journal of Solid-State Circuits, 43, 1384-1393. Zhang, Q., Zhao, W., Seabaugh, A. (2006). Low- subthreshold-swing tunnel transistors. IEEE Electron Device Letters, 27(4), 297-300.Ma, S., Wang, H., Li, H., Wei, F. (2024). The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits. arXiv:2402.17764v1 [cs.CL]",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 576,
    "tokens": 202
  },
  {
    "text": "TABLE I: Comparison of GenEDA with representative categories of circuit foundation models. Existing circuit encoders mainly leverage graph structure for prediction tasks, while circuit decoders focus on semantic text for generation tasks. GenEDA bridges the gap between these two widely explored branches by aligning encoders and decoders within a shared latent space. After alignment, GenEDA supports more challenging functionality generation tasks. netlist functional reasoning tasks are highly valuable in multiple aspects: (1) Practical applications: Reasoning high-level function- ality from bit-blasted netlists can support critical applications such as functional verification, datapath optimization, and malicious logic detection. (2) Unprecedented reasoning quality: These tasks shift from traditional gate-level structural analysis to generating the overall functionality of entire circuits, including specification and RTL code, offering a significant leap in reasoning quality. (3) Benchmarking model capability: Our proposed tasks introduce new benchmarks for evaluating the generative capabilities and netlist understanding of foundation models. Since these tasks generate human-readable circuit information, they help enhance the interpretability of circuit models. The contributions of this paper are summarized as follows: Aligned circuit encoder-decoder framework. We propose the first framework that aligns pre-trained circuit encoders with LLM decoders for generative tasks. It supports both trainable and frozen decoders through two alignment paradigms. Generative netlist foundation model. Building on this frame- work, we develop the first generative foundation model for netlists. By integrating structural and functional insights from netlist encoders, the model enhances LLM-based generation and enables reasoning over low-level, bit-blasted netlists. New netlist generative reasoning tasks and benchmarks. We introduce three novel generative functional reasoning tasks on netlists, advancing beyond prior gate-type predictions. We also release corresponding benchmarks to encourage follow-up research on these tasks. Boosting SOTA LLMs performance. Experimental results validate that GenEDA significantly boosts the performance of cutting-edge LLMs on all three new functional reasoning tasks after alignment with the pre-trained netlist encoder. II. RELATED WORK A. Method Related: Circuit Foundation Model Recent advances in foundation AI for EDA have enabled strong generalization and generation capabilities through the pretrain- finetune process.",
    "source": "2504.09485v1_GenEDA_Unleashing_Generative_Reasoning_on_Netlist_.pdf",
    "length": 2578,
    "tokens": 484
  },
  {
    "text": "should be explicitly cleared so that downstream logic can rely on deterministic, intentionally driven zeros. The golden solution makes that intent explicit with {{4 b0}, ...} concatenations. B Quality Filtering Automatic quality filtering proceeds in two stages. The first stage applies sanity checks to the test harness: it must pass with the reference solution and fail with the initial context. The former ensures consistency between the test harness and the reference solution, while the latter confirms that a correct solution is required and the initial context does not already satisfy the task. Of the 1,313 initial datapoints, 78 were excluded due to failing these sanity checks. The second stage of quality filtering uses LLM-based judging with four metrics: ambiguity, consis- tency, category match, and behavioral match (Figure 6). The prompt used for this LLM quality judge is shown in Listing 1. It also includes fields for prompt refinement, enabling automated revisions of ambiguous or incorrect prompts; however, we do not report results from that experiment in this work, as further vetting is needed to ensure such revisions do not result in overly descriptive or trivial prompts. When running CVDP in map mode with the LLM judge, the output is a scored JSONL file with additional metadata fields. Post-processing scripts then combine the four metrics into an aggregate score and remove low-scoring problems. For this work, we used a threshold of 8.0 for a passing score. The final filtered JSONL dataset excludes the scoring fields. The number of problems filtered per category is shown in Table 6, an expanded version of Table 1. Code Improvement (cid07) and Debugging (cid16) saw the most filtering, while Code Completion (cid02) saw the least. Pass rate changes resulting from quality filtering are shown in Table 7. Claude 3.7 showed a 10 14 point increase in pass rate for Code Improvement and Debugging, as did Spec- to-RTL (cid03) and Code Modification (cid04). Interestingly, Testbench and Assertion Generation (cid12 14) saw little improvement despite aggressive filtering, as shown in Table 6. This supports our findings in Section 4 that existing LLMs struggle fundamentally with generating correct and accurate SystemVerilog testbench code. You are an expert at refining code challenge datapoints. Analyze the provided datapoint and improve it , focusing ONLY on enhancing the prompt field.",
    "source": "2506.14074v1_Comprehensive_Verilog_Design_Problems_A_Next-Gener.pdf",
    "length": 2422,
    "tokens": 498
  },
  {
    "text": "Qwen technical report. arXiv preprint arXiv:2309.16609 (2023). [4] Batten, C., Pinckney, N., Liu, M., Ren, H., and Khailany, B. PyHDL-Eval: An LLM evaluation framework for hardware design using python-embedded dsls. In Proceedings of the 2024 ACM IEEE International Symposium on Machine Learning for CAD (2024), pp. 1 17. [5] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in Neural Information Processing Systems 33 (2020), 1877 1901. [6] Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021). [7] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. PaLM: Scaling language modeling with pathways. Journal of Machine Learning Research 24, 240 (2023), 1 113. [8] Devlin, J. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [9] Fried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F., Zhong, R., Yih, W.-t., Zettlemoyer, L., and Lewis, M. Incoder: A generative model for code infilling and synthesis. arXiv preprint arXiv:2204.05999 (2022).",
    "source": "2503.08823v2_ResBench_Benchmarking_LLM-Generated_FPGA_Designs_w.pdf",
    "length": 1403,
    "tokens": 481
  },
  {
    "text": "Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911 (Nov 2023). [59] Ruidong Zhu, Ziheng Jiang, Chao Jin, Peng Wu, Cesar A. Stuardo, Dongyang Wang, Xinlei Zhang, Huaping Zhou, Haoran Wei, Yang Cheng, Jianzhe Xiao, Xinyi Zhang, Lingjun Liu, Haibin Lin, Li-Wen Chang, Jianxi Ye, Xiao Yu, Xuanzhe Liu, Xin Jin, and Xin Liu. 2025. MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism. arXiv preprint arXiv:2504.02263 (Apr 2025).",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 502,
    "tokens": 164
  },
  {
    "text": "While these applications have shown promising results, the use of LLMs to port efficient machine code from one machine to another, while maintaining efficiency, remains underexplored and largely unsolved. Assembly languages present unique challenges due to their under-representation in training datasets, lack of human readability, extensive length, and fundamental differences in execution models across architectures. Guess Sketch (Lee et al., 2024) introduced a neurosymbolic approach combining language models with symbolic reasoning for translating assembly code between ARMv8 and RISC-V architectures. In our work, we extend the neural transpiliation direction with a focus on leveraging the existing efficiency in x86 programs to transpile into efficient ARM binaries, bridging architectural differences in ISA complexity and execution mod- els. Further, instead of fixing transpilations with symbolic approaches, as done in Guess Sketch, we focus on upfront data design and modeling methods to flexibly handle the increased scale and complexity of CISC-to-RISC transpilation. 3 Guaranteed Guess In this section, we explore the two primary components of building our GG transpiler: data generation and model training. 3.1 Data Collection As shown in Figure 1, our training dataset is derived from AnghaBench(Da Silva et al., 2021) and The Stackv2(Kocetkov et al., 2022). AnghaBench is a comprehensive benchmark suite that contains 1 million compilable C C programs extracted from major public C C repositories on GitHub. The Stack is a 3.1TB dataset of permissively licensed code in 30 languages for training and evaluating code LLMs. From these datasets, we randomly sampled 1.01M programs (16.16B tokens) from AnghaBench and 306k programs (4.85B tokens) from the stack to form our training set, equivalent to 1.32M samples. After we collected the whole samples, we removed boilerplates, deduplicated the data, and choose file that were neither too short ( 10 lines) nor too long ( 16k lines). These programs were then compiled for x86 (CISC) ARMv8 ARMv5 RISC-V (RISC). Each program was compiled to both x86 (CISC) ARMv8 ARMv5 RISC-V (RISC) targets under two optimization levels: -O0 (no optimization) and -O2 (aggressive optimization).",
    "source": "2506.14606v1_Guaranteed_Guess_A_Language_Modeling_Approach_for_.pdf",
    "length": 2245,
    "tokens": 494
  },
  {
    "text": "5 CONCLUSION AND FUTURE WORK In this paper, we have introduced AnalogGenie, a generative engine built on a GPT model to generate diverse analog circuits by predicting the next device pin to connect within a circuit. AnalogGenie addresses two key gaps in the field: building a comprehensive dataset of analog circuit topology and developing a scalable sequence-based graph representation universal to analog circuits. Exper- imental results across three types of analog circuit benchmarks demonstrate that our method can efficiently discover previously unseen circuit topologies in a scalable manner. Given the expressive nature of our representation and the bottom-up generative approach, we believe that our method has broader applicability beyond analog circuit topology generation and can be generalized to digital design as well. Ultimately, we consider our work to pave the way for the integration of generative 9 Published as a conference paper at ICLR 2025 AI into IC design, fostering a mutually beneficial relationship where IC design enhances generative AI capabilities, while generative AI accelerates IC design advancements. Limitations and future work. AnalogGenie is a comprehensive framework that combines a domain-specific generative engine for discovering analog circuit topologies with a genetic algo- rithm for optimizing the parameters (e.g., sizing and bias) of the generated topologies. The primary focus of our work is on discovering topologies with a high likelihood of achieving a superior FoM once sized. While the current sizing algorithm is effective, its sample efficiency can be improved by exploring more advanced alternatives (Wang et al., 2020). Additionally, for digital circuit develop- ment, we will consider combining AnalogGenie s graph generation approach with code generation work to enhance its ability. 6 REPRODUCEBILITY STATEMENT The main theoretical backbone of our paper is Theorem 3.2.1. We have already shown its proof in Section 3.2. Furthermore, we discussed our experiment setup and implementation details in Sec- tion 4.1. We also provide open-source code in supplementary material, including data augmentation, pretraining, and finetuning. The genetic algorithm sizing framework and Ngpsice simulation infras- tructure are also provided in the supplementary material. For our open-sourced circuit dataset, we provide its statistics in Appendix A.1. We will make our code and dataset public on Github in the future. 7 ACKNOWLEDGEMENTS This work was partially supported by NSF Award 2416375.",
    "source": "2503.00205v1_AnalogGenie_A_Generative_Engine_for_Automatic_Disc.pdf",
    "length": 2541,
    "tokens": 500
  },
  {
    "text": "Therefore, retrofitting such classical machines with smart sensors will help in preventing such failures and will allow taking predictive measures to increase production efficiency. To understand the implications and benefits of retrofitting sensing into these classical machines, we conducted a case study on the data collected from a grinding machine which had three types of sensors one power sensor and two accelerom- eters. It also incorporated the tool parameters like speed, feed 4 0 0.5 1 1.5 2 2.5 3 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 Inference Time (ms) Variance Threshold Edge-Cloud devices 2 Edge-Cloud devices 4 Edge-Peer devices 2 Edge-Peer devices 4 (a) Inference time vs threshold 0 1 2 3 4 5 6 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 Average Devices Encountered Variance Threshold Edge-Cloud devices 2 Edge-Cloud devices 4 Edge-Peer devices 2 Edge-Peer devices 4 (b) Communication through (aver- age) number of devices vs Threshold 0.78 0.8 0.82 0.84 0.86 0.88 0.9 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 Correlation Variance Threshold Edge-Cloud devices 2 Edge-Cloud devices 4 Edge-Peer devices 2 Edge-Peer devices 4 (c) Correlation vs threshold 0 0.2 0.4 0.6 0.8 1 1.2 1.4 0.86 0.861 0.862 0.863 0.864 0.865 100 200 300 400 500 600 700 800 Inference Time (ms) Correlation Cloud Estimators Correlation Inference Time (d) Accuracy (correlation) and infer- ence time vs cloud estimators Fig. 4: Sensitivity study and depth-of-cut, and measured the surface roughness of the grinding surface. The goal of our study was to correctly predict the surface roughness from sensor data. We divided the data into multiple chunks to emulate a multi machine setup (varying from 2 to 5).",
    "source": "EdgeClourRF.pdf",
    "length": 1695,
    "tokens": 476
  },
  {
    "text": "In addition, he had co-organized a summer workshop for visually-impaired students as a part of their Expeditions project. As the department Head, PI Das has been closely associated with many such activities. PI Kandemir is an adviser co-adviser of 5 female students, and PI Zhang has been advising 1 female PhD student, 4 female undergraduate stu- dents for their honors thesis projects, and serving on PhD dissertation committee for 9 female PhD students including 1 African American student. We plan to share our BPC effort outcomes in our NSF project reports and at different forums such as the annual Big Ten Department Heads meeting as well as in Tapia and CRA-W conferences. Mentoring Plan This project will accommodate a total of 4 PhD students, as discussed in our Management and Coordi- nation Plan. The PIs will perform the following mentoring activities for these PhD students: Orientation and Expectation Setting The PIs will engage in in-depth conversations with PhD students, to set clear expectations, goals, and deliverables for the project period. After the initial meetings with the PhD students, the students will be asked to complete a worksheet to ensure alignment on objectives. Regular annual review meetings will be conducted to assess progress and make necessary adjustments. Career Counseling Advising The PIs will provide on a regular basis career counseling and advising to the PhD students in the project as part of the mentorship. The students will also have access to individual career counseling appointments with Penn State, who specialize in career and professional development. Additionally, the students will be encouraged to attend career and professional development workshops offered by Penn State. Training in Paper Writing The PIs will discuss regularly with the PhD students the best paper-writing practices, to ensure that they gain the first-hand experience in best practices. To expedite the process, where it makes sense, the PIs will team up the new students with the older ones in paper writing process. Publications and Presentations The PhD students will receive guidance and training in the preparation of manuscripts for scientific journals and presentations at conferences and workshops. They will have access to courses on Effective Communi- cation and Presentation Skills. Improving Skills The students will participate in regular research group meetings, where they will describe their work to colleagues and collaborate on solving research problems, fostering communication, programming skills, and other types of technical skills.",
    "source": "NSF_LLM_Medium_Proposal.pdf",
    "length": 2588,
    "tokens": 483
  },
  {
    "text": "2024. Opening the Black-Box: A Systematic Review on Explainable AI in Remote Sensing. arXiv:2402.13791 [cs] [51] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, and Hartwig Adam. 2019. Searching for MobileNetV3. arXiv:1905.02244 [52] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. Mo- bileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. arXiv:1704.04861 [cs] [53] CH Huang, WT Chen, YC Chang, and KT Wu. 2024. An Edge and Trustworthy AI UAV System With Self-Adaptivity and Hyperspectral Imaging for Air Quality Monitoring. IEEE INTERNET OF THINGS JOURNAL 11, 20 (Oct. 2024), 32572 32584. [54] Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, and Kurt Keutzer. 2016. SqueezeNet: AlexNet-level Accuracy with 50x Fewer Parameters and 0.5MB Model Size. arXiv:1602.07360 [55] Cosimo Ieracitano, Nadia Mammone, Fanny Spagnolo, Fabio Frustaci, Stefania Perri, Pasquale Corsonello, and Francesco C. Morabito. 2024. An Explainable Embedded Neural System for On-Board Ship Detection from Optical Satellite Imagery. ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE 133, 108517 (July 2024). [56] Intel. 2016. Intel Movidius Myriad 2 Vision Processing Unit 1GB - Product Specifications. [57] ISPRS. 2022. 2D Semantic Labeling Contest - Potsdam.",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 1484,
    "tokens": 464
  },
  {
    "text": "The complete computation is divided into 16 segments. Loop unrolling by a factor of 4 is applied to the segments loop. ------------------------------------------------------------------------- for (int seg 0; seg 16; seg 4) { for (int j_in_o 0; j_in_o 4; j_in_o ) { uint32_t j_off j_in_o tile_dim; uint32_t acc_addr; will hold the computed accumulator address for preload --- Unrolled segment \"seg\" ------------------------------------------------- { uint32_t block_id seg 4; uint32_t sub_offset (seg 4) tile_dim; uint32_t A_seg_addr current_A_buffer block_id (tile_dim 4) sub_offset; If this is the very first compute for this sub -tile , use overwrite mode if (first_compute[j_in_o ]) { acc_addr cur_acc_base j_off; first_compute[j_in_o] 0; } else { acc_addr (cur_acc_base j_off) 0x40000000; } preload(new_B_base seg (tile_dim 4) j_off , acc_addr , tile_dim , tile_dim , tile_dim , tile_dim); compute_preloaded(A_seg_addr , ( uint32_t)0, tile_dim , tile_dim , tile_dim , tile_dim); } --- Unrolled segment \"seg 1\" ------------------------------------------------- { uint32_t block_id (seg 1) 4; uint32_t sub_offset ((seg 1) 4) tile_dim; uint32_t A_seg_addr current_A_buffer block_id (tile_dim 4) sub_offset; preload(new_B_base (seg 1) (tile_dim 4) j_off , (cur_acc_base j_off) 0x40000000 , tile_dim , tile_dim , tile_dim , tile_dim); compute_preloaded(A_seg_addr , ( uint32_t)0, tile_dim , tile_dim , tile_dim , tile_dim); } --- Unrolled segment \"seg 2\" ------------------------------------------------- { uint32_t block_id (seg 2) 4; uint32_t sub_offset ((seg 2) 4) tile_dim; uint32_t A_seg_addr current_A_buffer block_id (tile_dim 4) sub_offset; preload(new_B_base (seg 2) (tile_dim 4) j_off , (cur_acc_base j_off) 0x40000000 , tile_dim , tile_dim , tile_dim , tile_dim); compute_preloaded(A_seg_addr , ( uint32_t)0, tile_dim , tile_dim , tile_dim , tile_dim); } Figure 20: Example from Fig.",
    "source": "2505.18574v2_Autocomp_LLM-Driven_Code_Optimization_for_Tensor_A.pdf",
    "length": 1894,
    "tokens": 656
  },
  {
    "text": "IEEE Trans. Computers, 70(4):595 605. 10 Joshi, V., Gallo, M. L., Boybat, I., Haefeli, S., Piveteau, C., Dazzi, M., Rajendran, B., Sebastian, A., and Eleftheriou, E. (2019). Accurate deep neural network inference using computational phase-change memory. CoRR, abs 1906.03138. Kadlecov√°, G., Lukasik, J., Pil√°t, M., Vidnerov√°, P., Safari, M., Neruda, R., and Hutter, F. (2024). Sur- prisingly strong performance prediction with neural graph features. In International Conference on Machine Learning, pages 22771 22816. PMLR. Krestinskaya, O., Salama, K., and James, A. P. (2020). Towards hardware optimal neural network selection with multi-objective genetic search. In 2020 IEEE International Symposium on Circuits and Systems (ISCAS), pages 1 5. IEEE. Krizhevsky, A. and Hinton, G. (2009). Learning multiple layers of features from tiny images. Technical Report 0, University of Toronto, Toronto, Ontario. Lammie, C., Xiang, W., Linares-Barranco, B., and Azghadi, M. R. (2022). Memtorch: An open-source simulation framework for memristive deep learning systems. Neurocomputing, 485:124 133. Loshchilov, I. and Hutter, F. (2017). SGDR: stochastic gradient descent with warm restarts. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. Rasch, M. J., Mackin, C., Gallo, M. L., Chen, A., Fasoli, A., Odermatt, F., Li, N., Nandakumar, S. R., Narayanan, P., Tsai, H., Burr, G. W., Sebastian, A., and Narayanan, V. (2023).",
    "source": "2506.18495v1_AnalogNAS-Bench_A_NAS_Benchmark_for_Analog_In-Memo.pdf",
    "length": 1518,
    "tokens": 474
  },
  {
    "text": "[12] Nicolas Weber, Florian Schmidt, Mathias Niepert, and Felipe Huici. BrainSlug: Transparent Acceleration of Deep Learning Through Depth- First Parallelism, April 2018. arXiv:1804.08378 [cs]. [13] Jonathan Binas and Yoshua Bengio. Low-memory convolutional neu- ral networks through incremental depth-first processing, May 2019. arXiv:1804.10727 [cs]. [14] Simone Scardapane, Michele Scarpiniti, Enzo Baccarelli, and Aurelio Uncini. Why Should We Add Early Exits to Neural Networks? Cognitive Computation, 12(5):954 966, September 2020. [15] Matteo Gambella, Jary Pomponi, Simone Scardapane, and Manuel Roveri. NACHOS: Neural Architecture Search for Hardware Con- strained Early Exit Neural Networks, January 2024. arXiv:2401.13330 [cs]. [16] NUCLEO-U575ZI - STM32 Nucleo-144 development board with STM32U575ZIT6Q MCU, SMPS, supports Arduino, ST Zio and mor- pho connectivity - STMicroelectronics.",
    "source": "2503.16939v1_On-Sensor_Convolutional_Neural_Networks_with_Early.pdf",
    "length": 898,
    "tokens": 268
  },
  {
    "text": "Even with a single set of constraints, the output of a constrained random test generator (and the behaviour of the DUV) can be varied by changing the random seed and initial state. Industrial test generators can have over 1000 constraints, making their configuration non-trivial. Machine learning can be used to parametrize constrained test generators (Section 7.5), therefore it is important to realise the potentially large feature space and the need to identify the relevant features (parameters) to control. Coverage-directed generation is a mature industry-standard approach, well-defined in the SystemVer- ilog language [2] and Universal Verification Methodology [1], used by approximately 70 of real-world projects [35]. Its advantages include the ability to generate tests for devices with many inputs, cover functionality in a balanced way, and quickly create many test cases [83]. However, it is inherently computationally inefficient due to its reliance on pseudo-random generation. The effectiveness of a param- eterisation to increase coverage decreases over time [45], and the approach can be ineffective for hitting specific coverage points (e.g., coverage holes) [69]. Compared to expert-written tests, Coverage-directed generated tests are often longer, less targeted, and use more simulation resources to achieve the same result [89]. One topic of research is to use machine learning to increase the efficiency of CDG and enable tighter control over its output. Coverage-Directed Test Selection is a variant of CDG where pre-existing tests are selected based on their potential to increase coverage. This approach is especially beneficial when tests are computationally cheap to generate but expensive to simulate, and it is a focus of ML research [69]. 3.4 The Verification Environment The typical dynamic-verification environment makes use of a testbench as shown in Figure 3. The stimuli source can be either expert-written instruction sequences or those generated by a constrained-random test generator. These stimuli are translated into inputs compatible with the Design Under Verification (DUV), which is then simulated, and its response is monitored. A reference model, or golden model, checks if the response aligns with the design specifications. Most research using machine learning methods interface with a variant of this environment, discussed in Section 7. Dynamic-based verification also uses a repository to store information necessary for replicating tests and results from previous runs.",
    "source": "2503.11687v1_Review_of_Machine_Learning_for_Micro-Electronic_De.pdf",
    "length": 2523,
    "tokens": 483
  },
  {
    "text": "3: Similarity and Correlation Comparisons Between BiT (with Regular Softmax) and SPS Attention. 3) SPS Thresholds Search and Model Training: Figure 2 illustrates our method and search strategy for determining the SPS thresholds. Suppose two attention maps, A1 and A2, are obtained from the same layer Layeri. We define the Channel Distortion Rate (CDR) as the mean squared error (MSE) between these two maps: Distortion 1 n i√ø n 1 pA1 A2q2 (5) The thresholds Œª P RhÀÜn are predefined within the range r0, 1s with a search granularity of 0.05 and an initial value of 0(regular sign function). Our objective is to identify the optimal thresholds that minimize the output discrepancy be- tween BiT s softmax-based attention (Eq. 2) and our SPS- based attention (Eq. 4) for a given attention layer. Formally, we solve: Œª arg min Œª pDistortion pAtt probBiT, Att probSPSpŒªqqq (6) Here, Œª represents the per-head threshold. To avoid overfitting to any specific task, we uniformly sample 10 of each dataset (benchmark) to construct a small calibration set for the search process. This process identifies a unique threshold for each attention head in every attention block and is highly efficient, requiring only about five minutes in our experiments. We further evaluate different threshold granularities per layer, per head, and per row of the attention map which will be discussed in the next section. Balancing effectiveness and efficiency, we adopt the head-wise threshold configuration in our final design. This decision is well-supported, as different attention heads often capture distinct relational patterns among tokens and contribute to diverse representations, thereby en- hancing the model s expressivity. Figure 3 presents multiple similarity and correlation mea- surements between BiT (with Softmax) and SPS attention, demonstrating the validity of our SPS approximation. Finally, to further enhance COBRA s performance, we fix the searched thresholds Œª and fine-tune the model weights using the original training data to compensate for the information loss introduced by the SPS approximation.",
    "source": "2504.16269v2_COBRA_Algorithm-Architecture_Co-optimized_Binary_T.pdf",
    "length": 2100,
    "tokens": 468
  },
  {
    "text": "Minimizing TPOT latency for MoE models requires fine-grained expert parallelism, with each expert placed on a dedicated NPU die. In the DeepSeek- R1 model, 256 router experts are deployed, making large-scale expert parallelism (LEP) a core requirement. However, implementing LEP is non-trivial due to sequential dependencies in token processing and the significant communication overhead incurred when coordinating hundreds of NPU dies. To address these challenges, we introduce a set of hardware-aware optimization techniques tailored to the CloudMatrix384. First, we present our fused communication operator design that exploits the UB plane for low-latency, high-throughput MoE execution ( 4.2.1). Next, we detail our custom MLA implementation for the Ascend 910 ( 4.2.2) and describe a microbatch-based decode pipeline that overlaps two execution streams to hide latency ( 4.2.3). Finally, we explain how the CloudMatrix-Infer supports multiple-token prediction (MTP), a feature leveraged by DeepSeek-R1 to improve decode throughput ( 4.2.4). 4.2.1 Fused Communication Operators for LEP Figure 10a illustrates a basic MoE computation flow. After the gating mechanism selects the Top-ùêæ(ùêæ 8 in DeepSeek R1) activated experts for each token, two all-to-all communication steps are required before the feed-forward network (FFN) stage. The first all-to-all operation exchanges routing metadata such as token-to-expert assignments across all NPUs. The second all-to-all operation exchanges the actual token data, typically a 7,168-dimensional hidden state vector per token. This data, initially stored in BF16 format, is quantized to INT8 on each NPU to reduce communication and compute costs before being processed by its assigned FFN. After FFN computation, a third all-to-all communication sends the expert outputs back to their source ranks, where each NPU performs the final token combination step to reconstruct the output. However, this basic MoE implementation suffers from several inefficiencies: (1) Communication Overheads: The three all-to-all communications introduce significant latency, exacerbated by the large communication domain (hundreds of NPUs).",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2166,
    "tokens": 486
  },
  {
    "text": "2) Attention and MoE Disaggregation: Large-scale MoE models present unique challenges due to sparse expert activation and extreme memory demands. MegaScale-Infer [59] proposes disaggregating attention and expert components into separate execution services, enabling different parallelism strategies and hardware mappings. Attention layers, which process every token, are deployed using data-parallelism on memory-optimized nodes, while expert FFNs are distributed via expert parallelism across a dedicated resource pool. This disaggregated execution reduces contention, improves throughput, and allows independent scaling of attention and expert resources, which is critical for efficiently serving trillion-parameter MoE models. Together, these disaggregation techniques represent a shift toward viewing LLMs as collections of loosely coupled microservices, each with distinct performance profiles. This granularity allows better mapping to heterogeneous hardware and improves load balancing and scalability across a supernode. 6.2.2 Hybrid and Adaptive Deployment Once LLM inference is decomposed into components, which can be considered as fine-grained microservices, such as attention execution, FFN computation, KV cache management, or MoE expert gating, the serving system gains significant flexibility to adopt more sophisticated deployment strategies. These hybrid and adaptive deployment models enable the system to tailor resource allocation to each component s unique computational and memory requirements, improving overall utilization and scalability. 1) Hardware-aware Microservice Placement: Each microservice can be mapped to the most suitable hardware type based on its performance profile. For instance, attention layers, which are typically memory bandwidth-bound, should be prioritized on NPUs with high memory throughput; compute-intensive FFN modules benefit from allocation on NPUs with strong compute capabilities; and lightweight or latency-tolerant operations, such as KV cache indexing, can be offloaded to Serving Large Language Models on Huawei CloudMatrix384 53 pooled CPUs or lower-cost general-purpose accelerators. This fine-grained matching enables more efficient use of heterogeneous hardware and reduces cost without compromising performance. 2) Hybrid Microservice Co-location: Disaggregated microservices can also be dynamically co- located to improve resource utilization across the supernode. For example, memory-bound attention operations from the decode phase can be offloaded to memory-underutilized prefill instances [37].",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2566,
    "tokens": 478
  },
  {
    "text": "In contrast, adding multiple 3 3 convolutions consistently improves robustness, though the effect plateaus beyond four. Pooling and skip connections also exhibit an optimal range: architectures with two or three of either operation 4 20.9 17.5 32.2 9.5 19.9 (a) 16.9 16.2 28.1 26.1 12.6 (b) skip_connect zeroize nor_conv_3x3 nor_conv_1x1 avg_pool_3x3 Figure 3: Percentage of each operation type in (a) robust and (b) non-robust architectures. 0 1 2 3 4 0 20 40 60 80 100 Percentage (a) 0 1 2 3 4 5 (b) 0 1 2 3 4 5 6 (c) 0 1 2 3 4 5 (d) 0 1 2 3 4 (e) Operation Count Robust Non-Robust Figure 4: Distribution of operation counts for robust and non-robust architectures. From left to right: (a) skip connection, (b) zeroize, (c) 3 3 convolution, (d) 1 1 convolution, and (e) 3 3 average pooling. tend to be more robust, while having too few or too many reduces performance. Meanwhile, the zeroize operation shows no clear impact, suggesting sparsity alone does not determine robustness. Sequential Patterns. Operation counts alone do not fully explain robustness, as architectures with identical compositions can exhibit vastly different robustness levels. This suggests that the order and positioning of operations, beyond just their frequencies, play a crucial role. An example of this effect is shown in Figure 5. To better understand these sequential patterns, we examine the most frequent one-, two-, and three-operation paths in robust and non-robust architectures. Robust pathways consistently feature 3 3 convolutions combined with skip connections or pooling layers, as seen in sequences like (2,0), (2,4), (2,2,4), and (2,0,2). In contrast, non-robust pathways frequently include 1 1 convolutions, appearing in patterns like (2,3), (3,2,3), and (2,3,3), indicating that their presence propagates instability even when paired with other beneficial operations.",
    "source": "2506.18495v1_AnalogNAS-Bench_A_NAS_Benchmark_for_Analog_In-Memo.pdf",
    "length": 1865,
    "tokens": 472
  },
  {
    "text": "ABU [62] [13] 2023 Deep Belief Network Shallow NN - Zynq US - - - Segmentation RGB Urban areas Potsdam [57] [104] 2021 ENet Custom CNN - Zynq US 63.3 (mIoU) 0.36 4.06G [104] 2021 ESPNet Custom CNN - Zynq US 55.5 (mIoU) 0.33 3.71G [104] 2021 FPN Custom CNN - Zynq US 65.3 (mIoU) 5.84 17.30G [104] 2021 U-Net U-Net - Zynq US 61.4 (mIoU) 7.40 96.68G General Landsat-8-OLI (cust.) [99] 2021 RDBC Trad. ML - Virtex-6 37.45 (SSIM) - O(n) Cloud coverage 38-Cloud [83] [8] 2019 C-FCN Custom CNN - Cyclone V 79.30 (mIoU) 0.047 4.7K Object detection RGB Diverse det.",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 556,
    "tokens": 238
  },
  {
    "text": "This enables rapid design-space exploration and performance sensitivity analyses that are currently infeasible, e.g., in about an hour, we conducted a first-of-its-kind fine-grained performance attribution to different microarchitectural components across a diverse set of programs, requiring nearly 150 million CPI evaluations. 1 Introduction Microarchitecture simulators are a key tool in the computer archi- tect sarsenal[4,6,11,14,19,28,72,75,82].FromSimpleScalar[11]to gem5 [14, 59], simulators have enabled architects to explore new de- signs and optimize existing ones without the prohibitive costs of fab- rication. CPU simulation, in particular, has become increasingly im- portant as hyperscale companies like Google (Axion) [85], Amazon (Graviton) [7], Microsoft (Cobalt) [62] increasingly invest in devel- oping custom CPU architectures tailored to their specific workloads. The landscape of CPU performance modeling is characterized by a critical tension between model accuracy and speed. This trade-off manifests in the various levels of abstraction employed by different performance models [14, 32, 87]. At one end of the spectrum lie analytical models [2, 87], which provide simplified mathematical representations of microarchitectural components and their inter- actions. Although they are fast, analytical models often lack the detailed modeling necessary to capture the dynamics of modern pro- cessorsaccurately.Attheotherendofthespectrumresidecycle-level simulators like gem5 [14], which can provide high-fidelity results by meticulously modeling every cycle of execution. However, this level Work done at Google. of detail comesat a steep computational cost, becoming prohibitively slow for large-scale design space exploration [32, 47, 55], programs with billions of instructions, or detailed sensitivity studies. Recognizing the limitations of conventional methods, there has been growing interest in using machine learning (ML) to expedite CPU simulation [54, 55, 61, 71]. Rather than explicitly model ev- ery cycle and microarchitectural interaction, these methods learn an approximate model of the architecture s performance from a large corpus of data. A typical approach is to pose the problem as learning a function mapping a sequence of instructions to the tar- get performance metrics.",
    "source": "2503.23076v1_Concorde_Fast_and_Accurate_CPU_Performance_Modelin.pdf",
    "length": 2317,
    "tokens": 489
  },
  {
    "text": "Concretely, p 2 and m 5 for all of our experiments. Since each ti is shared for all pixels within the same bit plane, a small change of the latent threshold parameters may notably tamper the model s behavior. Therefore, to stabilize the model convergence during training, the update magnitude of the latent parameters tj is scaled by a factor Œ≤ 2 HWM. B. Latent parameter s initialization and constraint An important question is how to properly initialize the latent threshold parameters t. For the sake of simplicity, the straightforward option is to choose the initial values such that the resulting thermometer thresholds follow a linear ramp like proposed in [11]. In details, the threshold of the i-th bit-plane is defined as ti s(i 0.5) (2Nb 1) where s 2Nb M is the uniform step size and 1 (2Nb 1) is the normalization factor (e.g., 1 255 with 8b image data). Besides, the initialization of the non-normalized latent parameters t is scaled as: ti 0.5sk if i 1, sk if 1 i M, (0.5s 1)k if i M 1. (6) Here the scale factor k aims to balance the stabilization with the updatability of the GLT. For optimization purposes and with respect to training stability issues, the latent parameters are forced to be positive and large enough, i.e., ti 0.05. Coherently, the scaling value k is set such that the initial latent parameters remain higher than the aforementioned clipping threshold, e.g., by setting k M 1280. IV. BLOCK PRUNING WITH DISTRIBUTIONAL LOSS A. Proposed method BNNs usually need to be compressed further to match strict hardware specifications of edge devices, e.g., typically under 1Mb SRAM. In this section, we propose a method to gradually prune complex processing blocks and replace it with a lightweight convolution (LWC) block involving a g- group 3 3 convolution (GConv) of strides 2 followed by a channel shuffle (if g 1). Figure 3 illustrates how our idea is applied to MUXORNet-11 [21]. The number of channels is doubled after the GConv, thus allowing the pruned model to have the same data dimension as the baseline model. The LWC shows a far better hardware mapping than its corresponding block in the baseline model.",
    "source": "2505.13462v1_End-to-end_fully-binarized_network_design_from_Gen.pdf",
    "length": 2144,
    "tokens": 500
  },
  {
    "text": "(1) where x is normalized to the interval [0, 1] for ease of later optimization. The choice of the threshold values {ti}i [[1,M]] deeply impacts the quantity of relevant information preserved and thus model s performance, therefore our goal is to find optimal thresholds by minimizing any task loss. Let us denote t RM 1 as the learnable latent threshold parameters that are kept always positive during the optimization process. We first compute the normalized vector t as follows: t t PM 1 i 1 ti (2) With 0 ti 1 and PM 1 i 1 ti 1, the thermometer thresholds {ti}i [[1,M]] are then computed as a cumulative sum: ti i X j 1 tj (3) It can be easily verified that 0 ti ti 1 1 for i [[1, M]], and the optimal threshold values are indirectly learned through the optimization of t. Let us denote L as the task loss; I RH W as the input image data of resolution H W that is normalized within [0, 1]; and Ib i {0, 1}H W as the encoded bit plane at the bit position i [[1, M]]. Indeed, the gradient with respect to tj can be written by the chain rule as: L tj X x,y,i L Ib x,y,i Ib x,y,i ti ! ti tj X x,y,i L Ib x,y,i Heaviside(Ix,y ti) ti ! ti tj (4) where the term ti tj is calculated from Eqs. 2 and 3. The gradient vanishing problem caused by Heaviside function could be overcame using approximation techniques [18] [20]. Intuitively, near-threshold values should have higher influence (i.e., larger gradient magnitude) than far-threshold counterparts. Therefore, we employ the ReSTE approximation proposed in [20] and propose a modified version as follows: Heaviside(u) u 1 mmin 1 p u 1 p p , m (5) where p 1 controls the narrowness of the bell-shaped gradient curve and m is the clipping threshold. Concretely, p 2 and m 5 for all of our experiments. Since each ti is shared for all pixels within the same bit plane, a small change of the latent threshold parameters may notably tamper the model s behavior.",
    "source": "2505.13462v1_End-to-end_fully-binarized_network_design_from_Gen.pdf",
    "length": 1905,
    "tokens": 492
  },
  {
    "text": "Ho, B. Yu, and Y. Huang, Functionality matters in netlist representation learning, in Design Automation Conference (DAC), 2022. [8] W. Fang, S. Liu, J. Wang, and Z. Xie, Circuitfusion: multimodal circuit representation learning for agile chip design, in International Conference on Learning Representations (ICLR), 2025. [9] H. Wu, H. Zheng, Y. Pu, and B. Yu, Circuit representationlearning with masked gatemodeling and verilog-aigalignment, in International Conference on Learning Representations (ICLR), 2025. [10] C. Deng, Z. Yue et al., Less is more: Hop-wise graph attention for scalable and generalizable learning on circuits, in DAC, 2024. [11] C. Xu, P. Sharma, T. Wang, and L. W. Wills, Fast, robust and transferable prediction for hardware logic synthesis, in IEEE ACM International Symposium on Microarchitecture (MICRO), 2023. [12] Y. Zhang, Z. Yu et al., Mg-verilog: Multi-grained dataset to- wards enhanced llm-assisted verilog generation, arXiv preprint arXiv:2407.01910, 2024. [13] Z. Pei, H.-L. Zhen, M. Yuan, Y. Huang, and B. Yu, Betterv: Con- trolled verilog generation with discriminative guidance, arXiv preprint arXiv:2402.03375, 2024. [14] S. Liu, W. Fang, Y. Lu, Q. Zhang, H. Zhang, and Z. Xie, RTLCoder: Outperforming GPT-3.5 in design RTL generation with our open-source dataset and lightweight solution, IEEE LLM-Aided Design (LAD), 2023. [15] Z. Yan, W. Fang, M. Li, M. Li, Z. Yan, S. Liu, Z. Xie, and H. Zhang, AssertLLM: Generating and evaluating hardware verification assertions from design specifications via multi-LLMs, in ASP-DAC, 2025.",
    "source": "2504.09485v1_GenEDA_Unleashing_Generative_Reasoning_on_Netlist_.pdf",
    "length": 1570,
    "tokens": 472
  },
  {
    "text": "Latency Distribution: The end-to-end latency distribution for all policies for the Social Network application with the Twitter trace is plotted in Figure 12. In particular, Arch, Fifer and Kraken show comparable latencies, with P99 values re- maining well within the SLO of 1000ms. However, Arch and Fifer use 3.51x and 2.1x more containers than Kraken to 0 0.25 0.5 0.75 1 Arch Fifer DProb Kraken SProb Xanadu Energy Consumption Rate (a) Energy Consumption Rate. 0 300 600 900 1200 0.25 0.5 0.75 0.98 0.99 Latency (ms) CDF Kraken Comm Only Conn Only SLO (b) Response Time Distribution. Figure 13: Real System: Normalized Energy Consumption of all Schemes and Response Time Distribution of Kraken, Comm Only and Conn Only achieve this. The tail latency (measured at P99) for DProb almost exceeds the SLO, whereas it does so for SProb. Kraken manages to avoid high tail latency by assigning augmented weights to key functions, thus, helping it tolerate incorrect load probability estimations. SProb does worse than DProb at the tail because of its lack of adaptive probability estimation. Kraken makes use of 21 more containers to achieve the improved latencies. Xanadu experiences a sudden rise in tail latency, with it being 100ms more than that of Kraken, while using 96 more containers. This is due to Xanadu s MLP misprediction and the resultant container over-provisioning. Energy Efficiency: We measure the energy-consumption as total Energy consumed divided over total time. Kraken achieves one of the lowest energy consumption rates among all the policies considered, with it bettering existing policies, namely, Arch, Fifer and Xanadu by 26 , 14 and 3 respec- tively (for the workload mix of Media Service application with Wiki trace) as depicted in Figure 13a. These savings can go up to 48 compared to Arch for applications like Social Network. The resultant energy savings of Kraken are a direct consequence of the savings in computation and memory usage from the fewer containers spawned. Only DProb and SProb consume lesser energy than Kraken (4 lesser), due to their more aggressive container reduction approach.",
    "source": "kraken.pdf",
    "length": 2127,
    "tokens": 476
  },
  {
    "text": "The design of L3 mainly involves two primary components. First, to address the challenges posed by the two mis- matches, L3 implements an in-flight fine-grained relayout ( 4.1), which seamlessly integrates with conventional DIMM-based burst transfer without DDR protocol violations. In addition, cross-level PUs are configured to cooperate with two distinct mapping meth- ods of KV cache ( 4.2), which further enables kernel fusion with a bubble-free pipelined execution manner ( 4.3). Second, L3 maxi- mizes resource utilization across both GPU and DIMM-PIM sides with optimizations on two dimensions: dependency-aware communication- computation overlap ( 5.1), that reduces the cross-device data trans- fer overhead, and adaptive chunk-partitioned batch interleaving ( 5.2), that optimizes parallel execution across both sides. The fol- lowing sections elaborate on these two designs (DIMM-PIM hard- ware and PIM-powered inference system) in greater detail. 4 L3 DIMM-PIM Our architecture implements a hierarchical processing framework with strategically integrated PUs that cooperate across different levels: rank-level PUs positioned on the buffer chip and bank-level PUs embedded within each DRAM chip, as illustrated in Fig. 5-a. This cross-level coordination forms the foundation for three key innovations: (1) in-flight data re-layout mechanisms that resolve the bit-level mismatch while overcoming the challenges introduced by DDR burst [1] and timing constraints [2]; (2) distinct KV map- ping methods that organize co-processed elements contiguously at burst-transfer granularity, effectively addressing the element-level mismatch through the cross-level PU collaboration; and (3) com- prehensive kernel fusion that seamlessly integrates score computa- tion, softmax operations, and context computation in a bubble-free pipelined execution flow. Together, they enable L3 to overcome the fundamental mismatch challenges in DIMM-PIM while maximizing computational throughput during attention computation. 4.1 Zero-latency In-flight Re-layout We implement a re-layout unit in the rank PU to perform data trans- formation during GPU-to-CPU offloading (KV for prefilling, QKV for decoding).",
    "source": "2504.17584v1_L3_DIMM-PIM_Integrated_Architecture_and_Coordinati.pdf",
    "length": 2196,
    "tokens": 484
  },
  {
    "text": "The most computationally intensive operations in the critical execution path, such as large matrix multiplications in feed-forward networks (FFNs) and attention mechanisms, are quantized to INT8 to leverage the highest throughput. Conversely, sub-modules or specific operations that are more sensitive to quantization errors but constitute a smaller fraction of the overall memory access or computational burden (e.g., certain normalization layers or critical gating mechanisms) retain higher precision using BF16 or FP32. This flexible partitioning of bit-widths ensures that the entire model can execute efficiently within a unified hardware pipeline, while precision bottlenecks in critical, numerically sensitive pathways are avoided. Adaptive Scale Search. Effective quantization requires careful alignment of the dynamic range of floating-point values to the limited range of INT8 integers. For each weight tensor and activation tensor destined for INT8 quantization, we introduce a lightweight, adaptive scale search process. This process automatically determines the optimal scaling factor ùë† that minimizes the quantization error, effectively aligning the value distributions before and after quantization. The scale search is formulated as an optimization problem: ùë† arg min ùë† L(ùë†), where L(ùë†) ùëÑ(ùëä ùë†)(ùë† 1 ùëã) ùëäùëã (3) Here, ùëärepresents the weights, ùëãthe activations, and ùëÑ( ) denotes the quantization function. This formulation seeks to find scales ùë†for weights (and ùë† 1 for activations) such that the output of the quantized operation ùëÑ(ùëä ùë†)(ùë† 1 ùëã) is closest to the original floating-point output ùëäùëã. This entire scale determination process is performed offline during a post-quantization calibration step and therefore incurs no additional runtime overhead during inference. This concept involves transforming ùëãand ùëäwith appropriate scales before quantized multiplication. 38 Outlier Suppression and Structural Transformation. Certain components within large models, particularly specific expert subnetworks or gating structures in MoE architectures, can exhibit activation or weight distributions with long tails or significant outliers.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2147,
    "tokens": 484
  },
  {
    "text": "In fact, prior research on HVS has shown that human eyes are able to observe beyond 135 vertically and 160 horizontally, but see fine details within an only around 5 central circle (i.e., foveal vision). Motivated by such degradation of peripheral visual acuity, foveated rendering reduces computational costs for the peripheral region, and maintains high normal resolution only for the foveal re- gion [2, 22, 24, 25, 30, 47, 62]. For instance, a real-time gaze-tracked foveated rendering system is proposed to yield performance and memory savings by avoiding shading up to 70 of the pixels for VR headsets [47]. Similarly, a prototype AR display also takes advan- tage of foveated rendering by tracking the user s gaze and providing low-resolution images to the peripheral area to reduce computa- tion and improve display resolution [25]. More recently, another foveated rendering based CGH reconstruction technique has been proposed to accelerate calculations with negligible effect for the viewer [22]. We implemented such foveated rendering idea (de- noted as Inter-Holo design in Sec. 4.3) and found to reduce around 23 execution latency (in Sec. 5) for AR holograms. However, such performance gain from foveated rendering is still insufficient to close the 10 gap discussed above. Thus, in this paper, we want to go beyond the prior foveated rendering for further optimizations, by investigating the potential opportunities which are unique to the AR use cases and may have been missed out before. 2.2.3 What are the Potential Opportunities? Towards addressing this hologram bottleneck, various ap- proaches from both the software [33, 52, 54] and hardware [32, 35] sides have been proposed. These prior approaches either incorpo- rate additional memory for maintaining a lookup table for compu- tation reduction, or build an application-specific integrated circuit (ASIC) chip specifically for holographic processing, which is more power-efficient than generic processors. While such approaches im- prove the hologram execution to some extent, they do not consider the unique features of the AR applications. Recall that, in the AR holographic application discussed above in Fig.",
    "source": "HoloAR.pdf",
    "length": 2187,
    "tokens": 458
  },
  {
    "text": "1 46, 2021. [3] H. Wu, Z. He, X. Zhang, X. Yao, S. Zheng, H. Zheng, and B. Yu, Chateda: A large language model powered autonomous agent for eda, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2024. [4] K. Chang, Y. Wang, H. Ren, M. Wang, S. Liang, Y. Han, H. Li, and X. Li, Chipgpt: How far are we from natural language hardware design, arXiv preprint arXiv:2305.14019, 2023. [5] M. Akyash and H. M Kamali, Evolutionary large language models for hardware security: A comparative survey, in Proceedings of the great lakes symposium on VLSI 2024, 2024, pp. 496 501. [6] M. Akyash, K. Azar, and H. Kamali, Rtl : Graph-enhanced llm for rtl code generation, arXiv preprint arXiv:2505.13479, 2025. [7] F. Cui, C. Yin, K. Zhou, Y. Xiao, G. Sun, Q. Xu, Q. Guo, D. Song, D. Lin, X. Zhang et al., Origen: Enhancing rtl code generation with code-to-code augmentation and self-reflection, arXiv preprint arXiv:2407.16237, 2024. [8] S. Liu, W. Fang, Y. Lu, J. Wang, Q. Zhang, H. Zhang, and Z. Xie, Rtl- coder: Fully open-source and efficient llm-assisted rtl code generation technique, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2024. [9] Y. Zhu, J. Li, G. Li, Y. Zhao, Z. Jin, and H. Mei, Hot or cold? adaptive temperature sampling for code generation with large language models, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 1, 2024, pp. 437 445.",
    "source": "2507.02226v1_DecoRTL_A_Run-time_Decoding_Framework_for_RTL_Code.pdf",
    "length": 1436,
    "tokens": 467
  },
  {
    "text": ". , cma} of all valid program configurations for a given hardware platform a (ma Z ). For example, for CPU, a valid configuration from CCP U is a tuple of program configuration parameters for loop strip-mining, loop reordering, and format reordering (Table 1). The optimal program configuration minimizes the execution time of a sparse tensor program. For an input sparse matrix (sparsity pattern) M, the optimal program configuration on platform a can be given as, c arg minci Ca Ta(M, ci), where Ta is the execution time function for platform a (ground truth runtime). The execution time for the optimal program configuration is given by t Ta(Ml, c ). A.2. Cost Model Performance and Data Efficiency Objectives We approximate the ground truth runtime Ta using learned cost models. Usually, these cost models are trained with one objective: to achieve high accuracy. However, due to the high cost of simulation in emerging hardware, we also want to minimize the amount of data samples required from these platforms for model training. We formalize these two objectives as follows. Data Collection Objective (DCE). Let Da {(Ml, ci), ti i ma , l Z } be the dataset collected from hardware platform a, and let Œ≤a represent the average cost of collecting a single data sample from the platform. Our objective is to minDa Œ≤a Da . Accuracy Objective. Let CMa (which approximates Ta) be the learned cost model trained on dataset Da. If the best program configuration returned by the cost model (c CMa) has an actual execution time t CMa, our objective is to min t CMa t , where t is the execution time for the optimal configuration. For a set of input sparse matrices {M1, M2, . . . , Mk}, our objective can be extended to minimizing the Absolute Percentage Error (APE) across all matrices: APE 1 k k X l 1 t CMa,Ml t Ml t Ml 100 where t CMa,Ml denotes the execution times for the predicted best program configuration for the input sparse matrix Ml and t Ml denote the optimal program configurations for the same matrix. A.3. Evaluations for Cost Model Objectives To evaluate the cost model objectives, we conducted the following experiments for SpMM on SPADE. For simplicity in the calculations, we set Œ≤CPU 1 and Œ≤SPADE 1000.",
    "source": "2506.00424v2_COGNATE_Acceleration_of_Sparse_Tensor_Programs_on_.pdf",
    "length": 2221,
    "tokens": 497
  },
  {
    "text": "Sim. GPT Score GPT-3.5 3.34 28.20 10.46 25.11 20.36 0.740 0.510 GPT-4o 4.59 29.26 11.48 25.74 22.78 0.761 0.549 o1-preview 3.73 28.00 10.39 24.98 20.48 0.748 0.535 CodeV-DeepSeek 3.05 25.14 9.78 23.25 20.23 0.705 0.495 CodeV-CodeQwen 2.80 24.91 8.27 22.75 21.07 0.747 0.499 DeepRTL-220m 13.06 37.56 19.85 34.72 34.37 0.806 0.600 DeepRTL-16b 12.85 37.43 19.34 34.63 33.09 0.802 0.597 Llama-3.1 2.68 25.37 10.39 23.75 17.16 0.730 0.430 DeepSeek-Coder 2.56 24.52 7.72 22.45 22.83 0.756 0.571 DeepRTL21st-Direct (Llama) 11.28 34.29 16.35 33.63 27.73 0.754 0.580 DeepRTL21st-Direct (DeepSeek) 12.07 36.37 17.78 33.78 28.56 0.767 0.602 DeepRTL21st (Llama) 13.34 37.74 19.54 34.76 33.46 0.798 0.594 DeepRTL21st (DeepSeek) 13.53 37.52 19.68 34.68 33.28 0.814 0.612 DeepRTL2 (Llama) 13.84 37.97 20.69 34.42 34.75 0.813 0.603 DeepRTL2 (DeepSeek) 13.96 37.93 20.73 34.34 34.74 0.820 0.616 Table 3: The performance evaluation for RTL code understanding. BLEU-4 refers to the smoothed BLEU-4 score, while Emb. Sim. represents the embedding similarity metric. The best results are highlighted in bold, and the second-best results are underscored.",
    "source": "2506.15697v1_DeepRTL2_A_Versatile_Model_for_RTL-Related_Tasks.pdf",
    "length": 1132,
    "tokens": 496
  },
  {
    "text": "Furthermore, as illustrated in Figure 8, we present the power consumption distribution for each module in the FineQ PE array, where the Acc unit constitutes 71.8 , the PE Array 25.9 , and the Temporal Encoder accounts for only 2.3 . The simplification of multipliers has significantly reduced the overhead of the PE array. TABLE III THE AREA AND POWER BREAKDOWN OF ACCELERATOR CORE MODULES Architecture Setup Area Power Systolic Array 64 64 PEs 0.954 mm2 88.793 mw FineQ Decoder 64 0.008 mm2 0.187 mw FineQ PE Array 64 64 PEs 0.370 mm2 32.891 mw Energy Efficiency. Figure 9 illustrates the normalized energy efficiency of the accelerator at different sequence lengths under the same on-chip buffer size. Compared to the baseline design, PE Array (25.9 ) ACC (71.8 ) Temporal Encoder (2.3 ) PE Array ACC Temporal Encoder Fig. 8. Breakdown of power in FineQ PE array. the accelerator exhibits higher energy efficiency, attributed to the simplified design of most PEs and the reduction in the complexity of multipliers. Overall, the accelerator achieves up to 1.79 average energy efficiency on various models. 1.760 1.815 1.787 Fig. 9. Normalized energy efficiency over baseline accelerator. VI. CONCLUSION In this paper, we propose FineQ, a software-hardware co- design for low-bit fine-grained mixed-precision quantization of LLMs. The key insight is to handle outliers within clusters at a fine granularity. We address the issue of coarse quantiza- tion granularity in LLMs by proposing a fine-grained mixed- precision quantization algorithm, effectively balancing model accuracy and memory overhead. FineQ achieves higher model accuracy compared to the SOTA mixed-precision quantization algorithm at a close average bit-width. Additionally, we in- troduce an accelerator based on temporal coding to support this algorithm, simplifying multipliers in the systolic array and achieving up to 1.79 energy efficiency. REFERENCES [1] A. Vaswani, Attention is all you need, Advances in Neural Information Processing Systems, 2017. [2] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A.",
    "source": "2504.19746v1_FineQ_Software-Hardware_Co-Design_for_Low-Bit_Fine.pdf",
    "length": 2082,
    "tokens": 499
  },
  {
    "text": "As shown in Figure 3a, this results in a high volume of long memory accesses, where up to 86 of requests are more than 10 longer than L1D accesses, and up to 36 are more than 100 longer. However, compute units such as GPU SMs and LLC HBM CPU0 L1 Core TMU CPU1 L2 OutQ 8 cores, 8 LLC slices, 1 HBM stack N1-like OoO core 2.4 GHz, 224-entries ROB, 2x96-entries LSQ, 2x64B vector units One 64Bx64B matrix unit per core 64 KiB core, 4-way, 2-cycle access, 32 MSHRs, stride IMP prefetcher 512 KiB core, 8-way, 8-cycle access, 64 MSHRs, best offset prefetcher Mostly exclusive, 1 MiB slice, 16 ways, 12-cycle access, 128 MSHRs 2D mesh, 1 cycle routers, 1 cycle links, MOESI-like AMBA 5 CHI 300 GB s HBM2 8KB storage, 256 MSHRs, 1 GHz SoC: Core: Acc: L1D: L2: LLC: NoC: HBM: TMU: (a) (b) Figure 5: A DAE processor. Each traditional core offloads embedding lookup to an access unit like the TMU [61]. CPU cores are not dimensioned for such long memory access laten- cies [44]. For instance, as shown in Figure 3b and 3c, off-the-shelf CPU cores can only track a few of these long memory requests, achieving low throughput (i.e. loads cycle) and performance. Increasing the memory-level parallelism of a CPU core to track more in-flight requests requires scaling up the reorder buffer, load- store queue, and miss-status handling registers [58]. However, these resources scale inefficiently. As shown in Figure 4, doubling them only improves performance by up to 12 , with a 21 power over- head. Besides being inefficient, scaling up these resources intro- duces challenges in timing closure [59], which would require lower- ing the core s frequency and, consequently, compute performance. Scaling out the number of cores is also ineffective. As shown in Figure 3d, we would need 43 72 traditional CPU cores to saturate a single HBM2 stack.",
    "source": "2504.09870v1_Ember_A_Compiler_for_Efficient_Embedding_Operation.pdf",
    "length": 1831,
    "tokens": 501
  },
  {
    "text": "One predictor (i.e., one of LinReg, DNN, etc.) was trained per CPU architecture. Each training was performed 10 times with a random selection of training and test sets. Scores were subsequently calculated based on the median predictions from the test sets. Table III shows the results for x86, Table IV for ARM, and Table V for RISC-V. For the x86 architecture, the DNN, Bayesian optimization, and XGBoost yield good scores. XGBoost achieves the best average Rtop1 score, with a maximum of 2.5 . XGBoost also had the smallest prediction error, with an average of Etop1 2.54 . For ARM and RISC-V, the scores are slightly better. For ARM, DNNs, Bayesian optimization, and XGBoost achieve a Rtop1 score smaller or equals 2.5 . All Etop1 errors are below 5 , and often even below 2 . Bayesian optimization correctly predicts the optimum in three out of five cases and demonstrates the lowest Qlow and Qhigh. For RISC- V, even the linear model performs quite well, with exceptions in Etop1 for group 0 and 4. For other predictors, the optimum is often included in the top 2 of predictions. The DNN provides the best results with Etop1 3.6 and Rtop1 2 . In summary, it is possible to train predictors for forecasting. For RISC-V and ARM, a prediction error Etop1 of less than 5 is achievable. If the goal is to find the best sample, it is sufficient to re-execute the top 2 -3 of the predictions later on a real architecture.",
    "source": "2505.13357v1_Introducing_Instruction-Accurate_Simulators_for_Pe.pdf",
    "length": 1419,
    "tokens": 351
  },
  {
    "text": "At the RTL stage, even though the same RTL functionality might be present, different circuit structures can yield significantly varying PPA characteristics. By leveraging structural RTL information captured by RTL encoders, LLMs can enable structure-aware generation, potentially generating more opti- mized RTL code with better PPA characteristics. Additionally, at the layout stage, GenEDA can be adapted to handle cross-modal inputs, such as layout images and netlist graphs. This might enable the direct generation of macro positions on a chip by learning from image representations, thus enabling more efficient and optimized physical design generation. B. Potential Application of Netlist Generative Reasoning GenEDA can reason the detailed functionality from netlists, which can significantly benefit verification and optimization processes. Before loading the netlists into EDA tools, GenEDA can guide the selection of appropriate strategies for verifying or optimizing different parts of the design, such as datapaths or control logic. Additionally, it can assist in verifying the equivalence of netlists by transforming them into higher-level RTL code, making the verification process more scalable. In hardware security, GenEDA can be applied to detect malicious hardware trojans. By analyzing netlists, it can identify unexpected or unauthorized functional behaviors, helping to ensure the integrity and security of hardware systems. VIII. CONCLUSION AND FUTURE WORK In this paper, we present GenEDA, a framework that aligns multimodal circuit encoders and decoders for advanced generative circuit functional reasoning tasks. We align the state-of-the-art netlist encoder with both trainable and frozen LLM decoders through two alignment paradigms. Our experiments and ablation studies demon- strate the effectiveness of this approach, with GenEDA significantly enhancing the performance of state-of-the-art LLMs, showcasing the critical role of integrating both graph and text circuit modalities for complex netlist tasks. Future work will explore extending this alignment framework to other stages of the circuit design flow, such as RTL code generation and layout-stage tasks, further enhancing the capabilities of GenEDA for diverse EDA applications. REFERENCES [1] L. Chen, Y. Chen, Z. Chu et al., Large circuit models: opportunities and challenges, Science China Information Sciences, 2024.",
    "source": "2504.09485v1_GenEDA_Unleashing_Generative_Reasoning_on_Netlist_.pdf",
    "length": 2408,
    "tokens": 481
  },
  {
    "text": "Neurocomputing, 568:127063. Hanzhuo Tan, Qi Luo, Jing Li, and Yuqun Zhang. 2024. Llm4decompile: Decompiling binary code with large language models. arXiv. Sourabh Kumar Verma. 2024. Exploring win- dows on arm: The future of computing. educatordeveloperblog exploring-windows -on-arm-the-future-of-computing 4260186. Microsoft Tech Community Blog. Hao Wang, Wenjie Qu, Gilad Katz, Wenyu Zhu, Zeyu Gao, Han Qiu, Jianwei Zhuge, and Chao Zhang. 2022. Jtrans: Jump-aware transformer for binary code similarity detection. In Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis. Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859. Zhe Wang, John Smith, and Jane Doe. 2024. Evaluating the effectiveness of decompilers. In Proceedings of the 2024 ACM Conference on Software Analysis, New York, NY, USA. ACM. Xiangzhe Xu, Shiwei Feng, Yapeng Ye, Guangyu Shen, Zian Su, Siyuan Cheng, Guanhong Tao, Qingkai Shi, Zhuo Zhang, and Xiangyu Zhang. 2023. Improving binary code similarity transformer models by semantics-driven instruction deemphasis. In Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis. Zeping Yu, Rui Cao, Qiyi Tang, Sen Nie, Junzhou Huang, and Shi Wu. 2020. Order matters: Semantic-aware neural networks for binary code similarity detection. In Proceedings of the AAAI conference on artificial intelligence. Siyuan Zheng, Zhi Yang, Cedric Renggli, Yuxiang Pu, Zixuan Li, Mohammad Shoeybi, Lin Zhang, Dheevatsa Narayanan, Haotian Zhao, Zhewei Yao, and Tianqi Chen.",
    "source": "2506.14606v1_Guaranteed_Guess_A_Language_Modeling_Approach_for_.pdf",
    "length": 1698,
    "tokens": 476
  },
  {
    "text": "Their systems include an online spike sorting module configured by a shadow spike sorter block on an external module. Generally, spike compression offers superior CR since only the spike events (with an average firing rate of 40-60 Hz) are detected, extracted, compressed, and transmitted. In contrast, our work focuses on compressing local field potentials (LFPs), which are a different signal modality compared to spike waveforms. LFPs are usually low-frequency components of the neural signal (typically 300 Hz), and the information is encoded in the raw signal itself rather than in spikes. Therefore, in our proposed compression scheme, we compress the raw neural signal as opposed to extracting and compressing spike waveforms. The CR achieved by the proposed compression scheme is superior to the existing LFP compression methods [54], [59] [61]. V. CONCLUSIONS This paper introduces a novel neural signal compression scheme employing convolutional autoencoders. The encoder section of the CAE underwent several hardware-software co- optimizations and was subsequently deployed on the RAMAN tinyML accelerator specifically designed for edge computing applications. RAMAN leveraged weight and activation spar- sity to reduce latency, memory usage, and power consumption. A novel hardware-aware stochastic pruning technique was em- ployed to address workload imbalance issues across multiple parallel processing elements and reduce the indexing overhead associated with compressed weight storage, resulting in up to a 32.4 reduction in parameter memory requirements. Furthermore, since RAMAN inherently supports a wide range of neural network topologies, including standard con- volutions, depth-wise convolutions, point-wise convolutions, pooling layers, and dense layers, the encoder of the CAE was constructed based on depth-wise separable convolutional layers to minimize the number of MAC operations. The proposed CAE-based scheme performs compression in both spatial (channel) and temporal domains, achieving a superior compression ratio of up to 150. The CAE encoder model was pruned using the stochastic pruning scheme and quantized to 8 bits before deployment on the RAMAN tinyML accelerator. RAMAN was implemented on the Efinix Ti60 FPGA with 52.3k XLR cells and 61 DSP units, and the compressed neural data obtained at the output of RAMAN was decoded offline.",
    "source": "2504.06996v1_Neural_Signal_Compression_using_RAMAN_tinyML_Accel.pdf",
    "length": 2375,
    "tokens": 479
  },
  {
    "text": "Binary Transformer models require two types of binarization schemes, t 1, 1ubt 1, 1u and t 1, 1ubt0, 1u, and involve two distinct matrix multiplication operations: linear layer and attention computation. Some existing works aim to enhance the efficiency of binary transformers. VAQF [17] introduced a binary vision Trans- former (ViT) accelerator that exploits the speedup potential of binarization by converting multiplication into bitwise logical operations, as discussed in an earlier section. However, it only supports one execution mode and does not consider 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1.0 0.9 0.8 0.1 0.2 0.1 0.1 1.0 0.7 0.3 0.1 0.1 0.3 0.6 1.0 0.7 0.1 0.2 0.2 0.2 0.5 1.0 0.9 0.3 0.2 0.1 0.1 0.7 1.0 0.6 0.1 0.1 0.7 0.4 0.8 1.0 ùúÜùëñ [ùúÜùëñ,1, ùúÜùëñ,2, , ùúÜùëñ,ùëò] Original BiT Binary Attention Attention Map Distortion Rate Original BiT (Sofmax-Round-Clip) COBRA SPS Binary Attention Argmin Update 1 1 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 Similarity MSE Fig. 2: Shifted Polarized Softmax (SPS) Search the matrix multiplication of binarized attention. BETA [18] employs a Compressor Tree Loop to create an accelerator specifically for fully binary transformers, supporting both matrix multiplication types and achieving improved efficiency and throughput on edge platforms. BAT [19] offers a co-design approach, creating a custom binarized transformer model with a specialized hardware accelerator for edge deployment.",
    "source": "2504.16269v2_COBRA_Algorithm-Architecture_Co-optimized_Binary_T.pdf",
    "length": 1488,
    "tokens": 465
  },
  {
    "text": "From line 5 of Alg. 1 we know that the output is computed as follows: oi oi 1 ‚Ñìi 1emi 1 mi ‚Ñìi vi esi mi ‚Ñìi (1) From the definition of ‚Ñìi ‚Ñìi 1emi 1 mi esi mi in line 4 of Alg. 1 we get that: ‚Ñìi 1emi 1 mi ‚Ñìi esi mi Replacing this term to the output recursion (1) we get that oi oi 1 ‚Ñìi esi mi ‚Ñìi vi esi mi ‚Ñìi oi 1 1 esi mi ‚Ñìi vi esi mi ‚Ñìi (2) Setting wi esi mi ‚Ñìi (3) and replacing it in (2) we get an equivalent, but simplified form, for the recursive computation of the output in baseline FlashAttention: oi oi 1(1 wi) viwi (4) This newly rewritten form of (1) reveals that, effectively, the output is a weighted sum of the previously accumulated output, oi 1, and the new value vector, vi. The contribution of each part is determined by the value of wi which, according to (3), corresponds to an incrementally formed softmax since it includes the exponent of each attention score divided by the running sum of exponents accumulated in ‚Ñìi. Thus, by definition, wi is always positive and less than 1. B. Recursive weight computation Next, we aim to establish a recursive relation that links the current weight, wi to the weight from the previous iteration, wi 1 and the corresponding attention scores. Our goal is to use the planned recursive relation exclusively, replacing the recursive computation of ‚Ñìi and mi, as well. To do so, based on the definition of weight wi in (3), we can equivalently write that wi 1 esi 1 mi 1 ‚Ñìi 1.",
    "source": "2505.14201v1_FLASH-D_FlashAttention_with_Hidden_Softmax_Divisio.pdf",
    "length": 1430,
    "tokens": 409
  },
  {
    "text": "This process continues until the final target sparsity is reached. After the pruning process is complete, a final fine-tuning phase is performed to ensure the pruned model maintains its accuracy. The remaining non-zero weights are then stored us- ing a sparse matrix format, significantly reducing the model s memory footprint. The combination of gradual sparsity in- crease and iterative fine-tuning helps preserve the most im- portant features learned by the network while eliminating redundant parameters. This compressed format is particularly well-suited for deployment on the edge, as it reduces both storage requirements and computational complexity during inference. C. Quantisation scheme Our quantisation strategy operates in two stages: first, quan- tizing the model weights using quantisation-aware training, and second, quantizing the feature maps to align with hardware constraints. The model weights are quantised to 8-bit integers during training. This process applies quantisation during training, allowing the model to adapt to reduced precision, while still helping to preserve accuracy compared with lower bit quan- tisation during training. For ACAM deployment, we further quantise the feature maps. We use a mean-based thresholding approach for binary quantisation. This approach, using the mean rather than a fixed threshold, better adapts to the distribution of feature values. D. ACAM-aware optimisations Several architectural decisions in our network design are specifically guided by ACAM hardware constraints and effi- ciency considerations. The primary optimisation focuses on minimising the use of complex layer types and reducing the overall network depth while still aiming to maintain performance. A key design decision is the exclusion of fully connected layers throughout the network. Traditional neural networks of- ten employ multiple fully connected layers, particularly in their final stages, which significantly increases the parameter count and computational complexity. Instead, our architecture relies primarily on convolutional layers, which are more parameter- efficient due to weight sharing and maintain spatial relation- ships in the feature maps while also demonstrating superior memory access patterns. In convolutional operations, each loaded weight is reused multiple times as it slides across the input feature map and input values are reused across multiple filter applications. This repeated use of data once loaded from memory significantly improves the energy efficiency of the network, as the energy cost of memory access is spread over IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL., NO., FEBRUARY 2025 4 multiple computations.",
    "source": "2502.10089v1_A_Hybrid_Edge_Classifier_Combining_TinyML-Optimise.pdf",
    "length": 2697,
    "tokens": 498
  },
  {
    "text": "1369 1379, Sep. 2021. [81] P. Shrestha, A. Aversa, S. Phatharodom, and I. Savidis, EDA-schema: A graph datamodel schema and open dataset for digital design automation, Proceedings of the ACM Great Lakes Symposium on VLSI, pp. 1 8, Jun. 2024. [82] P. Shrestha and I. Savidis, EDA-ML: Graph representation learning framework for digital IC design automation, Proceedings of the International Symposium on Quality Electronic Design, pp. 241 246, Apr. 2024. [83] J. Jung, A. B. Kahng, S. Kim, and R. Varadarajan, METRICS2.1 and flow tuning in the IEEE CEDA robust design flow and OpenROAD, Proceedings of the IEEE ACM International Conference On Computer Aided Design, pp. 1 9, Nov. 2021. [84] Rongjian Liang, Anthony Agnesina, Geraldo Pradipta, Vidya A Chhabria, and Haoxing Ren, CircuitOps: An ML Infrastructure Enabling Generative AI for VLSI Circuit Optimization, Proceedings of the IEEE ACM International Conference on Computer Aided Design, pp. 1 6, 2023. [85] P. Shrestha, S. Phatharodom, A. Aversa, D. Blankenship, Z. Wu, and I. Savidis, Deep Representation Learning for Electronic Design Automation, arXiv preprint arXiv:2505.02105, May 2025.",
    "source": "2506.00007v1_Emerging_ML-AI_Techniques_for_Analog_and_RF_EDA.pdf",
    "length": 1147,
    "tokens": 348
  },
  {
    "text": "of 0.5 than Clipper (not plotted). The accuracy gain seen in CIFAR-100 is lesser than ImageNet dataset because the class-based weighted voting works effectively when handling large number of classes (100 in CIFAR vs 1000 in ImageNet). Nevertheless, Cocktail is able to deliver the accuracy at 2x lower latency than InFaaS and 1.35x lower cost than Clipper. 6.4 General Applicability of Cocktail To demonstrate the general applicability of Cocktail to other classiÔ¨Åcation tasks, we evaluated Cocktail using a Sentiment Analysis application for two datasets. The results reported are averaged across both the datasets. Figure 15b plots the average number of models used by the three policies for the top four constraints. As shown for Const1, Cocktail shows similar reduction (as image-classiÔ¨Åcation) with only using 4.8 models on average, which is 40 and 26 lower than Clipper and Clipper-X, respectively. Cocktail is also able to reduce the number of models by 30 and 50 for medium ensembles (Const2 Const3) as well. Figure 16b plots the latency reduction and accuracy gain, compared to InFaaS (baseline). While being able to reduce 50 of the models used in the ensemble, Cocktail also re- duces latency by up to 50 and improves accuracy by up to 1.3 . Both Cocktail and Clipper deliver the same overall accuracy (96 , 94.5 , 93.5 , and 92 )). Since sentiment analysis only has 2-3 classes, there are no additional accuracy gains by using the class-based weighted voting. However, the model selection policy effectively switches between differ- ent models based on the structure of input text (equivalent to classes in images). For instance, complex sentences are more accurately classiÔ¨Åed by denser models compared to smaller. Despite the lower accuracy gains, Cocktail is able to reduce the cost (Figure 17) of model-serving by 1.45 and 1.37 for Wiki trace compared to InFaaS and Clipper, respectively. 7 Concluding Remarks There is an imminent need to develop model serving systems that can deliver highly accurate, low latency predictions at re- duced cost.",
    "source": "cocktail.pdf",
    "length": 2061,
    "tokens": 469
  },
  {
    "text": "This way, the model can learn to deal with possible inaccuracies that will occur during (lower-bit) inference. For this, an observer is placed at the position of weights and activations during training , cap- turing the possible values during the current forward step. We fake-quantize each tensor into the desired precision, meaning while computations still happen in FP32, only a limited number of values are allowed. One important property for memristor computation is the need for a symmetric quantization range with a fixed zero-point at 0. This can be challenging for PTQ, as there is no control over the value range and the distribution the model learns during training. In contrast, QAT allows to directly induce this property during training, nudging the model to learn a symmetric distribution within the given bit precision. 5. Simulated Execution of Conformer Blocks During QAT, we have observers that track the statistics of each weight and input. Assuming that for a specific linear layer we would have tracked the largest absolute input value to be 4 and the largest absolute weight value to be 0.1, we would have the input scaling factor 0.25 and a weight factor of 70 for a 3- crossbar stack with level representations from -7 to 7. For each memristor in the simulated cell pairs representing each weight, we would apply a large positive or negative voltage to set the desired high or low conductance states. During execution, we quantize the normalized input values with the DAC settings and apply the resulting input voltage via the cell simulation function. After reading the results from the simulated crossbars, we apply the previously calculated correction factor c, apply the ADC quantization, bit-shift the results with respect to their weight bit level, and then divide both the input factor and weight factor to retrieve the desired output. This process is performed for all parameterized linear transformations in the conformer encoder with the sole exception of the depth-wise convolution. This means that only less than 1 of the trainable weights of the conformer blocks remain outside of the memristor crossbars. Parameter-free operations such as activation functions, gating, the dot product of the self attention, and the softmax operations are computed as usual. The DAC precision which corresponds to the activation quantization during QAT, as well as the ADC precision, is fixed to 8 bits.",
    "source": "2505.24721v1_Running_Conventional_Automatic_Speech_Recognition_.pdf",
    "length": 2425,
    "tokens": 486
  },
  {
    "text": "This is due to two reasons: (i) The creative-writing dataset typically has longer output lengths, which makes the decoding phase a larger bottleneck for end-to-end performance, thereby making PAPI acceleration more beneficial. (ii) Longer output lengths of the creative- writing dataset lead to more significant dynamic changes in parallelization levels, thereby allowing PAPI to further improve performance over prior schemes. We conclude that PAPI provides significant performance ben- efits in LLM inference over state-of-the-art PIM-based designs across various real-world configuration settings (speculation length, batch size) and using different real datasets. Energy Efficiency. Figures 8(b) and 9 (b) present the end- to-end energy efficiency, normalized to A100 AttAcc system, for the creative-writing and general-qa datasets. PAPI im- proves average energy efficiency by 3.4 and 3.1 for these datasets, respectively, over A100 AttAcc.",
    "source": "2502.15470v2_PAPI_Exploiting_Dynamic_Parallelism_in_Large_Langu.pdf",
    "length": 945,
    "tokens": 201
  },
  {
    "text": "Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code NL code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage distill-then- RL training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6 and 72.9 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12 20 , while matching or even exceeding the performance of 671B DeepSeek-R1. We will release our model, training pipeline, and dataset to facilitate research in EDA and LLM communities. 1 Introduction Large language models (LLMs) have recently demonstrated remarkable progress on reasoning tasks when trained via reinforcement learning with verifiable reward (RLVR). Notable examples include OpenAI-o1 [25] and DeepSeek-R1 [3], which exhibit emergent reasoning capabilities on problems endowed with explicit verification procedures such as software programming and mathematical problem solving. This success suggests a promising opportunity to apply RLVR within electronic design automation (EDA), specifically to the automatic generation of hardware description languages (HDLs) like Verilog from natural-language (NL) specifications [36]. However, the three foundational components required for effective RLVR (i) a reliable verification environment, (ii) high-quality NL-code data, and (iii) an efficient training algorithm each present significant challenges in training reasoning LLMs for Verilog generation: Preprint. Under review. (1) Automated verification of hardware designs remains difficult. RLVR requires a verification environment capable of providing accurate rewards. However, even in the data-rich software coding domain, such environments are rare. For example, most problems in the programming-contest dataset APPS [6] have only one or two sets of unit tests, and they exhibit a false-positive rate of up to 60 when evaluated with an average of 20 unit tests [12]. Consequently, the software community has adopted the practice of using LLMs to generate additional unit tests in order to improve verification quality [11, 39].",
    "source": "2505.24183v2_CodeV-R1_Reasoning-Enhanced_Verilog_Generation.pdf",
    "length": 2393,
    "tokens": 500
  },
  {
    "text": "HydraNets, alongside the other works detailed in this section, focus on CNNs. There exist non-energy-focused works on RNNs, utilizing dynamic entropy-based beam-width1 (BW) modification for adaptive inference [54, 56]. Recent works have also demonstrated adaptive inference on transformer-based DNNs, including cascaded transformers with increasing numbers of patches [160], and input-based patch, block, and head selection [100]; however, these are not generally applicable to constrained devices, and none directly considers energy as a metric. 3.2 Multi-Exit Networks Proliferation in DNN complexity is helpful only when classifying difficult inputs, which are inherently rare. The features learned at earlier network layers are often sufficient for accurately classifying the vast majority of the data population [28, 143, 159]. Multi-exit networks offer a form of adaptive right-sizing where inputs can exit the network at intermediate layers, bypassing the unnecessary processing of subsequent layers. Such networks obviate the need for re-execution in the case of a failed inference and can be combined with fixed optimizations. The design of a multi-exit network includes the addition of side-branch exit heads to a backbone network; Fig. 1 below outlines a conventional DNN alongside a multi-exit variant of the same architecture. With trivial modifications, any DNN can be made an implicit ensemble of networks by adding exit layers. The exit-point(s) of a multi-exit network can be determined either pre-execution (subnet-based inference [143]) or at inference time (progressive inference [7]). The former allows for more reliable estimation of inference energy requirements, enabling dynamic voltage and frequency scaling (DVFS). However, inference always continues until the selected 1BW refers to the number of candidate sequences evaluated at each step. Energy-Aware Deep Learning on Resource-Constrained Hardware 7 exit layer, potentially resulting in wasted energy for easy inputs and or overthinking [63]. Instead, with progressive inference, the exit-point is determined during network execution, typically based on the uncertainty of the exit classification or following an energy-management policy. This can reduce energy consumption via dynamic adaptation to input complexity, but necessitates more intricate hardware-level adjustments to do so.",
    "source": "2505.12523v1_Energy-Aware_Deep_Learning_on_Resource-Constrained.pdf",
    "length": 2367,
    "tokens": 468
  },
  {
    "text": "When summing partial products derived from multiplying 5-bit weight and 7-bit activations, we expect that 5 7 12 bit accumulation is required. However, Figure 5 shows one may use a narrow 9-bit accumulator to sum 10 values before needing to use a wider accumulator, on average. 15 10 5 0 5 10 15 0 2000 4000 6000 8000 Weight Distribution 60 65 70 75 80 85 90 0 5000 10000 15000 20000 25000 Input Activation Distribution 600 400 200 0 200 400 600 0 20 40 60 80 Partial Product Distribution 5 6 7 8 9 10 Accum Bitwidth 0 10 20 30 40 50 Mean Summation Length Modelled vs Measured Summation Length Before Overflow in MobileNetV2 Measured During Inference Modelled by Random Walk Figure 5: Plotting the empirical measured average dot prod- uct length versus expected dot product length based on our random walk model. 5-bit Weights follow a normal distri- bution in the range [-15, 15], while 7-bit activations have a half-normal distribution in the range [0,127] after ReLU. Note that the plot shows that with the accumulation bitwidth equal to 10, we do not expect overflow at a summation length of about 32. In contrast, a naive analysis would conclude that 17 5 7 5 bits are required to avoid overflows, noting that 5 log2 32. 5 Vikas Natesh, H.T. Kung, and David Kong 5 Dual-Accumulator MAC Design In this section, we describe the hardware for dual-multiply-accumulate (dMAC) units, leveraging our observation that the majority of dot product sums do not overflow when using narrow accumulators. We first introduce the dMAC for integer dot products and then show how this design enables narrow accumulation in FP8. 5.1 Integer dMAC The integer dMAC unit uses a narrow adder (green in Figure 6) for most summations and a wide adder (red) to handle partial sums that overflow the narrow adder. It has a slightly higher area over- head than a conventional MAC unit, containing two adders and additional overflow handling logic. However, dMAC consumes sig- nificantly less dynamic power by exploiting the low overflow rate in DNN dot products. In addition, we clock-gate the wider accumu- lator to reduce dynamic power usage further when not performing wide accumulations.",
    "source": "2504.09072v1_MGS_Markov_Greedy_Sums_for_Accurate_Low-Bitwidth_F.pdf",
    "length": 2168,
    "tokens": 498
  },
  {
    "text": "The input to the models has a dimension of 96 100, and the encoder output of the MobileNetV1-CAE(0.25x) and DS- CAE1 models are 1 1 256 and 1 1 64 (cf. Table IIa and IIb), respectively. Therefore, the MobileNetV1-CAE(0.25x) model has a CR of (96 100) 256 37.5, while the DS-CAE1 model achieves a CR of (96 100) 64 150. We investigated how CAE models generalize across diverse monkey recordings. Using data from monkeys K and L, we trained CAE models on a combined dataset with 80 of the recordings from each monkey and evaluated their performance on individual test sets. Table IV compares models trained separately on monkeys K and L with those trained on the x TABLE III: Comparison between stochastic pruning and standard magnitude-based pruning for 8-bit quantization.",
    "source": "2504.06996v1_Neural_Signal_Compression_using_RAMAN_tinyML_Accel.pdf",
    "length": 772,
    "tokens": 196
  },
  {
    "text": "GraNNite: Enabling High-Performance Execution of Graph Neural Networks on Resource-Constrained Neural Processing Units Arghadip Das, Shamik Kundu, Arnab Raha, Soumendu Ghosh, Deepak Mathaikutty and Vijay Raghunathan Abstract Graph Neural Networks (GNNs) are crucial for learning and reasoning over graph-structured data, with appli- cations in network analysis, recommendation systems, and speech analytics. Deploying them on edge devices, such as client PCs and laptops, enables real-time processing, enhances privacy, and reduces cloud dependency. For instance, GNNs can augment Retrieval-Augmented Generation (RAG) for Large Language Models (LLMs) and enable event-based vision tasks. However, irregular memory access, sparse graphs, and dynamic struc- tures lead to high latency and energy consumption on resource- constrained devices. Modern edge processors combine CPUs, GPUs, and NPUs, where NPUs excel at data-parallel tasks but face challenges with irregular GNN computations. To address these gaps, we present GraNNite, the first hardware-aware framework tailored to optimize GNN deployment on commercial-off-the- shelf (COTS) state-of-the-art (SOTA) DNN accelerators using a systematic three-step methodology: (1) enabling GNN execution on NPUs, (2) optimizing performance, and (3) trading accuracy for further performance and energy efficiency gains. Towards that end, the first category includes techniques such as GraphSplit for workload distribution and StaGr for static graph aggregation, while GrAd and NodePad handle real-time updates for dynamic graphs. Next, performance improvement is acquired through techniques such as EffOp for control-heavy operations and GraSp for sparsity exploitation. For Graph Convolution layers, PreG, SymG, and CacheG reduce redundancy and memory transfers. The final class of techniques deals with quality vs efficiency tradeoffs QuantGr applies INT8 quantization to lower memory usage and computation time, while GrAx1, GrAx2, and GrAx3 optimize graph attention, broadcast-add, and sample-and-aggregate (SAGE)-max aggregation for higher throughput with minimal quality loss.",
    "source": "2502.06921v2_GraNNite_Enabling_High-Performance_Execution_of_Gr.pdf",
    "length": 2125,
    "tokens": 468
  },
  {
    "text": "It is important to differentiate our approach from methods such as TreeLUT [18], NeuraLUT-Assemble [5], and DWN [7]. While da4ml optimizes the implementation of quantized traditional neural networks, these methods re- formulates the entire model into inherently hardware-native structures. In particular, TreeLUT defines the model as collection of quantized decision trees, while NeuraLUT-Assemble and DWN construct models directly mapped into LUTs with particular fan-in. These methods are highly specialized and may achieve exceptional efficiency in terms of resource utilization and throughput, while they often come at the cost of reduced generalizability and slight accuracy degradation. In contrast, da4ml focuses on optimizing the core CMVM operations of quantized, conventional neural networks, which would enable more straightforward integration with existing machine learning workflows and is applicable on more complex tasks. In particular, we show that when the network is properly quantized with HGQ [10], 3All-inclusive time, until routing finishes Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 21 High-level feature jet tagging network Implementation Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] II [cycles] This Work: HGQ da4ml (HLS) 76.9 31 (44.1 ns) 12,682 0 19,056 702.2 1 This Work: HGQ da4ml (RTL) 76.5 17 (23.1 ns) 6,165 0 7,207 735.8 1 This Work: HGQ hls4ml 76.9 42 (57.6 ns) 16,081 57 26,484 729.4 1 This Work: HGQ hls4ml 76.5 35 (67.2 ns) 8,548 30 14,418 520.8 1 QKeras hls4ml [34] 76.3 15 (105 ns) 5,504 175 3,036 142.9 2 DWN [7] 76.3 10 (14.4 ns) 6,302 0 4,128 695.",
    "source": "2507.04535v1_da4ml_Distributed_Arithmetic_for_Real-time_Neural_.pdf",
    "length": 1646,
    "tokens": 450
  },
  {
    "text": "Experimental Setup Data preparation. We collect circuits for pre-training from various sources, including benchmark RTL code from ITC99 [41], Open- Cores [42], Chipyard [43], and VexRiscv [44]. All RTL designs are synthesized into netlists using Synopsys Design Compiler using NanGate 45nm technology library and then undergo physical design with Cadence Innovus. Gate and design quality metrics and statistics are obtained using Synopsys PrimeTime. For symbolic logic expres- sions, we use PySMT [37], a symbolic reasoning toolkit for formal verification, to construct and manipulate the Boolean expressions. The statistical details of our dataset are shown in TABLE II. For the expression dataset used by ExprLLM, we collected 313k original expressions, each augmented with functionally equivalent transformations, resulting in a total of 626k expressions. As for the circuit netlist data, we collected 100k subcircuit cones after chunking, which were also functionally augmented to reach 200k samples in total. Additionally, we included 10k aligned RTL and layout cones for cross-stage alignment. When applying NetTAG to downstream tasks, we directly use the original task datasets if available. Otherwise, we utilize designs from the aforementioned open-source benchmarks. TABLE II: Statistics of circuit expression and netlist dataset. Source Gate Expression Circuit Netlist Data Tokens (Avg.) Data Nodes (Avg.) ITC99 47k 6,960 4k 1,025 OpenCores 76k 212 55k 173 Chipyard 109k 9,849 20k 2,813 VexRiscv 81k 5,289 21k 901 Total 313k 5,810 100k 855 Implementation details. For the implementation of NetTAG, we initialize ExprLLM with LLM2Vec [38], an open-source LLM-based encoder based on Meta Llama-3.1-8B [45], with 8k maximum input tokens. For the graph transformer backbone of TAGFormer, we adopt SGFormer [46]. Additionally, we use NV-Embed [47] (maximum 32k input tokens) as the pre-trained RTL text encoder, and the layout en- coder is pre-trained using a graph contrastive objective on SGFormer. Each MLP contains three layers with a hidden dimension of 256, and the output dimension of NetTAG is set to 768.",
    "source": "2504.09260v1_NetTAG_A_Multimodal_RTL-and-Layout-Aligned_Netlist.pdf",
    "length": 2119,
    "tokens": 501
  },
  {
    "text": "Finally, the aggregation step requires 2d ‚Ñìmultiplications and memory accesses, with only half as many summations. B.5 Other blocks In this section, we synthetically review the two main other blocks required for a complete analysis of the Transformer BERT architecture. Feed Forward block This block, typically appended to the Attention or Efficient Attention layers after a normalization step, consists of two fully connected layers with size d2 Œ±. These layers sequen- tially increase decrease the embedding dimension by a factor ofŒ± with an activation function applied in between, followed by a skip connection. The total parameter count for this block is: Wff 2d2 Œ± (9) and activation size of: Aff 2‚Ñì d ‚Ñì d Œ±. (10) Overall, the Feed Forward block requires 4‚Ñì d (d Œ± 1) memory accesses and half as many summations and multiplications, with the majority of these operations required by the matrix multiplications in the fully connected layers. Normalization Layer The normalization layer performs simple layer-wise normalization, requir- ing 2 d parameters to store the mean and variance values. During inference, it performs (‚Ñì 1) d 2 memory accesses and ‚Ñì d summations and multiplications. The required activation size is ‚Ñì d 2, which is negligible compared to the activation sizes of the more computationally intensive layers. 21 References [1] H. Kopetz, W. Steiner, Internet of things (2022). doi:10.1007 978-3-031-11992-7_13. URL [2] A. Ometov, V. Shubina, L. Klus, J. Skibi nska, S. Saafi, P. Pascacio, L. Flueratoru, D. Q. Gaibor, N. Chukhno, O. Chukhno, et al., A survey on wearable technology: History, state-of-the-art and current challenges, Computer Networks 193 (2021) 108074. [3] J. Lin, W.-M. Chen, Y. Lin, J. Cohn, C. Gan, S. Han, Mcunet: Tiny deep learning on iot devices, arXiv:2007.10319 [cs] (11 2020).",
    "source": "2502.10001v1_EmbBERT-Q_Breaking_Memory_Barriers_in_Embedded_NLP.pdf",
    "length": 1825,
    "tokens": 490
  },
  {
    "text": "In contrast, ROUGE counts how many n-grams from the reference appear in the generated text (focusing on recall). However, both metrics primarily capture lexical rather than semantic similarity, which may not fully reflect the accuracy of the generated descriptions. To address this limitation, we take the initiative to apply embedding similarity and GPT score for evaluation. Embedding sim- ilarity calculates the cosine similarity between vector representations of generated and ground truth descriptions, using embeddings derived from OpenAI s text-embedding-3-large model. Meanwhile, GPT score uses GPT-4 to quantify the semantic coherence between descriptions by assigning a sim- ilarity score from 0 to 1, where 1 indicates perfect semantic alignment. These metrics provide a more nuanced evaluation by capturing the semantic essence of the descriptions, thus offering a more accurate assessment than previous methods. For details on the prompt used to calculate the GPT score, please refer to Appendix I. 4.5 GENERATION EVALUATION To evaluate LLMs capabilities in Verilog generation, we adopt the latest benchmark introduced by Chang et al. (2024a), which is an expansion based on the previous well-established RTLLM benchmark (Lu et al., 2024). The benchmark by Chang et al. (2024a) encompasses a broad spectrum of complexities across three categories: arithmetic, digital circuit logic, and advanced hardware designs. This benchmark extends beyond previous efforts by incorporating a wider range of more challenging and practical Verilog designs, thus providing a more thorough assessment of the models capabilities in generating Verilog code. 3 8 Published as a conference paper at ICLR 2025 Table 2: Evaluation results on Verilog understanding using the benchmark proposed in Section 3.4. BLEU-4 denotes the smoothed BLEU-4 score, and Emb. Sim. represents the embedding similarity metric. Best results are highlighted in bold. Model BLEU-4 ROUGE-1 ROUGE-2 ROUGE-L Emb. Sim.",
    "source": "2502.15832v1_DeepRTL_Bridging_Verilog_Understanding_and_Generat.pdf",
    "length": 1984,
    "tokens": 421
  },
  {
    "text": "The performance of ZettaLith depends upon whether the final TRIMERA stacks are power or area limited, and whether the PE meets 12 GHz timing closure. As there are 31,407 million copies of the PE in a ZettaLith, this optimization is commercially very valuable. The transistor count of a single PE in the CASCADE array is estimated in Table 3. The minimum transistor count for functionality is 470 transistors. A cushion of 5 transistors is added to account for extra transistors which may be required to achieve timing closure or reduce power consumption, to make the total of 494 transistors. This is the actual transistors of each PE. However, a further 11 transistors is added as a share of the inter-array CASCADE and CREST mechanism to obtain the effective average of 505 transistors per PE used for the calculations in this document. While not strictly the number of transistors in a PE, using 505 makes the calculations consistent and avoids a long-winded explanation every time it is used, or the introduction of yet-another acronym. 10.6 PE optimizations The architectural optimizations and trade-offs include: The output of the multiplier is FP5, in E3M1 format, where ideally it should be FP7. The 1 bit mantissa of an FP4 E2M1 number (with implied leading 1) has two states: (1.0, 1.1). The product of two such numbers has four states: (1.0, 1.1, 1.1, 10.01), requiring a 3 bit mantissa. 10.01 is truncated to 10 and normalized to 1.0 (with an increase in exponent), allowing a 1 bit mantissa. The difference is minor and should be inconsequential in transformer calculation. The adder is not a full FP8 adder, but is FP8 FP5, as the only source of the number to be added to the FP8 partial sum is the FP5 output of the multiplier. This saves many transistors. The adder is truncated, not rounded. Again, this saves many transistors. The 1 bit full adders used are CLRCL, used for its 10T design, high speed and suitability for GAAFET process nodes. CLRCL directly uses pass-transistor structures to convey signals, often resulting in fewer intermediate nodes storing charge.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2086,
    "tokens": 485
  },
  {
    "text": "Several prior works have assigned different preci- sions to different layers (aiming to preserve more sensitive layers in higher precision) [8, 9, 30, 34]. Other works have performed structured mixed-precision quantization by preserving a subset of input channels in higher precision. [6] preserved large-magnitude activation input channels (and the corresponding input channels in weights) in high precision, while quantizing the remaining values. [1, 35] reordered input channels in order to group large-magnitude activation channels together at the end of the matrix, and then left a portion of the input channels at the end of the matrix in higher precision for both weights and activations. These works were able to leverage low-precision datapaths for part of the computation since they quantize both weights and activations. However, these methods were unable to adapt to the unstructured nature of impor- tant values in weights and activations. In contrast, our fine-grained mixed-precision approach can adapt to unstructured sensitive val- ues in both weights and activations, while still performing the majority of computation using low precision datapaths. There have also been prior works which searched for the opti- mal mixed-precision configuration, either by using reinforcement learning [30] or by formulated the search as a Neural Architecture Search problem [31]. However, existing automatic mixed precision search methods are challenging to apply for LLMs, as these methods require repeated evaluation of the network with different precision configurations for each layer, which can be prohibitively expensive. Additionally, these approaches have previously been applied for layer wise mixed precision. With mixed precision at the 1D block granularity, the search space of potential precision assignments is much greater than the layer wise precision assignment problem. It is therefore infeasible to leverage existing automatic methods to search for the optimal fine-grained mixed precision assignment. 3 2.3 Hardware Support for Mixed-Precision Quantization Several prior works have investigated methods for adding archi- tecture support for representing numerical outliers in order to improve low-precision quantization. GOBO [33] combines outlier- aware quantization with non-uniform quantization for better low- precision representation. However, this approach requires sepa- rately loading outliers and dense values (as the sparse values are stored in a separate sparse matrix), as well as separate datapaths for outlier processing.",
    "source": "2504.14152v1_FGMP_Fine-Grained_Mixed-Precision_Weight_and_Activ.pdf",
    "length": 2558,
    "tokens": 502
  },
  {
    "text": "Although the accuracy remains significantly below SOTA (82 ), their compressed YOLOv2 model enables on-the-edge computations. As a general note, comparing SOTA-driven studies 15Dataset accuracies were compared to the benchmarks available at Manuscript submitted to ACM FPGA-Enabled Machine Learning Applications in Earth Observation: A Systematic Review 23 with HW-aware FPGA implementations is challenging without a cost benefit analysis. Quantifying the computational cost of a 1 accuracy gain would clarify the trade-off between task performance and hardware efficiency, yet, this is often missing in SOTA literature. Compact and Fast ML Models. Encoding models directly on-chip avoids loading parameters from peripherals, granting significant speed-up, as demonstrated by the OSCAR-RT framework in Yang et al. [132]. By mapping all CNN layers onto on-chip FPGA resources and using mixed-width quantization and optimized pipeline scheduling, they consistently compress YOLOv2 with different lightweight backbones under 0.5MB while maintaining accuracies above 90 mAP. In particular, their MobileNetv1 [52] and MobileNetv2 [106] designs run in 0.7ms, the lowest latency of the surveyed DL models. Wei et al. [126] achieve LeNet-5 inference in 2.29ms via a custom QAT algorithm, though their ablation study hints at sub-optimal FPGA resource usage. Upadhyay et al. [119] classify cloud coverages with their SICNet CNN in 1.8 ms, the lowest latency of Vitis AI implementations. The lowest per-pixel latency in this survey is achieved by the 2D CNN implemented by He et al. [48], reaching 97ùúáùë†on a Zynq US . However, such performance is facilitated by the abundance of resources of the Zynq US series, which comes with a power consumption of 8.4W. Low-Power Implementations for Resource-Constrained Systems. As RQ3 highlights, power efficiency is the main concern for hardware designs, especially onboard SmallSats [64]. Traditional ML methods and shallow networks offer low power consumption, such as MLP [43], RDBC [99], Fuzzy ARTMAP [130], and SVM-based models [42] consuming less than 1W.",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 2091,
    "tokens": 490
  },
  {
    "text": "3. The architecture comprises compute, memory, and control subsystems. The computing subsystem comprises a processing element (PE) array for performing MAC operations, an activation sparsity engine to exploit sparsity in activations, and a post-processing module (PPM) responsible for the rectified linear unit (ReLU) activation, quantization, pooling, bias addition, and residual addition. The memory subsystem consists of a global memory to store activations and parameters and a cache to exploit temporal data reuse. The control subsystem includes a top-level controller that schedules and sequences different operations and issues commands to various processing and memory blocks. A com- prehensive description of the architecture is presented in [44]. In this paper, we introduce a novel stochastic pruning scheme for weight pruning, that eliminates the indexing overhead of the compressed weights compared to our previous implemen- tation that utilizes conventional magnitude-based pruning [7], [44], [46], [47]. B. RAMAN features The architectural features of the RAMAN tinyML acceler- ator tailored for edge computing are outlined as follows: 1) Sparsity: RAMAN exploits both input activation and weight sparsity to reduce latency, memory access, and Activation Parameter Cache Global Memory PE PE Array PE PE PE PE PE PE PE PE PE PE PE Post Processing Activation Sparsity Engine Top-Level Controller Instruction Memory Layer Config. Inst. Stream IAs OAs Parameters Column Router Row Router LFSR Block Fig. 3: Top-Level architecture of the RAMAN accelerator. storage. RAMAN can skip the processing cycles with zero activations and weights to minimize the processing latency. The sparse weight matrices are compressed and stored in the memory. The stochastic pruning scheme pre- sented in Section III-C generates the compressed weight indices on the fly, eliminating the need for explicit index storage in memory. 2) Dataflow: RAMAN employs Gustavson s inspired [48] dataflow with optimal input and output activation reuse to reduce memory access. The dataflow reduces the partial sums (Psums) within the processing element to eliminate the Psum writeback traffic. 3) Peak Activation Memory Reduction: The state-of-the-art accelerators typically segment the Input Activations (IAs) and Output Activations (OAs) within the memory, with the total activation memory being the sum of IA and OA memory spaces.",
    "source": "2504.06996v1_Neural_Signal_Compression_using_RAMAN_tinyML_Accel.pdf",
    "length": 2411,
    "tokens": 497
  },
  {
    "text": "1 9. [22] S. Liu, G. Tao, Y. Zou et al., ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters, Nov. 2024, arXiv:2402.10930. [23] N. Burgess, J. Milanovic, N. Stephens et al., Bfloat16 Processing for Neural Networks, in 2019 IEEE 26th Symposium on Computer Arithmetic (ARITH), Jun. 2019, pp. 88 91. [24] F. Schuiki, F. Zaruba, T. Hoefler et al., Stream Semantic Registers: A Lightweight RISC-V ISA Extension Achieving Full Compute Utilization in Single-Issue Cores, IEEE Trans. Comput., vol. 70, no. 2, pp. 212 227, 2021. [25] A. Belano, Y. Tortorella, A. Garofalo et al., A Flexible Template for Edge Generative AI with High-Accuracy Accelerated Softmax GELU, Dec. 2024, arXiv:2412.06321. [26] L. Bertaccini, G. Paulin, T. Fischer et al., MiniFloat-NN and ExSdotp: An ISA Extension and a Modular Open Hardware Unit for Low- Precision Training on RISC-V Cores, in 2022 IEEE 29th Symposium on Computer Arithmetic (ARITH), Sep. 2022, pp. 1 8. [27] G. Paulin, P. Scheffler, T. Benz et al., Occamy: A 432-Core 28.1 DP- GFLOP s W 83 FPU Utilization Dual-Chiplet, Dual-HBM2E RISC-V- Based Accelerator for Stencil and Sparse Linear Algebra Computations with 8-to-64-bit Floating-Point Support in 12nm FinFET, in 2024 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits), Jun. 2024, pp. 1 2.",
    "source": "2504.11227v1_VEXP_A_Low-Cost_RISC-V_ISA_Extension_for_Accelerat.pdf",
    "length": 1327,
    "tokens": 435
  },
  {
    "text": "Gamora: Graph learning based symbolic reasoning for large-scale boolean networks. In Design Automation Conference (DAC), 2023. [55] Lilas Alrahis, Abhrajit Sengupta, Johann Knechtel, Satwik Patnaik, Hani Saleh, Baker Mohammad, Mahmoud Al- Qutayri, and Ozgur Sinanoglu. Gnn-re: Graph neural networks for reverse engineering of gate-level netlists. IEEE A Survey of Circuit Foundation Model: Foundation AI Models for VLSI Circuit Design and EDA 55 Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), 2021. [56] Subhajit Dutta Chowdhury, Kaixin Yang, and Pierluigi Nuzzo. Reignn: State register identification using graph neural networks for circuit reverse engineering. In International Conference on Computer-Aided Design (ICCAD), 2021. [57] Zhuolun He, Ziyi Wang, Chen Bai, Haoyu Yang, and Bei Yu. Graph learning-based arithmetic block identification. In International Conference on Computer-Aided Design (ICCAD), 2021. [58] Yuzhe Ma, Haoxing Ren, Brucek Khailany, Harbinder Sikka, Lijuan Luo, Karthikeyan Natarajan, and Bei Yu. High performance graph convolutional networks with applications in testability analysis. In Design Automation Conference (DAC)), 2019. [59] Zhiyao Xie, Guan-Qi Fang, Yu-Hung Huang, Haoxing Ren, Yanqing Zhang, Brucek Khailany, Shao-Yun Fang, Jiang Hu, Yiran Chen, and Erick Carvajal Barboza. FIST: A feature-importance sampling and tree-based method for automatic design flow parameter tuning. In Asia and South Pacific Design Automation Conference (ASP-DAC), 2020. [60] Walter Lau Neto, Yingjie Li, Pierre-Emmanuel Gaillardon, and Cunxi Yu. Flowtune: End-to-end automatic logic optimization exploration via domain-specific multiarmed bandit. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), 2022.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 1791,
    "tokens": 481
  },
  {
    "text": "9) and also improves the accuracy by 1.1 1.5 by computing the normalizing factor independently for each point feature. Table 2: Ablation study results on ScanObjectNN. Reorder Point-wise Reduced ScanObjectNN Norm. Dims. mAcc( ) OA( ) PointODE-Naive 82.5 84.3 81.9 83.8 PointODE 83.4 84.9 81.2 83.1 PointODE-Elite 82.6 84.2 4The asterisk denotes surface normals are used along with point coordinates. 10 PointODE: Lightweight Point Cloud Learning with Neural ODEs on Edge A PREPRINT Table 3 shows the accuracy of FPGA-based PointODE-Elite. While the FPGA implementation uses a fixed-point data type for point clouds and extracted features, it maintains nearly the same accuracy as the PyTorch counterpart on both datasets. This suggests more aggressive quantization techniques could be applied to PointODE-Elite, which is left as a future work. Note the reduction of the mean accuracy by 0.5 0.6 indicates the model makes slightly more incorrect predictions for categories with fewer training samples. Table 3: Classification accuracy on the ZCU104 board. Method FPGA ModelNet40 ScanObjectNN mAcc( ) OA( ) mAcc( ) OA( ) PointODE-Elite 90.5 93.4 82.6 84.2 PointODE-Elite 90.0 93.7 82.0 84.3 Fig. 11 plots the accuracy of PointODE-Elite with respect to the number of ODE iterations C on ScanObjectNN. By increasing C from 1 to 8, the overall accuracy improves by 1.1 (83.6 to 84.7 ) at the cost of linearly increasing computational cost in ODEPBlocks, which closes the gap between PointODE-Elite and PointMLP. Setting C 10 results in a sudden accuracy drop, which may be attributed to cumulative numerical errors in the ODE solver. Higher- order ODE solvers (e.g., Runge-Kutta) could be used instead to alleviate this problem. 1 2 4 6 8 10 of ODE Iterations 80 81 82 83 84 85 Accuracy (\\ ) OA mAcc Figure 11: Number of ODE forward iterations C and accuracy of PointODE-Elite on the ScanObjectNN dataset.",
    "source": "2506.00438v1_PointODE_Lightweight_Point_Cloud_Learning_with_Neu.pdf",
    "length": 1900,
    "tokens": 500
  },
  {
    "text": "This is because the TTFT is related to many factors, like the swap cost and vis- ited frequency. Figure 5 shows the relationship between the visited frequency, LRU time, and swap cost of each KV cache and LoRA. In the figure, each point represents a LoRA or KV cache, and its x-axis or y-axis represents its corresponding ranks of LRU Time Frequency Swap Cost. As observed, the points are randomly distributed, which means there is not clear correlation among these key factors. Relying on LRU to manage the HBM space is not efficient to minimize the TTFT, even if dynamic HBM usage is enabled. 4 Performance-driven Cache Swapper Frequency LLM Engine HBM Root LoRA-1 LoRA-2 KV1-1 KV1-2 KV1-3 Main Memory Hit New Token Required Dependency-aware Cache Manager Query Cost Model LRU Time ...... Dependency Construct Dependency Maintain 2nd layers with LoRAs Swap-in on leaves ...... Usage Dependency Benefits to TTFT Swap-in Swap-out State LoRA Quantity INPUT: OUTPUT: Swap-out leaves Figure 6: Design overview of FASTLIBRA. 3 FASTLIBRA Methodology In this section, we summarize the challenges of FASTLIBRA, and introduce the overview of FASTLIBRA. 3.1 Challenges of FASTLIBRA According to the above analysis, two technical challenges should be addressed to resolve the above problems. Firstly, the neglect of the intra-LoRA usage dependencies between LoRAs and KV caches brings invalid KV caches in HBM. This can prevent other useful KVs or LoRAs from being loaded, increasing the cold-start overheads. A suitable scheme is required to construct the usage dependencies among the LoRAs and KV caches and consistently main- tain the dependencies when serving the queries. Secondly, when loads of different LoRAs vary, the required HBM space for caching LoRAs and the hotness of KV caches of different LoRAs change accordingly. Thus, we need to balance the HBM usage for LoRAs and KVs, and swap-in or swap-out the appropriate KVs and LoRAs to optimize the TTFT. An appropriate mechanism is needed to access the benefits or harms of swapping-in or out each LoRA or KV cache to the TTFT of future queries.",
    "source": "2505.03756v1_Improving_the_Serving_Performance_of_Multi-LoRA_La.pdf",
    "length": 2097,
    "tokens": 500
  },
  {
    "text": "Similarly, other approximation-via-data-reduction techniques, such as sub-sampling, did not perform inference with a desirable accuracy. Collectively, the aforementioned figures demonstrate that the harvested energy budget is insuf- ficient to perform all inferences with acceptable accuracy on currently proposed EH-WSN systems. Therefore, to complete all the scheduled computations, and thereby to improve ac- curacy, the system must rely on another device (e.g. a mobile phone), where sufficient resources are available to complete any remaining inference, if the data can be sent from the sensor. Said coordinating device completes the rest of the computations and finally, aggregates them with the ones completed in the sensor nodes. The challenge here is to send 1Throughout the paper we evaluate many of our motivation results using HAR as a workload as it is one such application, where the (EH-)WSN, used as body area network, fits perfectly with RF or body movement as the har- vesting source. HAR has the nuances of human introduced unpredictability and sensor induced noises. HAR has been pervasive enough given the rise of smart wearables and has been studied well enough to have ample access to resources to make a judicious evaluation. Further, we also evaluate one more emerging application from predictive maintenance domain. Algorithm Compression Ratio Accuracy Loss ( ) Fourier Decomposition 3 - 5 9.1 - 18.3 DCT 3 - 5 5.8 - 16.2 DWT 3 - 6 5.3 - 12.7 Coreset 3 - 10 0.02 - 0.76 Table 1: Accuracy trade-off of different compression techniques: Low-dimensional data loses important fea- tures under lossy compression, dropping inference ac- curacy significantly compared to the original data. De- tails on Coreset are available on Section 4. (Notations used: DCT: Discrete Cosine Transform, DWT: Discrete Wavelet Transform.",
    "source": "Seeker.pdf",
    "length": 1840,
    "tokens": 399
  },
  {
    "text": "Nevertheless, to fully harness the potential of approximate multipliers in AxDNNs, an exhaustive search to identify the most suitable layer-wise combination of approximate multipliers is indispensable [1]. Indeed, such an exhaustive search can help achieve the best trade-off between accuracy and energy efficiency in AxDNNs. However, the simulation of approximate multipliers does not scale well on CPUs and GPUs [3], which slows the overall search process and makes such an exhaustive search imprac- tical for large designs. Researchers have tried to propose different solutions to solve the above-mentioned problem. Since approximation errors originating from approximate mul- tipliers exhibit a Gaussian distribution, several studies exploit Gaussian noise injection to identify resilient DNN layers [4] and then apply approximate multipliers solely to these resilient This material is based upon work supported by the National Science Foundation (NSF) under Award Numbers: CCF-2323819. Any opinions, findings, conclusions, or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of the NSF. 0 50 100 0 5 10 15 20 25 30 Accuracy [ ] Noise Magnitude [dB] L1 L2 L3 L4 L5 (a) Approximation error resilience -50 50 150 250 350 5 10 18 34 50 GPU Hours Number of Layers (b) Execution time Fig. 1: Approximation error resilience of layers L1 to L5 in Lenet-5 using White Gaussian Noise and execution time of AxDNNs. Noise magnitude of 0 dB denotes Lenet-5 without approx. noise. layers to accelerate AxDNN simulations. Note that selecting the appropriate Gaussian noise magnitude and approximation multipliers involves a trial-and-error method, which can be prone to human error. As an alternative approach, a mathemat- ical model [2] estimates the magnitude of approximation noise in approximate multipliers of AxDNNs. However, it relies on the input values obtained from all convolutional layers, which may change in real-time applications. Also, when such a model is evaluated with a uniform distribution of activations, the resulting noise magnitude differs from the noise magnitude estimated for approximation multipliers in a pre-trained model. Other strategies involve exploiting the protobuf format [5] and Cartesian Genetic Programming (CGP) [6] to find the most suitable approximate multiplier serving multiple AxDNN layers.",
    "source": "2503.16583v1_Explainable_AI-Guided_Efficient_Approximate_DNN_Ge.pdf",
    "length": 2390,
    "tokens": 495
  },
  {
    "text": "Sustainable Computing: Informatics and Systems 40 (2023), 100926. doi:10.1016 j.suscom.2023.100926 [22] Xuanyi Dong and Yi Yang. 2020. NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search. arXiv:2001.00326 [cs.CV] [23] Ying-Jun Du, Jun Xu, Huan Xiong, Qiang Qiu, Xiantong Zhen, Cees G. M. Snoek, and Ling Shao. 2020. Learning to Learn with Variational Information Bottleneck for Domain Generalization. CoRR abs 2007.07645 (2020). arXiv:2007.07645 Energy-Aware Deep Learning on Resource-Constrained Hardware 17 [24] Cian Eastwood, Ian Mason, and Christopher K. I. Williams. 2022. Unit-level surprise in neural networks. In Proceedings on \"I (Still) Can t Believe It s Not Better!\" at NeurIPS 2021 Workshops (Proceedings of Machine Learning Research, Vol. 163), Melanie F. Pradier, Aaron Schein, Stephanie Hyland, Francisco J. R. Ruiz, and Jessica Z. Forde (Eds.). PMLR, 33 40. [25] Amir Erfan Eshratifar, Mohammad Saeed Abrishami, and Massoud Pedram. 2018. JointDNN: An Efficient Training and Inference Engine for Intelligent Mobile Cloud Computing Services. CoRR abs 1801.08618 (2018). arXiv:1801.08618 [26] Biyi Fang, Xiao Zeng, Faen Zhang, Hui Xu, and Mi Zhang. 2020. FlexDNN: Input-Adaptive On-Device Deep Learning for Efficient Mobile Vision. In 2020 IEEE ACM Symposium on Edge Computing (SEC). 84 95. doi:10.1109 SEC50012.2020.00014 [27] Terry Fawden, Lorena Qendro, and Cecilia Mascolo. 2023. Uncertainty-Informed On-Device Personalisation Using Early Exit Networks on Sensor Signals. 2023 31st European Signal Processing Conference (EUSIPCO) (2023), 1305 1309.",
    "source": "2505.12523v1_Energy-Aware_Deep_Learning_on_Resource-Constrained.pdf",
    "length": 1592,
    "tokens": 479
  },
  {
    "text": "The method without reward shaping is also omitted. Both GA and the non-reward shaping method converge faster, but our work achieves convergence with significantly better metrics. The results show that CORE requires fewer samples to achieve lower latency and LAS in both cases. Discussion. While our approach demonstrates strong performance across simulated metrics in efficiently navigating structured design spaces, its effectiveness depends on the fidelity of the un- derlying cost models. In the context of DNN accelerator design, our formulation further assumes static, compile-time mapping strategies. Extending the framework to support dynamic or runtime- adaptive dataflows presents a promising direction for future work. Interestingly, a recent method [10] independently proposes a similar idea to compute the advantage via intra-batch reward comparisons in the context of optimizing LLM training dynamics, such as token sampling and update strategies. This convergence highlights the generality of our approach, suggesting its applicability beyond DNN accelerator design to other complex, constraint-rich domains such as compiler tuning, robotic system co-design, and automated system architecture search, where simulation costs and structural constraints are similarly restrictive. 5 Conclusion We presented CORE, a critic-free, one-step reinforcement learning approach for constraint-aware, simulation-guided design space exploration. CORE addresses critical limitations of existing meth- ods by incorporating a surrogate objective based on relative advantages and enforcing feasibility constraints through a scaling-graph-based decoder. We validated CORE s effectiveness on the chal- lenging task of neural network accelerator hardware-mapping co-design, demonstrating significantly improved sampling efficiency and better optimization outcomes compared to state-of-the-art methods. Although demonstrated in the accelerator design domain, the CORE framework and its representa- tions generalize broadly to other complex design tasks. Future work includes extending CORE to 9 dynamic design environments, incorporating runtime adaptivity, and integrating it with symbolic reasoning or large language models for enhanced interpretability and scalability. References [1] John H Holland. Genetic algorithms. Scientific american, 267(1):66 73, 1992. [2] Bowen Lei, Tanner Quinn Kirk, Anirban Bhattacharya, Debdeep Pati, Xiaoning Qian, Raymundo Arroyave, and Bani K Mallick. Bayesian optimization with adaptive surrogate models for automated experimental design. Npj Computational Materials, 7(1):194, 2021.",
    "source": "2506.03474v1_CORE_Constraint-Aware_One-Step_Reinforcement_Learn.pdf",
    "length": 2613,
    "tokens": 487
  },
  {
    "text": "It also outperforms Exo Unoptimized code by 2.9 , and Gemmini s software library by 2.6 , and in all cases achieves at least 78 of the hardware FSM s utilization. 4.5 Fine-Grained Linear Algebra Finally, we optimize fine-grained linear algebra benchmarks from the TinyMPC model-predictive control library [40], specifically the forward and backward passes of the primal update step. These benchmarks contain sequences of floating-point matrix-vector multiplications, interleaved with element-wise addition and subtraction. The inclusion of CPU-accelerator dependencies, low reuse, and a high ratio of data movement to computation leads to low accelerator utilization and makes this code challenging to optimize. We compare Autocomp-generated code against Dong et al. [14] s unoptimized software-based implementation on a 4 4 FP32 accelerator. For this work, we additionally had an expert hand-tune a hardware FSM-based implementation. The unoptimized software-based implementation is used as the starting point for search, and we use the same search parameters as for convolution, except with T 15 iterations, which takes about 12 hours. However, some of the optimization menu options are different from those used for GEMM convolution (see Appendix B). As shown in Fig. 9, Autocomp outperforms even the expert-optimized hardware FSM implementation on the forward pass (by 1.6 ), and across benchmarks speeds up unoptimized code by a geomean of 2.7 . To outperform the hardware FSM implementation, Autocomp harnesses the flexibility of software- based implementation. It optimizes the code by hoisting data loads shared between kernels (reducing data movement beyond what is possible for the hardware FSM implementation), as well as utilizing fine-grained software pipelining and eliminating blocking operations where possible. This experiment highlights Autocomp s adaptability: we optimize a new benchmark, running on an accelerator with a new size and data type, with highly different performance characteristics from previous 8 Forward Pass Backward Pass 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 Speedup 1.0 1.0 3.0 2.4 1.9 2.4 SW Unopt Autocomp HW FSM Opt Figure 9: Speedup for fine-grained linear algebra benchmarks.",
    "source": "2505.18574v2_Autocomp_LLM-Driven_Code_Optimization_for_Tensor_A.pdf",
    "length": 2213,
    "tokens": 499
  },
  {
    "text": "This formula rep- resents the theoretical maximum performance of the realized hardware architecture. While the theoretical peak throughput indicates the maxi- mum achievable performance of the hardware design, actual throughput reflects the real-world performance. According to Eq. (6), the peak throughput is influenced by the number of MAC units, which is directly related to the parallelism of the hardware framework. Larger parallelism leads to more MAC units and increased computational capacity. Compute Efficiency: The theoretical peak throughput rep- resents the ideal maximum performance of a hardware system, acting as the upper limit of an accelerator s capacity. However, in practical CNN acceleration, the network parameters may not align with the computational parallelism of the design. This misalignment can cause unbalanced workloads among the processing elements (PEs) in the computation engine, leading to suboptimal utilization of available resources. Additionally, factors such as memory bandwidth and data transfer rates significantly impact throughput, often creating bottlenecks. As a result, actual throughput is lower than the theoretical peak performance due to these practical constraints. To evaluate the efficiency of a hardware design, compute ef- ficiency, also referred to as MAC efficiency, is employed [27]. This metric is defined as the ratio of the actual throughput to the peak theoretical throughput. It serves as a critical measure of hardware accelerators effectiveness by quantifying the proportion of useful MAC cycles relative to the total MAC units in the design. Compute efficiency is calculated using the formula: Compute EFF. Throughput 2 MACs f (7) Unlike other evaluation metrics mentioned above, compute efficiency is independent of the specific FPGA device or the number of MAC units utilized. Instead, it reflects the intrinsic efficiency of the accelerator design, making it a more reliable measure for assessing and comparing different hardware architectures for CNN acceleration. 5 Resource Efficiency: In CNN accelerators, resource ef- ficiency measures the system s throughput relative to the number of DSP units used [28]. This metric evaluates the contribution of each DSP unit to the computational task and is defined as: Res. EFF. Throughput DSP used (8) Higher resource efficiency reflects better DSP utilization, with each unit delivering significant computational throughput. This enhances hardware resource usage while maintaining high performance, reducing power consumption and idle re- sources, and improving overall system efficiency and stability.",
    "source": "2505.13461v1_FPGA-based_Acceleration_for_Convolutional_Neural_N.pdf",
    "length": 2618,
    "tokens": 477
  },
  {
    "text": "Original code: uint32_t b_offset 16 16 4 8 sizeof(int8_t); for (int_fast32_t y 0; y 8; y ) { uint32_t b_base_y 64 y; Load B matrix slice for (int_fast32_t zo 0; zo 8; zo ) { uint32_t b_zo_offset 4 16 zo; Number of columns per zo iteration for (int_fast32_t z 0; z 4; z ) { uint32_t b_index ((zo 4 z) ((16 4) 16)) 16; Divide number of elements by 16 since scratchpad is row -indexed mvin3( B[b_zo_offset 16 z][ b_base_y], b_offset b_index , 16 4, 16); }} for (int_fast32_t x 0; x 32; x ) { uint32_t res 1 31; uint32_t a_base_x 16 x; Load A matrix slice for (int_fast32_t zo 0; zo 8; zo ) { uint32_t a_index (zo (16 4) 16) 16; mvin2( A[a_base_x ][64 zo], a_index , 16 4, 16); } Computation for (int_fast32_t zo 0; zo 8; zo ) { uint32_t a_index (zo (16 4) 16) 16; for (int_fast32_t z 0; z 4; z ) { uint32_t preload_flag (zo 0 z 0) ? 0 : 0x40000000; for (int_fast32_t y_in_o 0; y_in_o 4; y_in_o ) { uint32_t preload_index ((zo 4 z) ((16 4) 16) y_in_o (16 16)) 16; Find correct scratchpad index to load B from preload(b_offset preload_index , res (y_in_o (16 16)) 16 preload_flag , 16, 16, 16, 16); compute_preloaded(a_index (z (16 16)) 16, (( uint32_t)0), 16, 16, 16, 16); }}} Store C matrix slice for (int_fast32_t y_in_o 0; y_in_o 4; y_in_o ) { mvout( C[a_base_x ][ b_base_y 16 y_in_o], res (y_in_o (16 16)) 16, 16, 16); Divide number of elements by 16 since accumulator is row -indexed }}} Retiled code uint32_t b_offset 16 16 4 8 sizeof(int8_t); for (int_fast32_t y 0; y 2; y ) { Reduce number of y dimension outer loop iterations uint32_t b_base_y 256 y; Load larger B matrix slice Tiling reduces redundant loads of B matrix , reducing data movement and increasing data reuse for (int_fast32_t zo 0; zo 8; zo ) { uint32_t b_zo_offset 4 16 zo; Number of columns per zo iteration for (int_fast32_t z 0; z 4; z ) { for (int_fast32_t y_in 0; y_in 4; y_in ) { uint32_t b_index (((zo 4 z) 4 y_in) ((16 4) 16)) 16; Divide number of elements by 16 since scratchpad is row -indexed mvin3( B[b_zo_offset 16 z][ b_base_y 64 y_in], b_offset b_index , 16 4, 16); }}} for (int_fast32_t x 0; x 32; x ) { uint32_t res 1 31; uint32_t a_base_x 16 x; Load A matrix slice Tiling reduces redundant loads of A matrix , reducing data movement and increasing data reuse for (int_fast32_t zo 0; zo 8; zo ) { uint32_t a_index (zo (16 4) 16) 16; mvin2( A[a_base_x ][64 zo], a_index , 16 4, 16); } Computation for (int_fast32_t zo 0; zo 8; zo ) { uint32_t a_index (zo (16 4) 16) 16; for (int_fast32_t z 0; z 4; z ) { uint32_t preload_flag (zo 0 z 0) ? 0 : 0x40000000; for (int_fast32_t y_in_o 0; y_in_o 16; y_in_o ) { Increase number of Y dimension inner loop iterations to increase tile size uint32_t preload_index (((zo 4 z) 4) ((16 4) 16) y_in_o (16 16)) 16; Find correct scratchpad index to load B from preload(b_offset preload_index , res (y_in_o (16 16)) 16 preload_flag , 16, 16, 16, 16); compute_preloaded(a_index (z (16 16)) 16, (( uint32_t)0), 16, 16, 16, 16); }}} Store C matrix slice for (int_fast32_t y_in_o 0; y_in_o 16; y_in_o ) { Move out a larger tile in the Y dimension mvout( C[a_base_x ][ b_base_y 16 y_in_o], res (y_in_o 16 16) 16, 16, 16); Divide number of elements by 16 since accumulator is row -indexed }}} Figure 15: In-context learning example of tiling, provided during the code generation phase in Sec.",
    "source": "2505.18574v2_Autocomp_LLM-Driven_Code_Optimization_for_Tensor_A.pdf",
    "length": 3304,
    "tokens": 1273
  },
  {
    "text": "[135] A. Jung, H. Lefeuvre, C. Rotsos, P. Olivier, D. O noro-Rubio, F. Huici, and M. Niepert, Wayfinder: Towards automatically deriving optimal os configurations, in Proceedings of the 12th ACM SIGOPS Asia-Pacific Workshop on Systems, 2021, pp. 115 122. [136] T. Chen, L. Zheng, E. Yan, Z. Jiang, T. Moreau, L. Ceze, C. Guestrin, and A. Krishnamurthy, Learning to optimize tensor programs, Ad- vances in Neural Information Processing Systems, vol. 31, 2018. [137] T. Chen and C. Guestrin, Xgboost: A scalable tree boosting system, in Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, 2016, pp. 785 794. [138] P. A. Vikhar, Evolutionary algorithms: A critical review and its future prospects, in 2016 International conference on global trends in signal processing, information computing and communication (ICGTSPICC). IEEE, 2016, pp. 261 265. [139] Y. Zhai, S. Yang, K. Pan, R. Zhang, S. Liu, C. Liu, Z. Ye, J. Ji, J. Zhao, Y. Zhang et al., Enabling tensor language model to assist in generating high-performance tensor programs for deep learning, in 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), 2024, pp. 289 305.",
    "source": "2506.05007v1_QiMeng_Fully_Automated_Hardware_and_Software_Desig.pdf",
    "length": 1198,
    "tokens": 341
  },
  {
    "text": "Fig. 10 shows different generations (g) of the AxDNN search process for the Table I: Best parameter configurations for approximate LeNet5 (L5), ResNet18 (R1), ResNet-34 (R2), and ResNet-50 (R3) architectures generated with XAI-Gen. M1 is an accurate multiplier. M2-M7 refers to approximate multipliers namely, KV8 (0.0018 MAE), KV9 (0.0064 MAE), KVP (0.051 MAE), L2J (0.081 MAE), L2L (0.23 MAE), and L2N (0.52 MAE) in Evoapprox8b [15] library.",
    "source": "2503.16583v1_Explainable_AI-Guided_Efficient_Approximate_DNN_Ge.pdf",
    "length": 443,
    "tokens": 160
  },
  {
    "text": "arXiv:2412.02732 [cs] [116] Mingxing Tan and Quoc V. Le. 2020. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. 1905.11946 arXiv:1905.11946 [cs] [117] VAMF Torres, BRA Jaimes, ES Ribeiro, MT Braga, EH Shiguemori, HFC Velho, LCB Torres, and AP Braga. 2020. Combined Weightless Neural Network FPGA Architecture for Deforestation Surveillance and Visual Navigation of UAVs. ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE 87 (Jan. 2020). [118] Yaman Umuroglu, Nicholas J. Fraser, Giulio Gambardella, Michaela Blott, Philip Leong, Magnus Jahre, and Kees Vissers. 2017. FINN: A Framework for Fast, Scalable Binarized Neural Network Inference. In Proceedings of the 2017 ACM SIGDA International Symposium on Field-Programmable Gate Arrays. 65 74. arXiv:1612.07119 [cs] [119] G Upadhyay, S Ghosal, S Kart, K Jain, SH Shantala, JT LalitKrushna, S Srividhya, and SA Balwantrao. 2024. Design and Implementation of CNN-based Custom Net Architecture with Improved Inference Time for Realtime Remote Sensing Application. In 2024 IEEE SPACE, AEROSPACE AND DEFENCE CONFERENCE, SPACE 2024. 647 651. [120] UPV EHU. 2021. Hyperspectral Remote Sensing Scenes - Grupo de Inteligencia Computacional (GIC). [121] P Vitolo, A Fasolino, R Liguori, L Di Benedetto, A Rubino, and GD Licciardo. 2024. Real-Time On-board Satellite Cloud Cover Detection Hardware Architecture Using Spaceborne Remote Sensing Imagery. In University of Salerno, MF Carlsohn (Ed. ), Vol. 13000. [122] Erwei Wang, James J. Davis, Peter Y. K. Cheung, and George A. Constantinides. 2019. LUTNet: Rethinking Inference in FPGA Soft Logic.",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 1613,
    "tokens": 494
  },
  {
    "text": "By utilizing these constraints, SCRec achieves model parallelism in the embedding layer through table-wise splitting of EMBs in the SRM. Three-level Sharding The EMBs assigned to SmartSSDs are split row-wise based on their access frequency patterns and are fetched into the FPGA s DRAM, BRAM, and SSD memory devices. The constraints are structured as follows to minimize EMB access latency: X i x dramji ICDFj(i) row lenj dimj dfj mem dramj j J (9) X i x dramji i stepj pct dramj j J (10) X i x dramji 1 j J (11) x dramji {0, 1} i 0, . . . , stepj j J (12) The above four equations constrain which rows of EMB j are fetched into the FPGA DRAM and determine the size of those rows. The variable x dram serves as a binary variable to compute the pct dram variable, which determines , which specifies the percentage of rows from EMB j are stored in DRAM, as described in Equation 10. At this point, the step size for EMB j is set to min(row lenj, 100). For the ICDF, the CDF obtained from the DSA is inverted and the interpolation interval is determined by stepj using piecewise linear interpolation, reducing the computational complexity of the cost model. Equation 9, formulated using the ICDF, calculates the size of EMB j to be fetched into DRAM, with mem dramj serving as a constraint on the FPGA DRAM capacity of the SmartSSD. X i x ptr ttji ICDFj(i) row lenj dimj dfj mem dramj mem ttj j J (13) X i x dramji i stepj X i x ttji i stepj X i x ptr ttji i stepj j J (14) X i x ttji i stepj pct ttj j J (15) X i x ttji X i x ptr ttji 1 j J (16) x ttji {0, 1} i 0, . . . , stepj j J (17) x ptr ttji {0, 1} i 0, . . .",
    "source": "2504.00520v1_SCRec_A_Scalable_Computational_Storage_System_with.pdf",
    "length": 1614,
    "tokens": 489
  },
  {
    "text": "[62] Xudong Kang. 2017. Airport-Beach-Urban Website. Retrieved 2024-11-08 from [63] JH Kim, Y Kim, DH Cho, and SM Kim. 2024. On-Orbit AI: Cloud Detection Technique for Resource-Limited Nanosatellite. INTERNATIONAL JOURNAL OF AERONAUTICAL AND SPACE SCIENCES (Dec. 2024). [64] Vivek Kothari, Edgar Liberis, and Nicholas D. Lane. 2020. The Final Frontier: Deep Learning in Space. In Proceedings of the 21st International Workshop on Mobile Computing Systems and Applications. ACM, Austin TX USA, 45 49. [65] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Classification with Deep Convolutional Neural Networks. In Advances in Neural Information Processing Systems, Vol. 25. Curran Associates, Inc. [66] Kung. 1982. Why systolic architectures? Computer 15, 1 (1982), 37 46. [67] Kevin Lange, Federico Fontana, Francesco Rossi, Mattia Varile, and Giovanni Apruzzese. 2024. Machine Learning in Space: Surveying the Robustness of on-Board ML Models to Radiation. arXiv:2405.02642 [68] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep Learning. Nature 521, 7553 (May 2015), 436 444. [69] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. 1998. Gradient-Based Learning Applied to Document Recognition. Proc. IEEE 86, 11 (Nov. 1998), 2278 2324. [70] George Lentaris, Konstantinos Maragos, Ioannis Stratakos, Lazaros Papadopoulos, Odysseas Papanikolaou, Dimitrios Soudris, Manolis Lourakis, Xenophon Zabulis, David Gonzalez-Arjona, and Gianluca Furano. 2018. High-Performance Embedded Computing in Space: Evaluation of Platforms for Vision-Based Navigation. Journal of Aerospace Information Systems 15, 4 (2018), 178 192. [71] Chao Li, Rui Xu, Yong Lv, Yonghui Zhao, and Weipeng Jing.",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 1705,
    "tokens": 496
  },
  {
    "text": "[174] propose RALAD (Re- trieve Augmented Large Language Model Aided Design), a framework leveraging LLMs and RAG to optimize HLS programs without requiring computationally expensive fine-tuning. HLS allows circuit design using high-level languages like C C , but manual optimization remains highly expertise-driven. RALAD mitigates this challenge by embedding user code and a knowledge base (e.g., FPGA textbooks), retrieving relevant code snippets via a top-k search, generating prompts that incorporate user instructions and retrieved snippets, and using an LLM like CodeLlama to produce optimized code. The study also explores the impact of manual annotations to further refine optimiza- tion quality, demonstrating the framework s effectiveness in automating HLS code improvements. A Survey of Circuit Foundation Model: Foundation AI Models for VLSI Circuit Design and EDA 41 VerilogReader Aug. 2024 ChIRAAG Jan. 2024 AssertLLM Feb. 2024 LLM4DV Oct. 2023 UVLLM Nov. 2024 AutoSVA2 Sep. 2023 NL2SVA Sep. 2023 VeriAssist May. 2024 MEIC May. 2024 RTLFixer Nov. 2023 HDLDebugger Mar. 2024 Works on Verification Works on Debugging FVEval Oct. 2024 AssertionBench Feb. 2025 Fig. 16. Timeline of RTL verification (Section 5.4) and debugging works (Section 5.5). 5.4 LLM for Hardware Code Verification In addition to circuit code generation, verifying the functional correctness of circuit designs is a critical yet highly labor-intensive task that heavily relies on human engineers. To address this challenge, LLM-based solutions have been explored to automate hardware verification. Table 8 summarizes existing works in this direction. Current LLM-based verification approaches focus on two primary directions: 1) Assertion generation with LLMs. These approaches leverage LLMs to generate assertions based on design specifications or RTL code [177, 178, 181, 182, 188 191]. The generated assertions are then used to validate whether the design under test (DUT) complies with its specifications, with either formal verification tools (e.g., Cadence JasperGold) for static formal property verification or simulation tools (e.g., Synopsys VCS) for dynamic verification on test benches. 2) Test bench generation with LLMs.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 2216,
    "tokens": 497
  },
  {
    "text": "4.5 Prediction Arbitration Whenever an H2P-resident branch is fetched, all engines fire con- currently: the baseline TAGE tables, the statistical corrector (SC), and Bullseye s two perceptrons. The final outcome is chosen by a lightweight, confidence-based arbiter: i Perceptron gate: Each perceptron supplies a two-bit conf field that encodes (a) its running win-rate over TAGE-SC-L (high if win-rate 55 ), and (b) whether output ùúÉfor the current instance. A perceptron asserts strong confidence only when both tests pass. ii TAGE gate: TAGE-SC-L asserts strong confidence when (a) the selected TAGE component s usefulness 3 or (b) the SC overrides with a magnitude 0. iii Decision rule: If at least one perceptron has strong confi- dence and TAGE-SC-L does not, Bullseye chooses the percep- tron outcome. Otherwise, the arbiter defaults to TAGE-SC-L. This rule preserves accuracy on easy branches while allow- ing neural takeover only during sustained benefits. iv Decision rule: If at least one perceptron has strong confi- dence, Bullseye chooses the perceptron outcome. Otherwise, the arbiter defaults to TAGE-SC-L. This rule preserves accu- racy on easy branches while allowing neural takeover only during sustained benefits. 4.6 Selective TAGE Filtering Once a branch has delivered 128 consecutive correct perceptron pre- dictions without a single TAGE-SC-L win, its updates are filtered i.e., subsequent outcomes bypass all TAGE and SC tables. Filtering prevents low-utility data from evicting well-trained entries and cuts energy by avoiding unneeded SRAM writes. If the perceptron later falters (confidence drops below the strong threshold), filtering is automatically revoked, ensuring that valuable geometric history is never lost permanently. Empirically, this mechanism yields a modest accuracy gain ( 0.3 BPC) for negligible extra storage. 5 Experimental Results and Analysis We present results for CycWpPKI and BrMisPKI metrics. Overall, Bullseye achieves an average CycWpPKI of 145.09 and an average BrMisPKI of 3.405.",
    "source": "2506.06773v1_Taming_Wild_Branches_Overcoming_Hard-to-Predict_Br.pdf",
    "length": 2035,
    "tokens": 485
  },
  {
    "text": "5, pp. 1773 1786, 2021. [21] J. Chen, F. Tu, K. Shao, F. Tian, X. Huo, C.-Y. Tsui, and K.-T. Cheng, AutoDCIM: An automated digital CIM compiler, in Proceedings of the 60th ACM IEEE Design Automation Conference (DAC). IEEE, 2023, pp. 1 6. [22] F. Tu, Y. Wang, Z. Wu, L. Liang, Y. Ding, B. Kim, L. Liu, S. Wei, Y. Xie, and S. Yin, ReDCIM: Reconfigurable digital computing-in-memory processor with unified FP INT pipeline for cloud AI acceleration, IEEE Journal of Solid-State Circuits (JSSC), vol. 58, no. 1, pp. 243 255, 2022. [23] G. Desoli, N. Chawla, T. Boesch, M. Avodhyawasi, H. Rawat, H. Chawla, V. Abhijith, P. Zambotti, A. Sharma, C. Cappetta et al., 16.7 A 40- 310TOPS W SRAM-based all-digital up to 4b in-memory computing multi-tiled NN accelerator in FD-SOI 18nm for deep-learning edge applications, in Proceedings of IEEE International Solid-State Circuits Conference (ISSCC). IEEE, 2023, pp. 260 262. [24] C. Lattner, M. Amini, U. Bondhugula, A. Cohen, A. Davis, J. Pienaar, R. Riddle, T. Shpeisman, N. Vasilache, and O. Zinenko, MLIR: Scaling compiler infrastructure for domain specific computation, in Proceedings of the IEEE ACM International Symposium on Code Generation and Optimization (CGO). IEEE, 2021, pp. 2 14. [25] IEEE Standard for Standard SystemC Language Reference Manual, IEEE Std., 2023, IEEE Std 1666-2023 (Revision of IEEE Std 1666- 2011).",
    "source": "2505.01107v1_CIMFlow_An_Integrated_Framework_for_Systematic_Des.pdf",
    "length": 1370,
    "tokens": 458
  },
  {
    "text": "TaskType {GEMM0, GEMM1, Combine} 22 E Actors E.1 Processor Algorithm 2: Processor Actor: executed by a block 1 begin 2 tQ GetTQ() 3 signal 0 4 shared memory variables 5 task {} 6 interrupt False 7 complete False 8 while interrupt False do 9 if warpId 0 then 10 if threadId 0 then 11 awaitTaskFromScheduler(interrupt, signal) 12 FencedNotifyRQ(ready) 13 end if 14 syncwarp() 15 warpReadTQ(tQ, signal, task) 16 end if 17 syncthreads() 18 if interrupt False then 19 switch task.Type do 20 case GEMM0 do 21 fused GEMM, epilogue and async tile staging 22 fGET(GEMM0, task) 23 if threadId 0 then 24 complete NotifyTileCompletion() 25 end if 26 syncthreads() 27 if complete True then 28 NotifySchedulerNextGEMM(tQ) 29 end if 30 end case 31 case GEMM1 do 32 fused GEMM, epilogue and async tile transfer 33 fGET(GEMM1, task) 34 end case 35 case Combine do 36 combine(task) 37 end case 38 end switch 39 end if 40 end while 41 end 23 E.2 Scheduler Algorithm 3: Scheduler Actor: executed by one warp 1 begin 2 scheduled 0 3 tTB 0 4 tqState {} 5 pTDB GetProcessorDoorbell() 6 sTDB GetSubscriberDoorbell() 7 taskBound GetTaskBound() 8 tTB AtomicLoad(taskBound) 9 circular buffer ready queue 10 rQ {} 11 Populate ready queue with Processor ids 12 PopulateRQ(rQ) 13 while scheduled tTB do 14 lt 0 15 do in parallel 16 Sweep doorbells and populate observed task counts into tqState 17 Aggregate locally observed task counts into lt 18 end 19 qS, taskTally 0 20 qS is the inclusive output 21 WarpInclusiveSum(lt, qS, tasktally) 22 while tasktally 0 do 23 Repopulate rQ with ready processor ids 24 do in parallel 25 Starting at rQ[qS], signal processors about task indices from tqState 26 end 27 end while 28 if threadId 0 then 29 tTB AtomicLoad(taskBound) 30 end if 31 tTB WarpBroadcast(tTB) 32 end while 33 InterruptSubscribers() 34 InterruptProcessors() 35 end 24 E.3 Subscriber Algorithm 4: Subscriber Actor: executed by three warps Input: Tœï R2 E C, Gœï RS E O RS H, X RE H D 1 begin 2 interrupt GetSharedInterrupt() 3 flags GetSymmetricFlags() 4 tQ GetTQ() 5 Predefined upper bound on the number of tasks. 6 We modulate this value to the actual task count computed 7 dispatch signals received from peer GPUs 8 taskBound GetTaskBound() 9 while AtomicLoad(interrupt) False do 10 dispatch flags 11 do in parallel 12 Visit dispatch flags 13 Atomically retrieve signal 14 if Signal is set and flag is not visited then 15 Mark visited 16 SelfCorrectTaskBound(taskBound, Signal) 17 Enforce memory consistency before consuming packet 18 Decode packet into a set of GEMM0 task descriptors using X 19 Write task descriptors to tQ 20 Notify Scheduler of decoded tasks 21 end if 22 end 23 Advance flags by number of dispatch flags length 24 Atomically retrieve signal 25 combine signals 26 do in parallel 27 Visit combine flags: one per tile 28 if Signal is set and flag is not visited then 29 Mark visited 30 Enforce memory consistency before consuming packet 31 Decode packet into a set of combine task descriptors using Tœï, Gœï, O 32 Write task descriptors to tQ 33 Notify Scheduler of decoded tasks 34 end if 35 end 36 end while 37 end 25 F Multi-Node Evaluation F.1 Setup In this experiment, we seek to evaluate FlashDMoE in the multi-node setting. We use 4 nodes, where each node comprises 4 A100 GPUs fully interconnected via NVLink.",
    "source": "2506.04667v2_FlashDMoE_Fast_Distributed_MoE_in_a_Single_Kernel.pdf",
    "length": 3313,
    "tokens": 887
  },
  {
    "text": "3 Preliminaries 3.1 Problem Formulation HLS DSE can be formulated as a MOOP, where the primary objective is to identify a set of Pareto-optimal configurations that simultaneously minimize latency and resource utilization. Given a behavioral description and optional synthesis directives [pragma1, pragma2, ..., pragman], the design space DS can be formulated as the Cartesian product of the combination of pragmas. The definition of Pareto frontier Pf is as follows [28]: A(df) A(di) and L(df) L(di) (1) where df Pf, di DS and Pf DS. The function A( ) and L( ) represent the resource utilization and the latency of df and di. A design configuration dj Pf is considered Pareto-optimal if no other configuration in the search space dominates it, that is, no alternative design simultaneously achieves both lower area utilization and reduced latency. Therefore, our objective is to efficiently identify the Pareto-optimal set without resorting to exhaustive searches across the entire configuration space and invoking computationally expensive HLS tools. 3.2 Graph Representation of Source Code To facilitate GNNs in capturing the features of the source code, we construct hierarchical graph representations that preserve both syntactic structures and semantic dependencies. A prevalent approach nowadays is to use LLVM [29] to convert the source code into IR, and then utilize ProGraML [30] to transform the IR into a control data flow graph. LLVM instructions encompass numerous low-level operations such as addition operations and memory reading operations. These operations are converted into certain nodes in the CDFG. If an operation needs to be performed on a certain variable, the corresponding nodes will be connected through some directed edges. 3.3 Graph Neural Network A simple graph neural network extracts the features of each node during the training process and aggregates the messages from its neighbors to update the node features.",
    "source": "2504.19649v2_Intelligent4DSE_Optimizing_High-Level_Synthesis_De.pdf",
    "length": 1946,
    "tokens": 397
  },
  {
    "text": "If the Checker Agent encounters issues such as compi- lation errors, logical inconsistencies, or incorrect simulation results it notifies the Supervisor Agent with such informa- tion, prompting a refinement loop where the Verilog Code Generator Agent or Verilog Testbench Generator Agent mod- ifies the output to resolve the detected errors. This iterative TABLE I EXPERIMENTAL RESULTS USING VERILOGEVAL [24] Model VerilogEval-Machine VerilogEval-Human GPT-4o-mini 60.1 62.2 65 44.2 49.4 56.4 Claude3.5-Sonnet [25] 74.8 76.9 79.7 56.4 60.9 69.9 CodeLlama-70B-Instruct [26] 54.5 58 63.6 40.4 43.6 48.7 DeepSeek-Coder-V2-Instruct [27] 63.6 73.4 78.3 48.1 52.6 57.1 PyraNet-DeepSeek [13] 77.6 84.6 89.5 58.3 62.8 67.9 RTLCoder-DeepSeek [28] 61.2 76.5 81.8 41.6 50.1 53.4 Origen-DeepSeek [18] 74.1 82.5 85.3 54.5 60.3 64.1 AutoVCoder-CodeQwen [17] 68.5 79.7 79.7 48.7 55.8 55.8 CodeV-DeepSeek [14] 77.9 88.6 90.7 52.7 62.5 67.3 VeriMind-GPT-4o-mini 60.1 68.5 72.6 44.2 54.5 62.8 VeriMind-Claude3.5-Sonnet 74.8 83.2 85.3 56.4 63.5 71.8 VeriMind-CodeLlama-70b-Instruct 54.5 65 70.5 40.4 48.7 53.8 VeriMind-Deep-Seek-Coder-V2-Instruct 63.6 76.2 81.8 48.1 58.3 65.4 VeriMind-PyraNet-DeepSeek 77.6 90.9 96.5 58.3 67.9 74.4 validation process ensures that the final Verilog module is both syntactically correct and functionally reliable before being delivered to the user. IV.",
    "source": "2503.16514v3_VeriMind_Agentic_LLM_for_Automated_Verilog_Generat.pdf",
    "length": 1366,
    "tokens": 533
  },
  {
    "text": "HARP [99] focuses on encoding the graph format of HLS CDFG, while ProgSG [100] extends this by adding textual input for richer multimodal circuit representation learning. We detail the HLS encoding techniques below. Self-supervised HLS graph encoder with masked pragma reconstruction. HARP [99] focuses on HLS control-data flow hierarchical graphs for representing circuit designs. Specifically, HARP [99] utilizes a hierarchical graph representation of HLS designs, incorporating both high- level and low-level views, where the high-level view combines C C code and LLVM intermediate representation (IR) to capture the program s structure and semantics, and the low-level view focuses on LLVM IR to capture detailed implementation details. This dual-level representation helps mitigate long-range dependencies within the program. The model employs a GNN to encode this hierarchical graph into circuit embeddings. During pre-training, it applies a self-supervised learning technique called masked pragma reconstruction, with paradigm demonstrated in Figure 5 (b). In this approach, certain pragmas (compiler directives) are masked, and the GNN model is trained to predict these masked pragmas based on the surrounding node embeddings in the graph. This enables the model to learn the specific effects of each pragma, enhancing its performance and improving its ability to transfer knowledge across tasks. Self-supervised HLS encoder enhanced via HLS graph-text mulimodal fusion. ProgSG [100] builds upon HARP [99] by integrating multimodal learning to improve HLS encoding. It combines two modalities: CDFG hierarchical graph used in HARP [99] and HLS C C source code text, A Survey of Circuit Foundation Model: Foundation AI Models for VLSI Circuit Design and EDA 19 HARP ICCAD 23 ProgSG MLCAD 24 Design2Vec NeurIPS 21 SNS v2 MICRO 23 CircuitFusion ICLR 25 CircuitEncoder ASP-DAC 25 HLS code text Multimodal fusion RTL graph code text Multimodal fusion RTL graph Contrastive Cross-stage RTL Code summary text Multimodal fusion HLS Encoder RTL Encoder HLS graph Mask-reconstruction self-supervised self-supervised supervised (a) Timeline for HLS encoders (b) Timeline for RTL encoders Fig.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 2189,
    "tokens": 501
  },
  {
    "text": "D Instance conÔ¨Åguration and Pricing Instance vCPUs Memory Price C5a.xlarge 4 8 GiB 0.154 C5a.2xlarge 8 16 GiB 0.308 C5a.4xlarge 16 32 GiB 0.616 C5a.8xlarge 32 64 GiB 1.232 Table 7: ConÔ¨Åguration and Pricing for EC2 C5 instances. E CIFAR-100 and BERT Models Table 8 shows the different models available for image predic- tion, that are pretrained on Keras using CIFAR-100 dataset. Model Params (M) Top-1 Accuracy( ) Latency (ms) Pf Albert-base [51] 11 91.4 55 7 CodeBert [32] 125 89 79 6 DistilBert [67] 66 90.6 92 5 Albert-large 17 92.5 120 4 XLNet [85] 110 94.6 165 3 Bert [30] 110 92 185 3 Roberta [55] 355 94.3 200 2 Albert-xlarge 58 93.8 220 1 Albert-xxlarge 223 95.9 350 1 Table 9: Pretrained models for Sentiment Analysis using BERT. Similarly Table 9 shows the different models trained for BERT-based sentiment analysis on twitter dataset. Model Params (M) Top1 Accuracy Latency (ms) Pf Squeezenet 4,253,864 70.10 43.45 10 MobileNEt V2 4,253,864 68.20 41.5 10 Inception V4 23,851,784 76.74 74 6 Resnet50 95,154,159 79.20 98.22 5 ResNet18 44,964,665 76.26 35 6 DenseNet-201 20,242,984 79.80 152.21 2 DenseNet-121 8,062,504 78.72 102.35 3 Xxception 22,910,480 77.80 119.2 4 NasNet 5,326,716 77.90 120 3 InceptionResnetV2 2,510,000 80.30 251.96 1 Table 8: Pretrained models for CIFAR-100 using Imagenet. F Spot Instance Price Variation We proÔ¨Åle the spot price of 4 types of C5 EC2 VMs over a 2-week period in August 2020.",
    "source": "cocktail.pdf",
    "length": 1425,
    "tokens": 501
  },
  {
    "text": "[The Code Snippet] verilog module top_module ( input [ 9 9 : 0 ] in , output [ 9 8 : 0 ] out_both , output [ 9 9 : 1 ] out_any , output [ 9 9 : 0 ] o u t _ d i f f e r e n t ) ; assign out_both in in [ 9 9 : 1 ] ; assign out_any in in [ 9 9 : 1 ] ; assign o u t _ d i f f e r e n t in { in [ 0 ] , in [ 9 9 : 1 ] } ; endmodule [Description] This Verilog module is used to compare two input signals and generate three output signals. The first input signal is a 100- bit wide vector, and the second input signal is the same vector shifted by one bit. The module has three output signals, out_both, out_any, and out_different. The out_both signal is generated by performing a bit-wise AND operation between the two input signals. This will result in a 99-bit wide vector, where each bit is 1 only if both the corresponding bits of the two input signals are 1. The out_any signal is generated by performing a bit-wise OR operation between the two input signals. This will result in a 100-bit wide vector, where each bit is 1 if either of the corresponding bits of the two input signals is 1. The out_different signal is generated by performing a bit-wise XOR operation between the two input signals. This will result in a 100-bit wide vector, where each bit is 1 only if the corresponding bits of the two input signals are different. The first bit of the out_different signal is generated by performing a bit-wise XOR operation between the first bit of the first input signal and the last bit of the second input signal. [Problem] You are given a 100-bit input vector in[99:0]. We want to know some relationships between each bit and its neighbour: (1) out_both: Each bit of this output vector should indicate whether both the corresponding input bit and its neighbour to the left are 1 . For example, out_both[98] should indicate if in[98] and in[99] are both 1. Since in[99] has no neighbour to the left, the answer is obvious so we don t need to know out_both[99].",
    "source": "2505.24183v2_CodeV-R1_Reasoning-Enhanced_Verilog_Generation.pdf",
    "length": 1964,
    "tokens": 500
  },
  {
    "text": "The symmetric tensor layout L is write-write conflict-free. To construct L, we start from the original token buffer T RS H, where S is the sequence length and H is the token embedding dimension. We first reorganize the sequence dimension S into three sub-dimensions representing the expert capacity (C), local expert slots (E), and the expert parallel world size (W), st: C E W C E S , where S S and E EW In the typical case of uniform expert distribution (illustrated in Figure 9a), we have S S and E EW , where EW is the total number of experts in the model. Thus, the size of the token buffer is Size(T) S H. In Figure 9a, each cell labeled Ei (with i {0, . . . , 3}) is a matrix of size (C, H). Extending prior work [33, 11], we introduce additional temporal dimensions R (communication rounds) and B (staging buffers). Each communication round has two fixed staging slots: one for outgoing tokens and another for incoming tokens. Each slot, indexed by dimension P, forms a tensor of shape (S , H). Therefore, the tensor size Size(L) is generally at least four times the original token buffer size, becoming exactly four times larger in the case of uniform expert distribution. Empirically, we find: Size(L) 4 Size(T) 3.2.1 In-place Padding for Payload Efficiency Due to the dynamic and uneven distribution of tokens in MoE dispatch [36], GPUs commonly receive fewer tokens than their predefined expert capacity. Current MoE frameworks [10] typically pad these buffers with null tokens before computation, unnecessarily increasing communication payloads and degrading performance. In contrast, we propose in-place padding, performing padding directly within the local symmetric tensor buffers and thus eliminating excess network communication. As we show in Figure 9a as a reference, each cell Ei is sized according to the expert capacity C. We further align this capacity to ensure divisibility by the tile block size bM 128, guaranteeing safe and aligned memory reads by Processor threads consuming remote tokens.",
    "source": "2506.04667v2_FlashDMoE_Fast_Distributed_MoE_in_a_Single_Kernel.pdf",
    "length": 2019,
    "tokens": 448
  },
  {
    "text": "A. el Ghany, Efficient sequence generation for hardware verification using machine learning, in 2021 28th IEEE International Conference on Electronics, Circuits, and Systems (ICECS), 2021, pp. 1 5. [38] D. N. Gadde, T. Nalapat, A. Kumar, D. Lettnin, W. Kunz, and S. Simon, Efficient stimuli generation using reinforcement learning in design verification, 2024. [39] R. Gal, E. Haber, and A. Ziv, Using dnns and smart sampling for coverage closure acceleration, in 2020 ACM IEEE 2nd Workshop on Machine Learning for CAD (MLCAD), 2020, pp. 15 20. [40] R. Gal, G. Simchoni, and A. Ziv, Using machine learning clustering to find large coverage holes, in 2020 ACM IEEE 2nd Workshop on Machine Learning for CAD (MLCAD), 2020, pp. 139 144. [41] M. A. A. E. Ghany and K. A. Ismail, Speed up functional coverage closure of cordic designs using machine learning models, in 2021 International Conference on Microelectronics (ICM), 2021, pp. 91 95. [42] S. Gogri, J. Hu, A. Tyagi, M. Quinn, S. Ramachandran, F. Batool, and A. Jagadeesh, Machine learning-guided stimulus generation for functional verification, in Proceedings of the Design and Verification Conference (DVCON-USA), Virtual Conference, 2020, pp. 2 5. [43] S. Gogri, A. Tyagi, M. Quinn, and J. Hu, Transaction level stimulus optimization in functional verification using machine learning predictors, in 2022 23rd International Symposium on Quality Electronic Design (ISQED), 2022, pp. 71 76. [44] L. Guo, J. Yi, L. Zhang, X. Wang, and D. Tong, Cga: Combining cluster analysis with genetic algorithm for regression suite reduction of microprocessors, in 2011 IEEE International SOC Conference, 2011, pp. 207 212.",
    "source": "2503.11687v1_Review_of_Machine_Learning_for_Micro-Electronic_De.pdf",
    "length": 1662,
    "tokens": 453
  },
  {
    "text": "Cohere, T., :, Aakanksha, Ahmadian, A., Ahmed, M., Alammar, J., Alizadeh, M., Alnumay, Y., Althammer, S., Arkhangorodsky, A., Aryabumi, V., Aumiller, D., Avalos, R., Aviv, Z., Bae, S., Baji, S., Barbet, A., Bartolo, M., Bebensee, B., Beladia, N., Beller-Morales, W., B√©rard, A., Berneshawi, A., Bialas, A., Blunsom, P., Bobkin, M., Bongale, A., Braun, S., Brunet, M., Cahyawijaya, S., Cairuz, D., Campos, J. A., Cao, C., Cao, K., Castagn√©, R., Cendrero, J., Currie, L. C., Chandak, Y., Chang, D., Chatziveroglou, G., Chen, H., Cheng, C., Chevalier, A., Chiu, J. T., Cho, E., Choi, E., Choi, E., Chung, T., Cirik, V., Cismaru, A., Clavier, P., Conklin, H., Crawhall-Stein, L., Crouse, D., Cruz-Salinas, A. F., Cyrus, B., D souza, D., Dalla-Torre, H., Dang, J., Darling, W., Domingues, O. D., Dash, S., Debugne, A., Dehaze, T., Desai, S., Devassy, J., Dholakia, R., Duffy, K., Edalati, A., Eldeib, A., Elkady, A., Elsharkawy, S., Erg√ºn, I., Ermis, B., Fadaee, M., Fan, B., Fayoux, L., Flet-Berliac, Y., Frosst, N., Gall√©, M., Galuba, W., Garg, U., Geist, M., Azar, M. G., Gilsenan-McMahon, E., Goldfarb-Tarrant, S., Goldsack, T., Gomez, A., Gonzaga, V. M., Govindarajan, N., Govindassamy, M., Grinsztajn, N., Gritsch, N., Gu, P., Guo, S., Haefeli, K., Hajjar, R., Hawes, T., He, J., Hofst√§tter, S., Hong, S., Hooker, S., Hosking, T., Howe, S., Hu, E., Huang, R., Jain, H., Jain, R., Jakobi, N., Jenkins, M., Jordan, J., Joshi, D., Jung, J., Kalyanpur, T., Kamalakara, S. R., Kedrzycki, J., Keskin, G., Kim, E., Kim, J., Ko, W.-Y., Kocmi, T., Kozakov, M., Kry sci nski, W., Jain, A. K., Teru, K. K., Land, S., Lasby, M., Lasche, O., Lee, J., Lewis, P., Li, J., Li, J., Lin, H., Locatelli, A., Luong, K., Ma, R., Mach, L., Machado, M., Magbitang, J., Lopez, B. M., Mann, A., Marchisio, K., Markham, O., Matton, A., McKinney, A., McLoughlin, D., Mokry, J., Morisot, A., Moulder, A., Moynehan, H., Mozes, M., Muppalla, V., Murakhovska, L., Nagarajan, H., Nandula, A., Nasir, H., Nehra, S., Netto-Rosen, J., Ohashi, D., Owers-Bardsley, J., Ozuzu, J., Padilla, D., Park, G., Passaglia, S., Pekmez, J., Penstone, L., Piktus, A., Ploeg, C., Poulton, A., Qi, Y., Raghvendra, S., Ramos, M., Ranjan, E., Richemond, P., Robert-Michon, C., Rodriguez, A., Roy, S., Ruder, S., Ruis, L., Rust, L., Sachan, A., Salamanca, A., Saravanakumar, K. K., Satyakam, I., Sebag, A. S., Sen, P., Sepehri, S., Seshadri, P., Shen, Y., Sherborne, T., Shi, S. S., Shivaprasad, S., Shmyhlo, V., Shrinivason, A., Shteinbuk, I., Shukayev, A., Simard, M., Snyder, E., Spataru, A., Spooner, V., Starostina, T., Strub, F., Su, Y., Sun, J., Talupuru, D., Tarassov, E., Tommasone, E., Tracey, J., Trend, B., Tumer, E., √úst√ºn, A., Venkitesh, B., Venuto, D., Verga, P., Voisin, M., Wang, A., Wang, D., Wang, S., Wen, E., White, N., Willman, J., Winkels, M., Xia, 28 C., Xie, J., Xu, M., Yang, B., Yi-Chern, T., Zhang, I., Zhao, Z., and Zhao, Z. (2025).",
    "source": "2506.20752v1_Characterization_and_Mitigation_of_Training_Instab.pdf",
    "length": 2908,
    "tokens": 1276
  },
  {
    "text": "In Proceedings of ACM Conference (Conference 17). ACM, New York, NY, USA, 7 pages. Legal name: Zhaoqin Li Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. Request permissions from Conference 17, July 2017, Washington, DC, USA 2025 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x YY MM... 15.00 1 INTRODUCTION Large Language Models (LLMs) like Transformers rely on intensive matrix multiplications for their self-attention and feed-forward blocks. As shown in the Figure 1, computing the Q, K, V, and output linear projections during Multi-Head Self-Attention in each Trans- former layer entails multiplying large matrices (input activation Wq Wk Wv Wo). DistilBERT [5], a compact Transformer vari- ant [4], still performs billions of multiply-accumulate operations per inference. These operations are memory-bandwidth heavy on general processors, motivating specialized hardware acceleration. FPGAs offer custom parallelism and energy efficiency for matrix math, but deploying LLM acceleration on edge FPGAs presents challenges due to limited resources and power. Motivated by the observation that the Q, K, and V projections form a significant computational bottleneck, we strategically con- centrated our efforts on accelerating these key components. This targeted approach allowed us to fully optimize our design for the resource constraints of the Xilinx Kria KV260 Vision AI Kit (hous- ing a Zynq UltraScale FPGA), while still delivering substantial performance improvements. Our approach exploits data reuse and parallelism to speed up matrix multiplication operations within the tight DSP and BRAM budget of an edge FPGA. We adopt a block tiled matrix multiply approach: one input matrix ( A ) re- mains in on-chip memory, while the other ( B ) is processed in smaller column blocks to maximize reuse.",
    "source": "2503.16731v3_Design_and_Implementation_of_an_FPGA-Based_Hardwar.pdf",
    "length": 2305,
    "tokens": 492
  },
  {
    "text": "One challenge for 6 40 ML-enhanced verification techniques is to operate with a range of coverage models, both structural and functional. There are different types of functional models commonly seen in research using ML for electronic- design verification: - Cross-product coverage models: These are named groups of states in the DUV s state space. They define cover points, which are specific points in the design to monitor, such as the values of signals or variables. A coverage cross is the cross product of two or more cover points, and a cross-product coverage model is a collection of these crosses [68]. A simplified version defines cover points in isolation, without relating them to other signals or variables. - Assertion-based models: An assertion expresses a property of the design, such as a safety property (something that should never happen) or a liveness property (something that should eventually happen). The purpose of an assertion model is to report the occurrence of an expected event [80]. Assertions are broadly divided into those defined only over primary input signals and those defined over input, internal, and output signals [64]. An advantage of assertion models is their suitability for static-based techniques, making them advantageous in projects that use both formal and test-based methods. However, this review found they are rarely used with machine learning for dynamic verification, potentially due to their association with static-based techniques. They are used in hybrid methods, such as Goldmine [67], an ML-based technique that uses simulation traces and formal methods to create assertions automatically. Some applications may define alternative functional coverage models. For example, [83] applies ML to verify a design at a system level, and the research uses Modular coverage, to record when a specific block (module) is activated. It is common to refer to the coverage of a test. In the case of functional coverage, it measures how well a test covers part of the functional specification. In the case of structural coverage, it measures how well the test covers the implementation of the specification [92]. The coverage of a test can be viewed as the percentage of the coverage model a test covers. Structural and functional coverage models have limitations. Structural coverage models are easy to create but only reveal how much of the design has been tested, not whether its behaviour is correct. Conversely, functional coverage models track how much of the specified behaviour has been tested but do not measure the quality and completeness of the verification environment [108].",
    "source": "2503.11687v1_Review_of_Machine_Learning_for_Micro-Electronic_De.pdf",
    "length": 2633,
    "tokens": 499
  },
  {
    "text": "A Survey of Circuit Foundation Model: Foundation AI Models for VLSI Circuit Design and EDA 31 5 FOUNDATION MODEL AS A CIRCUIT DECODER Another major paradigm of circuit foundation models is circuit decoders, which leverage LLMs for the automated generation of circuit-related content. These models facilitate the creation of RTL code (e.g., Verilog or VHDL), HLS code (e.g., SystemC or C ), design scripts (e.g., Tcl or Python), design descriptions, etc. As summarized in Figure 10, this section provides a comprehensive overview of Foundation model as circuit decoder RTL code generation (Section 5.1) DAVE [132], ChipGPT [133], VerilogEval [14], GPT4AIGChip [134], Chip-Chat [135], AutoChip [136], RTLLM [12], VeriGen [137], RapidGPT [138], CodeV [139], AutoVCoder [140], BetterV [141], ChipNemo [13], Chang et al. [142], OriGen [143], VerilogCoder [144], Thakur et al. [15], RTLCoder [145], MG-Verilog [146] CreativeEval [147], VHDL-Eval [148], Chang et al. [149], OPL4GPT [150], RTLSquad [151], MAGE [152], RTL-repo [153], OpenLLM-RTL [154], Sun et al. [155], DeepRTL [156], CraftRTL [157], [158 166] HLS code generation (Section 5.2) HLSPilot [167], C2HLSC [168], SynthAI [169], Liao et al. [170], Gai et al. [171] Design optimization (Section 5.3) BetterV [141], ChipGPT [133], RTLRewriter [172], Martine et al. [173], Xu et al. [174], Thorat et al. [175], Sandal et al. [161], DeLorenzo et al.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 1399,
    "tokens": 454
  },
  {
    "text": "Input instructions and datasets are crucial: The fine-tuning of pre-trained LLMs on HLS dataset can bring a significant improve- ment in the design quality, echoing findings from previous studies on Verilog code generation [25]. Additionally, during our evaluation, we found that employing simple CoT prompting largely improves hardware design quality. This result contrasts with the application of CoT in general-purpose programming languages, where a spe- cialized form of CoT is necessary [13]. Therefore, future efforts for further enhancement can focus on collecting high-quality datasets and exploring better refinement of input prompts. 3. Limitations: At the time of this research, more advanced rea- soning models, such as DeepSeek-R1 [8], were not available for evaluation. Additionally, test-time scaling approaches [30] could be incorporated to further enhance performance in the future. More- over, we observe that the diversity of hardware designs in the benchmark is limited, which may impact the generalizability of our findings. We intend to address these limitations in our future work. 6 CONCLUSION This paper explores automating hardware generation with code language models and High-Level Synthesis (HLS). We aim to inves- tigate the suitability of HLS over low-level hardware description languages for hardware design generation. To facilitate this, we propose benchmarks and code infrastructures for evaluating LLM- assisted HLS design generation. Our experimental findings reveal that, with the integration of advanced optimizations such as feed- back loops and chain-of-thought techniques, LLM-assisted HLS code generation shows substantial promise in designing complex hardware with high levels of syntax and functional correctness. Exploring Code Language Models for Automated HLS-based Hardware Generation ASP-DAC 25, January 20 23, 2025, Tokyo Odaiba Miraikan, Japan REFERENCES [1] Anonymous Authors. 2024. LLM-VeriPPA: Power, Performance, and Area-aware Verilog Code Generation and Refinement with Large Language Models. https: openreview.net pdf?id nZL6S0b-HcI [2] Omri Avrahami, Dani Lischinski, and Ohad Fried. 2022. Blended diffusion for text-driven editing of natural images. In IEEE CVF Conference on Computer Vision and Pattern Recognition. 18208 18218.",
    "source": "2502.13921v2_Exploring_Code_Language_Models_for_Automated_HLS-b.pdf",
    "length": 2290,
    "tokens": 494
  },
  {
    "text": "ISCA 25, June 21 25, 2025, Tokyo, Japan Kim, Hong, et al. Table 1: Hardware specification of NVIDIA A100 GPU and Oaken equipped with either HBM or LPDDR memory. NVIDIA A100 Oaken-HBM Oaken-LPDDR Peak FP16 TFLOPS 312 270 270 Operating frequency 1.4 GHz 1.0 GHz 1.0 GHz Memory type HBM HBM LPDDR Memory capacity 80 GB 80 GB 256 GB Memory bandwidth 2.0 TB s 2.0 TB s 1.1 TB s 6 Evaluation 6.1 Methodology Models and datasets. For evaluation, we use Llama2-7B, 13B, and 70B [66], OPT-6.7B, 13B, and 30B [84], Mistral-7B [25] and Mixtral-8x7B [26] models. Llama2, Mistral, and Mixtral models im- plement grouped-query attention [3], while Mistral and Mixtral models also incorporate a sliding window [6]. Additionally, Mixtral model further integrates mixture-of-experts (MoE) layers [62]. To evaluate model accuracy, we utilize Wikitext2, PIQA, Winogrande, and Hellaswag datasets, which are widely evaluated in prior stud- ies [22, 31, 33, 35, 41, 86, 87]. Wikitext2 [46] dataset consists of tokens extracted from Wikipedia articles, while PIQA [7], Wino- grande [58], and Hellaswag [81] constitute questions and answers. We report zero-shot accuracy ( ) for PIQA, Winogrande, and Hel- laswag datasets and perplexity for Wikitext2 dataset. Note that for perplexity, lower values indicate better performance. For real- world benchmarking, we use two open-source production traces from Azure LLM inference services, Conversation [47, 54] and Burst- GPT [68]. We follow the methodology established in prior work to simulate inference serving scenarios [20]. Requests are sampled from the trace over a time period, and batches are synthesized with varying input and output sequence lengths. We repeat this process across multiple batches, measuring the average performance. Accelerator platforms.",
    "source": "2503.18599v2_Oaken_Fast_and_Efficient_LLM_Serving_with_Online-O.pdf",
    "length": 1788,
    "tokens": 484
  },
  {
    "text": "On the one hand, architects have designed various archi- tectures for MM based on MAC (Figure 1(B), including 2D- matrix [30], weight stationary (WS) or output stationary (OS) systolic array [13], [20], [21], [33], and 3D-Cube [1], [27]. These architectures have not only gained widespread applica- tion in academia but have also been practically deployed in the industry, becoming foundational in TPE design. On the other hand, arithmetic logic units (ALUs) researchers focus on the approach from the perspective of single compu- tational (Figure 1(A)), striving to develop higher performance multipliers and adders. Typical designs include array multipli- ers [16], Booth multipliers [23], Baugh multipliers [40], carry lookahead adders [9], carry select adders [6], carry save adders [15], Wallace tree [43] and compressor tree [37], [42].",
    "source": "2503.06342v1_Exploring_the_Performance_Improvement_of_Tensor_Pr.pdf",
    "length": 842,
    "tokens": 207
  },
  {
    "text": "For this, we will collabo- rate with the Multicultural Engineering Graduate Student Association (MEGA) and Graduate Women in Engineering (GradWIE) programs. Moreover, the CoE (College of Engineering) is a partner with the National GEM Consortium that leads the Grad Lab, which facilitates the participation of populations underrepresented in computing for graduate studies in engineering and science. In addition to working with the GEM Grad lab, department has developed a partnership with the Black in AI (BAI) group to attract more students from populations underrepresented in computing to our graduate program. We also plan develop recruiting relationships with HBCUs. All these efforts should help us in broadening participation in this project. Aligned with the departmental BPC plan, the students working on this project will participate in various diversity, equity, inclusion and belonging (DEIB) DEIB activities. Summer Camp for Middle School Girls: PI Das has been involved with organization of week-long sum- mer camps (funded by the CSE Department) targeted at middle school girls since 2017. Our earlier camps have already introduced participants to basic concepts in programming, building vision systems that assist visually impaired and learning skills towards building a basic embedded vision system, program- ming for robotics and exposure to various emerging tools in computing. We will continue to develop new week-long courses introducing students to LLMs Generative AI and inspiring them through hands-on application and system building activities. PI Zhang has participated in the 2024 summer camp, and he will lead this effort for the future years. This summer camp has been very successful, and we plan to extend it to additional school districts. Summer Research Opportunities for High School Students and Science Teachers: The PIs has been participating in the organization of various summer activities with high school students and teachers. For example, PI Kandemir has previously participated in the organization of a workshop targeting high school teachers and gave a talk on ML and high-performance computing. The research team is planning to organize summer activities with high school students and teachers in the context of this project as well. More specifically, to complement the summer camp with a more in-depth exploration for the students, we will work with the local school science teachers and students to develop the inquisitiveness of the potential of LLM concepts and develop simple but interesting LLM-based applications to attract young students.",
    "source": "NSF_LLM_Medium_Proposal.pdf",
    "length": 2595,
    "tokens": 477
  },
  {
    "text": "This paper is included in the Proceedings of the 19th USENIX Symposium on Networked Systems Design and Implementation. April 4 6, 2022 Renton, WA, USA 978-1-939133-27-4 Open access to the Proceedings of the 19th USENIX Symposium on Networked Systems Design and Implementation is sponsored by Cocktail: A Multidimensional Optimization for Model Serving in Cloud Jashwant Raj Gunasekaran, Cyan Subhra Mishra, Prashanth Thinakaran, Bikash Sharma, Mahmut Taylan Kandemir, and Chita R. Das, The Pennsylvania State University Cocktail: A Multidimensional Optimization for Model Serving in Cloud Jashwant Raj Gunasekaran, Cyan Subhra Mishra, Prashanth Thinakaran, Bikash Sharma, Mahmut Taylan Kandemir, Chita R. Das The Pennsylvania State University, University Park, PA {jashwant, cyan, prashanth, mtk2, Abstract With a growing demand for adopting ML models for a variety of application services, it is vital that the frameworks serving these models are capable of delivering highly accurate predic- tions with minimal latency along with reduced deployment costs in a public cloud environment. Despite high latency, prior works in this domain are crucially limited by the accu- racy offered by individual models. Intuitively, model ensem- bling can address the accuracy gap by intelligently combining different models in parallel. However, selecting the appro- priate models dynamically at runtime to meet the desired accuracy with low latency at minimal deployment cost is a nontrivial problem. Towards this, we propose Cocktail, a cost effective ensembling-based model serving framework. Cock- tail comprises of two key components: (i) a dynamic model selection framework, which reduces the number of models in the ensemble, while satisfying the accuracy and latency requirements; (ii) an adaptive resource management (RM) framework that employs a distributed proactive autoscaling policy, to efÔ¨Åciently allocate resources for the models. The RM framework leverages transient virtual machine (VM) in- stances to reduce the deployment cost in a public cloud.",
    "source": "cocktail.pdf",
    "length": 2053,
    "tokens": 452
  },
  {
    "text": "Downloaded on June 29,2023 at 18:36:04 UTC from IEEE Xplore. Restrictions apply. bounding boxes for the object detection task and the feature maps for each layer during the inference are intermediately stored in memory as a checkpoint . By referring this, it can potentially beneÔ¨Åt an inference to be performed later by either completely skipping it (SI) or reducing the computation (PI). Skip Inference: When the current frame is identiÔ¨Åed as clonable by the previous frame, the CPU is released without any inference execution request. Instead, the previous results (the bounding boxes [including the classes and scores] in the previous frame), which have been memoized before, are directly loaded as the inference result for the current frame. Partial Inference: In this case, rather than processing the whole frame, only the RoI blocks are fed into the CPU and, with the memoized feature maps from previous frame, the CPU is able to generate the desired result for the current frame as accurate as performing inference on a full frame. Fig. 7: The proposed frame-level reuse and tile region-level reuse design blocks implementation; BB BBox: bounding boxes from the last FI; MV: motion vectors of the current frame; FM: feature maps for each layer from the last FI. V. EVALUATION Targeting the object detection task on edge devices, we compare our proposed frame-level scheme (Full-Inference and Skip-Inference, i.e., FI SI) and region-level scheme (Full- Inference, Skip-Inference and Partial-Inference, i.e., FI SI PI) against the baseline inference, which performs full infer- ence on every frame, as well as four state-of-the-art runtime techniques (DeepCache [8], Euphrates [9], Potluck [12], and MCDNN [7]), by quantifying the normalized execution time, energy consumption, and accuracy (mean Average Precision, mAP). We Ô¨Årst describe the design conÔ¨Ågurations, experi- mental platform, datasets, and measurement tools used in this work, and then analyze the collected results. A. Design conÔ¨Ågurations Baseline: We evaluate the baseline video object detection on an edge CPU1, where every frame is fully inferenced.",
    "source": "PCframeSim.pdf",
    "length": 2123,
    "tokens": 491
  },
  {
    "text": "Tiantu Xu, Luis Materon Botelho, and Felix Xiaozhu Lin. Vstore: A data store for analytics on large videos. In Proceedings of the Fourteenth EuroSys Conference 2019, pp. 1 17, 2019. Yang Yang, Zhi Guan, Huiping Sun, and Zhong Chen. Accelerating rsa with fine-grained parallelism using gpu. In Information Security Practice and Experience: 11th International Conference, ISPEC 2015, Beijing, China, May 5-8, 2015, Proceedings, pp. 454 468. Springer, 2015. Ziyu Ying, Shulin Zhao, Sandeepa Bhuyan, Cyan Subhra Mishra, Mahmut T. Kandemir, and Chita R. Das. Pushing point cloud compression to the edge. In 2022 55th IEEE ACM International Symposium on Microarchitecture (MICRO), pp. 282 299, 2022a. doi: 10.1109 MICRO56248. 2022.00031. Ziyu Ying, Shulin Zhao, Haibo Zhang, Cyan Subhra Mishra, Sandeepa Bhuyan, Mahmut T. Kandemir, Anand Sivasubramaniam, and Chita R. Das. Exploiting frame similarity for efficient inference on edge devices. In 2022 IEEE 42nd International Conference on Distributed Computing Systems (ICDCS), pp. 1073 1084, 2022b. doi: 10.1109 ICDCS54860.2022.00107. Hang Yue, Laurence R Rilett, and Peter Z Revesz. Spatio-temporal traffic video data archiving and retrieval system. GeoInformatica, 20:59 94, 2016. Haibo Zhang, Prasanna Venkatesh Rengasamy, Shulin Zhao, Nachiappan Chidambaram Nachiappan, Anand Sivasubramaniam, Mahmut T. Kandemir, Ravi Iyer, and Chita R. Das. Race-to-sleep content caching display caching: a recipe for energy-efficient video streaming on handhelds. In Proceedings of the 50th Annual IEEE ACM International Symposium on Microarchitecture, MICRO-50 17, pp. 517 531, New York, NY, USA, 2017. Association for Computing Machinery.",
    "source": "SaLT.pdf",
    "length": 1673,
    "tokens": 495
  },
  {
    "text": "2024. DuQuant: Distributing Out- liers via Dual Transformation Makes Stronger Quantized LLMs. arXiv preprint arXiv:2406.01721 (2024). [40] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. 2023. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978 (2023). [41] Yujun Lin , Haotian Tang , Shang Yang , Zhekai Zhang, Guangxuan Xiao, Chuang Gan, and Song Han. 2024. QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving. arXiv preprint arXiv:2405.04532 (2024). [42] Jing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei Cai, and Bohan Zhuang. 2023. Qllm: Accurate and efficient low-bitwidth quantization for large language models. arXiv preprint arXiv:2310.08041 (2023). [43] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. 2024. KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache. arXiv preprint arXiv:2402.02750 (2024). [44] Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. 2025. SpinQuant: LLM quantization with learned rotations. (2025). arXiv:2405.16406 [cs.LG] [45] Eitan Medina and Eran Dagan. 2020. Habana Labs Purpose-Built AI Inference and Training Processor Architectures: Scaling AI Training Systems Using Standard Ethernet With Gaudi Processor. IEEE Micro 40 (2020), 17 24.",
    "source": "2503.18599v2_Oaken_Fast_and_Efficient_LLM_Serving_with_Online-O.pdf",
    "length": 1468,
    "tokens": 485
  },
  {
    "text": ". , oG} from the current policy and compute rewards for each. The GRPO objective function is defined as: LGRPO(Œ∏) Eq D,{oi}G i 1 œÄŒ∏old( q) \" 1 G G X i 1 min (ri œÅi, clip(œÅi, 1 œµ, 1 œµ) ri) Œ≤ DKL(œÄŒ∏( q) œÄref( q)) (1) where: where œÅi œÄŒ∏(oi q) œÄŒ∏old(oi q) is the importance sampling ratio, ri is the normalized reward for candidate oi, œµ is a hyperparameter controlling the clipping range, Œ≤ is a coefficient balancing the KL divergence penalty, œÄref is a reference policy (typically the supervised fine-tuned model), and DKL is the Kullback-Leibler divergence. The advantage estimation in GRPO is simplified by normalizing rewards within each group, where ri R(oi) ¬µR œÉR Œ¥ , with R(oi) being the raw reward for output oi, ¬µR and œÉR are the mean and standard deviation of rewards within the group, and Œ¥ is a small constant for numerical stability. This group-based normalization provides several benefits: it eliminates the need for a separate value network, reduces variance in advantage estimation, and naturally compares alternative implementa- tions of the same specification, which aligns well with the goal of generating functionally correct Verilog code. 3.2 Reward Model Our reward function combines both structural correctness and functional validation to provide comprehensive feedback during training. The reward R for a generated Verilog implementation is computed as: R(o) 2.0, if functionally correct 0.1 1.0 ASTscore(o), if syntactically correct 0, otherwise (2) where: Functional correctness is determined by running the generated code through testbenches and comparing outputs with the expected behavior. Syntactic correctness is verified by successful parsing of the Verilog code. The ASTscore(o) measures structural similarity between the generated code s Abstract Syntax Tree (AST) and reference implementations, with values ranging from 0 to 1. The AST score provides a fine-grained measure of structural correctness even when the code is not functionally perfect.",
    "source": "2505.11849v1_VeriReason_Reinforcement_Learning_with_Testbench_F.pdf",
    "length": 1982,
    "tokens": 483
  },
  {
    "text": "In Figure 3, examples with higher presence of outliers have larger values of N. FLOW takes the normalized outlier distribution of each layer (NDl) into account to identify the optimal M l. In specific, for each layer weight tensor, we partition it into B non-overlapping blocks of dimension (128 128). Within a block b, we measure Dl b by calculating the pairwise L1 distance between all the outlier pairs following Equation 1. We obtain Dl as PB b 1 Dl b, and finally get NDl via normalizing the Dl values following Equation 1. A higher NDl indicates a farther distance between outliers within a layer, implying that they are sparsely distributed (Case 1 in Figure 3), whereas smaller NDl indicates a large number of closely located outliers (Case 3 in Figure 3). Intuitively, if a layer has many closely located outliers, a higher value of M l is preferred, providing more freedom while performing weight pruning and preventing inadvertent pruning of outliers. In specific, M l (1 NDl). Automated layer-wise N, M allocation. Given the relations of N l, M l, we formulate the problem of assigning layer-wise N:M values as an integer linear programming (ILP) problem: argmin L X l 1 Œ± N l k Ol Œ≤ M l h (1 NDl) (2) s.t. 1 L L X l 1 Sl S, 0 Sl 1, l {1, . . . , L} 1 N l M l, N l {1, 2, 4, 8}, M l {2, 4, 8} where Œ±, Œ≤ are the weighing factors to optimize N and M values, respectively. k and h are the hyper-parameters to keep the terms Ol and NDl in the same ballpark range as N l and M l choices, respectively. We empirically choose their values to be (Œ±, Œ≤, k, h) (1, 4, 8, 8). This formulation aims to assign values to N l and M l for each layer while minimizing deviations from desired characteristics, all while maintaining the target sparsity (Equation 2). N l is aligned with the normalized outlier count Ol, and M l is aligned with the outlier distribution (1 NDl).",
    "source": "2504.14365v1_Accelerating_LLM_Inference_with_Flexible_NM_Sparsi.pdf",
    "length": 1871,
    "tokens": 482
  },
  {
    "text": "In all experiments, we use a contrastive candidate set size of k 5, a contrastive penalty coefficient of Œª 0.5, and a base decoding temperature of 0.7. As summarized in Table VII and Table VI , our method outperforms standard decoding strategies, e.g., top-k sampling, showing that our proposed decoding framework significantly reduces functionality and synthesizability issues, consistently leading to higher success rate in both perspectives. E. Computational Efficiency of Contrastive Decoding We evaluated the computational efficiency of our contrastive decoding strategy on the QwenCoder-2.5-14B model. To en- sure a fair comparison of computational efficiency across de- coding strategies, we enabled Key-Value (KV) caching during inference. KV caching significantly accelerates autoregressive decoding by storing the intermediate key and value tensors computed at each step of generation. This allows the model to avoid recomputing the entire attention mechanism over the full sequence at each step and instead perform attention only over the new token, thereby reducing inference time from quadratic to linear complexity with respect to sequence length. As shown in Table VIII, the average decoding time per token increased marginally from 0.1383 to 0.1413 seconds when using contrastive re-ranking. Peak GPU memory usage remained nearly unchanged, with only a 0.5 increase. In our proposed contrastive decoding strategy, at each decoding step, we sample a small set of candidate tokens (e.g., top-3) and compute their modified scores by combining the model s predicted log-probabilities with a contrastive penalty based on semantic similarity. This re-ranking is efficient for two reasons, (i) it operates on a small subset of tokens, not the full vocabulary, limiting the cost of additional computation, (ii) the similarity computations are lightweight vector opera- tions (i.e., cosine similarity) between the precomputed token embeddings and the current context representation, avoiding any need for extra forward passes through the model. Because the token embeddings are already available at decoding time, the re-ranking step integrates seamlessly without disrupting the KV cache or triggering redundant computations. TABLE VIII: Decoding Efficiency Comparison on QwenCoder-2.5-14B. Method Avg.",
    "source": "2507.02226v1_DecoRTL_A_Run-time_Decoding_Framework_for_RTL_Code.pdf",
    "length": 2309,
    "tokens": 470
  },
  {
    "text": "For reference, ARM cores are subject to many hours of simulation-based testing running on high-performance clusters. A typical company using an open-source design does not possess the computational resources, expertise, or access to the EDA tools required to achieve similar levels of verification. Therefore, a need and opportunity exist for open research that can be used by small electronic design houses to verify their applications based on open-source core designs. 13 CHALLENGES FOR FUTURE RESEARCH The results of this review highlight the difficulties of applying machine learning to the verification of microelectronic devices in a real-world project. There are many examples of successful applications of machine learning, but also many configurations of elements that affect the learning. These elements include abstraction level of both the input and output spaces of the ML model, what the machine learning controls, whether the ML is used to target a single coverage hole or many holes, the hyper-parameters of the ML models, and more. What this review concludes is that while there are many successful applications of ML for verification, there is very little understanding of why the application was successful. This information is crucial to generalise a technique to different applications. To gain widespread adoption, the use of machine learning techniques for verification could look to the adoption of formal techniques as a case study. Once seen as requiring complex setup and specialist skills, formal techniques are now more accessible to verification engineers. This has been achieved by offering guided workflows to configure and run the tool as a push button operation in industrial EDA software suites. In summary, the questions for future research into the use of ML for verification are as follows. - Why does a machine learning technique work for a specific application? - How would the technique transfer between different applications? - What are the limitations of the technique? - What domain knowledge, assumptions, and constraints are needed to apply the technique? 14 ACKNOWLEDGMENTS The authors acknowledge the assistance of Maryam Ghaffari Saadat in the preparation of this review. REFERENCES [1] Ieee standard for universal verification methodology language reference manual, IEEE Std 1800.2- 2020 (Revision of IEEE Std 1800.2-2017), pp. 1 458, 2020. [2] Ieee standard for systemverilog unified hardware design, specification, and verification language, IEEE Std 1800-2023 (Revision of IEEE Std 1800-2017), pp.",
    "source": "2503.11687v1_Review_of_Machine_Learning_for_Micro-Electronic_De.pdf",
    "length": 2552,
    "tokens": 497
  },
  {
    "text": "Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM con- ference on recommender systems, pages 191 198, 2016. [25] Daniel Crankshaw, Peter Bailis, Joseph E Gonzalez, Haoyuan Li, Zhao Zhang, Michael J Franklin, Ali Ghodsi, and Michael I Jordan. The missing piece in complex analytics: Low latency, scalable model man- agement and serving with velox. arXiv preprint arXiv:1409.3809, 2014. [26] Daniel Crankshaw, Gur-Eyal Sela, Corey Zumar, Xiangxi Mo, Joseph E. Gonzalez, Ion Stoica, and Alexey Tumanov. Inferline: ML inference pipeline composition framework. CoRR, abs 1812.01776, 2018. [27] Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J. Franklin, Joseph E. Gonzalez, and Ion Stoica. Clipper: A low-latency online pre- diction serving system. In 14th USENIX Symposium on Networked Sys- tems Design and Implementation (NSDI 17), pages 613 627, Boston, MA, March 2017. USENIX Association. [28] Deepstudio. Deep Learning Dtudio, February 2020. . [29] J. Deng, W. Dong, R. Socher, L. Li, and and. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, June 2009. [30] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language un- derstanding, 2019. [31] Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander Smola. Autogluon-tabular: Robust and accurate automl for structured data, 2020. [32] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155, 2020.",
    "source": "cocktail.pdf",
    "length": 1739,
    "tokens": 499
  },
  {
    "text": "Model x86 ARMv5 ARMv8 RISC-V64 Qwen-Coder (Hui et al., 2024a) 4.28 2.89 3.62 3.62 DeepSeek-Coder (Guo et al., 2024) 3.74 3.51 4.28 4.28 GG-Qwen (Ours) 4.14 2.87 3.50 3.50 GG-DeepSeek (Ours) 3.47 3.26 3.99 3.37 Qwen ( ) 3.3 0.5 3.4 3.4 DeepSeek ( ) 7.2 6.9 6.8 6.8 Table 2: Tokenizer fertility rate (tokens words) across ISAs. Lower is better. 4 Model ARMv5 ARMv8 ARMv8 HumanEval HumanEval HumanEval HumanEval BringupBench BringupBench -O0 -O2 -O0 -O2 -O0 -O2 GPT-4o (OpenAI, 2024) 8.48 3.64 10.3 4.24 1.54 0 Qwen2.5-Coder-1.5B (Hui et al., 2024a) 0 0 0 0 0 0 Qwen2.5-Coder-3B (Hui et al., 2024a) 0.61 0 0 0 0 0 StarCoder2-3B (Lozhkov et al., 2024) 0 0 0 0 0 0 Deepseek-R1-1.5B (Guo et al., 2025) 0 0 0 0 0 0 Deepseek-R1-Qwen-7B (Guo et al., 2025) 0 0 0 0 0 0 GG-Deepseek-1.3B 79.25 12.80 75.15 10.3 3.08 0 GG-0.5B 90.85 23.03 86.06 25.45 27.69 3.08 GG-1.5B 93.71 50.30 99.39 45.12 49.23 15.38 Table 3: Models trained with our method outperform baselines across all benchmarks, at all optimization levels. 4 Experiments and Evaluation In this section, we describe our experimental setup, training methodology, evaluation benchmarks, and the metrics used to assess the accuracy and robustness of our CISC-to-RISC transpiler.",
    "source": "2506.14606v1_Guaranteed_Guess_A_Language_Modeling_Approach_for_.pdf",
    "length": 1222,
    "tokens": 502
  },
  {
    "text": "4, FLASH-D reduces the hardware area by more than 20 in all examined cases. These savings are a direct result of the restructured FlashAttention kernel. 1Publicly available on TABLE I PERCENTAGE OF SKIPPED OUTPUT UPDATES DURING INFERENCE ON DIFFERENT NLP BENCHMARKS LLM Benchmarks CSQA GSM8K QASC MMLU Date Object Tracking Microsoft 0.8 1.7 2.2 2 1.5 2 Phi-3-mini-4k-instruct DeepSeek 2.5 2.0 2.2 2.7 2.4 2.8 Qwen-1.5B Meta 1.8 1.6 2.6 2.3 1.6 2.3 Llama-3.1-1B Google 1.2 0.5 0.51 1.4 0.8 0.83 Gemma2-2B Standalone division is eliminated and fused within the sigmoid PWL function evaluation, effectively merging the exponential and division operations. One vector multiplier is saved in the output update module, and the sum-of-exponents and maximum logic are entirely removed. The reduction in hardware complexity also improves power consumption, which is reduced by more than 16 on average across all examined cases. Power savings are expected to in- crease further, as the computation-skipping criterion developed in FLASH-D would save additional memory power, which has not been quantified in the presented analysis. B. How often output update can be simplified To quantify how often the attention score differences fall outside the range [-6,11] described in Section III-C, and thereby simplify the output update in FLASH-D (line 9 in Alg.3), we implemented the FLASH-D kernel in Python and integrated it into the forward pass of various contemporary LLM models available on HuggingFace [33]. We then per- formed inference on these LLMs using the utilities and bench- marks provided by Microsoft s PromptBench workflow [34]. The results for each case are summarized in Table I. In all cases, there is small percentage of cases that output update can be simplified with either keeping the previous computed output or loading the new value vector without any further calculations. This percentage, even if small, is always a win scenario and does not represent any tradeoff across energy savings vs application-level performance.",
    "source": "2505.14201v1_FLASH-D_FlashAttention_with_Hidden_Softmax_Divisio.pdf",
    "length": 2032,
    "tokens": 497
  },
  {
    "text": "Laser: A scalable response prediction platform for online advertising. In Proceedings of the 7th ACM international conference on Web search and data mining, pages 173 182, 2014. [3] Ahmed Ali-Eldin, Jonathan Westin, Bin Wang, Prateek Sharma, and Prashant Shenoy. Spotweb: Running latency-sensitive distributed web services on transient cloud servers. In Proceedings of the 28th Inter- national Symposium on High-Performance Parallel and Distributed Computing, pages 1 12, 2019. [4] Amazon. Deepar estimator. sagemaker latest dg deepar.html,February2020. [5] Amazon. EC2 pricing. [6] Amazon. Sagemaker. February 2018. [7] Amazon. Azure Low priority batch VMs., February 2018. . [8] Amazon. EC2 C5 Instances., February 2018. . [9] Amazon. Google Preemptible VMs., February 2018. . [10] Azure. Machine Learning as a Service., February 2018. service . [11] Azure. Ensembling in Azure ML Studio., February 2020. module-reference multiclass-decision-forest . USENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation 1053 [12] Ataollah Fatahi Baarzi, Timothy Zhu, and Bhuvan Urgaonkar. Burscale: Using burstable instances for cost-effective autoscaling in the public cloud. In Proceedings of the ACM Symposium on Cloud Computing, New York, NY, USA, 2019. Association for Computing Machinery. [13] Marian Stewart Bartlett, Gwen Littlewort, Mark Frank, Claudia Lain- scsek, Ian Fasel, and Javier Movellan. Recognizing facial expression: machine learning and application to spontaneous behavior. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 05), volume 2, pages 568 573. IEEE, 2005. [14] Eric Bauer and Ron Kohavi. An empirical comparison of voting classiÔ¨Åcation algorithms: Bagging, boosting, and variants. Machine learning, 36(1-2):105 139, 1999. [15] William H Beluch, Tim Genewein, Andreas N√ºrnberger, and Jan M K√∂hler.",
    "source": "cocktail.pdf",
    "length": 1889,
    "tokens": 480
  },
  {
    "text": "We identify the severe limitations of the Protein Structure Prediction Model (PPM) in handling long sequences, primar- ily due to rapidly increasing activation size, which leads to high peak memory requirements and latency. We develop a Token-wise Adaptive Activation Quantization (AAQ) scheme that analyzes the token-wise characteristics of activations in the PPM and applies precision and outlier handling differently, suggesting a new approach to solve the activation size problem without accuracy degradation. Based on AAQ, we propose LightNobel, a hardware accel- erator that flexibly supports multi-precision and dynamic outlier configuration among quantized activations, and con- figures a dataflow that can maximize hardware utilization for different types of activations. Our experiments show LightNobel achieves up to 8.44 , 8.41 speedup and 37.29 , 43.35 higher power efficiency over the latest NVIDIA A100 and H100 GPUs while main- taining negligible accuracy loss and reducing peak memory requirements by 120.05 for proteins with long sequences. 2 Background 2.1 Attention-based Model Attention-based models are models with attention layers, widely used for various applications [18, 42]. The attention layer oper- ates in the following steps. First, Query (Q), Key (K), and Value (V) are computed from the input. Next, attention scores are calcu- lated through dot product operations between Q and K, followed by scaling. After applying a softmax function, the resulting attention weights reflect the importance of each element. This step captures the underlying relationships in the data. Finally, the output is ob- tained by multiplying these weights with V, typically followed by linear layers. In the case of Multi-head Attention (MHA), QKV generation and attention score calculation can be divided into mul- tiple groups called heads. Attention-based models have achieved remarkable success across various fields, owing to their applica- bility and scalability to numerous tasks. Their ability to consider interactions across all positions in a sequence has recently extended their application to the Protein Structure Prediction Model (PPM). 2.2 Model Quantization Model Quantization is a technique that represents high-precision values (e.g., FP32) as discrete lower-precision values (e.g., INT4, INT8).",
    "source": "2505.05893v1_LightNobel_Improving_Sequence_Length_Limitation_in.pdf",
    "length": 2324,
    "tokens": 480
  },
  {
    "text": "Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948 [cs.CL] [20] Fabrice Devaux. 2019. The true Processing In Memory accelerator. In 2019 IEEE Hot Chips 31 Symposium (HCS). 1 24. 8875680 [21] Alexandar Devic, Siddhartha Balakrishna Rai, Anand Sivasubramaniam, Ameen Akel, Sean Eilert, and Justin Eno. 2022. To PIM or not for emerging general purpose processing in DDR memory systems. In Proceedings of the 49th Annual International Symposium on Computer Architecture (New York, New York) (ISCA 22). Association for Computing Machinery, New York, NY, USA, 231 244. https: doi.org 10.1145 3470496.3527431 [22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs.CL] [23] Siying Feng, Xin He, Kuan-Yu Chen, Liu Ke, Xuan Zhang, David Blaauw, Trevor Mudge, and Ronald Dreslinski. 2022. MeNDA: a near-memory multi-way merge solution for sparse transposition and dataflows. In Proceedings of the 49th Annual International Symposium on Computer Architecture (New York, New York) (ISCA 22). Association for Computing Machinery, New York, NY, USA, 245 258. https: doi.org 10.1145 3470496.3527432 [24] Elias Frantar and Dan Alistarh. 2023.",
    "source": "2504.17584v1_L3_DIMM-PIM_Integrated_Architecture_and_Coordinati.pdf",
    "length": 1566,
    "tokens": 484
  },
  {
    "text": "As the DNN models were designed to infer on the full data, we retrain the DNN models to recognize the compressed rep- resentation of the data and infer directly from that (both from the importance sampling and clustering). As the coreset for- mation algorithms are fairly simple [7, 8, 36, 37], it does not take much latency or energy to convert the raw sensor data into the coreset form even while using a commercial-off- the-shelf micro-controller (like TI MSP430FR5969 [66]). This allows the EH-sensor to opt for coreset formation followed by data communication to the host device as an energy-viable alternative to local DNN inference on the original data. In our example case, transmitting the raw data (60 data points, 32bit floating point data type) needs 240 Bytes of data trans- fer, and with coreset construction and quantization we can limit it to 36 Bytes (for 12 clusters, each cluster center is represented by 2 Bytes of data, and radius represented by 1 Byte data), thereby reducing the data communication volume by 85 . The host runs inference on the compressed data to detect the activity (with an accuracy of 76 ). However, due to this reduced accuracy, the sensor only takes this option iff it does not have enough energy to perform the inference at the edge device (either in the 16bit or 12bit variant of the DNN - more details on DNN design is presented in Sec- tion 4). This raises a question: is it possible to generate a more useful approximation, via reconstruction, of the data that we lost while forming the coresets? This problem has not been explored in details, as coresets are typically considered as an ùõº approximate representation of the data (ùõºbeing the error approximation parameter) [7] and never needed proper recovery. However, thanks to the low dimensional nature of many sensor data, reconstruction of original data from coresets becomes an essential step. 3.2.1 Data Memoization: Given our focus on ultra low power energy harvesting devices, any opportunities to re- duce computation and communication can noticeably aug- ment the performance and efficiency of the entire system. We look into data memoization as one such opportunity. For two instances of the same class, there should be a very high correlation in the sensor data.",
    "source": "Seeker.pdf",
    "length": 2273,
    "tokens": 488
  },
  {
    "text": "RQ4 examines whether FuncGNN overcomes feature convergence, ensuring robust capture of global logical structures in deep architectures. RQ5 validates the necessity and contribution of each component, such as hybrid aggregation and multi-layer integration, to the model s effectiveness. 4.2 Experiment Settings 4.2.1 Datasets For FuncGNN, the subcircuit dataset was constructed by extracting subcircuits from four benchmark suites ITC 99 [7], IWLS 05 [1], EPFL [3], and OpenCore [28] and converting them into a unified And-Inverter Graph (AIG) format using the ABC tool. Logic simulation with up to 100,000 random input vectors was performed to obtain accurate signal probabilities for each node. As presented in Table 1, the dataset comprises 9,933 valid subcircuits, covering circuit sizes from tens to thousands of nodes with varying logic levels and exhibiting diverse circuit topologies and AND-to-NOT gate ratios. The column labeled \" Subcircuits\" specifies the number of subcircuits extracted from each benchmark. To visually illustrate the pronounced structural differences among AIGs in the dataset, J. ACM, Vol. 1, No. 2, Article 3. Publication date: May 2025. 3:12 Qiyun Zhao a statistical analysis of the proportion of NOT nodes in the AIGs of all circuits was conducted. The histogram distribution is presented in Figure 3, revealing an irregular distribution of NOT gate proportions. Specifically, the mean NOT proportion is 29.7 , with a standard deviation of 8.42. Furthermore, Table 1 highlights substantial variability in subcircuit scale (node counts ranging from 36 to 3,214), logic levels (3 to 24 levels), and subcircuit counts (828 to 7,560), underscoring the dataset s high heterogeneity in topology, scale, and logical complexity. Notably, the \" Gate Ratio\" column shows a wide range from 0.90 to 30.4, reflecting large disparities in the relative proportions of AND and NOT gates across subcircuits. This multi-dimensional structural variability poses significant challenges to model robustness. Training and evaluating on such a structurally heterogeneous dataset highlights FuncGNN s strong generalization capabilities and validates its effectiveness in handling real-world AIG variability.",
    "source": "2506.06787v1_FuncGNN_Learning_Functional_Semantics_of_Logic_Cir.pdf",
    "length": 2217,
    "tokens": 474
  },
  {
    "text": "Table 1: Description of various quantization schemes. Activation Quantization Scheme Weight Total Memory Footprint LLM.int8() [17] Token-wise INT8 FP16 Channel-wise INT8 FP16 85.83 GB 3.99 GB 89.82 GB MEFold [32] No Quant. FP16 Tensor-wise INT4 FP16 113.49 GB 3.93 GB 117.42 GB PTQ4Protein [51] Tensor-wise INT8 Tensor-wise INT8 94.60 GB 3.95 GB 98.55 GB INT4 INT8 INT16 LightNobel (AAQ) Token-wise No Quant. INT16 7.90 GB 65.60 GB 73.50 GB SmoothQuant [64] Token-wise INT8 Channel-wise INT8 83.80 GB 3.95 GB 87.75 GB No Quant. BaseLine [39] FP16 No Quant. FP16 113.49 GB 7.90 GB 121.39 GB Channel-Wise Tender [35] INT4 Channel-wise INT4 94.60 GB 1.98 GB 96.58 GB Footprint Precision Grouping Footprint Precision Grouping Size Precision Grouping Size Precision Grouping Activation Quantization Scheme Weight Total Memory Footprint LLM.int8() [17] Token-wise INT8 FP16 Channel-wise INT8 FP16 85.83 GB 3.99 GB 89.82 GB MEFold [32] No Quant. FP16 Tensor-wise INT4 FP16 113.49 GB 3.93 GB 117.42 GB PTQ4Protein [51] Tensor-wise INT8 Tensor-wise INT8 94.60 GB 3.95 GB 98.55 GB INT4 INT8 INT16 LightNobel (AAQ) Token-wise No Quant. INT16 7.90 GB 65.60 GB 73.50 GB SmoothQuant [64] Token-wise INT8 Channel-wise INT8 83.80 GB 3.95 GB 87.75 GB No Quant. BaseLine [39] FP16 No Quant.",
    "source": "2505.05893v1_LightNobel_Improving_Sequence_Length_Limitation_in.pdf",
    "length": 1272,
    "tokens": 432
  },
  {
    "text": "The software accepts a user-specified LLM and generates Ver- ilog solutions based on structured problem definitions. It produces detailed evaluation reports, indicating whether each design passes synthesis and functional correctness checks, along with a resource utilization summary. By automating the full evaluation pipeline, the framework facilitates large-scale benchmarking and comparative studies of different LLMs for HDL code generation. To maintain consistency, each problem follows a structured for- mat consisting of three components: a natural language problem description in plain English, a Verilog module header, and a pre- defined testbench. The problem description specifies the expected input-output format and functional constraints, ensuring that LLM- generated code aligns with real-world design requirements. The module header provides a consistent Verilog interface with de- fined input and output signals but leaves the internal logic for the LLM to fill. The testbench validates functional correctness through simulation by applying predefined test cases in the testbench and comparing outputs against a manually verified reference solution. The evaluation framework follows a structured process to evalu- ate LLM-generated designs. The evaluation of an LLM on a bench- mark problem consists of the following three steps: (1) The framework queries the selected LLM to generate multi- ple Verilog code snippets for a given problem. These gener- ated snippets are stored in text format along with references to their corresponding problem descriptions. Functional cor- rectness is then verified using predefined testbenches. De- signs that pass all test cases proceed to FPGA synthesis, while those that fail have their errors recorded for further analysis. (2) FPGA synthesis is performed to determine resource usage metrics such as LUT count, DSP utilization, and register count. For designs that fail synthesis, the resource count is set to , ensuring a consistent comparison framework. (3) The framework generates structured reports summarizing pass rates, synthesis success rates, and resource usage statis- tics. Users can visualize model performance through auto- matically generated comparisons of functional correctness and resource usage. By following this structured evaluation process, the framework provides a fully automated benchmarking solution that evaluates LLM-generated Verilog across all benchmark problems, focusing on both design correctness and resource usage.",
    "source": "2503.08823v2_ResBench_Benchmarking_LLM-Generated_FPGA_Designs_w.pdf",
    "length": 2507,
    "tokens": 459
  },
  {
    "text": "Springer International Publishing, Cham, 234 241. [103] V√≠t R≈Ø≈æiƒçka, Anna Vaughan, Daniele De Martini, James Fulton, Valentina Salvatelli, Chris Bridges, Gonzalo Mateo-Garcia, and Valentina Zantedeschi. 2022. RaV√Ün: Unsupervised Change Detection of Extreme Events Using ML on-Board Satellites. Scientific Reports 12, 1 (Oct. 2022), 16939. [104] Sebastian Sabogal and Alan George. 2021. A Methodology for Evaluating and Analyzing FPGA-accelerated, Deep-Learning Applications for Onboard Space Processing. In 2021 Ieee Space Computing Conference (Scc). 143 154. Manuscript submitted to ACM 34 C√©dric L√©onard, Dirk Stober, and Martin Schulz [105] Shaibal Saha and Lanyu Xu. 2025. Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies. arXiv:2503.02891 [cs] [106] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. 2018. MobileNetV2: Inverted Residuals and Linear Bottlenecks. In 2018 IEEE CVF Conference on Computer Vision and Pattern Recognition. 4510 4520. [107] SDMS. 2010. MSTAR Dataset. [108] Ahmad Shawahna, Sadiq M. Sait, and Aiman El-Maleh. 2019. FPGA-Based Accelerators of Deep Learning Networks for Learning and Classification: A Review. IEEE Access 7 (2019), 7823 7859. [109] Sherin C. Shibi and R. Gayathri. 2021. Onboard Target Detection in Hyperspectral Image Based on Deep Learning with FPGA Implementation. MICROPROCESSORS AND MICROSYSTEMS 85, 104313 (Sept. 2021). [110] Felix Siegle, Tanya Vladimirova, J√∏rgen Ilstad, and Omar Emam. 2015. Mitigation of Radiation Effects in SRAM-Based FPGAs for Space Applications. ACM Comput. Surv. 47, 2 (Jan. 2015), 37:1 37:34.",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 1660,
    "tokens": 492
  },
  {
    "text": "This effectively reduces data move- ment. ECQ [60] utilizes a genetic approach for exploring layer-wise weight activation bitwidth(s), optimizing dynamic in-memory energy consumption at negligible performance loss. The ECQ approach requires only a network s weight activation distributions in order to estimate its energy consumption, making it applicable across various DNNs. The utilization of in-memory accelerators offers vast potential in reducing DNN inference energy consumption by minimizing data movement. However, such accelerators are not yet widely applicable on memory-constrained devices; Kim et al. [65] provide a detailed overview of in-memory techniques for DL. The key limitation of EA compression methods lies in their accuracy in predicting the layer operation-wise energy consumption of DNNs pre-execution. 2.3 provides an overview of existing energy-estimation methods, and their supported hardware platforms and DNN operators. Future work should additionally explore the impact of hardware variation on non-uniform quantization techniques. 2.2 Neural Architecture Search Traditional works enabling on-device inference focused on hand-crafted DNN architectures, including SqueezeNet [51], MobileNet [46, 126], and EfficientNet [141]. The design of these architectures required substantial manual tuning, combined with ingenuity; and while efficient, they lack versatility for diverse task-specific optimiza- tions. Neural Architecture Search (NAS) methods revolutionized NN design by automating the discovery of 4 Millar et al. NN architectures that optimize various performance metrics for a given task, while also considering computa- tional hardware constraints. Numerous works have demonstrated NAS s capability to reveal NN designs with state-of-the-art performance [116]. Typical NAS methods explore the architecture space using evolutionary [90] or reinforcement-learning (RL) [53] approaches. These methods require vast computational resources and GPU hours, rendering them infeasible in most real-world applications. Consequently, more recent NAS methods use proxies for architecture evaluation to reduce search complexity at the expense of (marginally) suboptimal results. Favoured among these methods is Differentiable NAS (DNAS), which utilizes gradient descent to optimize both DNN weights and parameters. Typically, DNAS employs super-nets, large DNNs that include various implementations of subnetworks, and explores the architecture space by evaluating subnetwork paths.",
    "source": "2505.12523v1_Energy-Aware_Deep_Learning_on_Resource-Constrained.pdf",
    "length": 2508,
    "tokens": 483
  },
  {
    "text": "Additionally, FC kernels are responsible for most of the execution time, so im- proving their performance has the largest impact on overall speedup. 0 1 2 3 1 2 3 Batch size Speculation length 1 Speculation length 2 Speculation length 4 Speedup 4 16 64 4 16 64 4 16 64 Figure 11: Performance speedup of PIM-only PAPI over AttAcc- only in the decoding phase for the Dolly creative-writing dataset. Figure 12 presents the execution time breakdown per token for the AttAcc-only system and for the PIM-only PAPI system with Attn-PIM and FC-PIM devices. We make four key obser- vations. First, FC kernels dominate the total execution time. Therefore, it is valuable to enable higher execution parallelism in PIM hardware (as PAPI does with FC-PIM) to effectively cater to the high computation demands of the FC kernels. Sec- ond, the PIM-only PAPI design provides 2.9 speedup when processing FC kernels. Third, attention kernels run 1.7 slower on Attn-PIM (1P2B) than AttAcc-only (1P1B) due to our design choice that reduces FPU area overheads. Fourth, communica- tion takes up 28.2 of the total execution time in the decoding stage; thus, more advanced network technologies could be developed and integrated into the PAPI architecture to further improve performance. 0 2 4 6 8 10 PIM-only PAPI AttAcc-only Execution time per token (ms) Attention layer FC layer Communication Other Figure 12: Execution time breakdown per token in the decoding phase of LLaMA-65B model inference (batch size 4, speculation length 4) for AttAcc-only versus PIM-only PAPI. 10 8. Related Work To our knowledge, PAPI provides the first architecture and a runtime framework to tackle dynamically varying paralleliza- tion levels and hence dynamically varying computation and memory demands of real-world LLM workloads. We compre- hensively compare PAPI to two state-of-the-art PIM designs, AttAcc [23] and HBM-PIM [30], demonstrating PAPI s signifi- cant performance and energy benefits over them (Section 7.2). PIM-enabled LLM accelerators.",
    "source": "2502.15470v2_PAPI_Exploiting_Dynamic_Parallelism_in_Large_Langu.pdf",
    "length": 2014,
    "tokens": 483
  },
  {
    "text": "This means that the results on these two models cannot be used to differentiate between configurations, and we therefore excluded these models from this calculation. Table 3: Results on a selection of lm-eval-harness down- stream evaluation tasks for different models in the Llama-2, GPT3, and Nemotron model families. Llama-2-7B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.440 0.570 0.780 0.696 0.795 0.656 FP8 0.435 0.570 0.780 0.680 0.791 0.651 FP4 0.429 0.561 0.768 0.680 0.773 0.642 90 FP4 0.427 0.564 0.777 0.684 0.782 0.647 70 FP4 0.450 0.569 0.777 0.695 0.788 0.656 Llama-2-13B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.448 0.602 0.797 0.723 0.822 0.678 FP8 0.450 0.603 0.792 0.721 0.824 0.678 FP4 0.440 0.595 0.785 0.715 0.809 0.669 90 FP4 0.446 0.596 0.787 0.724 0.817 0.674 70 FP4 0.445 0.600 0.792 0.725 0.815 0.675 GPT3-1.3B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.363 0.439 0.742 0.591 0.648 0.557 FP8 0.365 0.439 0.737 0.580 0.642 0.552 FP4 0.359 0.429 0.731 0.577 0.642 0.548 90 FP4 0.362 0.433 0.727 0.590 0.644 0.551 70 FP4 0.360 0.435 0.739 0.594 0.637 0.553 GPT3-8.3B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.415 0.546 0.780 0.680 0.689 0.622 FP8 0.416 0.546 0.774 0.672 0.691 0.620 FP4 0.415 0.533 0.774 0.665 0.683 0.614 90 FP4 0.413 0.538 0.770 0.665 0.688 0.615 70 FP4 0.415 0.541 0.773 0.667 0.690 0.617 GPT3-22B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.434 0.579 0.789 0.707 0.746 0.651 FP8 0.434 0.577 0.790 0.702 0.751 0.651 FP4 0.441 0.568 0.788 0.699 0.745 0.648 90 FP4 0.441 0.573 0.784 0.707 0.740 0.649 70 FP4 0.435 0.575 0.792 0.700 0.740 0.648 Nemotron-4-15B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.471 0.620 0.812 0.756 0.787 0.689 FP8 0.482 0.625 0.813 0.757 0.736 0.683 FP4 0.457 0.615 0.791 0.743 0.689 0.659 90 FP4 0.478 0.618 0.807 0.753 0.720 0.675 70 FP4 0.471 0.623 0.809 0.770 0.751 0.684 ( FGMP (w o Global Threshold Clipping) ), and with applying the global threshold but not applying our sensitivity-weighted clip- ping method ( FGMP (w o Clipping) ).",
    "source": "2504.14152v1_FGMP_Fine-Grained_Mixed-Precision_Weight_and_Activ.pdf",
    "length": 2123,
    "tokens": 897
  },
  {
    "text": "This includes smarter scheduling, caching, prompt engineering, and hybrid scaling approaches that adapt to deployment constraints. By exposing the hidden costs of agentic reasoning and offering actionable insights into their infrastructure impact, we hope this work informs future system and algorithm co-design for scalable and sustainable AI agents. REFERENCES [1] A. Agrawal, N. Kedia, A. Panwar, J. Mohan, N. Kwatra, B. Gulavani, A. Tumanov, and R. Ramjee, Taming Throughput-Latency tradeoff in LLM inference with Sarathi-Serve, in Proceedings of the USENIX Symposium on Operating Systems Design and Implementation (OSDI), 2024. [2] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebron, and S. Sanghai, GQA: Training generalized multi-query transformer models from multi-head checkpoints, in Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. [3] Anthropic, Model Context Protocol (MCP), 2024. [Online]. Available: [4] P. A. Bao Tran, Semiconductor Manufacturing Energy Consumption: How Green Is the Chip Industry? 2025. [Online]. Avail- able: consumption-how-green-is-the-chip-industry-latest-stats [5] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski, L. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk, and T. Hoefler, Graph of thoughts: solving elaborate problems with large language models, in Proceedings of the AAAI Conference on Artificial Intelligence, 2024. [6] T. Cai, Y. Li, Z. Geng, H. Peng, J. D. Lee, D. Chen, and T. Dao, Medusa: Simple llm inference acceleration framework with multiple decoding heads, in Proceedings of the International Conference on Machine Learning (ICML), 2024.",
    "source": "2506.04301v1_The_Cost_of_Dynamic_Reasoning_Demystifying_AI_Agen.pdf",
    "length": 1688,
    "tokens": 467
  },
  {
    "text": "5 Standardization and Open Benchmark Dataset for Analog RF ML-EDA When applying ML to analog design, an important challenge to address is the lack of standardization in data, workflows, and eval- uation protocols. For example, analog sizing techniques are com- monly evaluated on OTAs that are inconsistently characterized across different technologies. Due to the lack of standardization, a fair comparison between newly proposed techniques is challenging to perform and often errorneously completed [81]. Efforts in digital EDA [81 85] have highlighted the importance of unified frameworks and open datasets for the application of ML to EDA problems. Establishing uniform benchmarks and evaluation criteria for analog design will improve comparability, reproducibil- ity, and model reusability of workflows while addressing limitations due to dataset and PDK disparities. 6 Concluding Remarks ML has proven to be a powerful tool in improving productivity and design quality in analog and RF circuit design, complement- ing, rather than replacing, traditional techniques. A summary of heuristic and ML-based techniques for each circuit task is shown in Fig. 2. The infusion of ML AI reduces the traditionally steep learning curve needed in analog and RF design. References [1] IBS Corporation, IBS Corporation, Industry Reports, 2003. [2] R. Phelps, M. Krasnicki, R. A. Rutenbar, R. Carley, and J. R. Hellums, Anaconda: Simulation-based Synthesis of Analog Circuits via Stochastic Pattern Search, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, Vol. 19, No. 6, pp. 703 717, Jul. 2000. [3] Tonmoy Dhar, Kishor Kunal, Yaguang Li, Meghna Madhusudan, Jitesh Poojary, Arvind K. Sharma, Wenbin Xu, Steven M. Burns, Ramesh Harjani, Jiang Hu, Desmond A. Kirkpatrick, Parijat Mukherjee, Soner Yaldiz, and Sachin S. Sapat- nekar, ALIGN: A System for Automating Analog Layout, IEEE Design Test, Vol. 38, No. 2, pp. 8 18, Apr. 2021.",
    "source": "2506.00007v1_Emerging_ML-AI_Techniques_for_Analog_and_RF_EDA.pdf",
    "length": 1952,
    "tokens": 477
  },
  {
    "text": "Specifically, FASTLIBRA works as follows. 1) The cache manager organizes LoRAs and KV caches in HBM and main memory, constructing their usage dependencies into a de- pendency tree. 2) During inference, it inserts newly loaded LoRAs into the second layer of the tree, and inserts or deletes KV cache nodes at the leaves of their corresponding LoRA branches. 3) After each monitor interval, the cache swapper retrieves the states of nodes from the cache manager, and decides the swapped-in out KV caches and LoRAs when the HBM is idle busy. The decisions are made using a cost model that considers metrics like LRU time, visit frequency, and loaded LoRA quantity, and accesses their impact on TTFT. 4) The swap-in out decisions are sent back to the cache manager for performing corresponding memory operations. 5) For a new query, if its LoRAs or KV caches are in main memory but HBM is full, the cache manager swaps out cold LoRAs or KVs based on the cache swapper s decisions, then swaps in the required ones. 6) This query proceeds for inference to generate the next token with the required LoRA and KVs. FASTLIBRA can be adapted to other LLM inference en- gines [1,45,53] by replacing their memory management mod- ule with few modifications. It applies to LLMs based on decoder-only transformer [8,15,39] that cover popular LLM practical scenarios. The design of FASTLIBRA does not rely on any specific hardware architecture, and it is applicable to other accelerators. 4 Dependency-aware Cache Manager In this section, we first analyze how to construct the usage dependencies among LoRAs and KV caches, then introduce their maintenance during serving the queries. 4.1 Usage Dependency Constructing As we analyzed in Section 2.3.1, a LoRA and its correspond- ing KV caches have their inherent usage dependencies.",
    "source": "2505.03756v1_Improving_the_Serving_Performance_of_Multi-LoRA_La.pdf",
    "length": 1814,
    "tokens": 418
  },
  {
    "text": "The GDSII data is transformed into a sequential format to make it suitable for processing by a language model. The key components include polygon shapes and spatial relationships between them, encoded as sequential tokens. The model architecture employs a BERT-based transformer specifically designed for layout patterns, utilizing multi-head self-attention to capture relationships between layout features both locally and globally. The architecture consists of an embedding layer, followed by LLM-HD [117] layers, and concludes with a classification layer. During pre-training, the model uses masked language modeling, an unsupervised task where portions of the input data are randomly masked, and the model predicts the masked portions. This pre-training enables the model to learn representations of layout patterns, before fine-tuning for specific tasks such as hotspot detection. 4.4.3 Downstream tasks for layout encoders. Circuit GNN [115] supports both congestion prediction and net wirelength prediction tasks. For congestion, it predicts routing congestion during both the logic synthesis and placement stages, evaluated using correlations and classification metrics like precision, recall, and F1-score. TAG [116] handles three layout-stage tasks: layout matching prediction (binary classification of layout constraints, evaluated by accuracy, TPR, FPR, PPV, and F1-score), wirelength estimation (HPWL evaluated with R2, MAE, and sMAPE), and net parasitic capacitance prediction (evaluated using R2 and MAE). LLM-HD [117] focuses on hotspot detection, a binary classification task identifying layout areas prone to manufacturing defects.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 1649,
    "tokens": 330
  },
  {
    "text": "PointODE omits the geometric affine transformation in PointMLP and instead performs a point-wise normalization to avoid the necessity of computing global statistics of input feature points, leading to the improved parallelism and accuracy. The FPGA design consists of a four-stage feature extraction pipeline to exploit the point-level parallelism. Thanks to the lower parameter size and reuse of blocks, parameters and intermediate activations of the entire model fit on the FPGA memory, eliminating most of the off-chip data transfers. The proposed accelerator is implemented on a Xilinx ZCU104 board and evaluated on classification datasets. It runs 4.9x faster than the ARM Cortex-A53 processor, while achieving a comparable accuracy to state-of-the-art networks. The contributions of this paper are summarized as follows: We propose PointODE and its lightweight counterpart PointODE-Elite as a simple yet highly-accurate DNN architecture for point clouds. To the best of our knowledge, this paper is the first to explore an FPGA design for the Neural ODE-based point cloud analysis. 2 Related Work DNNs for Point Clouds: PointNet [9] is a pioneering model that directly consumes point clouds. It extracts point- wise features using a shared MLP and aggregates them into a global representation via a symmetric pooling function. PointNet [10] is an extension that aims to capture fine geometric context through repeated sampling and grouping 2 PointODE: Lightweight Point Cloud Learning with Neural ODEs on Edge A PREPRINT of feature points. PointNet and PointNet have catalyzed the development of more sophisticated networks to learn better feature representations at the cost of increased complexity. One approach is to design custom convolution kernels for point clouds [11, 12, 31 35], while several works [13, 14, 36 38] employ GCNs (Graph Convolutional Networks) to process KNN graphs built from point clouds. Inspired by the tremendous success, transformer-based methods [15,16,39,40] employ a self-attention mechanism to capture the relationship between all points, while it incurs a quadratic computational cost in terms of input size. Another approach is to represent point clouds as curves (sequence of points) [41], polygon meshes [42], or umbrella surfaces [43].",
    "source": "2506.00438v1_PointODE_Lightweight_Point_Cloud_Learning_with_Neu.pdf",
    "length": 2279,
    "tokens": 476
  },
  {
    "text": "We assume a total number of weights nw and a number of feature maps fl to be multiplied by the same weight in a layer and the number of clock cycles cl needed to process a layer. Given these parameters, the number of groups of weights loaded into each half of such an accelerator can be calculated as wp 2 2nw (m p). Also, the number of memory accesses Nm can be determined as follows: Nm r c if (r c) ( m 2 ) ( m 2 ) max(fl (( r c 2 ) 1), 1) if ( m 2 ) (r c) m (m) max(fl (r c 1), 1) otherwise (1) The above-mentioned wp and Nm can be used to estimate the energy consumption (Ea) are given as follows: Ea Nm R . EM X l L cl . T. Ppod (2) where R and L represent a number of rows per memory bank and a total number of layers in the AxDNN model, respec- tively. The number of clock cycles is given as cl as cl wp. The Ppod is calculated as (m 2).Pmac (m 2).Pxmac. The Pmac and Pxmac denote the power consumed by accurate and approximate MAC units, respectively. The analytical model of this accelerator has been validated by comparing the results with the hardware implementation of the multi-pod systolic array accelerator in [22]. Our analytical model provides an accurate estimate of latency and energy consumption. IV. XAI-GEN: PROPOSED METHODOLOGY This section discusses our proposed approach and method- ology for XAI-guided AxDNN generation, and a case study. A. XAI Guided Approximate Computing We use XAI to analyze gradients within a neural network to assess the importance of individual neurons and entire layers. These insights are valuable for making informed decisions regarding selecting appropriate approximate multipliers in AxDNN layers. When analyzing the importance of neurons, it is crucial to understand their contributions relative to a baseline. Let us consider an AxDNN classifier F : Rn [0, 1], an input x (x1, ..., xn), and a baseline input x i.e., black image.",
    "source": "2503.16583v1_Explainable_AI-Guided_Efficient_Approximate_DNN_Ge.pdf",
    "length": 1887,
    "tokens": 470
  },
  {
    "text": "Given these contrasting strengths, one might assume it s best to offload all control-heavy tasks during GNN inference, such as computing initial masks (i.e., preprocessing in Fig. 4) for aggregation or calculating intermediate attention scores, to the CPU. However, a challenge arises when control-flow tasks exhibit a Read- after-Write (RAW) dependency on previous data-parallel tasks, necessitating the transfer of data back to the CPU. This results Fig. 8. GraphSplit, partitioned GNN inference using CPU and NPU: CPU handles graph preprocessing; NPU accelerates data-parallel GNN computation. in considerable communication overhead. To overcome this, GraNNite introduces an offline profiling phase during model calibration. In this phase, we build a cost model that measures real-time latencies of various operations on both the CPU and NPU. This cost model also factors in the overhead from data transfer and communication between the CPU and NPU. Using this information, GraphSplit identifies the most effec- tive partition points to minimize communication and latency. GraphSplit s partitioning strategy is designed to play to the strengths of each processing unit. Control-flow tasks, which require complex decision-making, are assigned to the CPU. Computationally heavy, data-parallel tasks, such as matrix multiplications, are sent to the NPU. This careful distribution improves graph processing performance by reducing the need for frequent data exchanges. For example, offloading initial in- put preprocessing to the CPU requires minimal communication with the NPU, resulting in better performance. As shown in Fig. 8, this partitioned inference setup for models such as GCN, GAT, and GraphSAGE effectively balances workload between CPU and NPU. StaGr: For applications involving static graph structures, GraNNite proposes an efficient methodology (StaGr) for im- plementing GNNs on hardware accelerators. Using a precom- puted mask tailored to a fixed input graph, StaGr transforms the aggregation of node features in Graph Convolution into a streamlined matrix multiplication operation (refer to GCN in Fig. 9), fully utilizing the capabilities of the NPU. This precomputed mask establishes node connections beforehand, significantly reducing irregular memory accesses and improving memory latency and energy efficiency, all without requiring extensive hardware modifications.",
    "source": "2502.06921v2_GraNNite_Enabling_High-Performance_Execution_of_Gr.pdf",
    "length": 2390,
    "tokens": 478
  },
  {
    "text": "Application- driven problems span machine learning, encryption, and financial computing, emphasizing the role of FPGAs in AI acceleration, security, and high-speed data processing. By adhering to these principles, the benchmark evaluates not only an LLM s ability to generate syntactically correct Verilog code but also its capability to produce hardware-efficient designs suited for FPGA deployment. 3.2 Comparison with Existing Benchmarks Table 3 highlights key differences between ResBench and existing HDL benchmarks. The differences are particularly significant in FPGA resource optimization awareness and problem diversity: FPGA resource optimization awareness. Most existing bench- marks, such as VerilogEval, HDLEval, and GenBen, focus primarily on functional correctness and HDL syntax quality but do not explicitly account for FPGA resource usage. Con- sequently, these benchmarks cannot distinguish between functionally correct designs that differ significantly in hard- ware resource utilization. In contrast, ResBench introduces optimization-aware problems specifically designed to expose variations in resource usage. This enables a more practi- cal comparison of LLMs based on their ability to generate resource-efficient designs for FPGAs. Problem diversity. Existing benchmarks primarily focus on fundamental HDL constructs such as basic logic, state ma- chines, and arithmetic operations, with limited diversity in FPGA applications. For example, VerilogEval emphasizes control logic and arithmetic, while HDLEval mainly evalu- ates digital circuits and state machines. In contrast, ResBench encompasses a significantly broader range of FPGA appli- cations, including machine learning, encryption, financial computing, and physics-based modeling. These domains rep- resent real-world FPGA workloads where resource efficiency is crucial for minimizing device cost and maximizing paral- lelism. By incorporating a diverse set of tasks, ResBench pro- vides a more comprehensive evaluation of LLM-generated HDL in practical FPGA design scenarios. 4 Evaluation Framework for ResBench To evaluate LLM-generated designs for FPGA design with Res- Bench, we implement a structured framework that examines both functional correctness and hardware efficiency. We build the software for the evaluation framework based on the lessons learned in testing LLM-based software code generation. The benchmarks for LLM-based software share a common eval- uation framework aimed at assessing whether generated code is both syntactically valid and functionally correct.",
    "source": "2503.08823v2_ResBench_Benchmarking_LLM-Generated_FPGA_Designs_w.pdf",
    "length": 2565,
    "tokens": 492
  },
  {
    "text": "In per-block quantization, the max is computed on a block of a matrix, which in our paper means a FlashAttention block. 3 FP4 Attention for Inference Acceleration This section presents our microscaling FP4 attention through three key components: (1) the fundamen- tal workflow for applying microscaling FP4 quantization to attention in Section 3.1, (2) the two-level quantization approach for the attention map in Section 3.2, and (3) critical hardware implementation optimization in Section 3.3. 0.0 0.5 1.0 Value 0 1 2 3 Frequency 1e7 (a) P Distribution Mean: 0.1758 0.00 0.05 0.10 0.15 E4M3 Value 101 102 103 104 Frequency (b) sP Distribution (Direct Quant) 0 200 400 E4M3 Value 101 102 103 104 Frequency (c) sP Distribution (Two-level Quant) Direct Quant Two-level Quant 0.000 0.002 0.004 0.006 0.008 Absolute Error (d) sP Error Compared to FP32 Mean 1 SD Direct Quant Two-level Quant 0.00 0.02 0.04 0.06 Absolute Error (e) Quantization Error of P Mean 1 SD Figure 3: Analysis of the benefit of two-level quantization. (a) shows the distribution of eP. (b) and (c) show the distribution of sP using direct quantization and two-level quantization. (d) and (e) show the error of sP and eP using direct quantization and two-level quantization. 3.1 Microscaling FP4 Attention FP4 microscaling quantization. Given a matrix X RN d, we quantize it to ÀÜX in FP4 data type with a scale factor matrix sX in FP8 data type. Specifically, X is partitioned into Xij R1 n blocks, where each 1 n block corresponds to one scale factor sij. The FP4 microscaling quantization ([ ÀÜX, sX œï(X)]) and dequantization (X œï 1( ÀÜX, sX)) can be formulated as follows.",
    "source": "2505.11594v1_SageAttention3_Microscaling_FP4_Attention_for_Infe.pdf",
    "length": 1643,
    "tokens": 461
  },
  {
    "text": "The return value is the minimum between the application request and the maximum dimension size allowed by the underlying microarchitecture, discussed in Sections III-A1 and III-A2, and the data-width of input and output matrices, SEWi, and SEWo. The ttypeio is a 3-bit immediate encoding of the input and output matrix element widths, SEWi and SEWo, for configuring the ttype CSR fields. 2) Matrix data movement: The tile load store, tl ts, and their transposed variants ttl tts, move GEMM matrix operands between memory and the vector register file. In the gen- eral case, these instructions access blocks of up to RLEN consecutive bits found at constant strides in memory. The load instructions are encoded with the GEMM operand type they produce, A, B, C, or BT to specify the relevant CSR dimension shape fields to the microarchitecture for instruction decoding. For instance, C tile operations require the CSR tm, and tn fields to compute the C tile shape. The special t{t}lbt instruction indicates that the B operand is organized in a col- major layout within the vector register state. Finally, MTE defines store instructions exclusively for the C matrix operand. MTE memory instructions require two scalar parameters: The base memory address from rs1, and the leading dimension offset from rs2, expressed in bytes. The MTE operands on memory instructions are directly associated to the BLAS software interface parameters pairs (A, lda), (B, ldb), and (C, ldc). The special case with stride zero corresponds to row or column broadcast operations where a single row column value in memory is replicated across all vector register rows - columns. MTE implementations may optimize row column broadcasts when rs2 describes the 0-stride scenario. 3) Matrix Multiplication: The tfmul and tmul matrix tile multiply instructions compute the product of the A and B matrix tiles and accumulate the result to a C tile matrix, i.e., they are MMA instructions. The former instruction operates on floating-point data types and the latter on integers. These instructions require three vector register operands, the C tile in vd, the A tile in vs1, and the B tile in vs2.",
    "source": "2507.03522v1_A_Flexible_Instruction_Set_Architecture_for_Effici.pdf",
    "length": 2162,
    "tokens": 488
  },
  {
    "text": "[37] Tae Jun Ham, Lisa Wu, Narayanan Sundaram, Nadathur Satish, and Margaret Martonosi. 2016. Graphicionado: A High-Performance and Energy-Efficient Accelerator for Graph Analytics. In MICRO. [38] Charles R. Harris, K. Jarrod Millman, St√©fan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern√°ndez del R√≠o, Mark Wiebe, Pearu Peterson, Pierre G√©rard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. 2020. Array Programming with NumPy. Nature (2020). [39] Muhammad Hassan, Chang Hyun Park, and David Black-Schaffer. 2021. A Reusable Characterization of the Memory System Behavior of SPEC2017 and SPEC2006. ACM TACO (2021). [40] RanjanHebbarSRandAleksandarMilenkoviƒá.2019. SPECCPU2017:Performance, Event, and Energy Characterization on the Core i7-8700K. In ICPE. [41] Qijing Huang, Po-An Tsai, Joel S. Emer, and Angshuman Parashar. 2024. Mind the Gap: Attainable Data Movement and Operational Intensity Bounds for Tensor Algorithms. In ISCA. [42] Christopher J Hughes, Vijay S Pai, Parthasarathy Ranganathan, and Sarita V Adve. 2002. RSIM: Simulating Shared-Memory Multiprocessors with ILP Processors. IEEE Computer (2002). [43] Engin √èpek, Sally A McKee, Rich Caruana, Bronis R de Supinski, and Martin Schulz. 2006. Efficiently Exploring Architectural Design Spaces via Predictive Modeling. ACM SIGOPS (2006). [44] PJ Joseph, Kapil Vaswani, and Matthew J Thazhuthaveetil. 2006.",
    "source": "2503.23076v1_Concorde_Fast_and_Accurate_CPU_Performance_Modelin.pdf",
    "length": 1625,
    "tokens": 498
  },
  {
    "text": "Models COLA SST-2 MRPC QQP MNLI-m MNLI-mm QNLI RTE WNLI STSB Score BERT(2MB) 3,93 75,20 63,88 81,03 64,95 63,65 62,75 49,38 70,68 6,74 54,22 MAMBA(2MB) 7,22 80,60 60,72 80,66 64,27 64,44 59,78 51,76 80,00 4,58 55,40 Embedder 9,65 78,90 62,25 83,28 62,06 62,17 65,40 52,73 77,20 15,58 56,92 Embedder Conv 9,25 79,10 60,50 82,98 61,98 60,93 62,08 52,00 79,16 16,10 56,41 BERT NE 5,22 77,34 63,18 81,12 64,14 64,53 66,76 51,14 85,62 4,72 56,38 BERT NE EA 2,66 78,68 61,90 83,16 62,65 61,63 62,36 48,38 85,62 8,94 55,60 EmbBERT 5,32 78,50 62,54 82,58 63,82 65,78 63,68 51,26 87,30 9,76 57,05 Table 12: Evaluation of pretrained and finetuned models on the GLUE benchmark (Embedder and Embedder Conv are directly trained on the donwstream datasets). We report SCC for STSB, MCC for CoLA, F1 score for QQP and MRPC, Accuracy for the remaining GLUE tasks. Models COLA SST-2 MRPC QQP MNLI-m MNLI-mm QNLI RTE WNLI STSB Score BERT(2MB) -0,86 71,28 64,66 73,04 60,56 61,58 60,82 48,24 66,20 15,48 52,10 MAMBA(2MB) 2,56 81,16 64,62 79,18 61,22 61,40 63,20 50,20 76,62 10,16 55,03 Embedder 9,65 78,90 62,25 83,28 62,06 62,17 65,40 52,73 77,20 15,58 56,92 Embedder Conv 9,25 79,10 60,50 82,98 61,98 60,93 62,08 52,00 79,16 16,10 56,41 BERT NE 9,04 78,82 65,04 79,96 63,08 63,30 63,20 51,76 87,30 13,46 57,50 BERT EA 10,06 79,44 66,48 78,88 60,82 60,82 63,50 50,76 22,24 17,92 51,09 BERT NE EA 18,70 79,60 65,36 82,66 67,06 67,44 67,44 53,66 86,74 23,34 61,20 BERT NE EA (8bit) 9,57 77,87 63,40 48,93 34,19 33,59 59,58 53,79 59,15 19,58 45,97 EmbBERT 11,01 79,33 69,19 83,25 67,83 68,63 68,92 49,96 87,61 49,25 63,50 EmbBERT-Q 9,56 80,96 67,99 82,45 67,10 68,05 68,06 47,29 87,32 49,28 62,81 overall view of their functionality.",
    "source": "2502.10001v1_EmbBERT-Q_Breaking_Memory_Barriers_in_Embedded_NLP.pdf",
    "length": 1712,
    "tokens": 776
  },
  {
    "text": "8.1 Impact of General Optimizations Figure 16 shows the performance impact of general embedding opti- mizations such as vectorization (emb-opt1), bufferization (emb-opt2), and queue alignment (emb-opt3) over unoptimized Ember-generated code (emb-opt0) for the SLS function and more compute-intensive MP models. For the SLS function, we evaluated the three DLRMs in Table 3, each one running representative synthetic inputs [18] with low (L0), medium (L1), and high (L2) locality. Overall, all optimizations com- bined (emb-opt3) achieve 6.6 , 12.1 , and 21 better performance over unoptimized code (emb-opt0) for RM1, RM2, and RM3, respec- tively. Vectorization is consistently the most impactful optimization with a 5.13 speedup and only 17 deviation, whereas other opti- mizations deliver widely-different performance improvements on different configurations. To better understand these results, Figure 17 shows how these optimizations impact the throughput at which the compute unit reads and the access unit writes into the L2 queue. Compute op- timizations move upward whereas memory optimizations move rightward. The blue line indicates where the compute-unit through- put equals the access-unit throughput. Only optimizing compute code cannot move above the blue line as the access unit would not be able to marshal enough data to process. This would only improve throughput up to 8 (RM3, emb-opt0) before the access unit starts to be the bottleneck (blue line). However, because of the SLC IR, Ember can perform global optimizations on both access and compute code and move both rightward and upward in the plot, improving performance by up to 21 (RM3, emb-opt3). For RM1, the most control-intensive model (shorter loops), vectoriza- tion already saturates throughput, leaving other optimizations little room for improvement. For RM2 and RM3, by reducing coordinate overhead, bufferization helps to move closer to the blue line.",
    "source": "2504.09870v1_Ember_A_Compiler_for_Efficient_Embedding_Operation.pdf",
    "length": 1936,
    "tokens": 438
  },
  {
    "text": "We specifically highlight CloudMatrix384, the first production- grade implementation of this innovative architectural concept. CloudMatrix384 is an AI supernode purpose-built for large-scale AI workloads, featuring a fully peer-to-peer interconnected hardware design. It comprises 384 Ascend 910 NPUs and 192 Kunpeng CPUs, interconnected via an ultra-high- bandwidth and low-latency network named unified bus (UB). In particular, this UB network enables direct all-to-all data exchange across all compute and memory components. Unlike conventional hierarchical architectures with uneven intra-node and inter-node interconnect bandwidth, Cloud- Matrix384 allows the entire supernode to operate as a logically unified, tightly coupled compute entity, embodying the fully peer-to-peer principle that everything can be pooled, treated equally, and combined freely . These architectural features are particularly beneficial for communication- intensive operations such as large-scale MoE expert parallelism and distributed KV cache access, making CloudMatrix384 a scalable and high-performance foundation for next-generation LLM serving. The initial design of CloudMatrix384 predates the widespread adoption of MoE architectures [15, 39, 49], as the design and deployment of such a comprehensive supernode system typically spans several years. Nonetheless, CloudMatrix384 was purpose-built to enhance interconnect bandwidth and communication efficiency core capabilities essential for scaling large training and inference workloads. The emergence of large-scale MoE models such as DeepSeek-R1 [13] validates this Serving Large Language Models on Huawei CloudMatrix384 5 architectural foresight, highlighting that communication bandwidth is as crucial as compute and memory bandwidth capabilities in modern LLM deployments. To fully exploit CloudMatrix384 s capabilities, we propose CloudMatrix-Infer, a comprehensive LLM serving solution that represents a best practice for deploying large-scale MoE models such as DeepSeek-R1. CloudMatrix-Infer introduces three core innovations. First, we design a novel peer-to-peer serving architecture that disaggregates the LLM inference system into three independent subsystems: prefill, decode, and caching. Peer-to-peer means that the three subsystems operate as equal and independent resource pools, without being orchestrated around a centralized entity.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2393,
    "tokens": 460
  },
  {
    "text": "[40] OpenCV, Similarity check (PNSR and SSIM) on the GPU. similarity gpu-basics-similarity.html , 2019. [41] OpenGL, Cubemaps - Learn OpenGL. Advanced-OpenGL Cubemaps , 2019. [42] OpenGL, The Industry s Foundation for High Performance Graphics. , 2019. [43] O. Rift, Oculus Rift - How Does Time Warping Work? youtube.com watch?v WvtEXMlQQtI , 2019. [44] Samsung, Samsung Gear VR, galaxy gear-vr . [45] Samsung, Explore New Dimensions. global galaxy gear-vr display , 2019. [46] SkySports, Sky VR Virtual Reality, mobile apps 10606146 sky-vr-virtual-reality , 2019. [47] Tom s HARDWARE, Nvidia s Jetson TX2 Powers GameFace Labs Standalone VR Headset. gameface-labs-standalone-steamvr-headset,37112.html , 2019. [48] R. Toth, J. Nilsson, and T. Akenine-M oller, Comparison of Projection Methods for Rendering Virtual Reality, in Proceedings of High Perfor- mance Graphics, ser. HPG 16, 2016, pp. 163 171. [49] C.-H. Tsai, H.-T. Wang, C.-L. Liu, Y. Li, and C.-Y. Lee, A 446.6 K-gates 0.55 1.2 V H. 265 HEVC decoder for next generation video applications, in 2013 IEEE Asian Solid-State Circuits Conference (A- SSCC), 2013, pp. 305 308. [50] A. Vlachos, Advanced VR Rendering in Valve. http: media.steampowered.com apps valve 2015 Alex Vlachos Advanced VR Rendering GDC2015.pdf , 2019. [51] F. G. VR360, Virtual guided tour of Paris. watch?v sJxiPiAaB4k , 2019. [52] Wikepedia, Equirectangular Projection. Equirectangular projection , 2019. [53] Wikipedia, Pixel 2, 2 . [54] Wikipedia, Active-Matrix Organic Light-Emitting Diode, wikipedia.org wiki AMOLED , 2019. [55] Wikipedia, Virtual Reality.",
    "source": "DejaView.pdf",
    "length": 1592,
    "tokens": 500
  },
  {
    "text": "It is well established that the decode stage of LLM inference is memory bound [1], [26], with weights loading constituting a sig- nificant bottleneck. Traditional Von-Neumann accelerators consume significant energy and time when moving weights from on-chip buffers to processing elements (PEs). This makes existing traditional accelerators with flexible structured sparsity support [14], [36] poor candidates to accelerate N:M sparse LLMs for inference. An al- ternative computing paradigm, namely, digital compute-in-memory (DCiM) offers an efficient solution to the above challenge by moving the compute within the memory arrays. This mitigates the memory access bottleneck. However, despite the advantages of DCiM over traditional digital accelerators, there has been limited exploration of flexible structured sparsity support for DCiM-based architectures. This can be associated with the rigid crossbar structure of memory arrays and the infeasibility of transferring the flexible structured sparsity techniques employed for digital accelerators to DCiM ( III). Our contributions. We present two significant contributions toward addressing the above-mentioned algorithmic and hardware limita- tions. Specifically, we present a pruning algorithm that can assign different N:M sparsity with a higher degree of freedom and then present the first DCiM accelerator to support such patterns with minimal overhead, as summarized below. 1Scalars with larger magnitude compared to the rest of the tensor. arXiv:2504.14365v1 [cs.LG] 19 Apr 2025 We first explore the efficient and optimal assignment of the N:M sparsity pattern to each layer of an LLM. Specifically, we identify that compared to static N or M, having the luxury to choose both N and M allows us to yield better sparse representational freedom necessary to maintain improved model accuracy. To this end, we propose FLOW flexible layer-wise outlier-density-aware N:M spar- sity selection method. FLOW is based on a key insight that both the presence of outliers and their distribution in different layers con- tributes to the layer heterogeneity, impacting their tolerance towards model pruning. We leverage these two characteristics presence and distribution of outliers to assign diverse N:M values with the ability to determine the optimal N and M for a layer simultaneously.",
    "source": "2504.14365v1_Accelerating_LLM_Inference_with_Flexible_NM_Sparsi.pdf",
    "length": 2337,
    "tokens": 473
  },
  {
    "text": "Commercial models (e.g., GPT) reduce the substantial costs of training and deploying LLMs but may raise security and intellectual property concerns. On the other hand, fine-tuning local open-sourced LLMs (e.g., Llama, DeepSeek) can address these security and IP issues but requires significant resources and limits the model size. Smaller-scale customized LLMs tend to be less general compared with large commercial solutions. The four distinct strategies for RTL generation using LLMs present unique advantages and challenges. Prompt engineering focuses on crafting precise prompts to guide LLMs in generating RTL code, which usually involves iterative refinement with EDA tools. In contrast, LLMs trained on private datasets ( Supervised Fine-Tuning , denoted as SFT in Table 5) enhance model performance by fine-tuning them with proprietary data tailored to specific organizational needs. However, this method demands computational resources for fine-tuning, and the private circuit dataset is not open-sourced to facilitate the advancement of the community. Alternatively, LLMs trained on open datasets with code only ( Unsupervised Fine-Tuning , denoted as UFT in Table 5) leverage open-source codebases for unsupervised fine-tuning, reducing the need for labor-intensive dataset preparation. However, this approach is less effective for RTL generation, which requires strict adherence to specific instructions including design descriptions. The UFT trains LLMs to predict 36 Wenji Fang, Jing Wang, Yao Lu, Shang Liu, Yuchao Wu, Yuzhe Ma, and Zhiyao Xie LLM Interaction Feedback Loop based on Mismatch or Syntax Error Synthesis Tool Simulator Tool Syntax Error Mismatch Cases Prompt Engineering Emphasizing Structure Design Principles LLMs RTL Code Generation Prompt No errors Testcases Yes Yes Rewrite Code Description Fig. 13. Illustration of the basic flow of RTL design using prompt engineering. Related works are covered in Section 5.1. The design specification will be combined with manually designed structure analysis and design principles (in VerilogCoder [144]), and then the LLMs will take the prompt to generate corresponding RTL code. Most early works (RTLLM [12], AutoVCoder [140]) on RTL generation stop at this step.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 2237,
    "tokens": 474
  },
  {
    "text": "Verification of the model on Loihi 2 indicates close alignment with the quantized PyTorch simulation. Note: All current comparisons are performed with FP16 baselines on non-Loihi hardware. We contrast the estimated performance of the 370M MatMul-free model on Loihi 2 against transformer-based baselines running on an NVIDIA Jetson Orin Nano. We selected the Jetson Orin Nano (8GB) as our comparison platform because it represents a state-of-the-art edge AI device with 1024 CUDA cores, 32 Tensor cores, and a maximum power consumption of 15W, making it a relevant benchmark for energy-efficient AI applications. The Orin Nano is designed specifi- cally for edge deployment scenarios similar to those targeted by neuromorphic hardware, enabling a fair comparison between platforms intended for similar operational environments. Efficiency and throughput metrics for Loihi 2 are estimates based on a preliminary implementation that is not fully optimized, see Appendix A.4.1 for details. We compare the MatMul-free LLM on Loihi 2 against two similarly-sized transformer-based LLMs available on HuggingFace. As a Llama series rep- 4 Published at ICLR 2025 Workshop (SCOPE) resentative model, we use Alireo-400M (Montebovi, 2024), a 400M parameter transformer-based LLM with 24 layers and a context window of 8,192. It should be noted that the Alireo model was trained specifically on Italian text, so its performance is not included in Table 1 as it would not be representative of a competitive general-purpose transformer-based model at this parameter scale. We further use Qwen2.5-500M (Yang et al., 2024; Qwen Team, 2024), a 500M parameter transformer- based LLM with 24 layers and a context window of 32,768 whose performance is also included in Table 1. Both models run in half-precision (FP16). We did not benchmark the MatMul-free LLM or the Transformer baseline from Zhu et al. (2024) because the Jetson Orin Nano does not support Triton.",
    "source": "2503.18002v2_Neuromorphic_Principles_for_Efficient_Large_Langua.pdf",
    "length": 1944,
    "tokens": 451
  },
  {
    "text": "E. Ablation Study In Fig. 9, we present ablation studies to demonstrate the effec- tiveness of our encoder-decoder alignment and encoder quality in improving generative reasoning tasks. We conduct experiments by selectively removing the encoder alignment or using less effective encoders to assess their impact on task performance. These studies allow us to isolate and understand the contributions of different components of GenEDA to its overall performance. Effectiveness of alignment with encoders. GenEDA alignment paradigm 1 is achieved by cross-modally fine-tuning the LLMs using encoder embeddings. In this ablation study, we remove the encoder alignment and only perform supervised fine-tuning (SFT) of the LLMs using the same prompts and labels as in GenEDA. As shown in Fig. 9 (a), removing the encoder alignment significantly decreases model performance on Task 1 and Task 2 across all metrics. Notably, the GPT scores drop sharply from 0.62 to 0.32 on Task 1 and from 0.51 to 0.21 on Task 2, highlighting the effectiveness of embedding alignment. Impact of encoder quality. GenEDA alignment paradigm 2 heavily relies on the accuracy of gate functionality classification from the encoder. In this ablation study, we replace the high-quality encoder NetTAG [19] with a less effective baseline, GNN-RE [36]. This results in a significant drop in classification accuracy from 97 to 83 . Consequently, the performance of reconstructed RTL code also degrades, with syntax accuracy dropping from 96 to 91 and functional accuracy from 69 to only 40 , as shown in Fig. 9 (b). This demonstrates the critical importance of using high-quality encoders in generative tasks that involve netlists. VII. DISCUSSION A. Extending GenEDA Alignment to Other Circuit Stages and Tasks Beyond the netlist stage addressed in this work, GenEDA s encoder-decoder alignment framework can be extended to various stages in the circuit design flow. At the RTL stage, even though the same RTL functionality might be present, different circuit structures can yield significantly varying PPA characteristics. By leveraging structural RTL information captured by RTL encoders, LLMs can enable structure-aware generation, potentially generating more opti- mized RTL code with better PPA characteristics.",
    "source": "2504.09485v1_GenEDA_Unleashing_Generative_Reasoning_on_Netlist_.pdf",
    "length": 2281,
    "tokens": 490
  },
  {
    "text": "The temperature during generation is 0.6 for the distillation model and 1.0 for the RL model, and 20 responses are generated per query to estimate the score for both VerilogEval and RTLLM. 3.2 Main Results Table 1: Comparison of CodeV-R1-7B against baselines on VerilogEval v1 and RTLLM v1.1. Type Model Open source VerilogEval-Machine ( ) VerilogEval-Human ( ) RTLLM v1.1 ( ) Foundation models GPT-4o 67.7 75.5 77.2 60.1 71.4 74.5 41.7 65.9 DeepSeek-R1-671B 81.0 87.4 89.5 81.5 87.6 88.5 64.8 82.9 DeepSeek-V3-671B 80.8 87.5 88.8 68.7 79.7 82.1 60.9 74.2 QWQ-32B 71.1 84.0 87.0 63.8 78.0 81.3 50.9 70.6 DeepSeek-R1-Distill-Qwen-32B 64.7 80.5 83.6 51.3 68.1 72.2 42.1 64.3 DeepSeek-R1-Distill-Qwen-7B 5.3 16.9 24.9 1.6 6.3 10.1 0.0 0.0 Qwen2.5-Coder-32B-Instruct 66.6 76.6 79.7 47.6 58.1 61.8 47.9 67.7 Qwen2.5-Coder-7B-Instruct 60.2 77.8 82.4 31.9 46.3 50.2 32.2 48.2 IT baselines RTLCoder-Mistral-7B 62.5 72.2 76.6 36.7 45.5 49.2 - 48.3 RTLCoder-DS-6.7B 61.2 76.5 81.8 41.6 50.1 53.4 - 48.3 BetterV-CL-7B 64.2 75.4 79.1 40.9 50.0 53.3 - - BetterV-DS-6.7B 67.8 79.1 84.0 45.9 53.3 57.6 - - BetterV-CQ-7B 68.1 79.4 84.5 46.1 53.7 58.2 - - CodeV-CL-7B 78.1 86.0 88.5 45.2 59.5 63.8 39.4 62.1 CodeV-DS-6.7B 77.9 88.6 90.7 52.7 62.5 67.3 42.4 55.2 CodeV-CQ-7B 77.6 88.2 90.7 53.2 65.1 68.5 36.6 55.2 CraftRTL-CL-7B 78.1 85.5 87.8 63.1 67.8 69.7 42.6 52.9 CraftRTL-DS-6.7B 77.8 85.5 88.1 65.4 70.0 72.1 53.1 58.8 CraftRTL-SC2-15B 81.9 86.9 88.1 68.0 72.4 74.6 49.0 65.8 CodeV-R1-7B-Distill 76.2 85.6 87.0 65.7 76.8 79.7 57.4 75.8 Ours CodeV-R1-7B 76.5 84.1 85.7 69.9 79.3 81.7 72.9 86.1 We evaluate the models with , while other results are sourced from their papers.",
    "source": "2505.24183v2_CodeV-R1_Reasoning-Enhanced_Verilog_Generation.pdf",
    "length": 1663,
    "tokens": 822
  },
  {
    "text": "Memory Usage: Memory usage is assessed by analyzing the linker (.ùëöùëéùëù) file generated by the compilation toolchain. This file provides a detailed breakdown of memory allo- cation, including code (.ùë°ùëíùë•ùë°), initialized data (.ùëëùëéùë°ùëé), and uninitialized data (.ùëèùë†ùë†) segments. Flash memory usage is calculated as the sum of the code and initialized data seg- ments (.ùë°ùëíùë•ùë° .ùëëùëéùë°ùëé), while RAM usage includes both the initialized and uninitialized data segments (.ùëëùëéùë°ùëé .ùëèùë†ùë†). For the MAX78000, with its dedicated NPU-only memory, the RAM usage is computed separately for CPU and NPU. 3.4 Performance Breakdown We break down each stage of model execution and mea- sure per-stage latency and power consumption. This gran- ular analysis helps identify specific bottlenecks in the in- ference pipeline, alongside measuring overall end-to-end 2MAX78k (C) denotes use of its Cortex-M CPU, and (R) its RISC-V CPU. HX-WE2 (S) denotes model compilation with the Vela Size optimization flag, and (P) with the Vela Performance flag.",
    "source": "2503.22567v2_Benchmarking_Ultra-Low-Power_ŒºNPUs.pdf",
    "length": 1009,
    "tokens": 316
  },
  {
    "text": "The output here can be either the name or code of different abstraction levels, depending on the API definition. TABLE I LIST OF CODE RETRIEVAL APIS API name Description Output ùë†ùëíùëéùëüùëê‚Ñéùëöùëúùëëùë¢ùëôùëí Search for module Name of module ùë†ùëíùëéùëüùëê‚Ñéùëèùëôùëúùëêùëò Search for block Code of block ùë†ùëíùëéùëüùëê‚Ñéùë†ùëñùëîùëõùëéùëô Search for signal Name of signal ùë†ùëíùëéùëüùëê‚Ñéùëöùëúùëëùë¢ùëôùëíùëèùëôùëúùëêùëò Search for block in the module Code of block ùë†ùëíùëéùëüùëê‚Ñéùëöùëúùëëùë¢ùëôùëíùë†ùëñùëîùëõùëéùëô Search for signal in the module Name of signal B. HDLSearch Benchmark Figure 10 demonstrates the template instructions used in the HDLSearch benchmark refinement. Through the annotations, we want to measure how relevant would these results are to your Verilog design. - You don't have to be absolutely certain about the correctness of the code. - You might be interested in copy-pasting the code, finding a project to use or just getting some understanding about how something is implemented. - You might be searching within your project (e.g., to reuse modules, signals, or testbench constructs) or to understand how a particular digital circuit is structured. Please annotate the results according to the following scheme: - 3: Exact match. This Verilog snippet is exactly what I was looking for. I would directly integrate it into my design with minimal adaptations. - 2: Strong match. The snippet largely meets my requirements. I might use it as a backbone for my hardware module, but some modifications or additional verification might be needed. - 1: Weak match.",
    "source": "2505.15701v1_HDLxGraph_Bridging_Large_Language_Models_and_HDL_R.pdf",
    "length": 1462,
    "tokens": 494
  },
  {
    "text": "Trans- ferring between hardware of different types (e.g. CPU-to- accelerator) poses unique challenges: Heterogeneous program configuration spaces. The pro- gram configurations for emerging sparse accelerators, which serve as the input feature space for cost models, can dif- fer significantly from those of general-purpose hardware. For example, emerging sparse accelerators have software- managed buffers instead of hardware-managed caches and specialized, rather than general-purpose, pipelines. This causes a disparity in program configuration spaces for general-purpose hardware and emerging accelerators, mak- ing it challenging to naively apply transfer learning. Exist- ing heterogeneous transfer learning techniques (Liang et al., 2019), such as feature augmentation (Daum e III, 2009; Duan et al., 2012), can be a viable approach. However, these tech- niques often produce feature representations that are too sparse for the cost model to effectively learn, specifically when accommodating a diverse set of program configura- tion across different hardware platforms. Figure 2 shows the results of applying popular heterogeneous transfer learning techniques feature augmentation (FA) and feature map- ping (FM) to a learned cost model, WACO (Won et al., 2023). Even when using data samples from 1000 matrices for fine-tuning on the SPADE accelerator, the best configura- tions found under WACO FA and WACO FM are far from optimal. Therefore, we need better techniques to handle the heterogeneity of program configurations across hardware. High sample efficiency requirement. Existing transfer learning solutions for learned cost models operating in ho- mogeneous feature spaces typically require at least 25 of the original dataset used in a non-transfer learning setup to achieve competitive performance on the target hardware plat- form (Sasaki et al., 2022). The target dataset requirement for these solutions can further increase due to the heterogeneous input feature spaces between general-purpose hardware and emerging accelerators. This makes it infeasible to adopt ex- isting solutions in their current form for accelerators in early design stages. Therefore, we need data-frugal techniques. COGNATE. In this paper, we present COGNATE, a novel framework for developing learned cost models that enable effective knowledge transfer (Figure 1) overcoming these challenges.",
    "source": "2506.00424v2_COGNATE_Acceleration_of_Sparse_Tensor_Programs_on_.pdf",
    "length": 2387,
    "tokens": 471
  },
  {
    "text": "For ex- ample, in Figure 1c, the Connectivity of ùê∂‚Ñéùëíùëêùëò_ùëÖùëíùë†ùëíùëüùë£ùëéùë°ùëñùëúùëõ is 2 5 since it has two descendants and there is a total of five functions. Bringing Connectivity into the weight estimation process helps Kraken assign a higher weight to critical func- tions, in turn, ensuring that more containers are assigned to them, resulting in improved response times for the functions themselves, as well as their descendants. Commonality: As described in Section 2, in addition to cold start spillovers, incorrect probability estimations may arise due to variability in workflow activation patterns. This may be due to change in user behavior manifesting itself as variable function input patterns. Such errors can lead to sub- optimal container allocation to DAG stages in proportion to the wrongly-calculated function weights. To cope with this, we introduce a parameter called Commonality, which is defined as the fraction of number of unique paths that the function can be a part of with respect to the total number of unique paths. This is how the procedure ùê∂ùëúùëöùëöcalculates Commonality in Algorithm 1. For example, in Figure 1a, the Commonality of the function ùê∂ùëúùëöùëùùëúùë†ùëí_ùëÉùëúùë†ùë°in the Social Network application is given by the fraction 4 7 as it is present in four out of the seven possible paths in the DAG. Using Commonality in the weight estimation process allows Kraken to tolerate function probability miscalculations by assigning higher weights to those functions that are statistically more likely to experience rise in usage because of their presence in a larger number of workflows. Note that we deal with the possibility of container overprovisioning due to the in- creased function weights by allowing both Connectivity and Commonality to be capped at a certain value. 4.1.2 Proactive Container Provisioning:Once function weights are assigned by considering the above factors, they are employed in estimating the number of containers needed per DAG stage (Estimate_Containers in Algorithm 1).",
    "source": "kraken.pdf",
    "length": 1996,
    "tokens": 484
  },
  {
    "text": "Lastly, lack of complex logical capability: Traditional LLMs struggle to handle the intricate interdependencies between components in hardware design, often failing to maintain consistency across module interfaces, state machines, and timing constraints. Without systematic reasoning about component relationships, mod- els produce circuits with logical inconsistencies or incomplete implementations that meet superficial requirements but fail under comprehensive verification. Recent advances in reasoning and reinforcement learning (RL) have introduced promising approaches to overcome these challenges. Reasoning-augmented models, such as those leveraging chain-of- thought prompting or iterative refinement, have demonstrated the ability to follow multi-step logical patterns, making them particularly suitable for hardware description languages like Verilog that require strict structural correctness and functional dependencies. These reasoning mechanisms help LLMs better understand circuit intent and adhere to design constraints, and can better ensure the alignment between natural language and result. Methods such as Guided Reward Proximal Optimization (GRPO)[16] combine the strengths of SFT with reward-driven RL, enabling models to learn effectively even with minimal data and explicit feedback. By employing RL-based strategies, LLMs are trained not merely on predicting the next token but on achieving specific, meaningful outcomes, thus improving their logical reasoning, alignment, and self-checking capabilities. Our Proposed Framework. To address the challenges in RTL generation with LLMs, we propose a novel framework, VeriReason, combining supervised fine-tuning (SFT) and GRPO reinforcement learning, specifically tailored for Verilog RTL generation with a specially designed dataset featuring reasoning steps and testbenches. Our approach systematically tackles four critical limitations that hinder existing LLM-based hardware design methods: data scarcity in domain-specific code, natural language-code alignment issues, lack of self-checking behavior, and insufficient complex logical capabilities. Each of these challenges requires specialized techniques that we incorporate into the VeriReason framework, as detailed in the following sections. Data Scarcity in Domain-Specific Code: We introduce a reasoning-distillation and testbench- generation pipeline to augment existing prompt code pairs with high-quality testbenches and human- style reasoning steps, producing a high-quality dataset. Furthermore, we demonstrate that even with as few as 20 annotated examples from the VeriReason dataset, GRPO yields substantial performance gains, dramatically lowering the bar for required training data.",
    "source": "2505.11849v1_VeriReason_Reinforcement_Learning_with_Testbench_F.pdf",
    "length": 2726,
    "tokens": 476
  },
  {
    "text": "Finally, 7 lists limitations and future work, followed by 8 concluding remarks. 2 Table 1: Comparison of Domain Characteristics across Different Datasets Domain ComputeEval Rodinia SHOC Poly Babel Ours Characteristics NVIDIA[19] Bench[20] [21] Bench[22] Stream[23] CUDA (source) SASS (assembly) RDNA3 (assembly) OpenCL (source) 2 Related Works In this section, we describe prior work in GPU translation efforts ( 2.1), assembly-level transpilation ( 2.2), and related benchmarks (and their shortcomings) in the space ( 2.3) 2.1 Translating from Nvidia to AMD The fragmentation of GPU software ecosystems has driven the need for robust CUDA-to-HIP translation tools. HIPIFY [14] statically converts CUDA source code into HIP, enabling ROCm compatibility via direct syntax substitution. Operating at a lower abstraction, CuPBoP-AMD [15] translates NVVM IR to HIP-compatible LLVM IR using the LLVM toolchain [16, 17], offering more flexible intermediate-level interoperability. Earlier, GPU Ocelot [18] explored dynamic binary translation, recompiling CUDA to AMD x86 ISAs at runtime. Although innovative, it was limited by poor scalability and high overhead, making it impractical for modern GPU workloads. All these tools have lacked consistent updates to keep up with CUDA advances, suffer from usability issues, and operate only at the source level. More recently, ZLUDA [6] introduced a runtime system for executing unmodified CUDA binaries on AMD GPUs without source access by intercepting CUDA APIs and translating PTX SASS into AMD-compatible code via LLVM. Originally targeting Intel, it now supports AMD RDNA3 through runtime patching. ZLUDA operates at the LLVM IR level rather than the hardware assembly. While a reasonable level in the stack to target, ZLUDA would not be able to benefit from low-level, backend Nvidia optimizations (operating below the PTX level), and is limited to the AMD stacks backend optimizations. In our work, we target assembly-to-assembly translation, in an effort to leverage hardware-specific optimizations below the intermediate representation (IR) level, that may be missing altogether in the corresponding AMD codebase.",
    "source": "2505.16968v3_CASS_Nvidia_to_AMD_Transpilation_with_Data_Models_.pdf",
    "length": 2161,
    "tokens": 480
  },
  {
    "text": "F. Checker Agent The Checker Agent ensures the accuracy of the generated Verilog module and its corresponding testbench by conducting compilation and simulation tests. Serving as a verification layer, it guarantees that the output complies with Verilog syntax rules and meets functional expectations. This agent leverages compilation and simulation tools, including Icarus Verilog [22] and Verilator [23], to systematically validate the correctness of the generated HDL code. Upon receiving the Verilog module and testbench, the Checker Agent conducts a structured validation process to ensure the correct functionality of the design. It first compiles the Verilog module and testbench, checking for syntax er- rors, missing dependencies, or structural inconsistencies. Once compilation is successful, the agent performs a simulation to verify functional correctness, ensuring that the module operates as expected across different test scenarios. During the simulation, it monitors signal behavior and assertions using display, or assertion-based verification methods to track discrepancies in output. If the Checker Agent encounters issues such as compi- lation errors, logical inconsistencies, or incorrect simulation results it notifies the Supervisor Agent with such informa- tion, prompting a refinement loop where the Verilog Code Generator Agent or Verilog Testbench Generator Agent mod- ifies the output to resolve the detected errors.",
    "source": "2503.16514v3_VeriMind_Agentic_LLM_for_Automated_Verilog_Generat.pdf",
    "length": 1443,
    "tokens": 268
  },
  {
    "text": "III, there exist signiÔ¨Åcant similarities between two successive frames (e.g., for more than 95 of the pixels between two adjacent frames) when considering pixel difference as the similarity metric. In the following two subsections, targeting inference-based video applications, we present two novel schemes that take advantage of this similarity: frame-level pruning and region-level pruning. A. Frame-Level Pruning 1) Shortcomings of Pixel-by-Pixel Comparison: We believe a solution based on exact pixel-by-pixel matching would be too strict and would not be suitable for DNN-based applications. This can be illustrated with a simple use-case scenario. Con- sider an object detection scenario where a multi-color LED bulb on a Christmas tree identiÔ¨Åed as an object of interest, glowing red in Framei, changes to blue in Framei 1. In such a case, as the pixel values of the identiÔ¨Åed object have changed, Framei 1 cannot reuse the result (LED bulb) from Framei even though it should ideally be able to do so in an object detection application. Hence, rather than relying on low-level raw pixel values, most DNN applications leverage high-level features, where new events or motions make more sense to employ in determining whether we can reuse the results from previous 1076 Authorized licensed use limited to: Penn State University. Downloaded on June 29,2023 at 18:36:04 UTC from IEEE Xplore. Restrictions apply. frame (or skip the current frame). We next discuss the motion vector concept, which is used in our approach. 2) Motion Vectors: The motion vector (MV), which is built upon pixel differences, can be a good candidate to capture the reusability in video analytics. A MV is a 2-D vector that provides an offset from the coordinates in the decoded frame to the coordinates in a reference frame [34], which can be directly obtained from the codec [35] without any post-processing. As opposed to the software-based optical Ô¨Çow solution widely used in the computer vision domain [36], collecting the MV from the codec hardware is quite light-weight. In fact, our proÔ¨Åling indicates that only tens of Œºs are needed to generate the MVs for one frame (negligible compared to milliseconds or even seconds that DNN inference takes).",
    "source": "PCframeSim.pdf",
    "length": 2234,
    "tokens": 500
  },
  {
    "text": "Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhewen Hao, Zhihong Shao, Zhiniu Wen, Zhipeng Xu, Zhongyu Zhang, Zhuoshu Li, Zihan Wang, Zihui Gu, Zilin Li, and Ziwei Xie. 2024. DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model. arXiv preprint arXiv:2405.04434 (May 2024).",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 333,
    "tokens": 127
  },
  {
    "text": "A more formal theoretical analysis is provided in the Appendix. Table 1(b) shows the accuracy of 4 two-level quantization against naive direct quantization, using real Q, K, V from layers of CogVideoX. Results indicate that two-level quantization boosts the accuracy. 3.3 Implementation and Optimization on Hardware Permutation for K. Unlike FP16, the FP32 accumulator s memory layout in FP4 MatMul [9] differs from its operand A s register layout (shown in Fig. 20 and 19). Performing thread shuffles to match operand A s layout would degrade kernel performance. Our solution transforms the accumulator layout (Fig. 21) by permuting the P tile s columns. To maintain correct MatMul, we correspondingly rearrange K s columns, which can be fused with the quantization kernel. Reuse shuffle. The in-kernel micro-scaling quantization of eP requires finding the max value of 16 consecutive row elements. However, as shown in Fig. 21, these 16 elements are distributed across four threads, necessitating intra-thread max reduction followed by inter-thread shuffling, significantly slowing down the kernel. We optimize this by fusing quantization with online softmax, which also computes row-wise maxima. First, we compute the max over 16 elements in S and reuse it in the subsequent softmax max-reduction. This fusion reduces redundant shuffles and max operations by 50 , yielding about 10 whole kernel speedup. Producer warp epilogue. In conventional warp-specialized kernels, consumer warps typically handle both MatMul and store operations while producers merely load inputs, with ping-pong scheduling between consumers enabling stage overlap [10]. However, register constraints make this approach infeasible for our FP4 attention kernel. Instead, we implement ping-pong scheduling between producer warps: while one producer loads inputs for the next MatMul operation, another concurrently stores outputs to global memory, with consumer warps solely responsible for transferring MatMul results from registers to shared memory. This novel design overlaps MatMul and global memory stores within register constraints, boosting throughput. 4 INT8 Attention for Training Low-bit quantization attention works, such as FlashAttention3 and SageAttention, are only for inference.",
    "source": "2505.11594v1_SageAttention3_Microscaling_FP4_Attention_for_Infe.pdf",
    "length": 2268,
    "tokens": 476
  },
  {
    "text": "x f32 , out: mref ? x ? x f32 ){ 3 Access: Iterate over segments in a batch 4 slc.for(stream s_b from 0 to num_batches){ 5 stream s_beg slc.mem_str(offs[s_b]); 6 stream s_end slc.mem_str(offs[s_b 1]); 7 Access: Iterate over embeddings in a segment 8 slc.for(stream s_ptr from s_beg to s_end){ 9 stream s_idx slc.mem_str(idxs[s_ptr]); 10 Access: Iterate over embedding vector elements 11 slcv.for vlen ((stream s_e, stream msk) from 0 to emb_len){ 12 stream s_val slcv.mem_str vlen (vals[s_idx,s_e], msk); 13 Execute: Reduce embedding vectors 14 slcv.callback{ 15 index b slc.to_val(s_b); 16 index e slcv.to_val(s_e)[0]; 17 vec vlen x f32 val slcv.to_val vlen (s_val); 18 vec vlen x f32 acc vload vlen (out[b,e]); 19 vstore vlen (acc val, out[b,e], acc); }}}}} (b) Vectorized code. 1 void sls(idxs: mref ? x index , offs: mref ? x index , 2 vals: mref ? x f32 , out: mref ? x ?",
    "source": "2504.09870v1_Ember_A_Compiler_for_Efficient_Embedding_Operation.pdf",
    "length": 876,
    "tokens": 348
  },
  {
    "text": "Syst. 22, 2, Article 33 (Jan. 2023), 21 pages. doi:10.1145 3555807 [25] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De- vendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, and William El Sayed. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825 (2023). [26] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L√©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th√©ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, and William El Sayed. 2024. Mixtral of Experts. [27] Qing Jin, Jian Ren, Richard Zhuang, Sumant Hanumante, Zhengang Li, Zhiyu Chen, Yanzhi Wang, Kaiyuan Yang, and Sergey Tulyakov. 2022. F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization. In International Conference on Learning Representations. [28] Norman P. Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing, Brian Towles, Cliff Young, Xiang Zhou, Zongwei Zhou, and David Patterson. 2023. TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings. arXiv preprint arXiv:2304.01433 (2023).",
    "source": "2503.18599v2_Oaken_Fast_and_Efficient_LLM_Serving_with_Online-O.pdf",
    "length": 1569,
    "tokens": 498
  },
  {
    "text": "Millimeter-wave power amplifiers, passives, and EM structures are physically placed utilizing a deep-learning-based inverse design Emerging ML-AI Techniques for Analog and RF EDA , framework [60]. A convolutional neural network (CNN) is utilized to predict scattering parameters for 2 D planar structures, while a genetic algorithm optimizes the spatial configuration [60]. The generated power amplifier achieves a power added efficiency (PAE) of 16 -24.7 across a 3-dB bandwidth of 30-94 GHz. The approach expands the design space beyond pre-defined templates, achieving efficiency performance in less time. 3.6 Routing Strategies in Analog RF Design Automation Routing significantly impacts the performance and manufacturabil- ity of a circuit. While the core objective of routing algorithms is to find the shortest physical path between two connected nodes of a graph, routing for analog and RF circuits must address additional performance-critical constraints including symmetry, minimiza- tion of parasitic impedances, and noise isolation, while adhering to geometric and electrical design rules. Analog routing is, therefore, a complex optimization problem despite the smaller number of devices. In addition, for fabrication technology nodes below 22 nm, the complexity of design rules has increased nearly tenfold, surpass- ing 10,000 rules [61]. The increase in circuit and design constraints requires novel methodologies to efficiently achieve optimal routing of analog and RF circuits. Analog routing techniques share similarities with digital routing, utilizing maze routing, A , integer linear programming (ILP), SAT- based, and ML-guided algorithms. Common-centroid (CC) rout- ing [62] is introduced to mitigate mismatch due to systematic varia- tions. Circuit layouts that utilize CC routing ensure that systematic variations across a linear gradient are mitigated by symmetrically placed elements. However, CC layouts often introduce complex routing challenges and increased parasitic effects due to the intri- cate routing paths required to interconnect symmetric elements. BAG2 [46] integrates modularity and process portability into the routing strategy, using separate engines to automate wire place- ment, spacing, and layer selection. By isolating process-specific details with the developed parametric workflow, BAG2 adapts de- signs across technology nodes while balancing automated layout generation with designer input. Similarly, LAYGO [63] utilizes a grid-based and template-driven routing algorithm developed for sub-28 nm technology nodes.",
    "source": "2506.00007v1_Emerging_ML-AI_Techniques_for_Analog_and_RF_EDA.pdf",
    "length": 2568,
    "tokens": 498
  },
  {
    "text": "The intel programmable and integrated unified memory architecture graph analytics processor. IEEE Micro, 43(5):78 87, 2023. Adams, A., Ma, K., Anderson, L., Baghdadi, R., Li, T.- M., Gharbi, M., Steiner, B., Johnson, S., Fatahalian, K., Durand, F., et al. Learning to optimize halide with tree search and random programs. ACM Transactions on Graphics (TOG), 38(4):1 12, 2019. Baghdadi, R., Merouani, M., Leghettas, M.-H., Abdous, K., Arbaoui, T., Benatchba, K., et al. A deep learning based cost model for automatic code optimization. Proceedings of Machine Learning and Systems, 3:181 193, 2021. Beltagy, I., Peters, M. E., and Cohan, A. Long- former: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Bozinovski, S. Reminder of the first paper on transfer learn- ing in neural networks, 1976. Informatica, 44(3), 2020. Chen, T. and Guestrin, C. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd inter- national conference on knowledge discovery and data mining, pp. 785 794, 2016. Chen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E., Shen, H., Cowan, M., Wang, L., Hu, Y., Ceze, L., et al. {TVM}: An automated {End-to-End} optimizing compiler for deep learning. In 13th USENIX Symposium on Operating Sys- tems Design and Implementation (OSDI 18), pp. 578 594, 2018a. Chen, T., Zheng, L., Yan, E., Jiang, Z., Moreau, T., Ceze, L., Guestrin, C., and Krishnamurthy, A. Learning to opti- mize tensor programs. Advances in Neural Information Processing Systems, 31, 2018b. Child, R., Gray, S., Radford, A., and Sutskever, I. Gen- erating long sequences with sparse transformers.",
    "source": "2506.00424v2_COGNATE_Acceleration_of_Sparse_Tensor_Programs_on_.pdf",
    "length": 1625,
    "tokens": 494
  },
  {
    "text": "Dynamic Beam Width Tuning for Energy-Efficient Recurrent Neural Networks. In Proceedings of the 2019 on Great Lakes Symposium on VLSI (Tysons Corner, VA, USA) (GLSVLSI 19). Association for Computing Machinery, New York, NY, USA, 69 74. doi:10.1145 3299874.3317974 [56] Daniele Jahier Pagliari, Francesco Panini, Enrico Macii, and Massimo Poncino. 2019. Dynamic Beam Width Tuning for Energy-Efficient Recurrent Neural Networks. In Proceedings of the 2019 on Great Lakes Symposium on VLSI (Tysons Corner, VA, USA) (GLSVLSI 19). Association for Computing Machinery, New York, NY, USA, 69 74. doi:10.1145 3299874.3317974 [57] Seunghyeok Jeon, Yonghun Choi, Yeonwoo Cho, and Hojung Cha. 2023. HarvNet: Resource-Optimized Operation of Multi-Exit Deep Neural Networks on Energy Harvesting Devices. In Proceedings of the 21st Annual International Conference on Mobile Systems, Applications and Services (Helsinki, Finland) (MobiSys 23). Association for Computing Machinery, New York, NY, USA, 42 55. doi:10. 1145 3581791.3596845 [58] Heinrich Jiang, Been Kim, Melody Y. Guan, and Maya Gupta. 2018. To Trust Or Not To Trust A Classifier. arXiv:1805.11783 [stat.ML] [59] Petar Jokic, Stephane Emery, and Luca Benini. 2021. Battery-Less Face Recognition at the Extreme Edge. In 2021 19th IEEE International New Circuits and Systems Conference (NEWCAS). 1 4. doi:10.1109 NEWCAS50681.2021.9462787 [60] Beomseok Kang, Anni Lu, Yun Long, Daehyun Kim, Shimeng Yu, and Saibal Mukhopadhyay. 2021. Genetic Algorithm-Based Energy- Aware CNN Quantization for Processing-In-Memory Architecture.",
    "source": "2505.12523v1_Energy-Aware_Deep_Learning_on_Resource-Constrained.pdf",
    "length": 1572,
    "tokens": 469
  },
  {
    "text": "One promising approach to improving tree-based model inference is leveraging in-memory com- puting (IMC), which integrates data storage and processing within the same location, thereby eliminating latency and energy costs associated with data access and transfer. While conventional SRAM-based IMC accelerators can effectively accelerate model inference, they come at the cost of high energy consumption and extensive area overhead [14], making 2 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS it impractical for resource-constrained environments. Recently, there has been significant interest in developing emerging non-volatile-memory-based (NVM-based) IMC ac- celerators [15], [16], [17], as the characteristics of emerg- ing NVM (e.g., high density, low power consumption, and low cost) make it well-suited for resource-constrained envi- ronments. Among potential candidates, non-volatile ternary content-addressable memory (nvTCAM) [18], [19] exhibits the best fit for accelerating tree-based models due to its capability to perform sequence matching with extremely high parallelism and its high reliability. Since every root-to-leaf path within tree-based models can be encoded into a binary sequence, nvTCAM can traverse all the paths in one shot, enabling unprecedented acceleration for model inference. Nevertheless, to support in-memory search, data must be organized in a spe- cialized format (refer to Section II-B for further explanation). Although nvTCAM effectively accelerates tree-based model inference in a cost- and energy-efficient manner, a considerable portion of memory cells is allocated for format alignment rather than storing actual model data. This leads to excessive memory consumption with substantial redundancy, which is highly inefficient and requires further optimization. To achieve resource-efficient acceleration, the enormous CAM capacity requirement with considerable redundancy is the major obstacle that must be addressed before practical implementation. Since model complexity is highly correlated with memory consumption, pruning models can significantly mitigate this issue. Modern tree-based models often employ pruning algorithms, such as limiting maximum depth or min- imum impurity decrease [20], in order to reduce overfitting and simplify model structure.",
    "source": "2506.05994v1_RETENTION_Resource-Efficient_Tree-Based_Ensemble_M.pdf",
    "length": 2336,
    "tokens": 475
  },
  {
    "text": "The accelerator configurations are given in Table 1. The proposed base B accelerator is the most resource efficient for LUTs and FFs and operates at the highest frequency. Additionally, on the same eFPGA chip, the Single Core (S) uses 2.5x fewer LUTs and 3.38x fewer FFs than MATADOR for MNIST. BRAMs for B, S and M are over-provisioned for more tunability later. The models were compressed into 16-bit instructions and used to program the proposed accelerator configurations. Fig. 9 shows the energy and latency of the configurations compared to MATADOR. The num- bers in red indicate the speed-up and energy reduction compared to a software implementation of the compressed Include encoded inference approach running on an STM32Disco MCU (RDRS) as presented by [15] claiming upto 5700x speedup compared with embedded BNNs - MCU comparisons will be discussed in more detail in Question 2. All B, S, M configurations are within one order of magnitude of the MATADOR results; in the case of CIFAR 2, B is the most energy efficient. The key point to note is that MATA- DOR is fixed to these latencies and energies; however, a real-time recalibration to a smaller model would improve the B, S, M results without resynthesis. Ultimately, the key point becomes application, if the trained model is always a good representation of the data it will infer, then MATADOR is a better option. If the data is subject to drift, or there is need for personalization or sensor reading degra- dation or environmental fluctuation, and there is an opportunity to use a system like Fig. 8, then the proposed accelerators are more viable. They will adapt in situ during deployment. Addressing Question 2: The flexibility to change model size and architecture at runtime makes the accelerator comparable to small RISC processors. Question 1 already demonstrated the energy and latency advantages against an Arm-based STM32Disco. This section explores how it compares to another even smaller, cheaper low-power MCU - the Espressif ESP32. This is done to understand whether the eFPGA is the best platform to be used in a system like Fig. 8. Once again, the ESP32 runs the same compressed model inference, but as a software task on the processor.",
    "source": "2502.07823v1_Runtime_Tunable_Tsetlin_Machines_for_Edge_Inferenc.pdf",
    "length": 2222,
    "tokens": 497
  },
  {
    "text": "Guess sketch: Language model guided transpilation. arXiv preprint arXiv:2309.14396, 2023. [26] Yifan Sun, Xiang Gong, Amir Kavyan Ziabari, Leiming Yu, Xiangyu Li, Saoni Mukherjee, Carter McCardwell, Alejandro Villegas, and David Kaeli. Hetero-mark, a benchmark suite for cpu-gpu collaborative computing. In 2016 IEEE International Symposium on Workload Characterization (IISWC), pages 1 10. IEEE, 2016. [27] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Noua- mane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024. [28] NVIDIA Corporation. CUDA Binary Utilities, 2025. cuda-binary-utilities index.html. [29] The Khronos Group. Opencl guide. 2025. Accessed: 2025-05-14. [30] Khronos Group. clbuildprogram - opencl 3.0 reference pages. org OpenCL sdk 3.0 docs man html clBuildProgram.html, 2020. Accessed: 2025- 05-14. [31] Maja Popovi c. chrf: character n-gram f-score for automatic mt evaluation. In Proceedings of the tenth workshop on statistical machine translation, pages 392 395, 2015. [32] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jia- jun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [33] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters.",
    "source": "2505.16968v3_CASS_Nvidia_to_AMD_Transpilation_with_Data_Models_.pdf",
    "length": 1507,
    "tokens": 473
  },
  {
    "text": "In Figure 2, each square in the MatMul Outputs represents a partial TCONV output, and the values inside these squares represent the output index of the final TCONV outputs (shown on the right); this output mapping is a function of S and the input dimensions (Ih, Iw). For example, all the 0 index partial outputs are summed and stored in the 0 index of the final output feature maps. Additionally, calculating the index map of the (light and dark) blue squares in the MatMul Outputs, we derive the compute mapping for the given TCONV problem, that is, the index map of partial outputs that are not dropped out via col2IM. Therefore, our first key insight is that by using the output mapping and the compute mapping, we can solve the IOM inefficiencies and enable an efficient accelerator architecture that can: (i) Skip ineffectual computations of the dropped partial outputs (gray squares); (ii) Remove the need for storing Algorithm 1: Tiled MM2IM Data: Initialize filter step, i end row 1 foreach c 0 to Oc by filter step do 2 SendWeightFilters(c, filter step) 3 starting 0 4 foreach h 0 to Oh do 5 rows to send i end row[h] 1 starting 6 if i end row[h] starting 1 then 7 SendInputRows(starting, rows to send) 8 ComputeOutRow(h, c, filter step) 9 StoreOutRow(h, c, filter step) 10 starting i end row[h] 1 partial sums in temporary memory and to be summed later; (iii) Map the outputs of the MatMul operation directly to the final output values. B. Acceleration Dataflow for Resource-Constrained Devices Data transfer between off-chip and on-chip memory can become a bottleneck, especially on resource-constrained edge devices. Hence, we co-designed Tiled MM2IM, a specialized tiling strategy for MM2IM that enables weight and output stationary dataflow minimizing data transfer redundancy, high- lighted in Algorithm 1. Tiled MM2IM loads filter step filters and produces the corresponding output channels within the outer loop. The filter step is determined by the number of processing modules within our MM2IM architecture (dis- cussed in Section IV).",
    "source": "2507.07683v1_Accelerating_Transposed_Convolutions_on_FPGA-based.pdf",
    "length": 2055,
    "tokens": 483
  },
  {
    "text": "[36] Koksal Mus, Yarkƒ±n Dor√∂z, M Caner Tol, Kristi Rah- man, and Berk Sunar. Jolt: Recovering tls signing keys via rowhammer faults. In 2023 IEEE Symposium on Se- curity and Privacy (SP), pages 1719 1736. IEEE, 2023. [37] Oleksii Oleksenko, Marco Guarnieri, Boris K√∂pf, and Mark Silberstein. Hide and seek with spectres: Effi- cient discovery of speculative information leaks with random testing. In 2023 IEEE Symposium on Security and Privacy (SP), pages 1737 1752. IEEE, 2023. [38] Oleksii Oleksenko, Bohdan Trach, Mark Silberstein, and Christof Fetzer. Specfuzz: Bringing spectre- type vulnerabilities to the surface. arXiv preprint arXiv:1905.10311, 2019. [39] Dag Arne Osvik, Adi Shamir, and Eran Tromer. Cache attacks and countermeasures: the case of aes. In Topics in Cryptology CT-RSA 2006: The Cryptogra- phers Track at the RSA Conference 2006, San Jose, CA, USA, February 13-17, 2005. Proceedings, pages 1 20. Springer, 2006. [40] Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, and Brendan Dolan-Gavitt. Examining Zero-Shot Vulnerability Repair with Large Language Models. In 2023 IEEE Symposium on Security and Privacy (SP). IEEE, 2023. 15 [41] Antoon Purnal, Furkan Turan, and Ingrid Verbauwhede. Prime scope: Overcoming the observer effect for high- precision cache contention attacks. In Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security, pages 2906 2920, 2021. [42] Hany Ragab, Enrico Barberis, Herbert Bos, and Cris- tiano Giuffrida. Rage against the machine clear: A sys- tematic analysis of machine clears and their implications for transient execution attacks. In 30th USENIX Security Symposium (USENIX Security 21), pages 1451 1468, 2021. [43] John Schulman.",
    "source": "2502.14307v1_ŒºRL_Discovering_Transient_Execution_Vulnerabilitie.pdf",
    "length": 1726,
    "tokens": 491
  },
  {
    "text": "Furthermore, optimally assigning precisions at a fine-grained granularity for LLMs calls for a scalable, low-complexity solution to handle massive parameter spaces. LowRA meets the need through a hierarchical ILP(Integer Linear Programming)-based precision assigner for performing fine-grained mixed-precision. Moreover, LowRA provides a weighted Lloyd-Max [29, 31] formulation of mapping threshold search for groupwise normalization, and achieves strong practical performance through its efficient solution. We conduct extensive evaluation of LowRA across 4 choices of widely used base LLMs and 4 choices of natural language applications, and compare LowRA against state-of-the-art baselines. Evaluation results demonstrate that LowRA: (1) achieves a superior performance-precision trade-off above 2 bits (per parameter) compared to baselines, and is the first method to enable accurate, efficient LoRA fine-tuning below 2 bits, (2) enables substantial memory footprint reduction in fine-tuning, and (3) incurs minimal additional overhead even with newly added components. In summary, we make the following contributions: Identifying Key Limitations: We identify three core limitations in existing quantized LoRA approaches, highlighting opportunities to exploit fine-grained precision assignment and mappings thresholds. Design and Implementation of LowRA: We introduce LowRA, an accurate, end-to-end framework that applies fine-grained quantization to LoRA fine-tuning of LLMs. We detail its key design choices including a mappings thresholds learner, precision assigner, and practical programming primitives ensuring both efficiency and high performance. Better Performance-Precision Trade-Off: Comprehensive evaluations show that LowRA outperforms baselines in performance-precision trade-off, enabling an average 0.86-bit reduction per parameter without sacrificing performance. Memory Footprint Reduction: LowRA cuts memory usage by 30 50 during fine-tuning and deployment with minimal performance loss. Moreover, it enables fine-tuning and deploying LLMs in ultra-resource-constrained settings at as low as 1.15 bits. Open-Source Framework: We will open-source our framework and artifacts to spur further research in ultra-low-bit LoRA fine-tuning. 2 This paper is organized as follows: Section 2 introduces LoRA fine-tuning, quantization for LoRA, and three key limitations of existing quantized LoRA methods.",
    "source": "2502.08141v1_LowRA_Accurate_and_Efficient_LoRA_Fine-Tuning_of_L.pdf",
    "length": 2418,
    "tokens": 498
  },
  {
    "text": "The target vector is the effective net capacitance Ceff. The training flow is also divided into 2 stages. We first use feature vectors and labels of net nodes to train and validate the classifier. Once the classifier is obtained, according to the classification result, net nodes in the training set are grouped into different sub-sets and are used to train their own regressors with the corresponding targets Ceff, respectively. The test set is invisible during the whole training process. In the 1st stage, we adopt Focal Loss [8] with a weight factor ùõº [0, 1] which applies a modulating term to the cross-entropy loss in order to focus learning on hard examples and down-weight the numerous easy negatives. L ùë£ VNET ùêπùêøùë£ ùë£ VNET ùõºùë°(1 ùëùùë£(ùë°))ùõælog(ùëùùë£(ùë°)), (3) where ùõæ 0 is a tunable focusing parameter, ùëùùë£(ùë°) [0, 1] is the model s estimated probability for the net node ùë£ VNET to be in class ùë° T. Here we set ùõºto be ùõºùë° 1 ùëìùë° , (4) where ùëìùë°is the proportion of class ùë°in the entire data samples. It can also be set to other values according to the classification results to increase the contribution of categories with fewer data to the total loss function. In the 2nd stage, we use the mean value of the squared percentage error as the loss function in the regression training, i.e. Lùë° 1 ùëÅùë° ùë£ VNET ùë° (ùë¶ùë£ ÀÜùë¶ùë£ ùë¶ùë£ ) 2 , (5) where symbol ùë¶denotes the target value and ÀÜùë¶the predicted value. ùëÅùë°is the number of nets with predicted label ùë°. 5 OVERALL WORKFLOW The workflow of the proposed method is illustrated in Figure 4.",
    "source": "2507.06549v1_Deep-Learning-Based_Pre-Layout_Parasitic_Capacitan.pdf",
    "length": 1514,
    "tokens": 490
  },
  {
    "text": "The final attention vector for one query vector q is computed by dividing the final output vector by the corresponding sum-of-exponents ‚ÑìN. B. Attention Hardware Accelerators State-of-the-Art The traditional attention mechanism faces three primary per- formance bottlenecks: high memory bandwidth demands due to the retrieval of query, key, and value matrices, particularly Algorithm 1 Attention with lazy softmax division 1: for each query q do 2: for i 1 : N do 3: si dot( q, ki) 4: mi max(mi 1, si) 5: end for 6: ‚Ñì0 0 7: for i 1 : N do 8: ‚Ñìi ‚Ñìi 1 esi mN 9: oi oi 1 esi mN vi 10: end for 11: attn( q, K, V ) oN ‚ÑìN 12: end for with long sequences; substantial computational overhead from the softmax operation across the full sequence; and limited parallelism due to reduction operations within softmax, such as maximum value determination and exponential summation. To mitigate memory traffic, attention accelerators employ a strategy of keeping key and value vectors static in local SRAM, while streaming query vectors to calculate attention scores [8], [17], [18]. This approach minimizes memory fetches by only reloading query vectors for each key and value vector. However, this method s effectiveness diminishes with increasing sequence lengths, as key, value, and score matrices are replaced into slower DRAM, leading to performance degradation. This issue is particularly relevant in modern NLP tasks that often involve extensive sequences [4]. To reduce the memory access overhead other approaches focus on in- memory computation of attention [19]. To decouple computational resources and local memory from the sequence length, several designs [9], [10] accumulate partial softmax results for each column of the attention scores. This method maintains accuracy while reducing the need to buffer and compute softmax results for the entire sequence at once, avoiding memory spills. Further optimization techniques involve computation skip- ping based on token similarity to reduce latency and power [20], [21], as well as quantization to minimize the cost of data transfers and enhance accelerator efficiency [22]. III.",
    "source": "2505.14314v2_Low-Cost_FlashAttention_with_Fused_Exponential_and.pdf",
    "length": 2127,
    "tokens": 467
  },
  {
    "text": "However, the scale and complexity of neural data pose significant chal- lenges in processing, storage, and interpretation, necessitating innovative approaches in both hardware and software. Machine learning (ML) has emerged as a transformative tool for data analysis, enabling the execution of sophisticated tasks that were previously unattainable. ML algorithms excel at identifying patterns in complex datasets, making them invaluable for applications such as brain-computer interfaces (BCIs) aimed at motor and communication restoration [7] [10]. Additionally, ML facilitates therapeutic applications, including mental state recognition, migraine detection, and seizure prediction [11] [15]. Moreover, brain-triggered neuro- modulation systems leverage ML to adaptively modulate brain These authors contributed equally to this work. activity in response to disease-specific neuromarkers, offering personalized and precise therapeutic interventions [5], [16], [17]. To achieve these advanced functionalities, integrating neural interfaces and ML into miniaturized systems has become a key research focus [16], [18] [22]. Existing works have demonstrated promising results in developing energy-efficient system-on-chip (SoC) platforms for real-time neural signal processing and decoding. However, significant challenges re- main in achieving hardware efficiency, ensuring scalability, and maintaining the reliability and interpretability of these systems. Addressing these challenges is essential for advanc- ing next-generation neural interfaces that combine compact designs with robust performance in real-world applications. II. NEURAL INTERFACES Fig. 1. Neural Interfaces: (a) Neural recording interfaces capture high- density neural signals and process them to reduce data rates, or wirelessly transmit them to an external computer for further processing. (b) Therapeutic neural interfaces extract neuro-markers to detect disease-related neurological symptoms or mental states, and may also integrate neurostimulation for functions such as seizure suppression or brain rewiring. (c) Prosthetic neural interfaces use ML to convert brain intention into actionable commands, enabling control of end-effectors like robotic hands. arXiv:2505.02516v1 [cs.AI] 5 May 2025 Diagnostic Therapeutic Applications ML can empower advanced therapeutic devices and transform the diagnosis and treatment of neurological and psychiatric disorders, en- abling real-time monitoring, adaptive neurostimulation, and personalized interventions (Fig. 1(b)).",
    "source": "2505.02516v1_Machine-Learning-Powered_Neural_Interfaces_for_Sma.pdf",
    "length": 2538,
    "tokens": 487
  },
  {
    "text": "[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, Attention is all you need, in Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS 17, (Red Hook, NY, USA), p. 6000 6010, Curran Associates Inc., 2017. [14] K. Xu, C. Li, Y. Tian, T. Sonobe, K. ichi Kawarabayashi, and S. Jegelka, Representation learning on graphs with jumping knowledge networks, arXiv:1806.03536, 2018. [15] R. Liang, S. Nath, A. Rajaram, J. Hu, and H. Ren, Bufformer: A generative ml framework for scalable buffering, in Proceedings of the 28th Asia and South Pacific Design Automation Conference, ASPDAC 23, (New York, NY, USA), p. 264 270, Association for Computing Machinery, 2023. [16] Y. Ye, T. Chen, Y. Gao, H. Yan, B. Yu, and L. Shi, Fast and accurate wire timing estimation based on graph learning, Design, Automation Test in Europe Conference (DATE), 2023. [17] H.-H. Cheng, I. H.-R. Jiang, and O. Ou, Fast and accurate wire timing estimation on tree and non-tree net structures, Design Automation Conference (DAC), 2020. [18] P. VeliÀáckovi c, G. Cucurull, A. Casanova, A. Romero, P. Li o, and Y. Bengio, Graph Attention Networks, arXiv:1710.10903, 2018. [19] A. B. Kahng and T. Spyrou, The OpenROAD project: Unleashing hardware innovation, in Proc. GOMAC, 2021. [20] K. Boese and A. Kahng, Zero-skew clock routing trees with minimum wirelength, in ASIC Conf., pp. 1.1.1 1.1.5, 1992.",
    "source": "2507.05681v1_GATMesh_Clock_Mesh_Timing_Analysis_using_Graph_Neu.pdf",
    "length": 1461,
    "tokens": 474
  },
  {
    "text": "FlightLLM (FPGA 24) maps entire LLM inference flows onto FPGAs, leveraging sparsity and mixed- precision; implemented on a high-end Alveo U280, it achieves 6 higher energy efficiency than an NVIDIA V100 GPU [6] by using sparse DSP chains and always-on-chip decoding. SSR (FPGA 24) explores launching multiple accelerators in parallel versus sequen- tially to balance latency and throughput. On a Versal ACAP VCK190, SSR attains up to 2.5 throughput versus an NVIDIA A10G GPU, with 8.5 energy efficiency [7]. FAMOUS focuses on the Trans- former s attention mechanism: a flexible multi-head attention core on Alveo U55C that sustains 328 GOPS and runs 2.6 faster than Tiled MatMul Accelerator for Transformer Self-Attention Conference 17, July 2017, Washington, DC, USA an NVIDIA V100 GPU and 1.3 faster than prior FPGA designs. Earlier, Qi et al. (GLSVLSI 21) combined model compression (prun- ing) with FPGA optimization to fit Transformers onto FPGAs. These works, however, target large FPGAs or datacenter contexts and often integrate advanced techniques (sparsity, multi-engine con- currency) less applicable to small-edge devices. In addition to large-scale accelerators, recent research has ad- vanced data-centric analytical frameworks to understand data reuse and mapping strategies in DNN accelerators. For instance, MAE- STRO [2] and the approach presented in Understanding Reuse, Per- formance, and Hardware Cost of DNN Dataflows: A Data-Centric Approach [3] provide comprehensive models for quantifying data movement, reuse, and the associated hardware costs. Although these frameworks target a broader range of DNN workloads, they share foundational principles with our work particularly the em- phasis on maximizing both temporal and spatial data reuse which informs our targeted accelerator design for Transformer self-attention on edge FPGAs. Our Approach vs Prior Art Unlike FlightLLM or SSR, which assume abundant resources (HBM memory, 9000 DSPs) and apply complex optimizations, our design emphasizes dense GEMM acceleration on a resource- constrained FPGA.",
    "source": "2503.16731v3_Design_and_Implementation_of_an_FPGA-Based_Hardwar.pdf",
    "length": 2076,
    "tokens": 483
  },
  {
    "text": "A microcontroller updates an embedding layer using few-shot spoken keywords, inducing a 35 FLOPS overhead compared to embedding computation alone. CL results We evaluate CL on Chameleon using the same TCN model as for FSL. CL is performed by learning one new class at a time, up to 250 classes, using 1, 2, 5, or 10 shots. As Chameleon is the only silicon-proven work to perform end-to-end fully on-chip CL, Fig. 15 compares its CL per- formance (continuous lines) to the FSL performance of prior 10 TABLE I. FSL test accuracy comparison between FSL accelerators that have reported results in silicon on the Omniglot dataset. The shows 95 confidence intervals over 100 tasks. 5-way 20-way 32-way 1-shot 5-shot 1-shot 5-shot 1-shot Kim et al. [7] 93.4 98.3 - - - SAPIENS [8] - - - - 72 FSL-HDnn [9] 79.0 - - 79.5 - This work 96.8 1.6 98.8 0.5 89.1 1.3 96.1 0.5 83.3 1.2 Uses an off-chip FP32 embedder. 5 2032 50 100 150 200 250 Number of ways 60 70 80 90 100 Accuracy ( ) 1-shot 2-shot 5-shot 10-shot [7] [8] [9] Fig. 15. End-to-end CL classification accuracies on the Omniglot dataset using Chameleon for 2 250 ways with 1, 2, 5 and 10 shots, compared to other FSL chips (points outlined in black). The shaded regions indicate 95 confidence intervals over 20 tasks. For 5 ways, shot count has little effect on accuracy; with more ways, additional shots help, with diminishing returns beyond 5 shots. works [7] [9] (single data points). Notably, in the similar few- shot class-incremental learning (FSCIL) [44] scenario, where a DNN learns a set of classes before deployment and a new set of classes online, there is currently also no scalable end-to- end fully on-chip design. For example, Karunaratne et al.",
    "source": "2505.24852v2_Chameleon_A_MatMul-Free_Temporal_Convolutional_Net.pdf",
    "length": 1708,
    "tokens": 477
  },
  {
    "text": "Berlin, Heidelberg: Springer-Verlag, 2023, p. 359 378. [Online]. Available: 19 [28] F. Minervini, O. Palomar, O. Unsal, E. Reggiani, J. Quiroga, J. Marimon, C. Rojas, R. Figueras, A. Ruiz, A. Gonzalez et al., Vitruvius : an area-efficient risc-v decoupled vector coprocessor for high performance computing applications, ACM Transactions on Architecture and Code Optimization, vol. 20, no. 2, pp. 1 25, 2023. [29] M. V. Maceiras, M. W. Azhar, and P. Trancoso, Vsa: A hybrid vector- systolic architecture, in 2022 IEEE 40th International Conference on Computer Design (ICCD). IEEE, 2022, pp. 368 376. [30] A. Armejach, H. Caminal, J. M. Cebrian, R. Gonz alez-Alberquilla, C. Adeniyi-Jones, M. Valero, M. Casas, and M. Moret o, Stencil codes on a vector length agnostic architecture, in Proceedings of the 27th International Conference on Parallel Architectures and Compilation Techniques, 2018, pp. 1 12. [31] C. G omez, F. Mantovani, E. Focht, and M. Casas, Efficiently running spmv on long vector architectures, in Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Program- ming, 2021, pp. 292 303. [32] S. R. Gupta, N. Papadopoulou, and M. Peric as, Challenges and oppor- tunities in the co-design of convolutions and risc-v vector processors, in Proceedings of the SC 23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis, 2023, pp. 1550 1556. [33] D. A. Patterson and J. L. Hennessy, Computer organization and design RISC-V edition: the hardware software interface. Morgan kaufmann, 2020.",
    "source": "2507.03522v1_A_Flexible_Instruction_Set_Architecture_for_Effici.pdf",
    "length": 1579,
    "tokens": 485
  },
  {
    "text": "The baseline quantized model, where we apply our approximations, already has an accuracy loss of 17 . design approaches considering approximate multiplier hete- rogeneity, not only to each DNN layer, but also to each filter and kernel. The results show that our most prominent configurations achieve up to 54 energy reduction and 4 accuracy loss, while providing 2 gains versus the straightfor- ward approximation with EvoApprox8b multipliers. Our future work includes evaluation on different DNN types, and study of the error propagation among the DNN nodes. REFERENCES [1] S. Mittal, A Survey of Techniques for Approximate Computing, ACM Computing Surveys, vol. 48, no. 4, Mar. 2016. [2] V. Leon et al., Cooperative Arithmetic-Aware Approximation Tech- niques for Energy-Efficient Multipliers, in Design Automation Confer- ence (DAC), 2019, pp. 1 6. [3] D. Hernandez-Araya et al., AUGER: A Tool for Generating Approximate Arithmetic Circuits, in IEEE Latin American Symposium on Circuits Systems (LASCAS), 2020, pp. 1 4. [4] V. Leon et al., Approximate Hybrid High Radix Encoding for Energy- Efficient Inexact Multipliers, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 26, no. 3, pp. 421 430, Mar. 2018. [5] V. Mrazek et al., EvoApprox8b: Library of Approximate Adders and Multipliers for Circuit Design and Benchmarking of Approximation Meth- ods, in Design, Automation and Test in Europe Conference (DATE), 2017, pp. 258 261. [6] V. Leon et al., Exploiting the Potential of Approximate Arithmetic in DSP AI Hardware Accelerators, in Int l. Conference on Field Programmable Logic and Applications (FPL), 2021, pp. 1 2. [7] V. Mrazek et al., ALWANN: Automatic Layer-Wise Approximation of Deep Neural Network Accelerators without Retraining, in Int l. Confer- ence on Computer-Aided Design (ICCAD), 2019, pp. 1 8.",
    "source": "2506.21371v1_MAx-DNN_Multi-Level_Arithmetic_Approximation_for_E.pdf",
    "length": 1839,
    "tokens": 494
  },
  {
    "text": "This integration of the tracker into the pipeline not only improves the throughput performance but also optimizes the GPU and CPU power consumption. The GPU power consumption is reduced by around 600 mW, where a reduction of around 50 is introduced to the CPU power consumption. TABLE III IOU and SORT Trackers ID Switches TABLE IV DCF Tracker ID Switches Figure 11: Tracker Effect on Throughput (FPS) Figure 12: Tracker Effect on GPU CPU Power Consumption V. DISCUSSION The developed face detection and face recognition pipeline showed significant improvement to existing studies in terms of throughput and power consumption with no reduction in accuracy. By pruning existing pretrained models to enable them to run on CUDA and Tensor cores, and by utilizing other hardware engines in the device Figure 10: Tracked faced in a test Video 202 298 0 50 100 150 200 250 300 350 Tracker Disabled Tracker Enabled FPS Throughput in Frames Per Second (FPS) 4379 2834 3817 1525 0 1000 2000 3000 4000 5000 GPU CPU Power in (mW) Tracker Effect on GPU CPU Power Consumption Tracker Disabled Tracker Enabled 9 like NVENC and DLA, the pipeline achieved 202 FPS on a test video of 1920x1080 input size with an average of 5 faces per frame. This throughput, which is achieved through hardware utilization alone, before integrating the tracker, is more than 4 times higher than the FPS reported in existing similar studies based on CNN models [20][39][40]. Through hardware optimization the average detection time is around 5.6 ms per frame whereas, average recognition time is around 11.7ms. The average time of detection and recognition is significantly reduced compared to what was achieved in one notable work presented in [20], where different NVIDIA cloud and edge devices were evaluated for a face recognition pipeline, built based on MTCNN for detection and FaceNet for recognition. In [20], Jetson Xavier edge device with 512 CUDA Cores achieved 4 FPS, where RTX 2080 Ti cloud device with 4352 CUDA Cores on the same developed pipeline achieved 20 FPS on three different sizes of the input videos (i.e., 480x480, 1280 x 720, and 1920 x 1080 pixels) with an average of 3.4 faces per frame. As a further improvement to the developed pipeline through hardware utilization, a tracker was integrated between detection and recognition stages. This tracker has allowed to avoid repetitive recognition stage to pre-recognized faces.",
    "source": "2505.04524v1_Edge-GPU_Based_Face_Tracking_for_Face_Detection_an.pdf",
    "length": 2417,
    "tokens": 493
  },
  {
    "text": "5.2 Cost Model to Access Benefits to TTFT When performing swap-in or swap-out operations for LoRAs and KV caches, the goal is to retain the most valuable KVs and LoRAs in HBM as much as possible, thereby optimizing the TTFT for incoming queries. To achieve this, our key idea is to design a cost model to evaluate the expected benefits to TTFT of retaining a specific KV cache or LoRA i in HBM. As analyzed in Section 2.3.2 and Section 5.1, the cost model needs to try to load sufficient LoRAs, and consider metrics with the visited frequency, the LRU time, and the cost of swap-in out of nodes. Thus, we first define the LoRA_Evai as the reward coefficient that encourages the loaded LoAR quantity to be close to the Lowlora (Equation 3) as: LoRA_Evali max(1, Lowlora NowLoRA ) (4) In this formula, LoRA_Evali gives a larger reward when NowLoRA is farther from the Lowlora, and is set to 1 when NowLoRA is greater than or equal to Lowlora. Then, we define the Retain_Evai to represent the expected benefit of retaining node i in HBM, which can be estimated as 7 In Out Cache Manager Cache Swapper HBM space Up Used Low ... ... ... ... Candidate nodes Cost model ùê∏ùë£ùëéùëô! ùêøùëúùëÖùê¥_ùê∏ùë£ùëéùëô! ùëÖùëíùë°ùëéùëñùëõ_ùê∏ùë£ùëéùëô! Sorting on Costs 1 3 2 ... 1 2 Decided nodes Figure 10: The operation workflow of the cache swapper. the expected cold-start latency reduction to TTFT for future queries after caching it in HBM. The definition is as: Retain_Evali costi probi (1 sigmoid(ti)) (5) In this formula, the first item transfer costi can be computed using the PCIe bandwidth and size of the KV or LoRA, and the second item visit frequency probability probi is based on the recorded data on the dependency tree.",
    "source": "2505.03756v1_Improving_the_Serving_Performance_of_Multi-LoRA_La.pdf",
    "length": 1678,
    "tokens": 493
  },
  {
    "text": "Thus, for applications where power consumption isn t a major constraint and workloads don t require frequent NPU initialization deinitialization, but low-latency and a compact form factor are key, the MILK-V could prove effective. Fig. 7 details the power consumption breakdown across all evaluated platforms; among these, the MAX78000 (10.87 80.41 mW) and NXP-MCXN947 (22.91 36.69 mW) exhibit the low- est power draw across the benchmark models, with the NXP showing the lowest variance in peak power across the exe- cution stages, enabling more reliable energy budgeting. Beyond peak power, idle power consumption is another key consideration for low-power deployments, particularly if workloads run infrequently idle power also varies sig- nificantly across our benchmark platforms. The MAX78000 demonstrates the lowest idle power of the various ùúáNPU platforms (10.87 mW with RISC-V and 13.21 mW under Cortex-M4). The HX-WE2 platform ranks highest (89.09 mW), raising concerns about its applicability in extremely power-constrained scenarios (such as ones in which long idle durations dominate overall energy usage). 4.2 Latency and Memory I O Breakdown 4.2.1 NPU initialization NPU initialization times vary sig- nificantly across the benchmark platforms, from as low as 0.07 ms on the MAX78000 to 12.94 ms on the GAP8. However, the actual initialization overhead, with respect to end-to-end latency, is almost negligible on most ùúáNPU platforms except the GAP8 (7.46 ms to 12.92 ms initialization latency across the various benchmark models). Such over- head could again be problematic for duty-cycled applications, where models must be frequently loaded unloaded. 4.2.2 Memory I O Table 5 details flash and RAM usage across our various benchmark platforms and models.",
    "source": "2503.22567v2_Benchmarking_Ultra-Low-Power_ŒºNPUs.pdf",
    "length": 1772,
    "tokens": 415
  },
  {
    "text": "Unlike Flexible designs, Specific designs only support a single network architecture. Deploying a different model requires re-implementation of the bitstream or is not supported. This allows the design to be hyper-specialized to the model, making full use of the customizability of the FPGA. Specific designs can use different datapaths for different pruning and quantization of individual layers. Differentiating model families ( Fam. ) is relevant as different ML models have vastly distinct requirements. For example, CNNs spend most of their time performing convolution, a task with high computational intensity [98], whereas GNNs require a high memory bandwidth [138]. The FPGA device also influences the performance, similar to CPUs, the number of transistors increases over the generations, enabling designers to use more resources such as DSPs, BRAM, and LUTs (see 3.4). Design Metrics. The Prec. column reports the precision of operations post-quantization. The most popular quantiza- tion is i8, with 51 of all implementations and 44 of manual ones using either i8 or a mixture of i8 with f32 or i32. Next, we report the model complexity C[OP] , an essential metric as a higher complexity requires more computational resources to produce the same temporal throughput FPS . In addition, the relationship between model complexity and memory footprint can indicate whether a model is compute- or memory-bound. Most of the designs do not reach the compute boundary nor provide an off-chip memory bandwidth analysis making a further discussion impossible. The next metric is the model footprint and the location of parameters. FPGAs, similarly to CPUs, contain restricted on-chip memory, e.g., 5MB for Zynq MPSoC chips, and a main off-chip memory (512MB to 32GB). Using on-chip memory eliminates most memory bandwidth concerns as distributed BRAM provides very high bandwidth. On-chip memory weights can either be stored in BRAM pre-loaded before execution or stored inside LUTs disabling any change in parameters. Specific designs mostly use on-chip memory with only 25 using off-chip memory, while 93 of Flexible designs use off-chip memory. In particular, the FINN compiler stores weights on-chip, such as BRAM and LUTs, while Vitis AI loads weights from external memory.",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 2279,
    "tokens": 485
  },
  {
    "text": "For example, mix6 runs on eight concurrent threads for the eight con- stituent workloads. Multi-programmed workloads typically exhibit higher I O request intensity and varied access patterns. Table 5: Characteristics of multi-programmed workloads Mix Constituent Workloads [113 115] Description mix1 ssd-02 and LUN4 ssd-02 is read-intensive and LUN4 is write-intensive mix2 LUN1 and ssd-04 LUN0 is write-intensive and ssd-04 is read-intensive mix3 YCSB-C and YCSB-F Both have near-equal read-write ratio mix4 ssd-00, ssd-04, YCSB-A and LUN0 Two read-intensive and two write-intensive workloads mix5 ssd-00, LUN0, YCSB-C and YCSB-F Read-intensive, write-intensive and two workloads with a near-equal read-write ratio mix6 YCSB-B, YCSB-D, LUN0, LUN1, LUN4, ssd-00, ssd-02, ssd03 Two with near-equal read-write ratio, three write-intensive and three read-intensive 6 Evaluation 6.1 Performance Analysis Average I O Request Latency and End-to-End Throughput. Figs. 8(a) and 8(b) show the performance of CDE, K-SVM, RNN- HSS, Sibyl, Sibyl K-SVM, Sibyl RNN-HSS, CDE RL-Migr, SAPM, Harmonia and Oracle on performance- and cost-optimized HSS configurations. Figs. 9(a) and 9(b) show their end-to-end throughput on performance- and cost-optimized HSS, respectively. 0 2 4 6 8 CDE K-SVM RNN-HSS Sibyl Sibyl K-SVM Sibyl RNN-HSS CDE RL-Migr SAPM Harmonia Oracle 0 30 60 90 120 SYSTOR17 RocksDB YCSB MLPerf AVG Normalized Avg.",
    "source": "2503.20507v2_Harmonia_A_Multi-Agent_Reinforcement_Learning_Appr.pdf",
    "length": 1413,
    "tokens": 467
  },
  {
    "text": "Adopting a hierarchical framework where block serve as the fundamental level, we implement a multi-stage generation process. Initial functional block de- scriptions are first generated, then systematically propagated through two parallel pathways, which are 1) Signal-level annotation: through contextual information, the semantics of a functional block can be inherited by its associated signals, thereby effectively annotating these signals with specific func- tionalities, and 2) Module-level abstraction by designing a set of explicit and tailored prompts for the LLMs, we enable it to analyze and summarize the interactions among individual functional blocks as a module-level description. This dual- path flow ensures consistent semantic alignment between fine- grained signal behaviors and coarse-grained module opera- tions. With all descriptions finished, repo-specific information such as module and signal s names are removed to generate a relatively ambiguous query. 3) Benchmark Refinement. To further ensure benchmark validity, we employ an iterative refinement process using tem- plated instructions (shown in Appendix B). Through multiple rounds of evaluation and regeneration, we gradually remove unsuitable queries and align the LLM-generated query outputs with practical engineering requirements till it reaches the defined termination count ùêæ. After that, manual adjustments are undertaken to address few gaps between LLM outputs and the actual search intent. IV. EXPERIMENTS A. Experimental Configuration and Platforms To explore the capabilities of the proposed HDLxGraph framework, we evaluate it on three HDL downstream tasks: code search, code debugging, and code completion. The task- specific benchmarks and experimental metrics are detailed in the following subsections. We equip HDLxGraph with three LLMs with different model sizes: Claude-3.5-Sonnet [41], a large model with strong coding ability; Qwen2.5-Coder- 7B [42], a coding-specific model of medium size; and LLAMA-3.1 [43], a general-purpose model with a relatively small size. We use top-p 1.0 and temperature 0.7 as our basic configuration. All experiments are run on a 2xA6000 Linux GPU Server and all benchmark evaluations conduct 10 0.72 0.64 0.56 0.48 0.4 BM25 Search HDLxGraph 0.5467 0.6671 MRR Similarity- based RAG 0.5017 Fig. 6. HDL search MRR comparison with baselines.",
    "source": "2505.15701v1_HDLxGraph_Bridging_Large_Language_Models_and_HDL_R.pdf",
    "length": 2368,
    "tokens": 496
  },
  {
    "text": "These outliers in activations significantly contribute to degradation. Source of quality degradation. Figure 4 illustrates two pri- mary ways in which large-magnitude values degrade quality. First, inliers within the same group as outliers are truncated because the shared group exponent is set to the largest exponent in the group, leading to precision loss in smaller values. Second, large-magnitude values experience significant quantization error when represented with a 4-bit mantissa, whereas small-magnitude values remain more precise. This work proposes a mixed-precision approach to mitigate these degradations caused by large-magnitude values. III. DESIGN This section describes the MX-based mixed-precision quan- tization scheme and the MixDiT accelerator architecture. A. Magnitude-based Mixed-Precision MX Quantization Mixed-precision quantization for linear layers. Figure 5 reports that large-magnitude values concentrate in specific Exponent Mantissa (a) Inlier truncation (b) Precision loss in large-magnitude BFP 22 1 0 1 0 2-1 1 0 1 0 21 1 1 0 0 1 0 0 1 20 1 1 1 0 24 1 0 1 0 0 0 0 0 0 0 0 1 24 1 0 0 1 0 0 0 1 Truncation in inliers Outlier (Max exp) 2-1 1 0 0 0 1 0 0 1 2-1 1 0 0 1 Small magnitude DiÔ¨Ä 0.027 25 1 0 0 0 1 0 0 1 25 1 0 0 1 Large magnitude DiÔ¨Ä: 1.75 Fig. 4. Impact of large-magnitude values on MX quantization degradation. token dim channel dim token dim channel dim channel dim channel dim Prompt : macaw Prompt : baseball player Fig. 5. Observation of linear layer activation s value magnitude in DiT-XL- 512. The colors mean same with Fig. 3. 0 1 2 3 4 5 0 1 2 3 4 5 4 1 3 0 2 5 4 1 3 0 2 5 Activation Weight Activation Weight MX9 MX6 MX6 token dim Channel dim Fig. 6. Channel-wise reordering and mixed precision in linear layer. The colors mean same with Fig. 3. channels, while outliers consistently appear in the same ones regardless of the prompt [5].",
    "source": "2504.08398v1_MixDiT_Accelerating_Image_Diffusion_Transformer_In.pdf",
    "length": 1892,
    "tokens": 484
  },
  {
    "text": "The resultant energy savings of Kraken are a direct consequence of the savings in computation and memory usage from the fewer containers spawned. Only DProb and SProb consume lesser energy than Kraken (4 lesser), due to their more aggressive container reduction approach. 6.1.4 Ablation Study:This subsection conducts a brick-by- brick evaluation of Kraken using Conn Only and Comm Only, 163 SoCC 21, November 1 4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al. Application Kraken Comm Only Conn Only Social Network (99.94 , 284) (99.91 , 276) (99.89 , 256) Media Service (99.73 , 572) (99.66 , 561) (99.64 , 552) Hotel Reservation (99.87 , 316) (99.77 , 290) (99.74 , 282) Table 5: Real System: Comparing (SLO Guarantees, Containers Spawned) against Comm Only and Conn Only. schemes that exclude Commonality and Connectivity com- ponents from Kraken, respectively. From Table 5, it can be seen that Comm Only spawns 8 more containers than Conn Only for Social Network. This difference is lesser for the other applications. Upon closer examination, we see that this is due to functions having different degrees of Commonality and Connectivity. Moreover, the majority of functions whose Commonality and Connectivity differ, have a high batch size, thereby reducing the variation in the number of containers spawned. Following this, we observe that the variation in the number of containers in Social Network is mainly due to the significant difference in the Commonality and Connectivity of the Compose Post function whose batch size is only one. There is lesser difference in containers spawned by Comm Only, Conn Only and Kraken for Media Service because we have implemented Kraken with a cap on the additional con- tainers spawned due to Commonality and Connectivity when the sum of their values exceeds a threshold. This threshold is exceeded in Media Service for the majority of functions. Due to the difference in container provisioning, the difference in response times between the three schemes is evident at the tail of the response time distribution (Figure 13b). Comm Only and Conn Only are seen to exceed the target SLO at the 99th percentile. The tail latency of Kraken, in comparison, grows slower and remains within the target SLO.",
    "source": "kraken.pdf",
    "length": 2255,
    "tokens": 495
  },
  {
    "text": "Although model sharing, instead of data sharing, solves some of the challenges [5], such approaches are not trivial to deploy in classical learning paradigms, like random forests. In smart manufacturing, multiple machines, even of the identical make and model, can generate different artifacts while encountering the same fault due to different physical interference such as resonant frequency and ambient tem- perature. The ability to capture diverse conditions from the different nodes, with or without sharing data, can lead to more robust models. We focus on extending random forests models that have been deployed in smart manufacturing [7] to explore edge-cloud partitioning strategies when multiple machines cooperate in contributing to better models. Constructing an accurate random forest model, while respecting data privacy of a distributed multi user sensor network is also challenging. Moreover, such a system demands accurately predicting the cases where the edge analytics were insufficient and the cloud must be employed for deeper analysis and accurate results. We propose a novel framework to perform intelligent edge- cloud partitioning for a distributed sensor network running random forest-based analytics. We propose novel inference strategies to maximize the number of predictions performed at the edge, while consulting the cloud only when the local results are not satisfactory. We also provide novel learning strategies, especially when the distributed sensors do not want to share the local data with the cloud, saving crucial communication latency and energy. Our contributions include: (1) Two different edge-cloud learning and inference policies, in a distributed sensor environment, to efficiently run ran- dom forest based data analytics. We explore the impact of privacy-preserving random forest training mechanisms to help protect sensitive data generated by the sensors. (2) Design of a threshold based edge-cloud partitioning policy which intelligently decides when to offload an inference to the cloud while maximizing the prediction accuracy and mini- mizing the communication overheads. (3) Evaluation of these policies on a publicly-available data set and also on data from real industrial grinding machines. We show that our privacy preserving partitioning approach outperforms edge- local prediction accuracy and achieves much of the accuracy in a data-sharing model. Finally, we provide a sensitivity analysis to understand the effect of different hyper-parameters on the accuracy and latency. II.",
    "source": "EdgeClourRF.pdf",
    "length": 2540,
    "tokens": 449
  },
  {
    "text": "32.2 Addressing data-center energy Global data-center power draw is on track to exceed 7 of world electricity by 2030 - equivalent to the entire electricity consumption of India's 1.4 billion people - driven largely by AI workloads. ZettaLith s 1,490 gain in PFLOPS Watt would enable scaling to zettascale-class inference within existing power envelopes, sharply reducing the carbon footprint per query and easing pressure on grid and cooling infrastructure. 32.3 From racks to edge The same design principles scale elegantly down - from ExaLith desktop cards ( 6 exaFLOPS sparse 500 W) to NEXAI mobile IP blocks in 1 mm¬≤ of SoC area delivering 10¬π¬≥ ops sec. Thus, ZettaLith can power everything from hyperscale data centers to power-constrained edge devices and smartphones, all sharing a unified inference engine. 32.4 Next steps This work is a pre-implementation design study. The immediate engineering milestones include: SPICE-level PE design timing closure at the TSMC A16 (or A14) node Full-chip RTL synthesis, physical layout and IR-drop analysis of CASCADE CREST Thermo-mechanical simulation of WSSCB stress relief and JETSTREAM 2-phase cooling manifold Prototype fabrication of a CASCADE column in an MPW shuttle for silicon validation Early use of AI-accelerated RTL generation and layout tools may cut development time and cost. Moving from paper to silicon will require a disciplined PPA-driven flow and multi- disciplinary teams, but the potential 1,000 leap in AI inference efficiency makes ZettaLith a compelling next step toward sustainable, widely available AGI-scale computing. 32.5 Why no simulation yet? The absence of simulation data reflects a deliberate methodological choice to first establish architectural feasibility across all subsystems before detailed validation. The effort required for simulation can prematurely crystallize the overall design into a configuration that fails for reasons far outside the scope of the simulation. The author has deliberately maintained ZettaLith in a flexible pre-simulation state until there is confidence that fundamental problems in achieving ZettaLith s performance, power, and cost goals that can be identified at this stage are addressed. This approach has proven effective in the author's previous work on complex multidisciplinary systems. 33 References Agah, Amir Fakhraie, S. Emami, Azita.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2364,
    "tokens": 501
  },
  {
    "text": "This demonstrates that by optimizing the cost model in its software stack, SCRec effectively mitigates low JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 11 0 Relative Energy Efficiency (times) 64 128 256 512 64 128 256 512 Embedding Dimension of GPUs 2 4 6 8 10 12 14 280 240 200 160 120 80 40 0 MELS 2021 MELS 2022 : of GPUs System Type ( : Multi-GPU : SCRec [Ours]) System Type ( : Multi-GPU : SCRec [Ours]) Fig. 10. Energy efficiency comparison between multi-GPU system and SCRec. core utilization caused by imbalances in processing overheads between the embedding and MLP layers. These results indicate that SCRec can optimize the system based on workload intensity across various DLRM configurations and improve processing performance through hybrid-parallel processing. D. Energy Efficiency We evaluated the energy efficiency of SCRec, deployed on 8 SmartSSDs, against a multi-GPU system by varying embedding dimensions and utilizing the MELS 2021 and 2022 datasets. The batch size was set to 1024, and model parallelism was applied similarly in both systems, with EMBs split table- wise and distributed to GPUs in a round-robin manner based on their index. Due to the large size of the model parameters in MELS, which could not fit into a single server, a hash table was applied to the EMBs during measurement to limit the maximum index of the EMBs. Energy efficiency was measured using the IPS per watt (IPS W) metric to assess rel- ative performance. As shown in Fig.10, SCRec demonstrated superior energy efficiency compared to the multi-GPU system across both datasets, except when the embedding dimension was set to 64. As the embedding dimension increased, the improvement in energy efficiency became more pronounced. Specifically, with an embedding dimension of 512, SCRec achieved energy efficiency improvements of 6.14 for the 2021 dataset and 13.35 for the 2022 dataset. This improve- ment stems from the fact that as the embedding dimension increases, the number of GPUs required to store DLRM parameters grows significantly. In contrast, SCRec can fully accommodate DLRM parameters within the 32 TB capacity of the SmartSSDs.",
    "source": "2504.00520v1_SCRec_A_Scalable_Computational_Storage_System_with.pdf",
    "length": 2153,
    "tokens": 472
  },
  {
    "text": "These FSMs are tightly coupled with design con- straints (i.e., partial specifications, clock gating, and reset logic), making generic symbolic systems fragile or incom- plete. Addressing these challenges requires deeper semantic analysis beyond simple pattern matching or generic AST templates. To enhance alignment between optimized code and the FSM minimization algorithm, we propose a Control Flow Optimization module utilizing an LLM-based sym- bolic system. An FSM can be formally represented as: M (Q, Œ£, Œ¥, q0, F), (6) where Q is the finite set of states, Œ£ is the input alphabet, Œ¥ : Q Œ£ Q is the transition function, q0 Q is the initial state, and F Q is the set of accepting states. For a partially specified FSM Mp, the transition function is extended to handle non-deterministic transitions: Œ¥p : Q Œ£ 2Q, (7) where 2Q represents the power set of Q. Classical minimization algorithms (e.g., Hopcroft s (Hopcroft Ullman, 1969) or Moore s (Moore et al., 1956)) are effective for fully specified deterministic FSMs but are limited by real-world complexities. Practical RTL designs often integrate control logic with data path constraints, and undefined states and transitions make FSM minimization an NP-complete problem with a general complexity of O(2 Q ). A single pre-built AST script cannot efficiently handle all such partial specifications. Let: œï : Q D B, (8) represent the data path constraints, where D is the data path state space and B is the boolean domain. Pure FSM-focused AST-based optimization scripts can overlook these data path side effects, failing to capture deeper control semantics. Inspired by (Hu et al., 2023), we propose leveraging LLMs to transform each circuit into a symbolic representation focused solely on FSM components, i.e., isolating states, transitions, and relevant outputs, as illustrated in Figure 3. Instead of relying on a one-size-fits-all script, we prompt the LLM to dynamically generate a specialized minimization 5 SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired Symbolic Reasoning script tailored to the specific FSM structure and constraints.",
    "source": "2504.10369v1_SymRTLO_Enhancing_RTL_Code_Optimization_with_LLMs_.pdf",
    "length": 2125,
    "tokens": 482
  },
  {
    "text": "For optimizing throughput in code generation with regular prefill decode, disaggregated batching is recommended at medium request rates,i.e.3-4 req s, and chunked batch is recommended at high request rate, i.e. 5-6 req s. Last three columns indicate the optimization objective, minimizing TTFT and maximizing throughput, throughput energy. 0.5 1.5 2.5 3.5 RPS (req s client) 0 2 4 6 Normalized Value Throughput Continuous Chunked 12P 20D 16P 16D 20P 12D 0.5 1.5 2.5 3.5 RPS (req s client) 0 2 4 6 Normalized Value Throughput energy (a) Conversation trace 0.5 1.5 2.5 3.5 RPS (req s client) 0 2 4 6 Normalized Value Throughput 0.5 1.5 2.5 3.5 RPS (req s client) 0.0 0.5 1.0 1.5 Normalized Value Throughput energy (b) Coding trace Fig. 11: Comparing different serving strategies when with RAG based pipeline for running Llama-3.1-70B on 32 clients of H100 (TP2). high input rates, chunked batching offers the best throughput, especially in scenarios with long input contexts like code generation. Across all experiments, we simulate diverse request com- positions over varied GPU setups, consuming 5,688.88 GPU hours equivalent to 33,658.67. HERMES was able to simu- 0.5 1.5 2.5 3.5 4.5 5.5 6.5 RPS (req s client) 0 5 10 15 Normalized Value Throughput Continuous Chunked 12P 20D 16P 16D 20P 12D 0.5 1.5 2.5 3.5 4.5 5.5 6.5 RPS (req s client) 0 5 10 15 Normalized Value Throughput energy (a) Conversation trace 0.5 1.5 2.5 3.5 4.5 5.5 6.5 RPS (req s client) 0 5 10 Normalized Value Throughput 0.5 1.5 2.5 3.5 4.5 5.5 6.5 RPS (req s client) 0 1 2 Normalized Value Throughput energy (b) Coding trace Fig.",
    "source": "2504.09775v3_Understanding_and_Optimizing_Multi-Stage_AI_Infere.pdf",
    "length": 1599,
    "tokens": 500
  },
  {
    "text": "------------------------------------------------------------------ for (int i 0; i NHORIZON; i ) { Compute u[i] -(Kinf x[i]) - d[i] config_ld(4, -1.000000 , 4, 1); mvin2(x[i][0], x_sp_addr , 1, 4); preload(x_sp_addr , acc_start_addr , 1, 4, 1, 4); compute_preloaded(Kinf_sp_addr , 0xffffffff , 4, 4, 4, 4); mvin2(x[i][4], x_sp_addr 4, 1, 4); preload(x_sp_addr 4, acc_start_addr (1 30), 1, 4, 1, 4); compute_preloaded(Kinf_sp_addr 4, 0xffffffff , 4, 4, 4, 4); mvin2(x[i][8], x_sp_addr 8, 1, 4); preload(x_sp_addr 8, acc_start_addr (1 30), 1, 4, 1, 4); compute_preloaded(Kinf_sp_addr 8, 0xffffffff , 4, 4, 4, 4); mvin2(d[i], x_sp_addr , 1, 4); config_ld(4, 1.000000 , 4, 1); mvin2(0, x_sp_addr 4, 1, 4); preload(x_sp_addr 4, acc_start_addr (1 30), 1, 4, 1, 4); compute_accumulated(x_sp_addr 4, x_sp_addr , 1, 4, 1, 4); mvout(u[i], acc_start_addr , 1, 4); if (i NHORIZON - 1) { Compute A_x Adyn x[i] mvin2(x[i][0], x_sp_addr , 1, 4); mvin2(x[i][4], x_sp_addr 4, 1, 4); mvin2(x[i][8], x_sp_addr 8, 1, 4); preload(x_sp_addr , acc_start_addr , 1, 4, 1, 4); compute_preloaded(A_sp_addr , 0xffffffff , 4, 4, 4, 4); preload (0xffffffff , acc_start_addr 4, 1, 4, 1, 4); compute_accumulated(A_sp_addr 12, 0xffffffff , 4, 4, 4, 4); preload (0xffffffff , acc_start_addr 8, 1, 4, 1, 4); compute_accumulated(A_sp_addr 24, 0xffffffff , 4, 4, 4, 4); preload(x_sp_addr 4, acc_start_addr (1 30), 1, 4, 1, 4); compute_preloaded(A_sp_addr 4, 0xffffffff , 4, 4, 4, 4); preload (0xffffffff , (acc_start_addr 4) (1 30), 1, 4, 1, 4); compute_accumulated(A_sp_addr 4 12, 0xffffffff , 4, 4, 4, 4); preload (0xffffffff , (acc_start_addr 8) (1 30), 1, 4, 1, 4); compute_accumulated(A_sp_addr 4 24, 0xffffffff , 4, 4, 4, 4); preload(x_sp_addr 8, acc_start_addr (1 30), 1, 4, 1, 4); compute_preloaded(A_sp_addr 8, 0xffffffff , 4, 4, 4, 4); preload (0xffffffff , (acc_start_addr 4) (1 30), 1, 4, 1, 4); compute_accumulated(A_sp_addr 8 12, 0xffffffff , 4, 4, 4, 4); preload (0xffffffff , (acc_start_addr 8) (1 30), 1, 4, 1, 4); compute_accumulated(A_sp_addr 8 24, 0xffffffff , 4, 4, 4, 4); Compute B_u Bdyn u[i] and accumulate onto A_x mvin2(u[i][0], x_sp_addr , 1, 4); preload(x_sp_addr , acc_start_addr (1 30), 1, 4, 1, 4); compute_preloaded(B_sp_addr , 0xffffffff , 4, 4, 4, 4); preload (0xffffffff , (acc_start_addr 4) (1 30), 1, 4, 1, 4); compute_accumulated(B_sp_addr 4, 0xffffffff , 4, 4, 4, 4); preload (0xffffffff , (acc_start_addr 8) (1 30), 1, 4, 1, 4); compute_accumulated(B_sp_addr 8, 0xffffffff , 4, 4, 4, 4); mvout(x[i 1][0], acc_start_addr , 1, 4); mvout(x[i 1][4], acc_start_addr 4, 1, 4); mvout(x[i 1][8], acc_start_addr 8, 1, 4); fence(); } } } Figure 26: Example from Fig. 25, continued. 33",
    "source": "2505.18574v2_Autocomp_LLM-Driven_Code_Optimization_for_Tensor_A.pdf",
    "length": 2677,
    "tokens": 1219
  },
  {
    "text": "The final aspect Advanced Topics discusses research directions that enhance the effectiveness, scala- bility, and resilience of optimized onboard learning. This includes hardware-software co-design to optimize AI, bridging model compression with CL to increase flexibil- ity while preserving efficiency, scalability, and standardization for efficient deployment across heterogeneous edge platforms [1, 11, 12]. By structuring onboard learning around these core aspects, this survey provides a comprehensive review of the methodologies optimizing AI for edge computing. The remainder of this paper is organized as follows. Section 2 explores techniques for model compression and efficiency. Section 3 discusses inference optimization strategies. 1Terms such as on-device learning, onboard learning, and edge-based learning or edge AI are often used interchangeably in the existing literature. In this paper, we will use onboard learning for standardization. 2 Table 1: Taxonomy of key techniques and challenges Topic Techniques Performance Constraint Latency Comm Hardware Adaptability Optimization Complexity HCC MSC HPC LI CO OC HC HEE TDB PT OC Model Compression Pruning - - - - - - Quantization - - - - - - Knowledge Distillation - - - - - - - Neural Architecture Search - - Efficient Inference Computation Offloading - - - - Model Partitioning - - - - - - Early Exit Strategies - - - - - - Decentralized Learning Federated Learning - - - - - - Continual Learning DL - - - - - - - Adaptive Learning DL - - - - - - Security Privacy Privacy Protection - - - - - - - Secure Model Execution - - - - - Explainable AI - - - - - - - - - Advanced Topics Model Compression CL - - - - - - Scalability Hardware Co-Design Performance Constraints: High Computational Cost (HCC), Memory Storage Constraints (MSC), High Power Con- sumption (HPC) Latency and Communication: Latency Issues (LI), Communication Overhead (CO) Hardware Adaptability: Hardware Compatibility (HC), Heterogeneous Edge Environments (HEE) Optimization Complexity: Training Deployment Bottlenecks (TDB), Performance Trade-off (PT), Optimization Com- plexity (OC) Section 4 covers decentralized and adaptive learning methods. Section 5 examines security and privacy considerations. Section 6 presents emerging advanced topics for optimized onboard learning.",
    "source": "2505.08793v1_Onboard_Optimization_and_Learning_A_Survey.pdf",
    "length": 2316,
    "tokens": 495
  },
  {
    "text": "Pruning with vector length n 1 shows the highest sparsities across all DNNs, however, n 1 does not introduce any structured sparsity. With an increasing n, the first couple of operators for all DNNs and the last FC operator of AlexNet and VGG16 drastically decrease in sparsity. 12 M. M. M√ºller and K. L√ºbeck et al. 4x4 8x8 16x16 4x4 8x8 16x16 4x4 8x8 16x16 4x4 8x8 16x16 FlexiSAGA Systolic Array Size 109 1010 Cycles (log) AlexNet VGG16 GoogLeNet ResNet50 dense sparse (a) csOS dWS dOS sWS sOS dataflow 0 50 100 150 200 selected 40.2 24.3 16.9 15.2 3.4 (b) Fig. 8. (a) Comparison of whole DNN runtimes in clock cycles for different FlexiSAGA systolic array sizes. (b) Distribution of selected dataflows with minimal runtime per DNN operator across all DNNs and all FlexiSAGA sizes. The overall sparsity shows only slight variations for different vector lengths and orientations and remains above 0.75 for AlexNet, VGG16, and GoogLeNet which is comparable to the sparsities reported for the same DNNs in [17]. To compare the sparse-over-dense inference speedup to other architectures, we used the DeepSparse [10] inference engine and deployed the four DNNs un- pruned (dense) and pruned (sparse) onto an Intel Xeon Platinum 8168 CPU with 24 cores and 48 threads and the Nvidia Jetson AGX Orin ARM Cortex- A78AE CPU with 12 cores. Each DNN was run using a single core and multiple cores threads. The sparse variant of the DNNs were pruned with œµ a 0.02 and n 1, as it provides the highest sparsity and the highest speedup when using DeepSparse and limiting the top-1 accuracy drop to 2 .",
    "source": "2506.01566v1_FlexiSAGA_A_Flexible_Systolic_Array_GEMM_Accelerat.pdf",
    "length": 1586,
    "tokens": 444
  },
  {
    "text": "A. Multi-Centroid AM Initialization In traditional HDC approaches, each class is represented by a single vector. The initialization method for these class vectors, including random sampling, has minimal impact on their ability to effectively represent the overall set of sample hypervectors for that class. Random sampling is commonly used for initialization due to the method s robustness to initial conditions. During the subsequent iterative learning process, all updates corresponding to a single class are applied to one class vector, allowing it to converge to a representative vector for the class with minimal dependence on its initial state. In contrast, multi-centroid models use multiple class vectors per class, each capturing distinct features of that class and learn- ing independently. This makes the initialization method critical for ensuring comprehensive class representation. Randomly- sampled initial class vectors fail to distribute the centroids evenly across point cloud of the class sample hypervectors, leading to poor representation. To address this, we propose the clustering-based initialization method that improves con- vergence and accuracy. 1) Classwise Clustering : First of all, as shown in Fig. 2-(a), MEMHD splits the encoded sample hypervectors by class and applies clustering for each class. We use K-means algorithm for clustering, which allows us to specify the number of clusters directly. The distance metric used in clustering is based on dot similarity, the same metric employed in associative search. This consistency ensures that the clustering process is optimized for subsequent associative search operations. We define a hyperparameter R as the proportion of columns used for initial clustering out of the total memory columns. This approach allows for cluster allocation, which is detailed in (2). Given R, the number of initial clusters n per class is determined by the equation: n max 1, CR k , where C is the total number of columns and k is the number of classes. After clustering, each cluster s centroid becomes an initial class vector. 2) Cluster Allocation : We allocate the remaining columns, C(1-R), based on a validation process using the entire training dataset. This process employs a confusion matrix to assess the distribution of misclassifications among different classes. Classes with higher misclassification rates are assigned addi- tional centroids to enhance their representation in the model. These additional centroids are allocated to classes with higher misprediction rates.",
    "source": "2502.07834v1_MEMHD_Memory-Efficient_Multi-Centroid_Hyperdimensi.pdf",
    "length": 2550,
    "tokens": 491
  },
  {
    "text": "2023. Gibbon: An Efficient Co-Exploration Framework of NN Model and Processing-In-Memory Architecture. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 42, 11 (2023), 4075 4089. doi:10.1109 TCAD.2023.3262201 [29] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. 2019. FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search. In 2019 IEEE CVF Conference on Computer Vision and Pattern Recognition (CVPR). 10726 10734. doi:10.1109 CVPR.2019.01099 [30] Zheyu Yan, Da-Cheng Juan, Xiaobo Sharon Hu, and Yiyu Shi. 2021. Uncertainty Modeling of Emerging Device based Computing-in-Memory Neural Accelerators with Application to Neural Architecture Search. In 2021 26th Asia and South Pacific Design Automation Conference (ASP-DAC). 859 864. [31] Zhihang Yuan, Jingze Liu, Xingchen Li, Longhao Yan, Haoxiang Chen, Bingzhe Wu, Yuchao Yang, and Guangyu Sun. 2021. NAS4RRAM: neural network architec- ture search for inference on RRAM-based accelerators. Science China Information Sciences 64, 6 (10 May 2021), 160407. doi:10.1007 s11432-020-3245-7 [32] Ramtin Zand, Arman Roohi, and Ronald F DeMara. 2018. Fundamentals, modeling, and application of magnetic tunnel junctions. In Nanoscale Devices. CRC Press, 337 368. [33] Xinbang Zhang, Zehao Huang, Naiyan Wang, Shiming Xiang, and Chunhong Pan. 2021. You Only Search Once: Single Shot Neural Architecture Search via Direct Sparse Optimization .",
    "source": "2505.22868v1_CrossNAS_A_Cross-Layer_Neural_Architecture_Search_.pdf",
    "length": 1536,
    "tokens": 439
  },
  {
    "text": "The number of Eulerian circuits will drastically increase with the number of devices. VIN1 VOUT1 PM1 NM1 VDD VSS VIN1 NM1_G NM1 NM1_D NM1_S NM1_B PM1_G PM1 PM1_S PM1_D PM1_B VDD VSS VOUT1 (a) Topology (b) Device pin level graph representation ['VSS' 'NM1_S' 'NM1' 'NM1_D' 'VOUT1' 'PM1_D' 'PM1' 'PM1_G' 'VOUT1''PM1_G' 'PM1_D' 'NM1_D' 'PM1_G' 'NM1_D' 'PM1_D' 'PM1_G' 'PM1' 'PM1_S' 'VDD' 'PM1_B' 'PM1' 'PM1_B' 'PM1_S' 'PM1_B' 'VDD' 'PM1_S' 'PM1' 'PM1_D' 'VOUT1' 'NM1_D' 'NM1' 'NM1_G' 'VIN1' 'NM1_G' 'NM1' 'NM1_B' 'VSS' 'NM1_B' 'NM1_S' 'NM1_B' 'NM1' 'NM1_S' 'VSS'] ['VSS' 'NM1_B' 'NM1' 'NM1_D' 'VOUT1' 'PM1_D' 'PM1' 'PM1_G' 'VOUT1''PM1_G' 'PM1_D' 'NM1_D' 'PM1_G' 'NM1_D' 'PM1_D' 'PM1_G' 'PM1' 'PM1_S' 'VDD' 'PM1_B' 'PM1' 'PM1_B' 'PM1_S' 'PM1_B' 'VDD' 'PM1_S' 'PM1' 'PM1_D' 'VOUT1' 'NM1_D' 'NM1' 'NM1_G' 'VIN1' 'NM1_G' 'NM1' 'NM1_S' 'VSS' 'NM1_S' 'NM1_B' 'NM1_S' 'NM1' 'NM1_B' 'VSS'] (C) Four Eulerian circuits we found by using DFS ['VSS' 'NM1_S' 'NM1_B' 'VSS' 'NM1_B' 'NM1' 'NM1_D' 'VOUT1' 'PM1_D' 'PM1' 'PM1_G' 'VOUT1' 'PM1_G' 'PM1_D' 'NM1_D' 'PM1_G' 'NM1_D' 'PM1_D' 'PM1_G' 'PM1' 'PM1_S' 'VDD' 'PM1_B' 'PM1' 'PM1_B' 'PM1_S' 'PM1_B' 'VDD' 'PM1_S' 'PM1' 'PM1_D' 'VOUT1' 'NM1_D' 'NM1' 'NM1_G' 'VIN1' 'NM1_G' 'NM1' 'NM1_S' 'NM1' 'NM1_B' 'NM1_S' 'VSS'] ['VSS' 'NM1_B' 'NM1_S' 'VSS' 'NM1_S' 'NM1' 'NM1_D' 'VOUT1' 'PM1_D' 'PM1' 'PM1_G' 'VOUT1' 'PM1_G' 'PM1_D' 'NM1_D' 'PM1_G' 'NM1_D' 'PM1_D' 'PM1_G' 'PM1' 'PM1_S' 'VDD' 'PM1_B' 'PM1' 'PM1_B' 'PM1_S' 'PM1_B' 'VDD' 'PM1_S' 'PM1' 'PM1_D' 'VOUT1' 'NM1_D' 'NM1' 'NM1_G' 'VIN1' 'NM1_G' 'NM1' 'NM1_B' 'NM1' 'NM1_S' 'NM1_B' 'VSS'] Figure 7: An example of analog circuit topology, its device pin level graph representation, four unique Eulerian circuits we found by using DFS. 15 Published as a conference paper at ICLR 2025 A.4 ANALOGGENIE S GENERATED CIRCUIT TOPOLOGY VISUALIZATION In this section, we show some of the novel circuits AnalogGenie generated and demonstrate its zero-shot capability by generating circuits that belong to a type that is not included in the dataset.",
    "source": "2503.00205v1_AnalogGenie_A_Generative_Engine_for_Automatic_Disc.pdf",
    "length": 2014,
    "tokens": 1115
  },
  {
    "text": "To simplify our AE design, we simply reuse the pattern captured in the 1st row, and do not consider the deeper information related to the row numbers. To study how this decision affects the video quality, we report 250 0 20 40 60 80 100 0 20 40 60 80 100 Baseline EA AE EA AE PTU PTU EA AE Baseline EA AE EA AE PTU PTU EA AE Baseline EA AE EA AE PTU PTU EA AE Baseline EA AE EA AE PTU PTU EA AE Baseline EA AE EA AE PTU PTU EA AE Baseline EA AE EA AE PTU PTU EA AE Rhinos Timelapse Rollercoaster Paris Elephants Avg. Total Energy Saving Compute Energy Consumption L R TotalEnergySaving Fig. 9: Normalized energy consumption and savings with different conÔ¨Ågurations and video inputs. The left y-axis shows the compute energy consumption normalized to the compute energy consumption in Baseline (the lower, the better). The right y-axis shows the amount of energy savings compared to the end-to-end total energy consumption in Baseline (the higher, the better). 0 20 40 60 80 V1 V2 V3 V4 V5 Avg. PSNR (dB) -10 -5 0 5 10 1 224 447 670 893 1116 1339 1562 1785 2008 2231 2454 2677 2900 3123 3346 3569 3792 Distance Pixel ID Delta-y Delta-x (a): PSNR (b): Pattern in CubeMap format Fig. 10: Sensitivity study. (a): Video quality metric (PSNR [25], [40]) across video inputs. (b): The pattern between left-eye and right-eye in the front face in Cube Mapping [41]. the averaged PSNR [25], [40] of the Ô¨Åve videos represented in Equirectangular format [52] in Fig. 10 a . These results indicate that, although we ignore the row-number related information, the resulting PSNR is still sufÔ¨Åcient (47.71 on average) for VR video applications [26], [62]. General Applicability of D ej a View: The above discussion assumes that the Equirectangular format [52] is used to represent the 360 videos.",
    "source": "DejaView.pdf",
    "length": 1781,
    "tokens": 476
  },
  {
    "text": "In both cases, many accesses cannot be Ember: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures Embedding Operation and Model Class Embedding operation reference implementation Ncompute Nlookups Memory footprint Temporal locality (reuse distance CDF) Spatial locality (vector dim.) EmbeddingBag (EB) or Sparse-Lenghts Sum (SLS) for Deep-Learning Recommendation Models (DLRMs) (Section 2.2.1) segment in batch category in segment lookup vector accumulate vector 1 1 All embedding tables require GBs [18] to TBs [23] 0 200 400 600 800 1000 Reuse Distance 0.0 0.2 0.4 0.6 0.8 1.0 CDF criteo ftr2 criteo ftr1 criteo ftr0 32-256 Sparse attention (SpAttn) for Large Language Models (LLMs) (Section 2.2.2) block in Q input block in K input token in K block lookup K vector token in Q block copy K vector 0 K-V tensors are MBs Out tensor is MB-GB 0 200 400 600 800 1000 Reuse Distance 0.0 0.2 0.4 0.6 0.8 1.0 CDF 8 emb blk 4 emb blk 2 emb blk 1 emb blk 8x8x512 Sparse Matrix-Matrix Multiplication (SpMM) for Graph Neural Networks (GNNs) (Section 2.2.3) vertex in graph neighbor of vertex lookup vector rescale and accumulate vector 2 1 Single embedding table GBs large 0 200 400 600 800 1000 Reuse Distance 0.0 0.1 0.2 0.3 CDF proteins products arxiv mag 8-349 Message Passing (MP) models (Section 2.2.3) vertex in graph lookup vertex vector neighbor of vertex lookup neighbor vector multiply vertex-neighbor vectors scale resulting vector reduce into tmp multiply tmp by vertex vector accumulate resulting vector 5 2 Two embedding tables, each GBs large 0 200 400 600 800 1000 Reuse Distance 0.0 0.1 0.2 0.3 0.4 0.5 0.6 CDF com-Youtube roadNet-CA web-Google wiki-Talk 128 Knowledge Graphs (KGs) (Section 2.2.3) sample in batch lookup vector compute head-rel-tail norm 4 3 Single embedding table GBs large 0 200 400 600 800 1000 Reuse Distance 0.0 0.1 CDF biokg wikikg2 512 Table 1: Characterization of the embedding operations in Section 2.2. The compute-per-lookup ratio captures code complexity.",
    "source": "2504.09870v1_Ember_A_Compiler_for_Efficient_Embedding_Operation.pdf",
    "length": 2028,
    "tokens": 576
  },
  {
    "text": "This task is particularly crucial for hardware design, as it enables code reuse, improves efficiency, and ac- celerates the transition from user specifications to RTL code. For this task, we reuse the dataset and benchmark from the RTL code understanding task. However, since the functional descriptions in the understanding dataset often contain specific iden- tifiers, introducing the risk of data leakage, and are too complex for direct use in practical code search, we employ GPT-4o to rephrase the descrip- tions into a user query format, as shown in Figure 2. The rephrasing ensures that the new descriptions meet the following conditions: (1) no references to specific identifiers, (2) retention of the core func- tionality and high-level logic, and (3) clarity and simplicity, resembling how a user would query for relevant code based on its functionality. After this rephrasing process, we obtain the natural language Code Internal Logic Extract Rewritten Code LEC Feedback LLM Generate Figure 3: The feedback-driven code rewrite process. code search dataset and benchmark in the format {(user_queryi, RTL_codei)}n i 1. For details on the prompt used to rephrase the functional descriptions, please refer to the Appendix B. 3.2.2 Functionality Equivalence Checking Functionality equivalence checking is a critical ver- ification step in hardware design, ensuring that dif- ferent RTL implementations exhibit identical be- havior despite structural differences. To construct a dataset for this task, we develop a feedback-driven CoT prompting strategy using GPT-4o, as shown in Figure 3. Given a Verilog module, we first prompt GPT-4o to introduce significant modifications to its internal logic while preserving its intended func- tionality. We then use Yosys (Wolf et al., 2013) to perform logic equivalence checking (LEC), which verifies whether the original and modified designs are functionally equivalent. Based on Yosys feed- back classified as equivalent, inequivalent, or syn- tax error we iteratively refine the modifications. Specifically, we incorporate the original design, rewritten design, and verification results into the prompt to guide GPT-4o in generating alternative implementations. This process is repeated for two to three rounds per design, ensuring a diverse set of functionally equivalent and inequivalent pairs.",
    "source": "2506.15697v1_DeepRTL2_A_Versatile_Model_for_RTL-Related_Tasks.pdf",
    "length": 2347,
    "tokens": 492
  },
  {
    "text": "An Minimizing Makespan Bin Packing model is developed to efficiently solve the offline scheduling problem. Additionally, we introduce a sorting and preemption method to handle the online request scheduling problem, and we develop a Lagrangian-based heuristic technique for solving the online iteration scheduling issue. In the online scheduling module, our method can provide decisions within 5 milliseconds, meet- ing the real-time decision-making requirements of practical application in LLM inference. The remainder of the paper is structured as follows. We review literature on efficient LLM inference in Section II. The problem definition and model formulation are presented in Section III. Then, we illustrate the difficulty of solving this problem and introduce our hybrid offline-online scheduling method to provide a timely solution in Section IV. Numerical studies using real cases and 100 generated cases are presented in Section V. Finally, the conclusion and future directions are provided in Section VI. II. LITERATURE REVIEW This section provides an overview of existing research and prevalent methodologies in inference optimization for LLMs. Firstly, we introduce the general techniques commonly em- ployed in model serving, which can be seamlessly integrated with our scheduling strategies. Subsequently, we elucidate several classical techniques widely adopted in LLM inference systems, which constitute the cornerstone of our framework and methodologies. In the following, we succinctly introduce recent advancements in inference optimization. Finally, we examine scheduling methods within the existing operations research domain. As LLM inference falls within the broader scope of model serving, a variety of general inference optimization techniques can be effectively utilized. Model compression is one of the quintessential optimization strategies for reducing model size, encompassing techniques such as quantization [11], spar- sification [12], [13], and distillation [14]. In addition, the design of more compact structures to replace the original ones is also common. For instance, employing multi-query attention [15] or grouped-query attention [16] in place of the original multi-head attention in Transformer architecture can reduce key-value heads, resulting in a more streamlined model.",
    "source": "2502.15763v1_Hybrid_Offline-online_Scheduling_Method_for_Large_.pdf",
    "length": 2319,
    "tokens": 422
  },
  {
    "text": "The 2 arrow indicates that the input tile in the input matrix moves arXiv:2503.19640v1 [cs.LG] 25 Mar 2025 TABLE I HIDDEN DIMENSION, PRE-DEFINED TOKEN LENGTH, PARAMETER SIZE AND TOTAL EMA VALUES FOR RESPECTIVE REPRESENTATIVE LARGE MODEL Model ViT-G 14 Wav2Vec2 -XLS-R GPT-3 Hidden Dimension 4,096 2,560 12,288 Token Length 518 1,536 2,048 Parameter Size (B) 1.8 2 175 Total EMA (G) 312.9 353.9 11,132.6 TABLE II COMPARISON OF EXTERNAL MEMORY ACCESS FOR VARIOUS STATIONARY SCHEMES Stationary Scheme Input Matrix Weight Matrix Output Matrix Total Na ƒ±ve K MN M NK N MK MNK 3 IS MN M m NK N n MK MNK ( 1 K 1 m 1 n) WS K k MN NK N n MK MNK ( 1 k 1 M 1 n) OS K k MN M m NK MK MNK ( 1 k 1 m 1 N ) IS-OS MN M m NK MK MNK ( 1 K 1 m 1 N ) WS-OS K k MN NK MK MNK ( 1 k 1 M 1 N ) Fig. 1. Matrix Mapping for Matrix-Matrix Multiplication with Conventional Stationary Schemes n units, while the weight tile shifts n units downward in the weight matrix. As the input tile transitions from west to east in the input matrix, it moves m units to the next row, repositioning the weight tile from right-bottom to left-top in the weight matrix. An arrow marked 2 produces the output tile. These steps repeat M m times, as indicated by the arrow marked 3 . b) Weight Stationary: The WS scheme loads each weight tile into internal memory once, enabling repeated use across multiple calculations, which reduces redundant memory ac- cess. Fig. 1(c) illustrates this dataflow. An arrow 1 indicates the fixed position of the weight tile along matrix columns, M, where each tile is reused M m times. Arrow 2 shows the input tile moving k units down the input matrix, while the stationary weight tile remains fixed.",
    "source": "2503.19640v1_An_Efficient_Data_Reuse_with_Tile-Based_Adaptive_S.pdf",
    "length": 1686,
    "tokens": 479
  },
  {
    "text": "Power then returns to the ground of the PSU via the ground connection chain, which is essentially the reverse of the power connection chain. The number of ground connections is often greater than the number of power connections to reduce ground bounce. Most of the voltage drop and power dissipation is in the PSU power and ground rails, as these are much longer than any other part of the interface. A parasitic power loss of 1.0 kW in power distribution may seem excessive, but this is only 1.2 of the total ZettaLith power of 84 kW. 22.4 Electromigration The Current Density column of Table 15 shows the current density through each structure in A cm2 . All the structures are made of copper except those identified as solder. The maximum current density for copper before electromigration is generally considered to be a problem is 106 to 107 A cm2. All the copper structures have current densities of less than 105 A cm2, so are more than an order of magnitude below the threshold for the onset of electromigration. Solder has an electromigration threshold of only around 104 A cm2. The various solder connections are well within this limit. Electromigration for the entire SLD die is easy to calculate due to its SHAPE architecture. All CASCADE array columns are identical. 23 ZettaLith High Current Density and Technique for PSU PCB Attachment A fundamental challenge for ZettaLith implementation is delivering 114,000 Amps of precisely regulated fast response power to the computational elements. 23.1 CGA columns Conventional CGA columns made of solder represent a critical failure point that would render the entire system non-functional, as they could catastrophically fail (melt) under ZettaLith's high current densities. This power delivery bottleneck represented a potential \"showstopper\" that could have invalidated the entire ZettaLith architecture. The solution is a CGA column design being 217 fine copper wires in a hex-close pack configuration. Each 80 Œºm diameter wire contributes to a robust 640 Œºm copper column that simultaneously could provide: low resistance and low voltage drop of 0.25 mV; total power loss of all CGA columns of only 0.33 W; highs current-carrying capacity without electromigration failure; thermal-mechanical compliance to accommodate differential expansion; elimination of elastoplastic deformation common in solder columns; and sufficient structural integrity for reliable system assembly.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2437,
    "tokens": 495
  },
  {
    "text": "For the last 2 columns, we quantize only the forward pass. The second column indicates the total FLOP count used for those values of tokens-to-parameter ratios (D N). Values are shown as differences with respect to bfloat16 baseline (lower is better).",
    "source": "2506.20752v1_Characterization_and_Mitigation_of_Training_Instab.pdf",
    "length": 251,
    "tokens": 56
  },
  {
    "text": "2. Matrix Mapping for Matrix-Matrix Multiplication with Proposed Stationary Schemes of the input matrix is noticeably less compared to others, highlighting the overall EMA reduction. Therefore, applying a fixed stationary scheme without considering the input length is inadvisable. For the second problem, external memory like DRAM cannot read and write data simultaneously. However, when computing linear projections, two matrices need to be read from external memory, while only one requires writing back. Though this can solved by extra buffers, the required buffer size could be quite large due to the large matrix size in current transformer models. Thus, an effective partial sum reuse scheme is demanded to solve this problem. III. METHODOLOGY This Section first shows the concept of the adaptive station- ary mechanism, then further enhances data reuse with a hybrid stationary scheme in a tile granularity, and finally presents the whole strategy. A. Adaptive Mechanism The adaptive mechanism is constructed to optimize the selection of the most efficient stationary data scheme in a tile granularity for specific EMA needs during matrix com- putations. The core decision focuses on choosing between IS or WS, determined by evaluating the EMA reduction for the matrix. In current accelerators, PE arrays are often organized in a square formation like 8 8 or 16 16. This design choice facilitates efficient tile computation mapping onto the processing elements, where m, n, and k are roughly equal. For existed accelerators, each PE arrays usually consists of a squared sized PEs, such that 8 8, or 16 16. That is, for the consideration of well porting tile computation to the processing elements, m, n, and k are approximately equivalent. As demonstrated in Table II, the input matrix primarily reduces total EMA when the input stationary scheme is applied, decreasing EMA from K (MN) to MN. In contrast, under the WS scheme, the weight matrix EMA is reduced by a factor of M. Thus, selecting either IS or WS depends simply on comparing MN and NK, corresponding to the input and weight matrix sizes, respectively. The decision is governed by the expression MN NK N(M K). When M K, the result is negative, suggesting IS is more efficient than WS. Conversely, when the result is zero or positive (M K), WS is more advantageous and preferred.",
    "source": "2503.19640v1_An_Efficient_Data_Reuse_with_Tile-Based_Adaptive_S.pdf",
    "length": 2348,
    "tokens": 473
  },
  {
    "text": "Furthermore, the scarcity of Verilog code datasets exacerbates the challenge of effectively learning Verilog s syntax. To address this issue, we propose a novel application of speculative decoding for Verilog code generation. By integrating conventional tokenization with Verilog s syntactical structure, our method allows the model to generate more accurate Verilog code more efficiently. B. Syntax-Aware Tokenization for Code Driven by recent developments in LLMs, a variety of pretrained models for programming languages have emerged [1] [4], [22] [27]. However, most of these models represent code as token sequences using the BPE algorithm for pre-training, which often hinders their ability to capture code s syntactic structure and fails to ensure the syntactic correctness of the generated output. To address this limitation, some models [28] [30] have attempted to explicitly incorporate syntax by representing code as AST sequences, which are obtained by traversing the AST in pre-order and recording the symbol of each node. However, to maintain the tree structure in the node sequences, these approaches introduce additional nodes, which significantly increase sequence length and GPU memory usage, potentially compromising model performance. Furthermore, these models only leverage the AST representation in the encoder and do not ensure syntactic correctness during code generation. Meanwhile, several non-pretrained code generation models [6] [10] have used grammar rule sequences to represent code. Specifically, they traverse the AST in pre-order and record the grammar rules used to expand each non-terminal. The integration of grammar rules has been shown to improve performance. However, these models have only been tested in non-pretrained settings and do not involve LLMs. When applied to pre-training with large code corpora, these models face a significant challenge due to the big vocabulary problem, which arises from the large number of user-defined identifiers and could degrade the model s performance. In contrast, BPE is designed to find a rel- atively small set of subtokens whose concatenations could represent a large token set. To integrate BPE with grammars, GrammarT5 [11] uses a variant of grammar rule sequence to represent code, called Tokenized Grammar Rule Sequence (TGRS). Despite this innovation, GrammarT5 still faces generalization issues when applied to token sequences and has only demonstrated marginal improvements in smaller models, leaving its effectiveness on larger models unexplored.",
    "source": "2503.14153v1_Speculative_Decoding_for_Verilog_Speed_and_Quality.pdf",
    "length": 2538,
    "tokens": 499
  },
  {
    "text": "The expert-combine operation, used in architectures like GShard [33] and DeepSeek [1], merges outputs from multiple experts by computing a weighted combination based on their affinity scores: Ci k X j 1 gi,e (2) hi k X j 1 gi,e Ci hk i (3) In these equations, i 0, S 1 represents an input token index, e Ei,k identifies the k-th expert selected for token i, and gi,e is the affinity score indicating how relevant expert e is for token i. Unified task abstraction. We unify the FFN and combine operations under a common abstraction called a task. Tasks provide a uniform interface for communicating tile-level work among Subscribers, Schedulers, and Processors. Formally, a task descriptor t T is defined as a tuple: t (M, , œï) 7 where M is a set of metadata (e.g., device ID, tile index), is a binary tensor operation (specifically, matrix multiplication or Hadamard product ), and œï is an element-wise activation function (e.g., ReLU or identity). We define a task t operating on input tensors A, B, D, producing output tensor C, as follows: Ft(A, B, C, D) : C œï (A t B D) (4) The operator t (instantiated from ) may behave differently depending on the task metadata M, and the result of A t B is accumulated into D. We provide an example of task metadata in D. In practice, we implement each task defined by Equation 4 as a single fused __device__ decorated function which the Processor (Algorithm 2) invokes at runtime. Fusion for t entails applying œï and the succeeding addition operation to registers storing the results of the binary operator t. To illustrate its flexibility, we show how the FFN and expert-combine operations can be expressed using this task framework. Note that we omit the matrix multiplication symbol ( ) for simplicity. Also, œï1 can be any activation function, while œï2 is the identity function.",
    "source": "2506.04667v2_FlashDMoE_Fast_Distributed_MoE_in_a_Single_Kernel.pdf",
    "length": 1823,
    "tokens": 448
  },
  {
    "text": "This technique integrates multiple modalities of circuit data, such as textual, structural, and physical information, to create richer, more comprehensive representations. The model is pre-trained to fuse these different modalities, enabling it to capture a broader range of circuit characteristics. In this way, the model supports complex tasks that require information from different modalities. In the following subsections, we summarize existing circuit encoders based on their target circuit design stages, including HLS stage (Section 4.1), RTL stage (Section 4.2), netlist stage (Section 4.3) and layout stage (Section 4.4). A detailed comparison and summary of these circuit encoders are provided in Table 2 and Table 3, respectively. Section 4.5 will cover the emerging and more advanced circuit encoder techniques. For each stage, we first summarize the employed circuit dataset, including detailed statistics and data collection process, then detail the proposed encoding techniques, including circuit preprocessing, ML model architecture, and pre-training techniques, and finally discuss the supported downstream tasks with evaluation metrics. 4.1 Circuit Encoder for HLS In the context of HLS, the circuit encoder plays a pivotal role in representing and optimizing the design space for HLS circuits. HLS involves the transformation of high-level programming languages (e.g., C C ) into hardware description languages (e.g., Verilog), with the goal of improving the design, performance, and power efficiency of hardware systems. Efficient exploration of this design space is critical, and HLS encoders are explored to learn meaningful representations of the circuit designs, enabling better optimization and decision-making. As shown in Figure 6 (a), two notable methods in this domain are HARP [99] and ProgSG [100], both pre-train HLS encoders with self-supervised learning, improving the exploration of the HLS design space. 4.1.1 Dataset for HLS circuits.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 1972,
    "tokens": 403
  },
  {
    "text": "Input and weight registers are 8-bit long, and output register is 32-bit long. TABLE IV: Hardware implementation parameters Implementations Area, mm2 Power, W Max frequency, MHz Baseline SA 1.726 0.158 402 PM-DMR0-TMR3 1.937 0.177 357 PM-DMR0-TMR4 1.929 0.176 372 PM-DMRA-TMR3 2.129 0.193 303 PM-DMRA-TMR4 2.091 0.190 302 B. Reliability analysis Two common CNN models were used for reliability evalua- tion, including AlexNet, trained on the CIFAR-10 dataset, and VGG-11, trained on the ILSVRC-2012 dataset. Models and datasets were selected to represent different network scales. For the fault injection experiments, networks were quantized to 8-bit integer format. Experiments were performed on NVIDIA GeForce RTX 3090 24G GPU. Architectural vulnerability factor (AVF) was used as a reliability measure. AVF is the probability that a fault in the UNDER REVIEW IN AN IEEE JOURNAL 8 CONV0 CONV2 CONV4 CONV5 CONV7 Top1-class Top1-acc Top5-class Top5-acc Top1-class Top1-acc Top5-class Top5-acc Top1-class Top1-acc Top5-class Top5-acc Top1-class Top1-acc Top5-class Top5-acc Top1-class Top1-acc Top5-class Top5-acc 0 2 4 6 8 10 12 14 16 18 AVF [ ] No FT DMR0 DMRA TMR Fig.",
    "source": "2503.04426v1_FORTALESA_Fault-Tolerant_Reconfigurable_Systolic_A.pdf",
    "length": 1170,
    "tokens": 361
  },
  {
    "text": "Recent advances in LLM-based AI agents have introduced diverse workflows that combine language-based reasoning with external tool use. ReAct [64] interleaves reasoning and tool invocation through step-by- step decision making, while Reflexion [47] enhances agent behavior through self-evaluation and feedback. LATS [68] employs a tree-based search strategy to explore multiple reasoning paths in parallel, and LLMCompiler [18] plans tool usage in advance by generating action sequences. Building on these single-agent frameworks, multi-agent systems ex- tend agent capabilities through collaborative task execution. CAMEL [22] implements role-based dialogue among agents, and AutoGen [58] formalizes multi-agent orchestration via structured roles, message passing, and execution control. While these workflows substantially improve reasoning capabilities and behavioral flexibility, their system-level im- plications, such as resource usage, latency dynamics, and energy consumption, remain underexplored. To the best of our knowledge, this work is the first to address this gap through a comprehensive system-level analysis of representative AI 13 agent designs, offering new insights into the efficiency and scalability of agentic systems. Structured agent interfaces for tool-augmented reason- ing. In parallel with behavioral advancements in AI agents, recent efforts have focused on standardizing AI agent APIs and protocols to facilitate broader integration and deployment. OpenAI s function-calling interface [30] defines a structured mechanism for API invocation, enabling agents to interact with tools in a verifiable and consistent manner. Anthropic s Model- Context-Protocol (MCP) [3] further formalizes how agents manage context and interact with tools, enabling decoupled reasoning logic and modular deployment across platforms. Google s Agent-to-Agent (A2A) protocol [13] complements these efforts by specifying a standard for multi-agent com- munication. A2A enables heterogeneous agents to exchange structured messages, delegate tasks, and coordinate behavior, supporting modular and scalable agentic systems. Although these efforts focus on unifying the agent pro- gramming model, our work takes an orthogonal system-level perspective, uncovering the AI infrastructural challenges posed by agentic workloads under test-time scaling. System-level optimization of AI agents.",
    "source": "2506.04301v1_The_Cost_of_Dynamic_Reasoning_Demystifying_AI_Agen.pdf",
    "length": 2389,
    "tokens": 474
  },
  {
    "text": "This concept of equivalence has led to the use of circuit equivalent transformations as a data augmentation technique, allowing for the generation of multiple, functionally equivalent representations of a circuit. Furthermore, it has inspired cross- design-stage alignment in circuit foundation models, enabling these models to capture and align information across different stages of the design flow. This alignment enhances the model s ability to transfer knowledge between stages and improves cross-stage consistency. Multimodal circuit format. As shown in Figure 4, circuit data inherently can be represented in multiple formats and modalities, each capturing different aspects of the circuit, including: Text. This modality includes hardware description languages (HDLs) such as Verilog and VHDL, along with high-level specifications in natural language. Text-based representations define circuit functionality, behavioral constraints, and design requirements, emphasizing semantic information of circuits. Graph. Circuit structures are naturally represented as graphs, where nodes correspond to components (e.g., logic gates, registers, functional blocks) and edges capture connectivity (e.g., data flow, control dependencies). Graph-based formats, including control-data flow graphs and gate-connected graphs, preserve the topological relationships, which are crucial for structural reasoning in EDA tasks. Image. The physical layout of circuits, particularly at the post-synthesis stage, can be represented in two-dimensional visual formats, similar to the format of images. These images capture geometric features, including component placement and interconnect routing, which are critical for the physical design process and manufacturability. Each of these modalities provides a unique perspective of the circuit, and fusing them enables a com- prehensive understanding of the design, facilitating advancing foundation AI techniques for circuits. Multiple objectives. The ultimate target objectives in circuit design are PPA (i.e., power, per- formance, and area) and functionality.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 2093,
    "tokens": 384
  },
  {
    "text": "arXiv, 2409.13724, September 2024. doi: 10.48550 arXiv.2409. 13724. Fang, W., Lu, Y., Liu, S., Zhang, Q., Xu, C., Wills, L. W., Zhang, H., and Xie, Z. Masterrtl: A pre-synthesis ppa esti- mation framework for any rtl design. In 2023 IEEE ACM International Conference on Computer Aided Design (IC- CAD), pp. 1 9, 2023. doi: 10.1109 ICCAD57390.2023. 10323951. Gupta, R., Benson, D., and Fang, J. Z. Path profile guided partial dead code elimination using predication. In Pro- ceedings 1997 International Conference on Parallel Ar- chitectures and Compilation Techniques, pp. 102 113. IEEE, 1997. Hopcroft, J. E. and Ullman, J. D. Formal languages and their relation to automata. Addison-Wesley Longman Publishing Co., Inc., 1969. Hu, Y., Yang, H., Lin, Z., and Zhang, M. Code prompting: a neural symbolic method for complex reasoning in large language models, 2023. URL abs 2305.18507. Knoop, J., R uthing, O., and Steffen, B. Partial dead code elimination. ACM Sigplan Notices, 29(6):147 158, 1994. LaForest, C. E. and Steffan, J. G. Efficient multi-ported memories for fpgas. In Proceedings of the 18th An- nual ACM SIGDA International Symposium on Field Pro- grammable Gate Arrays (FPGA 10), pp. 41 50, Mon- terey, CA, USA, 2010. ACM. doi: 10.1145 1723112. 1723122. Langley, P. Crafting papers on machine learning. In Langley, P. (ed. ), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 1207 1216, Stan- ford, CA, 2000. Morgan Kaufmann.",
    "source": "2504.10369v1_SymRTLO_Enhancing_RTL_Code_Optimization_with_LLMs_.pdf",
    "length": 1474,
    "tokens": 459
  },
  {
    "text": "Evaluations for Cost Model Objectives To evaluate the cost model objectives, we conducted the following experiments for SpMM on SPADE. For simplicity in the calculations, we set Œ≤CPU 1 and Œ≤SPADE 1000. However, a CPU execution typically takes milliseconds, while a SPADE execution can extend up to two weeks. We explored 11 distinct models across four different categories, differentiated by the number of data samples they were trained on, while the cost model architecture remained the same. Category I consists of models (NT d) trained exclusively on data samples from d matrices executed on SPADE. Category II includes transfer-learned models (TL d), which were pre-trained with data samples from 100 matrices on CPU (10,000 data samples) and then fine-tuned on SPADE with data samples from d matrices. Category III consists of models (CPU d) pre-trained with varying numbers of data samples from d matrices on CPU and then fine-tuned on data samples from 5 matrices on SPADE. Finally, we did zero-shot inference (Zero-Shot) from a model pre-trained on CPU with data samples from 100 matrices without additional fine-tuning on SPADE. Models trained exclusively on SPADE data samples (NT d) generally exhibit increasing speedup and decreasing APE as the number of SPADE data samples increases. For example, NT 1000, trained on 100,000 SPADE data samples, achieves the highest speedup of 1.43 and an APE of 7.06. However, the data collection overhead for these models rises significantly with the number of SPADE samples, making the use of them impractical due to the long simulation times. In contrast, the TL 13 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning models, which are pre-trained on CPU data and fine-tuned on SPADE samples, demonstrate an excellent balance between speedup, APE, and DCE. TL 5 model, for instance, delivers a competitive speedup of 1.40 and a low APE of 7.28, while maintaining an excellent DCE of 0.51.",
    "source": "2506.00424v2_COGNATE_Acceleration_of_Sparse_Tensor_Programs_on_.pdf",
    "length": 1979,
    "tokens": 441
  },
  {
    "text": "These advancements enhance the security and reliability of onboard learning, making it a viable solution for mission-critical edge applications. 6 Advanced Topics Onboard learning continues to evolve, requiring advanced techniques to balance effi- ciency, adaptability, and robustness in resource-constrained environments. This section explores three critical areas: (1) bridging model compression with continual learning to improve adaptability while preserving efficiency, (2) enhancing scalability and stan- dardization to enable seamless deployment across heterogeneous edge platforms, and (3) leveraging hardware-software co-design to optimize AI for next-generation edge devices. 17 6.1 Bridging Model Compression with Continual Learning Continual Learning (CL) enables AI models to incrementally learn new tasks without catastrophic forgetting, making it crucial for long-term deployment in dynamic envi- ronments. However, CL is constrained by high memory requirements, computational overhead, and knowledge retention issues challenges that are especially critical for onboard AI. Model compression techniques such as pruning, knowledge distillation, and quantization offer solutions by optimizing memory usage and inference speed while preserving task knowledge. Pruning for Continual Learning. Integrating pruning into CL helps onboard models adapt efficiently while maintaining a compact structure. Sparse Continual Learning (SparCL) [159] employs weight sparsity [160], gradient sparsity [161], and dynamic masking to selectively retain critical weights and prune redundant ones, minimizing memory footprint. Task-aware dynamic masking (TDM) further enhances retention by ensuring that essential parameters are preserved. Alternative methods such as gradient pruning [162] and continual prune-and-select (CP S) [163] further improve memory allocation by structuring models into reusable subnetworks optimized for incremental learning. Knowledge Distillation for Continual Learning. Traditional experience replay is often infeasible for onboard AI due to memory constraints and privacy concerns. Knowledge Distillation (KD) mitigates catastrophic forgetting by transferring past knowledge to new models without requiring explicit data storage [164, 165]. Learning without Forgetting (LwF) [164] preserves past knowledge by constraining parameter updates, while deep model consolidation [165] further refines inter-task representa- tions. Prototype-sample relation distillation [166] and batch-level distillation [167] improve adaptation in non-stationary environments by optimizing feature transfer between tasks.",
    "source": "2505.08793v1_Onboard_Optimization_and_Learning_A_Survey.pdf",
    "length": 2624,
    "tokens": 493
  },
  {
    "text": "With this benchmark in place, more NAS algorithms and estimators will be developed by researchers, making it more practical to compare them in AIMC scenarios. 9 Acknowledgements. We thank the computational support from AiMOS, an AI supercomputer made available by the IBM Research AI Hardware Center and Rensselaer Polytechnic Institute s Center for Computational Innovations (CCI). References Banner, R., Nahshan, Y., and Soudry, D. (2019). Post training 4-bit quantization of convolutional networks for rapid-deployment. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 7948 7956. Benmeziane, H., Lammie, C., Boybat, I., Rasch, M. J., Gallo, M. L., Tsai, H., Muralidhar, R., Niar, S., Ouarnoughi, H., Narayanan, V., Sebastian, A., and Maghraoui, K. E. (2023). Analognas: A neural network design framework for accurate inference with analog in-memory computing. In IEEE International Conference on Edge Computing and Communications, EDGE 2023, Chicago, IL, USA, July 2-8, 2023, pages 233 244. IEEE. Boybat, I., Kersting, B., Sarwat, S. G., Timoneda, X., Bruce, R. L., BrightSky, M., Le Gallo, M., and Sebastian, A. (2021). Temperature sensitivity of analog in-memory computing using phase-change memory. In 2021 IEEE International Electron Devices Meeting (IEDM), pages 28 3. IEEE. Chellapilla, K., Puri, S., and Simard, P. (2006). High performance convolutional neural networks for document processing. In Tenth international workshop on frontiers in handwriting recognition. Suvisoft. Chen, T. and Guestrin, C. (2016). Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 785 794.",
    "source": "2506.18495v1_AnalogNAS-Bench_A_NAS_Benchmark_for_Analog_In-Memo.pdf",
    "length": 1818,
    "tokens": 475
  },
  {
    "text": "Another important challenge in approximate DNNs is the approximation localization, i.e., where to insert approximations, in order to maximize the energy efficiency while keeping accuracy in acceptable levels. In this paper, we introduce the MAx-DNN framework to ad- dress the aforementioned design challenges, by examining ap- proximation at different levels of the DNN architecture, i.e., in network s layers, layer s filters, or filter s kernels. Considering that the majority of the computations in DNNs are multiply- accumulate [9], we focus on optimizing the multiplications. MAx-DNN adopts ALWANN [7] for generating approximate DNN hardware accelerators without retraining, and extends it with our approximation localization, which is based on the ROUP multiplication library [2]. Our design space is larger compared to that of the original ALWANN framework, which introduces the same inexact multipliers in each convolutional layer (multipliers may differ among layers). Interestingly, the proposed approximation approaches assign different approxi- mate ROUP multipliers either in each convolutional layer, fil- ter, or kernel. To evaluate our approximations, we employ the quantized ResNet-8 network, trained on the CIFAR-10 dataset, and use standard-cell ASIC technology with the TSMC 45- nm library. At first, we examine the sensitivity of the DNN layers to approximations in both standalone and combined fashion, and then, we perform an extensive design space exploration and accuracy energy Pareto analysis to extract the most prominent configurations of our approaches. The results show that, compared to the quantized model, our designs deliver up to 54 energy gains in exchange for an accuracy loss of 4 , while providing 2 energy efficiency versus the default ALWANN approximations. The paper contribution lies in proposing an improved fine-grained method to approximate the DNN multiplications, and quantifying the results from various approximation approaches and configurations. The remainder of the paper is organized as follows. Section II introduces the ALWANN framework [7]. Section III defines our approximation space. Section IV reports the experimental evaluation. Finally, Section V draws the conclusions. 2022 IEEE. Personal use of this material is permitted.",
    "source": "2506.21371v1_MAx-DNN_Multi-Level_Arithmetic_Approximation_for_E.pdf",
    "length": 2288,
    "tokens": 459
  },
  {
    "text": "2021. Toward Resource-Efficient Federated Learning in Mobile Edge Computing. IEEE Network 35, 1 (2021), 148 155. doi:10.1109 MNET.011.2000295 [183] Jinliang Yuan, Shangguang Wang, Hongyu Li, Daliang Xu, Yuanchun Li, Mengwei Xu, and Xuanzhe Liu. 2024. Towards Energy-efficient Federated Learning via INT8-based Training on Mobile DSPs. In Proceedings of the ACM Web Conference 2024 (Singapore, Singapore) (WWW 24). Association for Computing Machinery, New York, NY, USA, 2786 2794. doi:10.1145 3589334.3645341 [184] Arber Zela, Julien Siems, and Frank Hutter. 2020. NAS-Bench-1Shot1: Benchmarking and Dissecting One-shot Neural Architecture Search. CoRR abs 2001.10422 (2020). arXiv:2001.10422 [185] Liekang Zeng, En Li, Zhi Zhou, and Xu Chen. 2019. Boomerang: On-Demand Cooperative Deep Neural Network Inference for Edge Intelligence on the Industrial Internet of Things. IEEE Network 33, 5 (2019), 96 103. doi:10.1109 MNET.001.1800506 [186] Qunsong Zeng, Yuqing Du, Kin K. Leung, and Kaibin Huang. 2019. Energy-Efficient Radio Resource Allocation for Federated Edge Learning. arXiv:1907.06040 [cs.IT] [187] Yu Zhang, Tao Gu, and Xi Zhang. 2022. MDLdroidLite: A Release-and-Inhibit Control Approach to Resource-Efficient Deep Neural Networks on Mobile Devices. IEEE Transactions on Mobile Computing 21, 10 (2022), 3670 3686. doi:10.1109 TMC.2021.3062575 [188] Yuchen Zhao, Sayed Saad Afzal, Waleed Akbar, Osvy Rodriguez, Fan Mo, David Boyle, Fadel Adib, and Hamed Haddadi. 2022. Towards battery-free machine learning and inference in underwater environments. In Proceedings of the 23rd Annual International Workshop on Mobile Computing Systems and Applications (HotMobile 22).",
    "source": "2505.12523v1_Energy-Aware_Deep_Learning_on_Resource-Constrained.pdf",
    "length": 1676,
    "tokens": 489
  },
  {
    "text": "5: Instruction tuning data pair of alignment paradigm 1. specific datasets (i.e., from Task 1 2) to achieve encoder-decoder alignment. As shown in Fig. 5, we generate a multimodal instruction- response pair for each netlist, where the input includes: (1) A task-specific instruction (e.g., request for function description or implementation details). (2) Input and output signals extracted from the netlist code. (3) The netlist graph format, with the encoder capturing the structural and functional information. The ground truth is the golden specification according to the task. This creates a unified format for multimodal instruction-following sequences. GenEDA is instruction-tuned on prediction tokens using the auto-regressive training objective, maximizing the likelihood of the target ground- truth specification text sequence y with length L, formulated as: Lalign1 L X i 1 log p(yi y i, GN ) (2) where y i represents the previously generated tokens before the current token yi, and GN denotes the input netlist graph for the encoder. During instruction tuning, we leverage a two-step procedure for multimodal embedding alignment: 1) Pre-training for modality alignment. In this stage, the netlist encoder fenc and decoder fdec remain frozen. Only the connector MLP fc is trained to maximize the likelihood of the generated tokens of the auto-regressive loss, as formulated in Equation (2). This step aligns the netlist embeddings Hg with the pre-trained LLM word embeddings Ht, effectively acting as a modality adapter (i.e., netlist tokenizer ) for the LLM. 2) Fine-tuning end-to-end. In this stage, the encoder fenc remains frozen, while both the connector fc and the LLM decoder fdec are fine-tuned also with the auto-regressive loss (i.e., Equation (2)). This end-to-end training step further refines the alignment, allowing the model to perform generative tasks seamlessly across graph and text modalities. B. Paradigm 2: Prediction-Based Alignment Paradigm 2 overview. In addition to trainable open-source LLMs, advanced LLMs are often frozen due to commercial or computational limitations, yet they excel in reasoning and support longer input contexts.",
    "source": "2504.09485v1_GenEDA_Unleashing_Generative_Reasoning_on_Netlist_.pdf",
    "length": 2170,
    "tokens": 499
  },
  {
    "text": "buffer_size rank_num max_tokens msg_size, (1) where max_tokens local_batch min(topK, experts_per_die) (2) max_tokens is the worst-case number of tokens an NPU may send to a single peer, and msg_size is the per-token message length (7.5 KB after INT8 quantization for token dispatch and 14 KB for token combine). With this space pre-allocated, both FusedDispatch and FusedCombine directly write data into the target NPU memory buffer via AIV-direct communication, avoiding an intermediate local copy and the subsequent remote read, thus reducing memory traffic and synchronization latency. Because FusedDispatch and FusedCombine execute back-to-back, sharing a single buffer would create a race: a faster NPU could launch FusedCombine and overwrite a peer s buffer before that peer finishes consuming the prior FusedDispatch payload, corrupting data. We eliminate this hazard with double buffering: distinct buffers are reserved for FusedDispatch and FusedCombine, ensuring that one buffer is always free for writers while the other is being read. The pre-allocation memory overhead is modest. In our experimental setup, each die handles a local batch of at most 96 tokens and hosts up to two experts, yielding max_tokens 96 min(8, 1) 96. Across a communication domain of 320 ranks, the dispatch buffer occupies 320 96 7.5 KB 225 MB, and the combine buffer 320 96 14 KB 420 MB. The two buffers together consume only about 645 MB memory per die. 4 Data-Sending Pipeline: Remote data writes require computing the target offset within a peer NPU s pre-allocated memory buffer. However, performing this calculation and the transfer sequentially would stall execution. To avoid this, we design a data-sending pipeline inside each fused operator as shown in Figure 12, which pipelines the following three stages: (1) copy the next token into the local UBuffer; (2) compute the remote buffer offset and apply INT8 quantization if enabled; (3) issue the AIV-Direct write to the peer NPU s memory. Tokens flow through this pipeline as one-token microbatches. While Stage 3 of a microbatch transmits data, Stages 1 and 2 of the following microbatches execute in parallel.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2160,
    "tokens": 496
  },
  {
    "text": "A Survey of Circuit Foundation Model: Foundation AI Models for VLSI Circuit Design and EDA WENJI FANG , Hong Kong University of Science and Technology (HKUST), Hong Kong JING WANG , Hong Kong University of Science and Technology (HKUST), Hong Kong YAO LU, Hong Kong University of Science and Technology (HKUST), Hong Kong SHANG LIU, Hong Kong University of Science and Technology (HKUST), Hong Kong YUCHAO WU, Hong Kong University of Science and Technology (HKUST), Hong Kong YUZHE MA, Hong Kong University of Science and Technology (Guangzhou) (HKUST(GZ)), China ZHIYAO XIE , Hong Kong University of Science and Technology (HKUST), Hong Kong Artificial intelligence (AI)-driven electronic design automation (EDA) techniques have been extensively explored for VLSI circuit design applications. Most recently, foundation AI models for circuits have emerged as a new technology trend. Unlike traditional task-specific AI solutions, these new AI models are developed through two stages: 1) self-supervised pre-training on a large amount of unlabeled data to learn intrinsic circuit properties; and 2) efficient fine-tuning for specific downstream applications, such as early-stage design quality evaluation, circuit-related context generation, and functional verification. This new paradigm brings many advantages: model generalization, less reliance on labeled circuit data, efficient adaptation to new tasks, and unprecedented generative capability. In this paper, we propose referring to AI models developed with this new paradigm as circuit foundation models (CFMs). This paper provides a comprehensive survey of the latest progress in circuit foundation models, unprecedentedly covering over 130 relevant works. Over 90 of our introduced works were published in or after 2022, indicating that this emerging research trend has attracted wide attention in a short period. In this survey, we propose to categorize all existing circuit foundation models into two primary types: 1) encoder-based methods performing general circuit representation learning for predictive tasks; and 2) decoder-based methods leveraging large language models (LLMs) for generative tasks. For our introduced works, we cover their input modalities, model architecture, pre-training strategies, domain adaptation techniques, and downstream design applications. In addition, this paper discussed the unique properties of circuits from the data perspective. These circuit properties have motivated many works in this domain and differentiated them from general AI techniques.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 2547,
    "tokens": 498
  },
  {
    "text": "While a reasonable level in the stack to target, ZLUDA would not be able to benefit from low-level, backend Nvidia optimizations (operating below the PTX level), and is limited to the AMD stacks backend optimizations. In our work, we target assembly-to-assembly translation, in an effort to leverage hardware-specific optimizations below the intermediate representation (IR) level, that may be missing altogether in the corresponding AMD codebase. 2.2 Assembly-to-Assembly Translation Translating assembly across ISAs is challenging due to divergent instruction sets and execution models. Recent work employs language models for this task, including CRT [24], a lightweight transpiler from x86 assembly (CISC) to ARM (RISC), and Guess Sketch [25], which integrates language models with symbolic reasoning to translate between ARMv8 and RISC-V. These recent successes open the door for assembly-to-assembly translation in the unexplored GPU-to-GPU space. A key contributing factor to their success is the large CPU-centric dataset enabling training from one ISA to another. Given the lack of such a rich dataset in the GPU space, a primary goal of this work is to enable such an exploration and transpilation across GPU vendors, democratizing compute in the critical GPU and ML-acceleration landscape, where Nvidia CUDA currently dominate the market. 2.3 Datasets and Benchmarks for CUDA and HIP As shown in table 1, existing benchmarks in the GPU space generally focus on runtime performance, do none target the assembly level, and do not have paired aligned data across Nvidia AMD codebases. ComputeEval [19] includes only CUDA code for hardware evaluation. Rodinia [20] and SHOC [21] provide heterogeneous benchmarks using CUDA OpenCL OpenMP but omit AMD code and assem- bly. PolyBench [22] evaluates compilers with CUDA OpenCL kernels, yet lacks assembly-level or AMD support. BabelStream [23] benchmarks HIP CUDA OpenCL memory bandwidth but excludes assembly and domain diversity. Hetero-Mark [26] targets CPU GPU workloads where GPU code is minimal.",
    "source": "2505.16968v3_CASS_Nvidia_to_AMD_Transpilation_with_Data_Models_.pdf",
    "length": 2054,
    "tokens": 445
  },
  {
    "text": "The first two capacitors swap their roles according to the value of ùëß, as shown for exemplary activations across three time steps. B More detailed schematic of the switching scheme, including the circuits representing the input activation ùë•ùëñ of a row, represented by a binary value. Weights ùë§‚Ñé and ùë§ùëß are stored in local 2 b SRAM cells, which determine the potential to sample from through turning on one of the 4 switches. ùúéùëß(ùë•) { { { { {0 if ùë• 3 , 1 if ùë• 3 , ùë• 6 1 2 otherwise. (5) The result is then quantized to 6 b. 3 Circuit implementation The MINIMALIST architecture encompasses multiple stacked GRU layers, each connected through feedfor- ward projections. Depending on their dimensionality, these GRU blocks can be mapped to one or multiple cores, which are connected through an event-based routing fabric. The following paragraphs introduce the design of those cores, namely the switched-capacitor IMC and state update circuitry. 3.1 Mixed-signal computing cores The computing cores capture the functionality of a GRU block and the subsequent application of the output acti- vation function ùúé‚Ñé. To that end, they first calculate the gating variable ùëß as well as the new proposal state ‚Ñé that both result from linear input projections through IMC. They also implement the subsequent state update mechanism. 3.1.1 Switched-capacitor-based IMC input projections MINIMALIST realizes the linear input projections repre- senting ùëæh and ùëæz through switched-capacitor IMC matrices. The two resulting matrix-vector multiplica- tions share the same input vector ùë•, and can thus be merged into a single matrix as indicated in Figure 2 A. Each GRU circuit is thus connected to a column of ‚Ñé and ùëß synapses, each. An ‚Ñé synapse fulfills two distinct roles: it participates in the respective matrix-vector multiplication, but is also responsible for maintaining the previous hidden state ‚Ñéùë° 1. It thus features two identical capacitors.",
    "source": "2505.08599v1_MINIMALIST_switched-capacitor_circuits_for_efficie.pdf",
    "length": 1931,
    "tokens": 487
  },
  {
    "text": "Mapping Mapping for p in [0:P): for q in [0:Q): for h in [0:H): for w in [0:W): Transformed Loops Innermost loop for MVM G_LI R7, 418816 G_LI R9, 0 G_LI R10, 16 CIM_MVM R7, R10, R9 SC_ADDI R7, R2, 1 SC_ADDI R2, R7, 0 JMP -26 Loop back Software Dim. Hardware Dim. k c r s w h for p in [0:P): for q in [0:Q): for h in [0:H Th ): -------------------- for w in [0:W Tw): for th in [0:Th): for tw in [0:TW): Loop Nest CIM OP Translation Mem. Acc. Annotation Tile size Searching Fig. 4. Compilation flow and mapping optimization strategies in CIMFlow. flexible spatial mapping strategies. Within each MG, weights are typically organized along the output channel, enabling efficient input data broadcast across macros for parallel in- memory MVM operations. The vector compute unit handles auxiliary DNN operations such as activation, pooling, and quantization. The scalar compute unit executes control flow operations through scalar arithmetic computations. Instruction Design. To support efficient execution across the hardware hierarchy, CIMFlow implements a unified 32- bit instruction format with specialized variations for different operation types. Instructions are categorized into compute, communication, and control flow instructions, with compute instructions further specialized for CIM, vector, and scalar compute units. Each instruction contains a 6-bit operation specifier (opcode) and multiple 5-bit operand fields. Certain instruction types may also include supplementary fields, such as a 6-bit functionality specifier, execution flags, or immediate values of 10 or 16 bits. The instruction format supports up to four operands depending on the operation type, providing flex- ibility for complex operations while maintaining encoding effi- ciency. The instruction set is designed for extensibility through incorporating a customized instruction description template, which enables seamless integration of new operations into the framework when provided with their associated performance parameters. C. Compilation Flow The CIMFlow compiler bridges the semantic gap between high-level DNN models and low-level CIM operations through a two-level optimization strategy, as illustrated in Fig.",
    "source": "2505.01107v1_CIMFlow_An_Integrated_Framework_for_Systematic_Des.pdf",
    "length": 2200,
    "tokens": 498
  },
  {
    "text": "These programs were then compiled for x86 (CISC) ARMv8 ARMv5 RISC-V (RISC). Each program was compiled to both x86 (CISC) ARMv8 ARMv5 RISC-V (RISC) targets under two optimization levels: -O0 (no optimization) and -O2 (aggressive optimization). These flags were selected to expose models to both raw, semantically transparent code (-O0) and real-world, performance-optimized binaries (-O2), enabling the model to learn both unoptimized and optimized ISA patterns. Compilation for ARMv5 and RISC-V64 was performed via cross-compilation on an Ubuntu 20.04 machine with a Ryzen 7 CPU, using arm-linux-gnueabi-gcc (Radcolor, n.d.) and gcc-riscv64-linux-gnu (Project, 2025), respectively. ARMv8 binaries were compiled natively on an Apple M2 Pro (macOS) using clang (Lattner, 2008), ensuring architectural fidelity for performance-critical ARM targets. 3.2 Training All hyperparameter optimization experiments were conducted on a small 500k portion of AnghaBench. We tested various hyperparameter settings on this subset of our benchmark. After identifying the optimal configuration, we scaled up the training data to 1.31M samples. We trained three models: DeepSeek-Coder1.3B (Guo et al., 2024), Qwen2.5-Coder (1.5B and 0.5B) (Hui et al., 2024b). Given the dataset size of 1.3M million samples, with an average of 13k tokens per sample, we opted for smaller models. Training was done on A100 GPUs (40GB each). Training with 1.3M 3 GG-Instruct AnghaBench C C Samples Deduplication CLANG Compilation {-O0,O2} CISC Arch. RISC Arch. Data Collection and Compilation ldr r1, r2 Tokenizer Extension ldr r1, r2 x86 Training and Benchmarking Test Cases Test Cases Test Cases GG Guesser Predictions Compute Platforms MAC Link Execute Test Accuracy RoPE Extrapolation Stackv2 Training and Tuning BringupBench Benchmarks C C HumanEval Figure 1: GG System Overview.",
    "source": "2506.14606v1_Guaranteed_Guess_A_Language_Modeling_Approach_for_.pdf",
    "length": 1846,
    "tokens": 495
  },
  {
    "text": "We also thank the NSF Chameleon Cloud project CH-819640 for their generous compute grant. All product names used here are for identification purposes only and may be trademarks of their respective companies. 165 SoCC 21, November 1 4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al. References [1] [n.d.]. Twitter Stream traces. Accessed: 2020-05-07. [2] 2019. Airbnb AWS Case Study. case-studies airbnb . [3] 2019. Provisioned Concurrency. lambda latest dg configuration-concurrency.html. [4] 2020. Amazon States Language. functions latest dg concepts-amazon-states-language.html. [5] 2020. AWS Lambda. Serverless Functions. lambda . [6] 2020. Azure Durable Functions. us azure azure-functions durable. [7] 2020. hey HTTP Load Testing Tool. [8] 2020. IBM-Composer. cloud-functions-pkg_composer. [9] 2020. Kubernetes. [10] 2020. Microsoft Azure Serverless Functions. com en-us services functions . [11] 2020. OpenFaaS. [12] 2020. Prometheus. [13] 2021. AWS Lambda Cold Starts. coldstarts aws . [14] 2021. Azure Functions Cold Starts. coldstarts azure . [15] 2021. Expedia Case Study - Amazon AWS. serverless coldstarts azure . [16] Feb 24, 2020. Intel Power Gadget. energy-meter. [17] February 2018. Google Cloud Functions. functions docs . [18] Istemi Ekin Akkus et al. 2018. SAND: Towards High-Performance Serverless Computing. In ATC. [19] Mamoun Awad, Latifur Khan, and Bhavani Thuraisingham. 2008. Pre- dicting WWW surfing using multiple evidence combination. The VLDB Journal 17, 3 (2008), 401 417. [20] M. A. Awad and I. Khalil. 2012. Prediction of User s Web-Browsing Behavior: Application of Markov Model.",
    "source": "kraken.pdf",
    "length": 1623,
    "tokens": 467
  },
  {
    "text": "2.4 Challenges in LLM Benchmarking for FPGA Design Despite advancements in LLM-driven Verilog generation, existing models primarily focus on producing syntactically and functionally correct HDL but fail to address critical hardware constraints essen- tial for FPGA deployment, such as resource efficiency, timing con- straints, and power consumption. Unlike ASIC design, FPGA-based development demands careful consideration of resource usage, in- cluding lookup tables (LUTs), flip-flops (FFs), block RAM (BRAM), and digital signal processing (DSP) blocks. However, current LLMs for Verilog generation lack an understanding of FPGA-specific re- quirements, often producing designs that are functionally correct but inefficient and impractical for real-world FPGA deployment. The inability to evaluate and optimize LLM-generated Verilog for FPGA resource constraints highlights the need for advancing resource-aware Verilog generation and motivates this study. To establish LLMs as a practical solution for HDL automation, it is essential to equip them with a deeper understanding of FPGA de- sign constraints. Achieving this requires developing new training datasets, designing robust evaluation frameworks, and refining LLM training strategies to enhance their capability in hardware-aware code generation. This paper focuses on constructing a benchmark that provides an evaluation of LLMs performance in generating HDLs that are both functionally correct and optimized for FPGA- specific resource constraints. HEART 25, May 26 28, 2025, Kumamoto, Japan C. Guo and T. Zhao 3 Design of ResBench This section presents the design of ResBench, outlining its guiding principles and structured problem set for evaluating LLM-generated Verilog code. Additionally, we compare ResBench with existing benchmarks for LLM-generated HDL code. 3.1 Design Principles and Benchmark Problems ResBench is designed to evaluate LLM-generated Verilog across a di- verse range of FPGA applications, with a primary focus on resource optimization awareness. The benchmark consists of 56 problems categorized into 12 domains, each representing a key area of FPGA applications. The problems in ResBench are carefully structured to evaluate both functional correctness and resource efficiency. The benchmark covers a wide range of FPGA design tasks, from fundamental digital logic and mathematical computation to more complex, application-driven domains such as machine learning, cryptography, and financial computing.",
    "source": "2503.08823v2_ResBench_Benchmarking_LLM-Generated_FPGA_Designs_w.pdf",
    "length": 2492,
    "tokens": 497
  },
  {
    "text": "When no caching is used, all 8 model instances concurrently loading the model from OBS experience a cold start latency of approximately 2,560 seconds each, due to severe contention on the shared 2.5 GB s OBS bandwidth. Local DRAM caching offers no improvement in this cold start latency, as each node still independently fetches the full model from OBS. In contrast, EMS reduces cold start latency to only 320 seconds by enabling shared loading through the memory pool and reusing model blocks across instances. Beyond latency, EMS also improves memory efficiency. Local DRAM caching results in an 8 DRAM overhead where each of the 8 instances stores a full model replica. EMS, in comparison, requires only 1 DRAM footprint to serve all instances, while maintaining an identical warm start latency of 5 seconds. In model switching scenarios, EMS achieves a 100 cache hit rate with an average switch latency of 5 seconds, significantly outperforming local DRAM caching, which yields only a 12.5 hit rate and a latency of 281 seconds. These results highlight EMS as a highly effective solution for minimizing both model access latency and memory resource overhead in large-scale inference environments. 4.5 INT8 Quantization To achieve high-throughput, low-latency inference for large-scale MoE models such as DeepSeek- V3 R1 on the Ascend 910 platform, we have designed and implemented a training-free, hierarchical INT8 quantization scheme for model weights and activations. This scheme is engineered to maximize computational efficiency and reduce memory footprint while carefully managing potential accuracy degradation. The core components of our approach are detailed below: Mixed-Precision Strategy. Our quantization scheme employs a mixed-precision strategy that classifies different operators within the model based on a trade-off between their impact on overall performance (e.g., computational load, memory access) and their sensitivity to numerical precision. The most computationally intensive operations in the critical execution path, such as large matrix multiplications in feed-forward networks (FFNs) and attention mechanisms, are quantized to INT8 to leverage the highest throughput. Conversely, sub-modules or specific operations that are more sensitive to quantization errors but constitute a smaller fraction of the overall memory access or computational burden (e.g., certain normalization layers or critical gating mechanisms) retain higher precision using BF16 or FP32.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2492,
    "tokens": 478
  },
  {
    "text": "Consider two Load instructions accessing the same cache line, with the data not present in cache. In the in-order cache simulation, 4 Concorde: Fast and Accurate CPU Performance Modeling with Compositional Analytical-ML Fusion Algorithm 1 A trace-driven state machine for memory for allùëêùëéùëê‚Ñéùëí_ùëôùëñùëõùëído State variable initialization exec_times[ùëêùëéùëê‚Ñéùëí_ùëôùëñùëõùëí] Execution times of load instructions accessing exec_times[ùëêùëéùëê‚Ñéùëí_ùëôùëñùëõùëí] ùëêùëéùëê‚Ñéùëí_ùëôùëñùëõùëífrom in-order cache simulation access_counters[ùëêùëéùëê‚Ñéùëí_ùëôùëñùëõùëí] 0 Number of accesses last_req_cycles[ùëêùëéùëê‚Ñéùëí_ùëôùëñùëõùëí] 0 Cycle of last request last_resp_cycles[ùëêùëéùëê‚Ñéùëí_ùëôùëñùëõùëí] 0 Cycle of last response end for function RespCycle(ùëüùëíùëû_ùëêùë¶ùëêùëôùëí,ùëñùëõùë†ùë°ùëü) ùëêùëéùëê‚Ñéùëí_ùëôùëñùëõùëí ùëñùëõùë†ùë°ùëü.ùëêùëéùëê‚Ñéùëí_ùëôùëñùëõùëí ùëüùëíùëû_ùëêùë¶ùëêùëôùëímust be non-decreasing for requests to the same cache line Assert ùëüùëíùëû_ùëêùë¶ùëêùëôùëí last_req_cycle[ùëêùëéùëê‚Ñéùëí_ùëôùëñùëõùëí] if is_load(ùëñùëõùë†ùë°ùëü) then Adjustment for load instructions only prev_resp_cycle last_resp_cycles[ùëêùëéùëê‚Ñéùëí_ùëôùëñùëõùëí] access_number access_counters[ùëêùëéùëê‚Ñéùëí_ùëôùëñùëõùëí] exec_time exec_times[ùëêùëéùëê‚Ñéùëí_ùëôùëñùëõùëí][access_number] resp_cycle max(ùëüùëíùëû_ùëêùë¶ùëêùëôùëí exec_time,prev_resp_cycle) last_resp_cycles[ùëêùëéùëê‚Ñéùëí_ùëôùëñùëõùëí] resp_cycle access_counters[ùëêùëéùëê‚Ñéùëí_ùëôùëñùëõùëí] else Nothing special for non-load instructions resp_cycle ùëüùëíùëû_ùëêùë¶ùëêùëôùëí estimated execution time of ùëñùëõùë†ùë°ùëü end if return resp_cycle end function the first Load is labeled as a main memory access (200 cycles), while the second Load is labeled as an L1 hit (4 cycles). Now suppose these Loads are issued at around the same time in the actual OoO core, e.g., first Load at cycle 0 and second Load at cycle 1.",
    "source": "2503.23076v1_Concorde_Fast_and_Accurate_CPU_Performance_Modelin.pdf",
    "length": 1518,
    "tokens": 929
  },
  {
    "text": "In many real-world applications, especially in IoT and edge computing, there is a critical need for smaller, energy-efficient models that can operate autonomously without reliance on batteries. These tiny, reusable devices contribute to reducing embodied carbon and represent a significant step toward sustainability. Moreover, we believe that advancing the capabilities of smaller models in intermittent environments is crucial for widespread adoption of sustainable, battery-free devices in various domains, including environmental monitoring, industrial IoT, and healthcare. By addressing the challenges of intermittent computing, our work contributes to the broader goal of enabling pervasive, sustainable intelligence at the edge. NExUME is especially advantageous in intermittent environments, and its utility extends to ultra- low-power or energy scavenging systems. However, the efficacy of DynFit and iNAS is contingent upon the breadth and depth of the available dataset. Additionally, profiling devices to ascertain their energy consumption, computational capabilities, and memory footprint necessitates detailed micro- profiling using embedded programming. This process, while informative, yields only approximate models that are inherently prone to errors. DynFit, with its stochastic dropout features, occasionally leads to overfitting, necessitating meticulous fine-tuning. While effective in smaller networks, our studies involving larger datasets (such as ImageNet) and more complex network architectures (like MobileNetV2 and ResNet) reveal challenges in achieving convergence without precise fine-tuning. DynFit tends to introduce multiple intermediate states during the training process, resulting in approximately 14 additional wall-time on average. The development of DynInfer requires an in-depth understanding of microcontroller programming and compiler directives. The absence of comprehensive library functions along with the need for computational efficiency frequently necessitates the development of in-line assembly code for certain computational kernels. 5 Conclusions This study presents NExUME, an advanced framework designed to optimize the training and inference phases of deep neural networks within the constraints of intermittently powered, energy-harvesting devices. By integrating adaptive neural architecture and energy-aware training techniques, NExUME significantly enhances the viability of deploying machine learning models in environments with limited and unreliable energy sources. The results from our extensive evaluations demonstrate that NExUME can substantially outperform traditional methods in energy-constrained settings, with improvements in accuracy and efficiency that facilitate real-world applications in remote and wearable technology.",
    "source": "NexUME.pdf",
    "length": 2796,
    "tokens": 469
  },
  {
    "text": "This involves a trade-off between computation- and communication energy-costs, which are dependent on local DNN efficiency, the number of participating devices, and the number of local iterations in each FL round. Most works on energy-aware FL focus on optimizing the energy costs of on-device network training and are largely based on static DNN optimizations. Q-FedUpdate [183] supports training with full integer quantization on energy-efficient mobile Digital Signal Processors (DSPs). These are found universally on mobile phones and most other IoT devices. Their use for DNN execution can yield 11.3x 4.0x reduction in energy usage over mobile CPU GPUs. Training is supported by maintaining a global full-precision (ùëñ.ùëí. FP32) network, which continuously accumlates network updates instead of having updates erased by quantization. Q-FedUpdate is integrated with efficient batch quantization as well as a pipelining-based approach to enable simultaneous CPU-based quantization and DSP training. The latter reduces computational overheads associated with repeated network quantization. Other works focus on reducing FL communication-energy costs; approaches include gradient quantization via SignSGD [191], gradient sparsification [37, 152, 162], and dynamic batch-sizing [87, 88, 133], as well as dynamically adjusting the number of local iterations between two global aggregation rounds [157]. In addition, energy-aware device scheduling can reduce communication costs. Sun et. al [138], for example, maximize the number of devices scheduled for gradient update at each iteration under an energy-constraint. Zeng et. al [186] use energy-efficient bandwidth allocation and scheduling to reduce the energy consumption of FL over wireless networks. Numerous recent works on energy-aware FL also concentrate on DNN partitioning to enable adaptivity towards heterogenous device energy budgets. Yang et. al [13] utilize layer-wise partitioning, partitioning local DNNs into shallow and deep-layers, with shallow layers updated more frequently in FL rounds as they capture more general features. Rong et al. [182] instead partition CNNs into subnetworks via width-wise partitioning, grouping filters into blocks and exchanging filter blocks among subnetworks periodically to ensure equal training across all devices while reducing training costs.",
    "source": "2505.12523v1_Energy-Aware_Deep_Learning_on_Resource-Constrained.pdf",
    "length": 2346,
    "tokens": 478
  },
  {
    "text": "The input and output sequence lengths are set to 1K:1K. Table 2: Perplexity results on Wikitext2, and zero-shot accuracy results on PIQA, Winogrande, and Hellaswag datasets with effective bitwidth of each quantization technique. Mistral Mixtral 7B 13B 70B 6.7B 13B 30B 7B 8x7B 7B 13B 70B 7B 13B 70B 7B 13B 70B 7B 13B 70B Task Metric Original 5.47 4.88 3.32 10.86 10.13 9.56 5.25 3.84 79.05 80.52 82.70 69.13 72.80 80.20 75.98 79.38 83.82 16.00 16.00 16.00 KVQuant 5.49 4.94 3.33 10.88 10.14 9.58 5.33 3.87 78.35 79.33 82.21 67.80 71.74 77.98 75.82 79.25 83.70 4.82 4.81 5.01 KIVI 5.50 4.90 3.33 10.88 10.16 9.58 5.34 3.84 78.07 79.05 78.07 67.84 70.96 76.81 75.57 78.97 83.47 4.99 4.99 4.99 Tender 6.42 5.74 4.25 11.80 11.05 10.44 5.54 NaN 74.27 76.12 77.91 62.90 65.69 74.59 73.89 77.16 75.04 4.07 4.07 4.10 Atom 5.62 4.98 3.37 11.01 10.22 9.64 5.42 4.05 76.17 76.99 81.34 66.46 67.09 75.77 72.22 76.21 80.52 4.25 4.25 4.63 QServe 5.67 5.12 3.36 10.95 10.28 9.62 5.42 4.03 77.37 77.48 81.77 65.29 66.80 76.09 74.41 76.69 83.24 4.25 4.25 4.25 Oaken 5.53 4.93 3.34 10.88 10.16 9.58 5.35 3.90 78.29 79.71 82.59 67.64 70.56 76.64 73.72 78.24 83.50 4.82 4.82 4.89 Effective Bitwidth Llama2 OPT Llama2 Llama2 Llama2 Llama2 PIQA Winogrande Hellaswag Model - Perplexity ( ) Accuracy ( ) Accuracy ( ) Accuracy ( ) Wikitext2 faces challenges in accommodating large models such as Mixtral- 8x7B and Llama2-70B, or handling large batches due to its insuffi- cient memory capacity.",
    "source": "2503.18599v2_Oaken_Fast_and_Efficient_LLM_Serving_with_Online-O.pdf",
    "length": 1469,
    "tokens": 636
  },
  {
    "text": "Task 3: arithmetic RTL code reasoning from netlist. Task and benchmark description. Task 3 arithmetic RTL code reasoning targets the reverse generation of RTL code from gate- level netlists, specifically for arithmetic circuits, as shown in Fig. 3. Unlike existing forward RTL code generation benchmarks [38], [39], which rely on human-readable specifications as input, our reverse task begins with post-synthesis netlists. This makes the task significantly more challenging due to the irreversible nature of logic synthesis and the lack of high-level functional information. We propose the first benchmark for reverse RTL code reasoning. Given the task s complexity, we begin by focusing on arithmetic blocks. Specifically, we extend the GNN-RE gate-level arithmetic function prediction dataset [36] into a generative benchmark, incorpo- rating golden RTL code and testbenches. For each of the 9 arithmetic designs in our benchmark, we provide: Netlist text. Gate-level netlist from GNN-RE dataset [36], originally used for gate function prediction task. Question prompt. Instructions to first infer the word-level arithmetic function, and then implement it using RTL code. Ground-truth RTL code. The original RTL design used for synthesis, which serves as the reconstruction target. Testbench. A verified testbench with predefined module name and IO ports, containing multiple input-output cases for func- tional validation. Evaluation metrics. Similar to the existing forward RTL genera- tion evaluation metrics [38], [39], we validate both syntax correctness and arithmetic functionality of the reversely generated RTL code using our provided golden testbenches. Additionally, GPT-4o is employed to assign a function similarity score, measuring how closely the generated RTL matches the ground truth. We provide detailed evaluation method implementation and prompts for obtaining GPT- score in Section VI. V. GENEDA ENCODER-DECODER FRAMEWORK Fig. 4 presents our proposed two paradigms for the encoder- decoder alignment, accommodating both trainable open-source LLMs and frozen commercial LLMs. Through alignment, GenEDA integrates rich netlist information captured by the encoder to enhance the generative reasoning capabilities of decoder LLMs. These two paradigms with their supported tasks are introduced in Section V-A and V-B, respectively.",
    "source": "2504.09485v1_GenEDA_Unleashing_Generative_Reasoning_on_Netlist_.pdf",
    "length": 2350,
    "tokens": 487
  },
  {
    "text": "For falling transitions, a separate arbiter is used, comprising a NOR SR latch and an AND gate to produce the comparison result and completion signal, respectively. Metastability may occur if two PDLs trigger output tran- sitions at nearly the same time. However, this can usually be resolved by increasing the delay difference between high- and low-latency nets of all delay elements, which improves resolution and ensures a sufficient gap between the transition arrival times, even when the two PDLs receive a nearly identical (but not the same) ones from their corresponding input vectors. Therefore, metastability may only occur if two PDLs receive equal number of ones (same Hamming weight). For certain ML operations like argmax, where two inputs are identical, the argmax function is designed to either arbitrarily select one input or consistently return a specific index. In both cases, this might be interpreted as an incorrect decision, as the result would essentially be based on a random or predetermined guess (basically, the result of inference may not match the class label in the training set)1. B. Implementation We present the FPGA implementation flow for the time- domain popcount design in Fig. 3. While some steps require manual intervention, these can be performed using the example 1The discussion of the interpretation of the classification metastability is outside the scope of this paper. It is worth noting that the problem of non- unique classification using argmax is sometimes mitigated by the techniques such as Softmax and Softermax [18]. 3 Tcl scripts provided as references. Each step is repeated for every delay element.",
    "source": "2505.02181v1_Efficient_FPGA_Implementation_of_Time-Domain_Popco.pdf",
    "length": 1655,
    "tokens": 328
  },
  {
    "text": "These include incorrect immediate values, 13 register mismanagement, and mismatched memory offsets that compromise program correctness despite appearing superficially similar to the ground truth. Together, Tables 7, 8, and 6 reveal that syntactic deviation does not necessarily imply failure. On the contrary, these examples support the argument that token-level metrics alone are insufficient to evaluate low-level transpilation tasks, and that functional correctness should take precedence in model assessment. 14",
    "source": "2506.14606v1_Guaranteed_Guess_A_Language_Modeling_Approach_for_.pdf",
    "length": 515,
    "tokens": 90
  },
  {
    "text": "Datasets Full Power Arduino on RF AP PT iNAS PT NExUME Better FMNIST 98.70 74.44 79.63 83.61 90.44 8.17 CIFAR10 89.81 58.11 63.91 65.01 79.60 22.44 MHEALTH 89.62 63.52 67.40 74.30 83.86 12.87 PAMAP 87.30 61.39 67.24 69.45 77.00 10.87 AudioMNIST 88.20 66.11 74.28 76.60 78.87 2.97 Table 6: Accuracy of NExUME on Arduino nano board using WiFi based RF harvester. Better refers to the improvement over iNAS PT baseline. 14 Datasets Full Power Arduino on Thermal AP PT iNAS PT NExUME Better FMNIST 98.70 77.04 80.44 83.08 89.90 8.20 CIFAR10 89.81 60.38 65.90 66.98 80.70 20.48 MHEALTH 89.62 65.74 69.88 72.41 85.75 18.42 PAMAP 87.30 62.76 65.93 71.46 81.27 13.73 AudioMNIST 88.20 69.12 73.86 77.79 83.54 7.39 Table 7: Accuracy of NExUME on Arduino nano board using thermocouple based thermal harvester. Better refers to the improvement over iNAS PT baseline. B Details on Energy Harvesting A typical energy harvesting (EH) setup captures and converts environmental energy into usable electrical power, which can then support various electronic devices. Here s a simplified breakdown of the process: 1. Energy Capture: The setup begins with a harvester, such as a solar panel, piezoelectric sensor, or thermocouple. These devices are designed to collect energy from their surround- ings light, mechanical vibrations, or heat, respectively. 2. Power Conditioning: Once energy is harvested, it often needs to be converted and stabilized for use. This is done using a rectifier, which transforms alternating current (AC) into a more usable direct current (DC). 3. Voltage Regulation: After rectification, the power might not be at the right voltage for the device it needs to support.",
    "source": "NexUME.pdf",
    "length": 1676,
    "tokens": 479
  },
  {
    "text": "RTL represents hardware structure and behavior. While typical software code defines sequential algo- rithms, RTL (e.g., Verilog) directly determines the structure and behavior of a digital circuit. For example, generating a natural language description of a Verilog counter requires not only explaining its behavior (incre- menting a counter) but also describing the underlying hardware, such as the 4-bit register and adder that op- erate synchronously on each clock cycle. 2. Distinct syntactic structure of RTL. For example, while state-updating operations in software are typi- cally expressed as sequential function calls or object state modifications, in Verilog, logic is typically de- fined within always blocks. These blocks execute con- currently, independent of each other, and are triggered by specific sensitivity conditions, such as clock edges or signal changes. Syntactic structures like this must be handled separately than typical software code. 3. Understanding RTL requires foundational knowl- edge in digital logic. Reading RTL requires a funda- mental understanding of hardware design and digital logic concepts. As a result, LLMs for RTL-to-NL tasks must also possess foundational knowledge of these con- cepts. Figure 4 illustrates how typical LLMs, which are primarily trained on software code, struggle to answer basic questions that require an understanding of digital logic design. 2.2. Impact of Interpretability in the Design Cycle As hardware designs continue to grow in scale and modular- ity (Ernst, 2005; Ziegler et al., 2017; Gomes et al., 2022), understanding the vast range of interconnected subsystems has become increasingly challenging. In this context, RTL interpretability can have a significant impact on the speed of the design cycle. 2.2.1. LEGACY CODE AND IP INTEGRATION Understanding code written by others is often cited as one of the most challenging and time-consuming aspects of engi- neering projects (Sun et al., 2023b; Oliveira et al., 2025), and chip design is no exception. Modern chip designs now often consist of many smaller subcomponents (IP) developed by individual teams or external vendors (Kim et al., 2020; ipx, 2023).",
    "source": "2504.08852v1_ML_For_Hardware_Design_Interpretability_Challenges.pdf",
    "length": 2183,
    "tokens": 465
  },
  {
    "text": "Wordcraft: story writing with large language models. In Proceedings of the 27th International Conference on Intelligent User Interfaces, pages 841 852, 2022. [178] Ted Zadouri, Ahmet √úst√ºn, Arash Ahmadian, Beyza Ermi s, Acyr Locatelli, and Sara Hooker. Push- ing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning. arXiv preprint arXiv:2309.05444, 2023. [179] Haibo Zhang, Shulin Zhao, Ashutosh Pattnaik, Mahmut T. Kandemir, Anand Sivasubramaniam, and Chita R. Das. Distilling the essence of raw video to reduce memory usage and energy at edge devices. In Proceedings of the 52nd Annual IEEE ACM International Symposium on Microarchitecture, MICRO 52, page 657 669, New York, NY, USA, 2019. Association for Computing Machinery. [180] Hengrui Zhang, August Ning, Rohan Baskar Prabhakar, and David Wentzlaff. Llmcompass: En- abling efficient hardware design for large language model inference. In 2024 ACM IEEE 51st Annual International Symposium on Computer Architecture (ISCA), pages 1080 1096, 2024. [181] Nan Zhang, Yanchi Liu, Xujiang Zhao, Wei Cheng, Runxue Bao, Rui Zhang, Prasenjit Mitra, and Haifeng Chen. Pruning as a domain-specific LLM extractor. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Findings of the Association for Computational Linguistics: NAACL 2024, pages 1417 1428, Mexico City, Mexico, June 2024. Association for Computational Linguistics. [182] Wei Zhang, Chen Liu, Xinyu Wang, and Jian Li. Efficient load balancing for distributed training of deep neural networks. In Proceedings of the IEEE International Conference on Big Data, pages 1234 1243. IEEE, 2019. [183] Yuanrui Zhang, Wei Ding, Mahmut Kandemir, Jun Liu, and Ohyoung Jang. A data layout optimiza- tion framework for nuca-based multicores.",
    "source": "NSF_LLM_Medium_Proposal.pdf",
    "length": 1777,
    "tokens": 483
  },
  {
    "text": "Input Embedding times due to database search, especially for long sequences. While FastFold [14] and ColabFold [43] attempt to ad- dress this issue, it still remains a bottleneck, highlighting ESM- Fold [39] as a strong baseline. Among the models that use protein language models for Input Embedding, including ESMFold [39], ColabFold [43], PTQ4Protein [51], and MEFold [32], LightNobel archives the best performance. This result is attributed to LightNo- bel s superior acceleration of the Protein Folding Block, which is a major bottleneck of the overall latency. Hardware Performance. We evaluate the performance of Light- Nobel hardware and NVIDIA A100 and H100 GPUs, focusing on the Protein Folding Block. We use CAMEO, CASP14, CASP15, and CASP16 datasets for the experiment. Figure 14(b) shows the nor- malized latency across datasets. LightNobel achieves 3.85-8.44 , 3.67-8.41 lower latency with the chunk option and 1.22 , 1.01 lower latency without the chunk option compared to A100 and H100. The chunk option significantly increases GPU latency due to kernel overhead from frequent kernel calls and returns, high- lighting LightNobel s advantage in handling long sequence lengths. Moreover, despite H100 s 5 higher INT8 computing resources compared to A100 (e.g., 3,026 TOPS vs. 624 TOPS), performance gains remain minimal due to the large portion of the PPM work- load being memory-bounded, leading to low utilization of compute resources [70]. Despite LightNobel having only 537 TOPS of compu- tational resources, it demonstrates significantly better performance compared to A100 and H100 under the same 2TB s bandwidth. These results demonstrate the superior performance efficiency of LightNobel and suggest that similar trends will be observed with the NVIDIA H200, the state-of-the-art GPU [47]. In experiments across the entire dataset, GPUs face out-of-mem- ory (OOM) issues. Therefore, for a fair comparison, we exclude the proteins that cannot be processed on GPUs without the chunk option and conduct experiments on the remaining datasets. The CAMEO dataset is excluded because it can already be fully pro- cessed without the chunk option.",
    "source": "2505.05893v1_LightNobel_Improving_Sequence_Length_Limitation_in.pdf",
    "length": 2159,
    "tokens": 496
  },
  {
    "text": "D. Batch Processing Workloads Analysis Table IV presents performance for high-throughput batch processing, measuring both maximum throughput (tokens second) and latency stability (coefficient of variation in token generation time) for different batch sizes. TABLE IV BATCH PROCESSING PERFORMANCE (LLAMA-2-70B, VARYING BATCH SIZES) For batch processing workloads, the results show a reversal of strengths compared to interactive workloads. NVIDIA Blackwell, Google TPU v7, and Cerebras WSE-3 achieve the highest throughput, with Cerebras showing particularly strong performance due to its massive on-wafer parallelism. Latency stability, measured by the coefficient of variation (CoV) in token generation times, shows substantial differences between archi- tectures. Groq LPU delivers exceptional determinism (CoV 0.02) due to its static scheduling approach, while more general-purpose archi- tectures show higher variability. The scaling efficiency metric reveals how well performance scales when using multiple accelerators in parallel, with Cerebras WSE-3 and Google TPU v7 demonstrating the best scaling characteristics due to their optimized interconnect designs. E. Cross-Architecture Insights Our quantitative analysis reveals several cross-cutting insights applicable to accelerator selection and future architecture development: 1. Memory bandwidth dominates for small batches: For batch size 1, performance correlates strongly with internal memory bandwidth (r 0.88), while compute capability becomes more significant at larger batch sizes. 2. Latency-throughput tradeoff persists: Despite architectural innovations, a fundamental tradeoff remains between minimizing latency and maximizing throughput, with specialized architectures like Groq LPU optimized heavily for one end of this spectrum. 3. Software matters as much as hardware: Performance variations of up to 40 were observed for the same hardware with differ- ent software stack versions, highlighting the critical importance of software optimization. 4. Energy efficiency varies dramatically: The most energy-efficient architecture for interactive workloads (Groq LPU) is 18.5 more efficient than the least efficient (Cerebras WSE-3), though this relationship inverts for high-throughput batch workloads. These insights emphasize the importance of matching accelerator selection to specific operational requirements, as no single architecture provides optimal performance across all inference regimes.",
    "source": "2506.00008v1_AI_Accelerators_for_Large_Language_Model_Inference.pdf",
    "length": 2471,
    "tokens": 477
  },
  {
    "text": "It replaces traditional SRAM in ultra-high- bandwidth applications like transformer inference, though it is not a general SRAM alternative. JETSTREAM: JET Surface Thermal Regulation via Evaporative Array Manifold. A two phase immersion cooling (2-PIC) system that incorporates a precise manifold to direct an array of liquid 2-PIC coolant jets to microchannel heatsink fins etched in the back surface of silicon chips. NEXAI: Neural Engine eXecution Accelerator for Inference. A high-performance, edge-optimized semiconductor IP block designed for transformer inference and AI workloads in power- and thermally constrained environments, derived from ZettaLith technologies. SCB: Silicon Circuit Board. A silicon substrate that serves as a circuit board, replacing traditional PCBs. An SCB is somewhat like a silicon interposer but is more robust and doesn t carry the high speed signals. SHAPE: Simple Hybrid Array of Processing Elements. A novel processing architecture that uses a SOTA Logic Die (SLD) - a high-density array of ultra-simple processing elements that can be full custom designed in a new process before the availability of standard cell libraries, SRAM, analog, mixed signal, or IP blocks. All circuits requiring these are on other die, hybrid bonded to the SLD die. Silicon springs: Micromechanical structures in silicon that provide thermal and mechanical stress relief. This stress relief can isolate sources of thermal and mechanical stress by orders of magnitude, effectively limiting propagated stress to small regions of approximately 1 cm2. SLD: SOTA Logic Die, part of a TRIMERA stack. A state- of-the-art semiconductor die manufactured in the most advanced available process node at the time (e.g., TSMC A16 or A14), containing digital logic circuits optimized for high performance and low power consumption. TRIMERA: TRIchip Module for Exascale Reasoning Applications. A high-performance 3D integrated circuit architecture consisting of three vertically stacked silicon dies that are hybrid bonded together to create a dedicated transformer inference accelerator. ZettaLith: A zetta-scale transformer inferencing system that combines a passive WSSCB with CASCADE arrays in SHAPE format in TRIMERA stacks, CREST fault tolerance, and advanced cooling. V-beam: A type of silicon spring design optimized for signal routing.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2347,
    "tokens": 491
  },
  {
    "text": "We explain instance pack- ing factor and its relationship with latency in Section 2.3.2. In this paper, we focus on improving the accuracy and latency from the model selection perspective and consider instances types from a cost perspective. A majority of the model serving systems [6,83,86] in public cloud support individual model selection from available models. For instance, InFaas [83] can choose variants among a same model to maintain accu- racy and latency requirements. However, denser models tend to have up to 6 the size and twice the latency of smaller models to achieve increased accuracy of about 2-3 . Besides using dense models, ensembling [15] techniques have been used to achieve higher accuracy. Why Ensembling? An Ensemble is deÔ¨Åned as a set of clas- siÔ¨Åers whose individual decisions combined in some way to classify new examples. This has proved to be more accurate than traditional single large models because it inherently re- duces incorrect predictions due to variance and bias. The commonly used ensemble method in classiÔ¨Åcation problems is bagging [33] that considers homogeneous weak learners, learns them independently from each other in parallel, and combines them following some kind of deterministic aver- aging process [18] or majority voting [49] process. For fur- ther details on ensemble models, we refer the reader to prior works [14,57,58,61,64,77,78,88]. 2.2 Related Work Ensembling in practice: Ensembling is supported by com- mercial cloud providers like Azure ML-studio [11] and AWS Autogluon [31] to boost the accuracy compared to single models. Azure initially starts with 5 models and scales up to Features Clipper [27] RaÔ¨Åki [80] Infaas [83] MArk [86] Sagemaker Swayam [34] Cocktail Predictive Scaling SLO Guarantees Cost Effective Ensembling Heterogeneous Instances Dynamic ensemble selection Model abstraction Table 2: Comparing Cocktail with other related frameworks. 200 using a hill-climb policy [17] to meet the target accuracy. AWS combines about 6-12 models to give the best possible accuracy. Users also have the option to manually mention the ensemble size.",
    "source": "cocktail.pdf",
    "length": 2115,
    "tokens": 477
  },
  {
    "text": "International Journal of Computer Vision 129(6), 1789 1819 (2021) [33] Wang, L., Yoon, K.-J. : Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks. IEEE transactions on pattern analysis and machine intelligence 44(6), 3048 3068 (2021) [34] Sepahvand, M., Abdali-Mohammadi, F., Taherkordi, A.: Teacher student knowl- edge distillation based on decomposed deep feature representation for intelligent mobile applications. Expert Systems with Applications 202, 117474 (2022) [35] Crowley, E.J., Gray, G., Storkey, A.J. : Moonshine: Distilling with cheap convo- lutions. Advances in Neural Information Processing Systems 31 (2018) 23 [36] Xiao, H., Fu, L., Shang, C., Bao, X., Xu, X.: A knowledge distillation compres- sion algorithm for ship speed and energy coordinated optimal scheduling model based on deep reinforcement learning. IEEE Transactions on Transportation Electrification (2024) [37] Itahara, S., Nishio, T., Koda, Y., Morikura, M., Yamamoto, K.: Distillation- based semi-supervised federated learning for communication-efficient collabora- tive training with non-iid private data. IEEE Transactions on Mobile Computing 22(1), 191 205 (2021) [38] Qu, X., Wang, J., Xiao, J.: Quantization and knowledge distillation for efficient federated learning on edge devices. In: 2020 IEEE 22nd International Conference on High Performance Computing and Communications; IEEE 18th International Conference on Smart City; IEEE 6th International Conference on Data Science and Systems (HPCC SmartCity DSS), pp. 967 972 (2020). IEEE [39] Luo, H., Chen, T., Li, X., Li, S., Zhang, C., Zhao, G., Liu, X.: Keepedge: A knowledge distillation empowered edge intelligence framework for visual assisted positioning in uav delivery.",
    "source": "2505.08793v1_Onboard_Optimization_and_Learning_A_Survey.pdf",
    "length": 1770,
    "tokens": 445
  },
  {
    "text": "IV. INTRA-FRAME COMPRESSION DESIGN As discussed in Sec. III-A, PCC takes several seconds to execute, which is signiÔ¨Åcantly higher than the ideal real- time demand (100ms). The main reason for this inefÔ¨Åciency is the sequential updates in the state-of-the-art octree- based algorithms [47] (illustrated in Fig. 2). Furthermore, we also observed in Sec. III-B that Morton codes can reveal opportunities for both geometry similarity (owing to the fact that the Morton code itself is the reÔ¨Çection of the geometrical relationship between points) and attribute similarity (the RGB attributes of two adjacent points are more likely to be similar). Unlike the prior octree-based works [56], [72] which mainly focus on the compression efÔ¨Åciency (i.e., attaining higher compression ratio and good quality simultaneously) with sequential updates and longer execution latency, in this work, we focus primarily on speeding up the PCC at the edge and achieving the real-time target mentioned above without losing much quality or compression ratio. A. Intra-frame Compression In this subsection, we Ô¨Årst present the state-of-the-art intra- frame geometry and attribute compression techniques and discuss their inefÔ¨Åciencies. We then introduce our proposed intra-frame geometry and attribute compression schemes which are discussed in detail in Sec. IV-B and Sec. IV-C. Figure 4: Intra-frame PCC pipelines. 1) Prior Intra-Frame Compression InefÔ¨Åciencies: State- of-the-Art Intra-Geometry Compression5: As discussed in Fig. 2 and Sec. III, most of the existing G-PCC tech- niques [56], [72] are based on octree data structure. We illustrate the generic pipelines (for 1 geometry compression, and for 2 attribute compression) employed by the state-of- the-art intra-frame compression techniques in Fig. 4 a and 4 b . SpeciÔ¨Åcally, the SOTA geometry compression pipeline includes Ô¨Åve stages which can be summarized as follows: Raw Frame (Input): The input raw PC frame contains several (usually millions of) points, carrying both geometry and attribute information.",
    "source": "PCcompress.pdf",
    "length": 2046,
    "tokens": 492
  },
  {
    "text": "Model Method example1 state example2 state example3 state example4 state example5 state Power (mW) Time (ns) Area (¬µm2) Power (mW) Time (ns) Area (¬µm2) Power (mW) Time (ns) Area (¬µm2) Power (mW) Time (ns) Area (¬µm2) Power (mW) Time (ns) Area (¬µm2) Original 0.042 1.21 833.0 0.056 2.25 549.4 0.052 1.35 589.6 0.055 2.18 597.1 0.055 2.18 597.1 GPT-3.5 0.043 1.27 870.6 0.056 2.25 549.4 0.052 1.35 589.6 0.059 2.17 972.3 0.055 2.18 597.1 GPT4o-mini 0.055 1.08 1021.1 0.062 2.23 579.2 0.063 1.08 714.9 0.055 2.18 597.1 0.053 2.18 634.7 GPT-4-Turbo 0.053 2.97 993.5 0.067 2.28 737.6 0.065 1.22 810.3 0.055 2.18 273.5 0.029 2.25 366.8 GPT-4o 0.053 2.97 1002.5 0.056 2.25 549.4 0.052 1.35 589.6 0.055 2.18 273.5 0.055 2.18 597.1 RTLCoder-DS 0.042 1.21 833.0 0.056 2.25 549.4 0.052 1.35 589.6 0.055 2.18 597.1 0.055 2.18 597.1 SymRTLO 0.047 0.36 534.4 0.024 1.17 271.0 0.023 1.15 268.5 0.024 2.17 273.5 0.026 2.18 270.9 Improvement( ) 11.90 70.25 35.85 57.14 48.00 50.67 55.77 14.81 54.46 56.36 0.46 54.1952 52.73 0.00 54.63 Original Compiler Opt. 0.041 2.85 564.51 0.035 2.24 316.11 0.038 1.23 358.77 0.044 2.19 451.59 0.045 2.19 451.59 SymRTLO Compiler Opt. 0.021 2.64 240.85 0.018 0.6 175.61 0.018 2.42 180.63 0.020 2.17 185.65 0.019 2.27 188.169 Improvement( ) 48.78 7.37 57.33 48.57 73.21 44.45 52.63 96.74 49.65 54.55 0.91 58.89 57.78 3.65 58.33 Table 4.",
    "source": "2504.10369v1_SymRTLO_Enhancing_RTL_Code_Optimization_with_LLMs_.pdf",
    "length": 1352,
    "tokens": 689
  },
  {
    "text": "For accelerating NSAI algorithm on six reasoning tasks featuring different difficulties. We can observe in Fig. 5 that NSFlow accelerator consistently outperforms other devices, offering 31 18 speedup over TX2 and NX, more than 2 over GPU, up to 8 speedup over TPU-like systolic array, and more than 3 speedup over Xilinx DPU on some standard workloads. Ablation study. To further showcase the scalability of NSFlow and validate the necessity of proposed DSE algorithm, in Fig. 6 we sum- marize the runtime of an NSFlow-generated architecture (32 32 8) w and w o the proposed mapping and hardware techniques, evaluated on a NVSA-like workload with varying vector-symbolic data propor- tions alongside a ResNet18. We observe that despite slight overhead caused by array partition when symbolic part is minimal ( 1 ), (1) with symbolic ratio going up NSFlow speedup against traditional systolic array grows steadily, reaching up to more than 7 when symbolic data occupies 80 of the memory. (2) The performance gain from our two-phase DSE algorithm (compared to only having array folding, or Phase I) can reach 44 when symbolic workload is balanced with NN (symbolic memory percentage 20 ). These findings highlight NSFlow s scalability to adapt to varying workloads and its efficiency in handling symbolic-heavy scenarios. VII. CONCLUSION To enable efficient NSAI for real-time cognitive applications, we propose NSFlow, the first end-to-end design automation framework to accelerate NSAI systems. NSFlow leverages the unique NSAI workload characteristics, explores dataflow and architecture design space, and generates scalable designs for FPGA deployment. We believe NSFlow paves the way for advancing efficient cognitive reasoning systems and unlocking new possibilities in NSAI. REFERENCES [1] J. Mao, C. Gan, P. Kohli, J. B. Tenenbaum, and J. Wu, The neuro- symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision, International Conference on Learning Representa- tions (ICLR), 2019. [2] C. Han, J. Mao, C. Gan, J. Tenenbaum, and J. Wu, Visual concept- metaconcept learning, Advances in Neural Information Processing Systems (NeurIPS), vol. 32, 2019.",
    "source": "2504.19323v2_NSFlow_An_End-to-End_FPGA_Framework_with_Scalable_.pdf",
    "length": 2190,
    "tokens": 498
  },
  {
    "text": "It spans 16 diverse GPU domains with execution-verified source and assembly pairs, providing a standardized testbed for future work in low-level translation and performance-aware code generation. Our contributions are summarized as follows: CASS Dataset. We introduce CASS, the first large-scale dataset for GPU transpilation, containing 70k semantically aligned Nvidia AMD pairs at both the source (CUDA HIP) and assembly levels (SASS RDNA3), covering 16 real-world GPU domains. CASS-Bench. We contribute the first evaluation benchmark for cross-architecture GPU translation, with 40 curated tasks across 16 domains, including functionally verified outputs and aligned CUDA HIP source and SASS RDNA3 assembly. CASS Models. We release domain-specialized CASS LLMs trained for cross-architecture code translation. Our 7B model achieves 95 source and 37.5 assembly accuracy, outperforming GPT-4o and Claude (0 ) on CASS-Bench. Crucially, 85 of translated assemblies preserve execution runtime and memory compared to native, confirming semantic and performance fidelity. CASS Dataset Pipeline. We designed a scalable pipeline for scraping, synthesizing, transpiling, and compiling CUDA HIP code into aligned host and device assemblies across Nvidia and AMD GPUs. The rest of the paper is organized as follows: 2 reviews prior work on Nvidia-to-AMD and assembly translation. 3 describes our data collection, conversion, and filtering pipeline. 4 analyzes dataset structure and coverage. 5 outlines model training and evaluation, with results and ablations in 6. Finally, 7 lists limitations and future work, followed by 8 concluding remarks. 2 Table 1: Comparison of Domain Characteristics across Different Datasets Domain ComputeEval Rodinia SHOC Poly Babel Ours Characteristics NVIDIA[19] Bench[20] [21] Bench[22] Stream[23] CUDA (source) SASS (assembly) RDNA3 (assembly) OpenCL (source) 2 Related Works In this section, we describe prior work in GPU translation efforts ( 2.1), assembly-level transpilation ( 2.2), and related benchmarks (and their shortcomings) in the space ( 2.3) 2.1 Translating from Nvidia to AMD The fragmentation of GPU software ecosystems has driven the need for robust CUDA-to-HIP translation tools.",
    "source": "2505.16968v3_CASS_Nvidia_to_AMD_Transpilation_with_Data_Models_.pdf",
    "length": 2223,
    "tokens": 487
  },
  {
    "text": "This process involves converting the feature maps produced by the neural network into binary representations that can be efficiently stored and processed in ACAM architecture. Two primary methods for this binary thresholding have been ex- plored; mean-based and median-based approaches. In conventional signal processing, median-based threshold- ing is often preferred for its robustness to outliers and ability to maintain a balanced distribution of values in the binary templates [14] [16]. However, our analysis (see Fig. 1) reveals that for neural network feature maps, mean-based thresholding consistently outperformed the median approach. In the mean- based method, the average value of each feature across all training samples is calculated to serve as the threshold, with feature values above the mean set to 1 and those below set to 0. While this approach is traditionally considered more sensitive to outliers, it can capture more nuanced distributions in the feature space which is advantageous in the context of neural network activations. Neural networks, particularly those em- ploying ReLU activations, produce feature maps characterised by significant sparsity, with numerous zero values. These zero activations substantially lower the mean threshold compared to the median, creating a more discriminative binary repre- sentation. The lower threshold enables the binary templates to capture subtle, non-zero activations that often carry important class-specific information, whereas a median-based approach might eliminate these features by setting a threshold that effectively treats many informative, low-magnitude activations as noise. Therefore, the mean-based method is more sensitive to the actual distribution of active features, preserving more relevant information for classification. Fig. 1. Comparison of mean (red) and median (green) thresholding for each feature output from the front-end classifier. We also explore the possibility of generating multiple templates for each class, specifically exploring configurations with one, two, or three templates per class. This approach acknowledges the variability within class distributions and aims to capture a more comprehensive representation of each class s feature space. By creating multiple templates, we can encompass different sub-clusters within a class, poten- tially leading to improved classification accuracy. The multi- template strategy is particularly beneficial for classes with high intra-class variability. In our implementation, these multiple templates are generated using a clustering approach on the feature representations of each class.",
    "source": "2502.10089v1_A_Hybrid_Edge_Classifier_Combining_TinyML-Optimise.pdf",
    "length": 2636,
    "tokens": 478
  },
  {
    "text": "4.1 Research Questions To systematically assess FuncGNN s performance and robustness in capturing logical structures across diverse circuit topologies, the following research questions (RQs) are investigated: RQ1:Does FuncGNN improve SPP performance with lower resource consumption? RQ2: Does FuncGNN enhance accuracy on the TTDP task? RQ3: Does FuncGNN stabilize training for heterogeneous AIGs? RQ4: Does FuncGNN mitigate feature convergence in deep layers? RQ5: Are FuncGNN s components necessary and effective? RQ1 evaluates whether FuncGNN achieves higher accuracy in SPP with reduced model com- plexity, leveraging efficient feature integration. RQ2 assesses FuncGNN s capability to capture J. ACM, Vol. 1, No. 2, Article 3. Publication date: May 2025. FuncGNN: Learning Functional Semantics of Logic Circuits with Graph Neural Networks 3:11 Fig. 3. NOT Proportion Distribution Histogram Table 1. The Statistics of AIG Datasets [17] Benchmark Subcircuits Node Level Gate Ratio (A N) EPFL 828 [52-341] [4-17] [1.27-30.4] ITC99 7,560 [36-1,947] [3-23] [1.16-8.56] IWLS 1,281 [41-2,268] [5-24] [0.90-22.2] Opencores 1,155 [51-3,214] [4-18] [1.08-14.29] Total 10,824 [36-3,214] [3-24] [0.90-30.4] fine-grained functional similarities in TTDP, as required for precise logic equivalence checking. RQ3 investigates whether FuncGNN stabilizes training across AIGs with diverse circuit topologies, addressing the challenge of structural heterogeneity. RQ4 examines whether FuncGNN overcomes feature convergence, ensuring robust capture of global logical structures in deep architectures. RQ5 validates the necessity and contribution of each component, such as hybrid aggregation and multi-layer integration, to the model s effectiveness.",
    "source": "2506.06787v1_FuncGNN_Learning_Functional_Semantics_of_Logic_Cir.pdf",
    "length": 1734,
    "tokens": 477
  },
  {
    "text": "This approach enables the embed- ding layer to be trained at GPU memory speed. AIBox [20] integrates GPU and SSD devices to store large embedding layers, implementing an SSD cache management system to provide low-latency access to embedding layers stored in SSDs. This allows industrial-scale recommendation models to be trained on a single node. In addition to caching methods, sharding schemes that split model parameters across multiple devices have also been extensively researched. RecShard [19] leverages multiple GPUs, placing frequently accessed data in the GPU s HBM and less frequently accessed data in unified virtual memory (UVM), which encompasses host DRAM and GPU HBM. By employing a cost model before runtime, RecShard effectively partitions the DLRM across GPU devices, significantly en- hancing training throughput. Similarly, AutoShard [44] uti- lizes a cost model to estimate table computation costs and incorporates deep reinforcement learning (DRL) to address the EMB partitioning challenge across multiple GPUs, achieving balanced sharding among GPUs. B. Tensor-train Decomposition In addition to the challenges posed by large EMB sizes in DLRM, the substantial vocabulary size in transformer models presents a significant hurdle in natural language processing (NLP). To address this, TT-embedding [42] was introduced to efficiently compress EMBs using tensor-train (TT) decom- position [31]. This method enables seamless integration into any deep learning framework and supports training via back- propagation. TT-Rec [32] applies the TT-embedding format to effectively compress EMBs in DLRM, introducing a scheme that initializes TT-core weights based on a sampled Gaussian distribution. It demonstrated negligible accuracy loss when tested on the CDA and Criteo Terabyte datasets. Similarly, EL-Rec [45] utilized the TT-format to compress the embedding layer and developed a TT-based training pipeline system. This approach mitigated host communication latency between the CPU and GPU, improving training performance on a single GPU. VI. CONCLUSION In this paper, we present SCRec, a scalable computational storage system with statistical sharding and TT decomposition for recommendation models. On the software side, SCRec leverages three-level statistical sharding to meet the high bandwidth requirements of DLRM by storing hot data in high- bandwidth memory devices and cold data in a low-bandwidth memory device.",
    "source": "2504.00520v1_SCRec_A_Scalable_Computational_Storage_System_with.pdf",
    "length": 2442,
    "tokens": 494
  },
  {
    "text": "To evaluate the model s performance, we apply both traditional ma- chine translation metrics BLEU (Papineni et al., Model BLEU-4 ROUGE-1 ROUGE-2 ROUGE-L METEOR Emb. Sim. GPT Score GPT-3.5 3.34 28.20 10.46 25.11 20.36 0.740 0.510 GPT-4o 4.59 29.26 11.48 25.74 22.78 0.761 0.549 o1-preview 3.73 28.00 10.39 24.98 20.48 0.748 0.535 CodeV-DeepSeek 3.05 25.14 9.78 23.25 20.23 0.705 0.495 CodeV-CodeQwen 2.80 24.91 8.27 22.75 21.07 0.747 0.499 DeepRTL-220m 13.06 37.56 19.85 34.72 34.37 0.806 0.600 DeepRTL-16b 12.85 37.43 19.34 34.63 33.09 0.802 0.597 Llama-3.1 2.68 25.37 10.39 23.75 17.16 0.730 0.430 DeepSeek-Coder 2.56 24.52 7.72 22.45 22.83 0.756 0.571 DeepRTL21st-Direct (Llama) 11.28 34.29 16.35 33.63 27.73 0.754 0.580 DeepRTL21st-Direct (DeepSeek) 12.07 36.37 17.78 33.78 28.56 0.767 0.602 DeepRTL21st (Llama) 13.34 37.74 19.54 34.76 33.46 0.798 0.594 DeepRTL21st (DeepSeek) 13.53 37.52 19.68 34.68 33.28 0.814 0.612 DeepRTL2 (Llama) 13.84 37.97 20.69 34.42 34.75 0.813 0.603 DeepRTL2 (DeepSeek) 13.96 37.93 20.73 34.34 34.74 0.820 0.616 Table 3: The performance evaluation for RTL code understanding.",
    "source": "2506.15697v1_DeepRTL2_A_Versatile_Model_for_RTL-Related_Tasks.pdf",
    "length": 1106,
    "tokens": 503
  },
  {
    "text": "For After Sequence Position 0 Position 1 Position 2 Position 3 Position n Base Model module [FRAG] [FRAG] Head 1 [FRAG] [FRAG] d Head 2 [FRAG] d _ Head 3 [FRAG] d _ f Head 4 [IGNORE] _ f lip Head 5 [IGNORE] f lip _ Head 6 [IGNORE] lip _ f Head 7 [IGNORE] _ f lop Head 8 [IGNORE] f lop [FRAG] Head 9 [IGNORE] lop [FRAG] [IGNORE] Head 10 [IGNORE] [FRAG] [IGNORE] [IGNORE] Before Sequence Position 0 Position 1 Position 2 Position 3 Position n Base Model module [FRAG] [FRAG] Head 1 [FRAG] [FRAG] d Head 2 [FRAG] d _ Head 3 [FRAG] d _ f Head 4 d _ f lip Head 5 _ f lip _ Head 6 f lip _ f Head 7 lip _ f lop Head 8 _ f lop [FRAG] Head 9 f lop [FRAG] [PAD] Head 10 lop [FRAG] [PAD] [PAD] Parallel Algorithm Step 1: Initialize the fragment mask has_frag_mask (Labels[1:,:] [FRAG]).sum(dim 0).bool() Step 2: Iterate over heads in reverse for i in range(Head number-1,0,-1): Identify non-[FRAG] positions temp_mask (Labels[i,:] ! [FRAG]) Update fragment mask has_frag_mask temp_mask if has_frag_mask.any() False: Early termination break Mask positions with [IGNORE] Labels[i, has_frag_mask] [IGNORE] ‚ë†Initialize the positions in the mask without [FRAG] to 0 ‚ë°Traverse reversely along the head dimension. If find [FRAG], update has_frag_mask by combining it with temp_mask using a logical AND operation, the position equals to 1 are set to [IGNORE] ‚ë¢Stop when has_frag_mask full of 0 - has_frag_mask: a boolean mask identifying positions with [FRAG] tokens across all heads - temp_mask: a boolean mask identifying positions in the current head without [FRAG].",
    "source": "2503.14153v1_Speculative_Decoding_for_Verilog_Speed_and_Quality.pdf",
    "length": 1550,
    "tokens": 504
  },
  {
    "text": "The average rank is defined as the arithmetic mean of the rank numbers being the most accurate method as 1. The data in the table, except for DFR, are from [8]. MLP FCN Res Encoder MCD Time- TWI DFR_ DFR_ Dataset Net CNN CNN ESN DRS DPRR ARAB 96.9 99.4 99.6 98.1 95.9 95.8 85.3 27.8 98.0 AUS 93.3 97.5 97.4 93.8 85.4 72.6 72.4 34.4 95.6 CHAR 96.9 99.0 99.0 97.1 93.8 96.0 92.0 33.9 96.2 CMU 60.0 100.0 99.7 98.3 51.4 97.6 89.3 93.1 100.0 ECG 74.8 87.2 86.7 87.2 50.0 84.1 73.7 67.0 88.0 JPVOW 97.6 99.3 99.2 97.6 94.4 95.6 96.5 62.4 97.8 KICK 61.0 54.0 51.0 61.0 56.0 62.0 67.0 60.0 80.0 LIB 78.0 96.4 95.4 78.3 65.1 63.7 79.4 30.0 78.3 NET 55.0 89.1 62.7 77.7 63.0 89.0 94.5 92.5 95.9 UWAV 90.1 93.4 92.6 90.8 84.5 85.9 75.4 26.2 86.2 WAF 89.4 98.2 98.9 98.6 65.8 94.8 94.9 90.2 99.0 WALK 70.0 100.0 100.0 100.0 45.0 100.0 94.4 100.0 100.0 Average Accuracy 80.3 92.8 90.2 89.9 70.9 86.4 84.6 59.8 92.9 Average Rank 5.92 2.25 3.08 3.42 7.58 5.50 5.92 7.08 2.50 Table 7. Synthesis results for different machine learning methods. Latency is in cycles. The dataset used is ARAB.",
    "source": "2504.11981v1_Hardware-Friendly_Delayed-Feedback_Reservoir_for_M.pdf",
    "length": 1075,
    "tokens": 499
  },
  {
    "text": "Table 2: Perplexity results on Wikitext2, and zero-shot accuracy results on PIQA, Winogrande, and Hellaswag datasets with effective bitwidth of each quantization technique. Mistral Mixtral 7B 13B 70B 6.7B 13B 30B 7B 8x7B 7B 13B 70B 7B 13B 70B 7B 13B 70B 7B 13B 70B Task Metric Original 5.47 4.88 3.32 10.86 10.13 9.56 5.25 3.84 79.05 80.52 82.70 69.13 72.80 80.20 75.98 79.38 83.82 16.00 16.00 16.00 KVQuant 5.49 4.94 3.33 10.88 10.14 9.58 5.33 3.87 78.35 79.33 82.21 67.80 71.74 77.98 75.82 79.25 83.70 4.82 4.81 5.01 KIVI 5.50 4.90 3.33 10.88 10.16 9.58 5.34 3.84 78.07 79.05 78.07 67.84 70.96 76.81 75.57 78.97 83.47 4.99 4.99 4.99 Tender 6.42 5.74 4.25 11.80 11.05 10.44 5.54 NaN 74.27 76.12 77.91 62.90 65.69 74.59 73.89 77.16 75.04 4.07 4.07 4.10 Atom 5.62 4.98 3.37 11.01 10.22 9.64 5.42 4.05 76.17 76.99 81.34 66.46 67.09 75.77 72.22 76.21 80.52 4.25 4.25 4.63 QServe 5.67 5.12 3.36 10.95 10.28 9.62 5.42 4.03 77.37 77.48 81.77 65.29 66.80 76.09 74.41 76.69 83.24 4.25 4.25 4.25 Oaken 5.53 4.93 3.34 10.88 10.16 9.58 5.35 3.90 78.29 79.71 82.59 67.64 70.56 76.64 73.72 78.24 83.50 4.82 4.82 4.89 Effective Bitwidth Llama2 OPT Llama2 Llama2 Llama2 Llama2 PIQA Winogrande Hellaswag Model - Perplexity ( ) Accuracy ( ) Accuracy ( ) Accuracy ( ) Wikitext2 faces challenges in accommodating large models such as Mixtral- 8x7B and Llama2-70B, or handling large batches due to its insuffi- cient memory capacity. Mistral-7B, Mixtral-8x7B, and Llama2-70B models employ grouped-query attention to reduce KV cache size, helping to alleviate bandwidth bottlenecks even without KV quan- tization.",
    "source": "2503.18599v2_Oaken_Fast_and_Efficient_LLM_Serving_with_Online-O.pdf",
    "length": 1592,
    "tokens": 674
  },
  {
    "text": "4 a and 4 b . SpeciÔ¨Åcally, the SOTA geometry compression pipeline includes Ô¨Åve stages which can be summarized as follows: Raw Frame (Input): The input raw PC frame contains several (usually millions of) points, carrying both geometry and attribute information. Only the geometry data are forwarded to the upper geometry compression pipeline. Octree Construction: With the input geometry data, the octree construction algorithm is invoked to add the points and update the tree (e.g., the maximum depth required for inclusion of a point, occupancy information for nodes, etc.) in a point-by-point fashion. This point-by-point update makes this stage difÔ¨Åcult to parallelize. Octree Serialization: After the octree has been constructed, the tree is traversed in a top-to-bottom manner, in order to extract the occupy bits for each node, and record them in a predeÔ¨Åned order (e.g., via depth-Ô¨Årst traversal), such that the decoder can recover the octree with these occupy bits as well as the serialization order. Note that this step is also time-consuming, as shown in Fig. 2. This is because that all the nodes in the tree are traversed sequentially. Entropy Encoding: To further compress the generated occupy bits vector, a typical encoding technique Entropy Encoding [35], [60] is employed. Compressed Geometry Stream (Output): The Ô¨Ånal com- pressed geometry output stream is ready to be stored in the memory or streamed over the network. State-of-the-Art Intra-Frame Attribute Compression: As shown in Fig. 4 b , to compress the attribute data, similar steps Raw Frame Input, Attribute Transform and Quantize, Entropy Encoding, and Compressed Attribute Output Stream are employed. Only the Transform and Quantize step differs from the geometry compression pipeline, which takes both the raw frame s attribute data as well as the constructed octree as its inputs. With these inputs, the Transform step 5We consider the octree-based technique [56], [72] and RAHT [14], [56] as SOTAs for geometry and attribute compression respectively. 287 Authorized licensed use limited to: Penn State University. Downloaded on August 10,2023 at 18:50:20 UTC from IEEE Xplore.",
    "source": "PCcompress.pdf",
    "length": 2159,
    "tokens": 501
  },
  {
    "text": "ACKNOWLEDGMENT We thank the anonymous reviewers for their helpful feedback and suggestions towards improving the paper content. This research is supported in part by NSF grants 1629915, 1763681, 1931531, 2008398, 2116962, 2122155, 2028929 and 2211018. REFERENCES [1] E. E. Aksoy, S. Baci, and S. Cavdar, Salsanet: Fast road and vehicle segmentation in lidar point clouds for autonomous driving, in 2020 IEEE intelligent vehicles symposium (IV). IEEE, 2020, pp. 926 932. [2] Andrei Frumusanu, The Snapdragon 888 vs The Exynos 2100: Cortex-X1 5nm, Who Does It Better? ly 3OF66Tw , 2021. [3] Apple Inc., , 2021. [4] S. Ayukawa, N. Tokudome, S. Enokida, and T. Nishida, Time- series lidar data superimposition for autonomous driving, Proc. of ICT-ROBOT, ThBT3, vol. 3, 2016. [5] J. B edorf, E. Gaburov, and S. P. Zwart, A sparse octree gravitational n-body code that runs entirely on the GPU processor, Journal of Computational Physics, pp. 2825 2839, 2012. [6] M. Beg, Y. C. Chang, and T. F. Tang, Performance evaluation of error resilient tools for mpeg-4 video transmission over a mobile channel, in 2002 IEEE International Conference on Personal Wireless Communications, 2002, pp. 285 289. [7] P. J. Besl and N. D. McKay, Method for registration of 3-d shapes, in Sensor fusion IV: control paradigms and data structures, 1992, pp. 586 606. [8] Charles Loop, Qin Cai, Sergio Orts Escolano, and Philip A Chou, JPEG Pleno Database: Microsoft Voxelized Upper Bodies - A Voxelized Point Cloud Dataset, jpeg.org pc microsoft . [9] Charles Thomson, Reality capture 101: point clouds, pho- togrammetry and LiDAR, , 2019.",
    "source": "PCcompress.pdf",
    "length": 1612,
    "tokens": 477
  },
  {
    "text": "acc. ppl. ROUGE1 ROUGE2 ROUGEL QLoRA 4.00 6.22 0.583 3.52 4.79 0.628 3.25 39.07 16.31 31.09 41.18 18.32 27.58 LoftQ 4.00 5.26 0.613 3.48 4.79 0.628 3.23 40.34 17.06 31.92 41.12 18.29 27.54 LowRA 4.00 5.25 0.612 3.48 4.79 0.628 3.23 40.27 17.18 32.06 40.95 18.12 27.54 QLoRA 3.00 7.13 0.566 4.56 6.06 0.588 3.88 17.60 2.68 13.93 15.34 1.12 10.44 LoftQ 3.00 6.87 0.571 4.42 5.91 0.591 3.79 37.23 14.34 29.30 40.47 17.75 26.88 LowRA 3.00 5.84 0.593 3.87 5.24 0.611 3.50 38.84 15.68 30.57 40.85 18.12 27.23 QLoRA 2.50 8.05 0.546 5.17 6.84 0.568 4.36 15.33 1.97 12.55 13.68 1.04 9.99 LoftQ 2.50 7.72 0.552 4.98 6.70 0.572 4.21 34.48 12.26 27.05 39.81 17.19 26.57 LowRA 2.50 6.23 0.582 4.11 5.51 0.601 3.64 37.69 14.76 29.53 40.88 18.06 27.01 QLoRA 2.25 8.67 0.534 5.59 7.31 0.588 4.64 16.37 2.22 12.84 11.90 1.32 10.25 LoftQ 2.25 8.22 0.543 5.24 6.96 0.564 4.46 32.71 10.94 25.37 39.36 16.87 26.29 LowRA 2.25 6.40 0.578 4.21 5.66 0.597 3.73 37.29 14.36 29.12 41.01 18.19 27.23 QLoRA 2.00 9.17 0.526 6.07 7.64 0.551 5.02 DNC 4.84 0.00 4.36 LoftQ 2.00 8.63 0.536 5.68 7.27 0.558 4.75 31.89 10.18 24.59 38.88 16.49 25.85 LowRA 2.00 6.60 0.574 4.35 5.79 0.593 3.84 36.75 13.93 28.61 40.15 17.48 26.67 QLoRA 1.90 LoftQ 1.90 LowRA 1.90 7.13 0.562 4.94 6.16 0.583 4.22 34.05 11.74 26.49 39.19 16.84 26.35 QLoRA 1.80 LoftQ 1.80 LowRA 1.80 7.50 0.553 5.24 6.48 0.575 4.59 33.29 11.19 25.85 39.20 16.69 26.07 QLoRA 1.75 LoftQ 1.75 LowRA 1.75 7.76 0.548 5.43 6.65 0.569 4.76 33.09 11.05 25.69 38.54 16.38 25.99 Table 2: Performance comparison of different methods on LLaMA-2-7B, LLaMA-2-13B, and BART-large.",
    "source": "2502.08141v1_LowRA_Accurate_and_Efficient_LoRA_Fine-Tuning_of_L.pdf",
    "length": 1591,
    "tokens": 823
  },
  {
    "text": "GNN-based solvers frame the coloring problem as a multi-class node classification problem and 4 employ unsupervised training by formulating a loss function. In contrast, the Tabucol technique searches the ground state energy solution by moving small steps and maintains a tabu list to avoid cycling around local minima. We implement Tabucol heuristics on Intel Xeon Gold 6330 pro- cessor and solve the graph coloring problem instances 200 times with 1000 iteration steps each. For accuracy comparison, we also run Ising and vectorized mapping framework with single-flip Gibbs sampling on Nvidia A100 Tensor Core GPU and solve the same problems 200 times with 1000 iteration steps each. The results comparing the described methods are shown in Table 1. It includes easy, medium, and hard problem instances labeled in work 41. The Ising approach per- forms worse by only being able to solve small and easy problem instances. Among GNN-based solvers, the GraphSage architecture (PI-SAGE) offers better solution accuracy but it suffers from longer training times 33. By contrast, proposed vectorized mapping gives competitive coloring results compared to Tabucol heuristics and PI-SAGE GNN while having slightly lower accuracy for hard-to-solve problem instances. Employing the described parallel tempering with vectorized mapping reduces the error up to 50 on these hard problems and therefore performs better than other methods. We implement the proposed vectorized architecture (Fig. 2) onto VCU118 FPGA to estab- lish the performance acceleration and energy efficiency benchmarks. The FPGA accelerator uses the memory-mapped IO interface used by software applications to program the problem weights and take the solution out (see Supplementary Section 3 for more details). The accelerator (Fig. 1) fetches the weight data from the memory and does neuron states-weight multiplication. The accumulated product is passed through a sigmoid activation implemented using LUT and further compared with a Linear Feedback Shift Register (LFSR) based random number generator (RNG) to get the updated node value. The architecture runs at 90 MHz clock frequency where only one node gets updated in each cycle. Further, it supports graph coloring problem sizes up to 256 nodes all-to-all connected with a maximum chromatic number of 16, equivalent to 1024 vectorized nodes in total. The framework achieves the same accuracy for dataset problems as the GPU-based vec- torized mapping (see Table 1).",
    "source": "2505.20250v1_Efficient_Optimization_Accelerator_Framework_for_M.pdf",
    "length": 2484,
    "tokens": 501
  },
  {
    "text": "3.4 Multi-Objective Optimization The Multi-Objective Optimizer iteratively explores both model and hardware search spaces, taking the ML model and hardware template as inputs. It utilizes the Ax platform Meta Platforms (2024) and BoTorch Ba- landat et al. (2020) to perform multi-objective Bayesian optimization via the qNEHVI algorithm Daulton et al. (2021). The optimization targets maximizing accuracy while minimizing latency, energy, and total carbon (embodied operational). It explores hardware parameters (Table 1) and model dimensions (Figure 3) within a compute constraint in terms of Tera Operations per Second (TOPS) based on publicly available edge accelerators. Our framework supports four optimization modes, all subject to a number of compute constraint: (1) Accuracy Total Carbon (given a latency constraint), (2) Accuracy Latency, (3) Accuracy Energy, and (4) Accuracy, Total Carbon, and Latency. 6 3.5 Outputs CATransformers outputs a combination of model and hardware configurations, which when used together will provide efficiency improvements. After Multi-Objective Optimizer module has identified carbon efficient model architectures that are pruned versions of our base model, we fine-tune our models to reach their final accuracy. Based on preliminary studies, our pruned models require significantly fewer training steps compared to what is used to pre-train the model in prior works Radford et al. (2021); Xu et al. (2024); Wu et al. (2023) to recover the accuracy loss from pruning. We train our CarbonCLIP models with the MetaCLIP Xu et al. (2024) dataset with 2 epochs, using only 40 of the training steps needed for pre-training in previous studies Radford et al. (2021); Xu et al. (2024); Wu et al. (2023). 4 Experimental Settings Model: We use CATransformers to optimize the CLIP-ViT-B 16 architecture, pre-trained on DataComp- 1B Gadre et al. (2023), for various metrics and use cases. We generate CarbonCLIP model and hardware configurations and compare them with (1) CLIP ViT-B 16 and (2) TinyCLIP, a state-of-the-art small CLIP model.",
    "source": "2505.01386v2_Carbon_Aware_Transformers_Through_Joint_Model-Hard.pdf",
    "length": 2071,
    "tokens": 486
  },
  {
    "text": "The SUMGNNs and MEANGNNs mentioned here are two fundamental variants of MPNNs that differ in their neighborhood aggregation functions, with SUMGNNs using summation-based aggregation and MEANGNNs employing mean-based aggregation. Additionally, we evaluate baseline models including GNN-DSE [32], HGP SAGE GF [15], IronMan-Pro [20], PNA-R [12], and PowerGear [14]. While PNA-R and PowerGear are primarily designed for post-implementation prediction tasks (PowerGear proposes GNN to predict power), we adapt these baseline models for post-HLS scenarios by modifying their input dimensions to align with the feature space of the GNN-DSE dataset and restructuring their architectures using the PyTorch Geometric framework. These adaptations enable competitive performance in post-HLS prediction tasks, demonstrating the models flexibility across different hardware abstraction levels. As shown in Table 1, the HLS prediction dataset primarily comprises benchmarks such as aes and gemm-blocked. GNN-DSE [32] employs a Redis-based service to implement an online dataset provisioning mechanism, ensuring strict separation between training and inference datasets while maintaining benchmark diversity across both data splits. As shown in Table 2, we conducted extensive experiments on this dataset and recorded the RMSE of models. RMSE is quared-error metric measuring prediction accuracy via root-averaged discrepancies between predicted and actual values, preserving data units. For latency prediction, CoGNNs(Œ≤, Œ±) achieves the best performance, followed by CoGNNs(Œ±, Œ≥) with prediction errors of 0.3557 and 0.4131, respectively. In contrast, CoGNNs predicts multiple targets within an integrated framework, achieving high accuracy overall. For DSP and FF resource prediction, CoGNNs(Œ≤, Œ±) 11 Running Title for Header Figure 5: Reduction rates of RMSE loss achieved by CoGNNs(Œ≤,Œ±) over other CoGNNs (For FF, we present the reduction rates of RMSE loss achieved by CoGNNs(Œ≤,Œ≤) over other CoGNNs). The bar chart quantifies each model s prediction error reduction ratios (PERR) relative to the top-performing model, while the line chart tracks the evolving PERR trend across evaluation metrics.",
    "source": "2504.19649v2_Intelligent4DSE_Optimizing_High-Level_Synthesis_De.pdf",
    "length": 2185,
    "tokens": 484
  },
  {
    "text": "To collect the training dataset, we performed the sparse op- erations SpMM and SDDMM on three distinct hardware platforms: an Intel Xeon Gold 6348 CPU with 1TB of RAM, an NVIDIA A100 GPU paired with an Intel Xeon Platinum 8358, and SPADE, a simulated sparse accelera- tor with 32 processing elements operating at 0.8GHz. To ensure practical feasibility across hardware platforms, the program configuration search space was constrained to 256 configurations for SPADE and approximately 300 configura- tions for SparseTIR (GPU). We gathered data samples using 1,500 matrices for each hardware platform, with up to 1,000 matrices used for model training under various scenarios and the remainder was set aside for validation. For each matrix, we randomly sampled 100 program configurations per hardware platform to have diverse and representative training datasets across all experiments. 6 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning Program Configuration Search Space for SPADE. The program configuration search space considered for the SPADE accelerator was derived from a combination of key tunable parameters including tiling, synchronization barriers, cache bypassing, and matrix reordering. As sum- marized in Table 1, the parameters for barrier insertion, cache bypassing, and matrix reordering are binary (i.e., ei- ther enabled or disabled). Tiling is controlled by three numerical parameters: the number of row panels, column panels, and the split factor. These values were chosen to resemble those explored in the original SPADE work (Gero- giannis et al., 2023), as those values were expected to show more significant performance deviations for differ- ent sparse matrices. Specifically, we used 4 values for row panels {4, 32, 256, 2048}, 4 values for column pan- els {1024, 16384, 65536, NUM MATRIX COLS} (where NUM MATRIX COLS depends on the input matrix) and 2 values for the split factor {32, 256}. Although COGNATE is designed to perform under limited data availability, we conducted extensive data collection to rigorously evaluate and justify its effectiveness. This in- cluded a range of experiments and ablation studies, some of which required performance data samples from up to 1,000 matrices for training. Altogether, this effort demanded ap- proximately 4 million CPU hours.",
    "source": "2506.00424v2_COGNATE_Acceleration_of_Sparse_Tensor_Programs_on_.pdf",
    "length": 2347,
    "tokens": 488
  },
  {
    "text": "iii. D (Detailed): Descriptions specific to the block following a top-down structure. If details are absent, they may be omitted; do not guess based on signal names. 2. Processes: Initially, we provide engineers with a set of descriptions generated by GPT-4 for reference. They are then expected to revise and enhance these GPT-generated descriptions using their expertise and relevant supplementary materials, such as README files and register tables. H: This module implements the UART FIFO's read and write logic. D: This Verilog module implements a parameterized RAM with configurable data width, depth, and memory mode. It supports both read and write operations, with the ability to pipeline the read and write signals based on the memory mode. The module includes combinational processes for generating the next memory state and a read multiplexer function for selecting the read data. It also has a sequential process for updating the memory and pipelining signals based on the clock and reset signals. module D_apb_uart_bcm57 ( clk, rst_n, init_n, wr_n, data_in, wr_addr, rd_addr, data_out ); ... H: This always block implements the logic of writing to the FIFO buffer when receiving a character or preparing for transmitting a UART character. M: This always block is used to implement the logic of writing to the RAM, according to the data width and memory depth. D: This Verilog code is designed to manage the update process for a memory array named 'next_mem' with conditional write operations. Initially, 'next_mem' is assigned the value of 'mem'. A write operation to 'next_mem' is only performed if 'write_en_n' is active low. The memory address matching 'write_addr' is updated with the value of 'write_data'. Here, data is written bit-by-bit to 'next_mem' in a nested loop construct, maintaining the exact sequence as in 'write_data'. The address pointer 'k' increments by 'DATA_WIDTH' after each iteration.",
    "source": "2502.15832v1_DeepRTL_Bridging_Verilog_Understanding_and_Generat.pdf",
    "length": 1924,
    "tokens": 446
  },
  {
    "text": "Achieves 28 utilization. 23 C.1.2 Exo Optimized Code Example void test(int8_t A[12544][256] , int8_t B[256][64] , int8_t C[12544][64]) { config_st ((64)); config_ex(WEIGHT_STATIONARY , NO_ACTIVATION , 1, false , false); config_ld ((64), 1.0f, 16, 2); config_ld ((256) , 1.0f, 16, 1); config_ld(0, 1.0f, 0, 0); uint32_t res 1 31; uint32_t a 0; uint32_t b 16 16 4 4 8 sizeof(int8_t) 16; for (int_fast32_t io 0; io 98; io ) { for (int_fast32_t i 0; i 8; i ) { mvin( 0, res ((i) (1024)) 16, (16), (16) ); mvin( 0, res ((i) (1024) 256) 16, (16), (16) ); mvin( 0, res ((i) (1024) (2) (256)) 16, (16), (16) ); mvin( 0, res ((i) (1024) (3) (256)) 16, (16), (16) ); for (int_fast32_t ko 0; ko 4; ko ) { mvin2( A[(16 i 128 io)][64 ko], a ((i) (4096) (ko) (1024)) 16, 16 (4) , (16) ); if (io 0) { if (i 0) { mvin3( B[(64 ko)][0], b ((ko) (4096)) 16, 16 (4) , (16) ); } } if (io 0) { if (i 0) { mvin3( B[(16 64 ko)][0], b ((ko) (4096) 1024) 16, 16 (4) , (16) ); } } if (io 0) { if (i 0) { mvin3( B[(32 64 ko)][0], b ((ko) (4096) (2) (1024)) 16, 16 (4) , (16) ); } } if (io 0) { if (i 0) { mvin3( B[(48 64 ko)][0], b ((ko) (4096) (3) (1024)) 16, 16 (4) , (16) ); } } preload(b ((ko) (4096)) 16, res ((i) (1024)) 16 0x40000000 , (16), (16), (16), (16)); compute_preloaded(a ((i) (4096) (ko) (1024)) 16, (( uint32_t)0), (16), (16), 16, 16); preload(b ((ko) (4096) 256) 16, res ((i) (1024) 256) 16 0x40000000 , (16), (16), (16), (16)); compute_preloaded(a ((i) (4096) (ko) (1024)) 16, (( uint32_t)0), (16), (16), 16, 16); preload(b ((ko) (4096) (2) (256)) 16, res ((i) (1024) (2) (256)) 16 0x40000000 , (16) , (16), (16), (16)); compute_preloaded(a ((i) (4096) (ko) (1024)) 16, (( uint32_t)0), (16), (16), 16, 16); preload(b ((ko) (4096) (3) (256)) 16, res ((i) (1024) (3) (256)) 16 0x40000000 , (16) , (16), (16), (16)); compute_preloaded(a ((i) (4096) (ko) (1024)) 16, (( uint32_t)0), (16), (16), 16, 16); preload(b ((ko) (4096) 1024) 16, res ((i) (1024)) 16 0x40000000 , (16), (16), (16), (16)); compute_preloaded(a ((i) (4096) (ko) (1024) 256) 16, (( uint32_t)0), (16), (16), 16, 16); preload(b ((ko) (4096) 1024 256) 16, res ((i) (1024) 256) 16 0x40000000 , (16), (16), (16), (16)); compute_preloaded(a ((i) (4096) (ko) (1024) 256) 16, (( uint32_t)0), (16), (16), 16, 16); preload(b ((ko) (4096) 1024 (2) (256)) 16, res ((i) (1024) (2) (256)) 16 0 x40000000 , (16), (16), (16), (16)); compute_preloaded(a ((i) (4096) (ko) (1024) 256) 16, (( uint32_t)0), (16), (16), 16, 16); preload(b ((ko) (4096) 1024 (3) (256)) 16, res ((i) (1024) (3) (256)) 16 0 x40000000 , (16), (16), (16), (16)); compute_preloaded(a ((i) (4096) (ko) (1024) 256) 16, (( uint32_t)0), (16), (16), 16, 16); ... Unrolling continues } mvout( C[(16 i 128 io)][0], res ((i) (1024)) 16, (16), (16) ); mvout( C[(16 i 128 io)][16], res ((i) (1024) 256) 16, (16), (16) ); mvout( C[(16 i 128 io)][32], res ((i) (1024) (2) (256)) 16, (16), (16) ); mvout( C[(16 i 128 io)][48], res ((i) (1024) (3) (256)) 16, (16), (16) ); } } fence(); } Figure 18: Example of hand-optimized matrix multiplication code from Ikarashi et al. [26], used as a baseline in Sec.",
    "source": "2505.18574v2_Autocomp_LLM-Driven_Code_Optimization_for_Tensor_A.pdf",
    "length": 3103,
    "tokens": 1420
  },
  {
    "text": "Automated Front-end Design Automating the design of general-purpose computer CPUs has been a pivotal research challenge since the 1950s, drawing the attention of AI pioneers like Turing and Church [10]. With the advancement of AI technologies, various methods such as decision trees [30], LLMs [32], and RL [29] have been attempted for automated circuit design. Nevertheless, the absence of well-defined formal representations of circuits limits the precision of existing methods, restricting current capabilities to circuits of roughly thousands of gates without guaranteeing accuracy at larger-scale circuits. To achieve automated design for large-scale processors and chips, we employ the module generation feedback loop based on automated verification and repair within the Hardware Design Agent to ensure functional correctness, while adopting 1 10 100 1000 10000 100000 1000000 10000000 100000000 80386 VAX 8600 A3010 QiMeng-CPU-v1 80486SX Intel Xeon 5160 QiMeng-CPU-v2 ARM A53 ARM A75 Performance comparison with human-designed commercial CPUs Fig. 5. Performance of QiMeng-CPU series (blue) compared with human- designed commercial CPUs (grey) on the official ARM CPU benchmark Dhrystone. The results show that QiMeng-CPU-v1 is comparable to Intel 486 (1990s CPU), while QiMeng-CPU-v2 is comparable to Arm Cortex A53 (2010s CPU). Binary Speculation Diagrams (BSD) as the circuit s graph- based representation [16]. BSD exhibits two key characteristics for combinational logic circuits: 1) design accuracy improves monotonically with the number of design nodes, and 2) accuracy asymptotically converges to 100 as the number of data sampling increases. The implementation initializes with a randomly generated BSD, based on the automated verification and repair feedback, and iteratively verifies the current BSD in a simulator. When errors are detected, the corresponding BSD nodes are repaired by Shannon expan- sion, thereby monotonically increasing the functional accuracy of the BSD. By iteratively cycling the automated verifica- tion and repair steps, the functional accuracy progressively converges to 100 .",
    "source": "2506.05007v1_QiMeng_Fully_Automated_Hardware_and_Software_Desig.pdf",
    "length": 2121,
    "tokens": 446
  },
  {
    "text": "The right figure shows accessibility scopes: on-chip registers are scoped to a thread; on-chip shared memory is visible to all threads in a block; and off-chip global memory is accessible by all threads on device. the feed-forward network (FFN) and expert-combine operations. Figure 7 show the chain of actor interactions, demonstrating how FlashDMoE enforces DMoE functional dependencies. Determining tile dimensions in FlashDMoE. Selecting appropriate tile dimensions in FlashDMoE is crucial to ensure efficient GPU utilization. An undersized tile underutilizes the GPU, while excessively large tiles create register pressure, causing performance-degrading register spills to local memory. After careful parameter sweeps, we choose tile dimensions of (128, 64). Our key insights are: increasing tile width significantly raises the register usage per thread, potentially triggering costly spills; increasing tile height without adjusting thread count increases workload per thread, harming performance. Raising the thread count per block beyond our fixed value of 128 threads reduces the number of concurrent blocks, negatively affecting SM occupancy. Larger thread-block sizes also increase overhead from intra-block synchronization (__syncthreads() barriers), further degrading performance. Thus, our chosen tile dimensions balance register usage, shared-memory constraints, and GPU occupancy to deliver optimal performance. 3.1 Task Abstraction for Computation Computational operators. The FFN operator is a standard position-wise feed-forward network widely used in Transformer architectures [5], composed of two linear transformations separated by a nonlinear activation œï (e.g., GELU or ReLU): FFN(x) W2 œï(xW1 b1) b2 (1) Here, W1 and W2 represent learnable weight matrices, and b1 and b2 are biases. The expert-combine operation, used in architectures like GShard [33] and DeepSeek [1], merges outputs from multiple experts by computing a weighted combination based on their affinity scores: Ci k X j 1 gi,e (2) hi k X j 1 gi,e Ci hk i (3) In these equations, i 0, S 1 represents an input token index, e Ei,k identifies the k-th expert selected for token i, and gi,e is the affinity score indicating how relevant expert e is for token i. Unified task abstraction.",
    "source": "2506.04667v2_FlashDMoE_Fast_Distributed_MoE_in_a_Single_Kernel.pdf",
    "length": 2270,
    "tokens": 501
  },
  {
    "text": "Figure 3 presents multiple similarity and correlation mea- surements between BiT (with Softmax) and SPS attention, demonstrating the validity of our SPS approximation. Finally, to further enhance COBRA s performance, we fix the searched thresholds Œª and fine-tune the model weights using the original training data to compensate for the information loss introduced by the SPS approximation. 4) SPS Results Analysis: Table I presents the accuracy performance of our binary COBRA model on the GLUE benchmarks [21], compared against the original BERT and other binary BERT variants. Our model outperforms both BinaryBERT [11] and BiBERT [12] by a significant margin, with only a less than 2 minor average performance drop compared to the state-of-the-art BiT [13]. Despite this small accuracy reduction, our model relies exclusively on true bi- nary operations and provides substantial hardware advantages, which will be further discussed in Section IV-C. We also evaluate different granularities of the SPS threshold. The attention layer-wise threshold performs adequately, with minimal overhead and the shortest search time. However, as previously discussed, the head-wise threshold yields the best performance, incurring similarly negligible overhead com- pared to the layer-wise case, and the search can be completed relatively quickly. Additionally, we conducted experiments using a more fine-grained row-wise threshold at the attention map.",
    "source": "2504.16269v2_COBRA_Algorithm-Architecture_Co-optimized_Binary_T.pdf",
    "length": 1443,
    "tokens": 294
  },
  {
    "text": "Equation 2,4 denotes the latency on the DIMM-PIM, which contains the latency of decoding MHA, QKV transferring for de- coding, and the overlapped transferring of prefilling requests KV. Scheduling algorithm. The scheduling algorithm constructs sub- batches for each iteration with the following steps: (1) As long as there is space in the host memory, decoding re- quests are fetched from the decoding queue (Fig. 4), which will be processed in this iteration. (2) These decoding requests are divided into two sub-batches, en- suring that the total number of tokens in the two sub-batches is as even as possible, thereby making the computation time for decoding MHA as even as possible. For example, four requests with 2k, 3k, 4k and 5k tokens can be divided to (2k, 5k) and (3k, 4k). (3) Prefilling requests are incrementally added to sub-batch 1 to gradually increaseùëáGPU1 untilùëáGPU1 ùëáPIM0, at which point the process stops. Similarly, prefilling requests are added to sub-batch 0 until ùëáGPU0 ùëáPIM1. (4) The last prefilling request of each sub-batch is chunked to make ùëáGPU as close as possible to the ùëáPIM of the other sub- batch. The chunk size is a multiple of 16, better aligning with the requirements of GPU tiling. After this iteration ends, the chunked prefilling request is placed at the head of the prefilling queue. Runtime profiling and modeling. Now we describe how L3 mod- els the latencies of various inference operations. First, to model ùëáPIM, since the latencies of ùë°ùëë, ùë°comm and ùë°overlap are almost linearly related to the number of computed transferred tokens, we can use a linear model for prediction, which is simple and fast. Second, to model ùëáGPU, we leverage Random Forest Regression (RFR) for its three advantages: capability of incremental learning, low latency, and high accuracy.",
    "source": "2504.17584v1_L3_DIMM-PIM_Integrated_Architecture_and_Coordinati.pdf",
    "length": 1808,
    "tokens": 472
  },
  {
    "text": "Details are listed in Table 3. Workloads. We use four real-world LLM inference datasets: OpenR1- Math-220K [5], Dolphin-r1 [4], OpenThoughts-114k-math [6] and LongBench [12]. All of these datasets are used for evaluating the performance of LLM inference on long sequences. The details about the traces are shown in Table 4. 6.2 End-to-end Performance We evaluate the end-to-end throughput for each LLM model and workload using various baselines. For each evaluation, we randomly select and execute 1,000 requests from the traces. We measure the overall throughput by dividing the total number of output tokens by the total execution time. Throughput with real-world traces. Fig. 8 presents the normal- ized throughput of L3 compared with four baselines. The results show that L3 achieves the highest throughput over all baselines with various settings. Specifically, L3 achieves up to 6.1 higher throughput than the HBM-PIM baseline, 5.0 than the GPU-only baseline, and 9.2 higher than R-PIM baseline. These improve- ments are attributed to the following reasons. First, L3 achieves much larger batch sizes compared to the HBM-based baselines. For example, for GPT-175B, L3 has 2 TB of memory on the host memory for KV cache storage, while GPUs have only about 310 GB of memory in total after deploying the model. Second, L3 enables parallel execution of prefilling and decoding. In contrast, the HBM- PIM baseline suffers from PIM bubbles if prefilling requests are added to a batch. Third, the CPU offloading baseline suffers from a performance bottleneck due to the limited channel bandwidth. Finally, compared with R-PIM, L3 achieves fewer bubbles on both devices and offers about 8 bandwidth with bank-level PIM. We observe that HBM-PIM s sub-batch strategy demonstrates promising efficiency in bubble reduction, though our analysis re- veals certain limitations when processing Dolphin trace on OPT- 66B. It faces challenges in efficiently utilizing GPU resources when handling inherently small batch sizes, potentially leading to GPU- side execution bottlenecks that may result in performance be- low conventional GPU-only implementations.",
    "source": "2504.17584v1_L3_DIMM-PIM_Integrated_Architecture_and_Coordinati.pdf",
    "length": 2146,
    "tokens": 478
  },
  {
    "text": "The host writes the latest copy of the completed iteration (in epoch granularity) into the STT- RAMs (STT-RAM-N for the upper 128 SAs, and STT-RAM- S for the lower 128SAs, Fig. 5a). In case of a complete power failure, the compute in Ô¨Çight are rejected and, once the system starts working, the work queues get invalidated and the host starts the compute again from the last checkpoint. Along with that, the most common intermittent software libraries and software designs [26], [52] (and most DNN training libraries like PyTorch, TensorFlow) also offer periodic checkpoints. Note that the power-up sequence for a tile runs in the exact opposite order of the powerdown sequence (a tile becomes computationally active 512 cycles after it gets the power up signal). Us. as uses two kinds of scheduling policies to handle the graceful powerdown and work queue rearrangement. Conservative Scheduling: The most important part of the Us. as accelerator design is to ensure proper compute place- ment even under a power emergency or power scaling. Fig. 6:Accelerator level provides a high-level overview of the compute scheduling (where the redacted part of the hard- ware is turned off because of the lack of power). The key components of the scheduler are the moving average power predictor and the micro-proÔ¨Åler . In the ith kernel scheduling iteration, given the power budget and power prediction, the micro-proÔ¨Åler decides the required training conÔ¨Åguration, and the control logic (conservatively) enables suitable number of tiles (say ti tiles of the 256 tiles). Those ti tiles fetch ti unique kernels from the 1Byte wide, 256 deep global kernel dispatch queue (GKDQ, ti kernels scheduled in parallel ). Note that the power requirement of each tile is known in advance (please refer to V, TABLE I for details). Once the scheduled (ti) tiles are completed, the micro-proÔ¨Åler again Ô¨Ånds the right conÔ¨Åguration for the i 1th iteration and the scheduler again conservatively enables ti 1 number of tiles suitable for the power budget. The ti 1 tiles fetch the next ti 1 kernels from the GKDQ and the process continues.",
    "source": "Usas.pdf",
    "length": 2112,
    "tokens": 481
  },
  {
    "text": "EVALUATION A. Experiment Setup We established an experimental environment using a GPU server and cycle-accurate simulators to evaluate the effective- ness of implementing DLRM in SCRec, with detailed specifi- TABLE III DLRM EVALUATION DATASET SPECIFICATION Dataset Criteo Kaggle Meta Synthetic 2021 2022 MLP Layer O X X of EMB Table 26 856 788 Avg. EMB Rows 1,298,560 2,720,716 4,841,017 Avg. PF 1 8.34 13.6 of Data Sample 45,840,617 65,536 131,072 cations outlined in Table II. DLRM training and inference were conducted on a server equipped with Nvidia A40 GPUs to assess accuracy based on Meta DLRM [34] when applying the TT-format to the embedding layer. Additionally, we utilized Ramulator [35] and MQSim [36], simulators designed to evaluate the performance of DRAM and SSD, respectively, to measure processing performance during SCRec s runtime operation. To compare SCRec with a multi-GPU system, we measured the power consumption of SSD and DRAM using SimpleSSD [37] and VAMPIRE [38]. FPGA power consump- tion was profiled using the Xilinx Vivado power analysis tool to evaluate the power of the implemented core logic. We also designed an in-house cycle-accurate core simulator, similar to the memory simulators, to measure performance for simulating the hardware design implemented on the FPGA chip. To synchronize the memory simulators and the core simulator, modifications were made to enable transmission and reception of control signals and data between simulators via inter-process communication (IPC) using the TCP protocol [39]. This synchronization ensured accurate emulation of real- time system behavior by aligning the simulators at each cycle. Regarding the datasets used for evaluation, large-scale in- dustrial DLRM datasets are not publicly available. Therefore, we utilized the Criteo Display Advertising (CDA) dataset [40] and the MELS dataset [33]. As the characteristics of these datasets differ, as shown in Table III, the appropriate dataset was selected based on the evaluation metrics. Specifically, the CDA dataset supports the evaluation of the entire DLRM, including both the embedding and MLP layers.",
    "source": "2504.00520v1_SCRec_A_Scalable_Computational_Storage_System_with.pdf",
    "length": 2138,
    "tokens": 492
  },
  {
    "text": "Experiments demonstrate that our testbench achieves 64.5 fewer false negatives than the LLM-generated counterpart and detects 62.5 more injected errors in fuzzing tests for sequential circuits. Detailed experimental results are presented in Section 3.3.4. (2) Round-trip data synthesis for high-quality NL code pairs. Leveraging our testbench generation framework, we propose the round-trip data synthesis approach that can automatically synthesize high- quality NL code pairs from code snippets. Specifically, candidate code snippets are first paired with LLM-generated NL descriptions, and then verified by regenerating the code from NL and comparing against the original for equivalence with our testbench. Only code that passes the testbench is retained and combined with the NL to form high-quality data for reinforcement learning. We theoretically prove that, given strong LLMs and an ideal verification environment, this procedure yields NL code pairs of sufficiently high quality for RLVR with a high probability. (3) Two-stage training with adaptive DAPO for cost-effective RLVR. We adopt a two-stage distill-then-RL training pipeline to cold start LLMs reasoning ability through SFT and reduce the overall training cost. Specifically, we use DeepSeek-R1 as the NL-to-code LLM in our round-trip data synthesis to produce (NL, Thought, Code) triplets, based on which we perform SFT on our base LLM to obtain a distilled LLM with basic reasoning ability. Then, we apply RLVR on the distilled LLM using the equivalence-checked high-quality data to further enhance its Verilog generation capability. Additionally, recognizing that RLVR s bottleneck lies in sampling and verification [20], we extend dynamic sampling policy optimization (DAPO) [42] with an adaptive mechanism that dynamically adjusts the number of samples per training step based on past sample discard rates. This approach significantly reduces unnecessary sampling and verification overhead, thereby achieving a 1.25x acceleration. Based on these techniques, we develop CodeV-R1-7B, the first specialized reasoning LLM for Verilog generation with only around 2,656 A100-GPU-hours. On the VerilogEval v2 [27] and RTLLM v1.1 v2 [19] benchmarks, CodeV-R1-7B achieves 68.8 and 72.9 68.0 respectively.",
    "source": "2505.24183v2_CodeV-R1_Reasoning-Enhanced_Verilog_Generation.pdf",
    "length": 2269,
    "tokens": 495
  },
  {
    "text": "Given a template T with upper and lower bounds [T L, T U], the distance score D for features outside the template bounds is: D(Q, T) N X i 1 (Qi T U i )2 if Qi T U i (T L i Qi)2 if Qi T L i 0 otherwise (9) The hit ratio H measures the proportion of features falling within the template bounds: H(Q, T) 1 N N X i 1 (T L i Qi T U i ) (10) The final similarity score combines both metrics: Ssim(Q, T) H(Q, T) 1 Œ±D(Q, T) (11) where Œ± is a scaling parameter that controls the influence of the distance penalty. For both methods, the final classification decision C for a query Q across M classes is determined by: C(Q) arg max j {1,...,M} S(Q, Tj) (12) where S represents either Sfc or Ssim depending on the chosen matching method. III. RRAM-CMOS ACAM HARDWARE DESIGN AND OPERATION In this section, we are introducing the hardware description of the RRAM-CMOS ACAM system used to replace the final dense layers in conventional DNNs. The specific technology employed for this ACAM is called Template piXeL (TXL) and depending on the specific requirements of each edge application, different circuit and or system implementations can be used. Each and every implementation discussed is used IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL., NO., FEBRUARY 2025 6 Feature Extraction Front-End Feature Map Vector Classification Output Convolutional Neural Network Input Similarity Comparison Memory-Centric Classifier (Back-End) Analogue Back-End Edge Classifier Image Capture Front-End Fig. 2. Combined Deep Neural Network (DNN), for feature extraction and dimensionality reduction, with Analogue Content Addressable Memory (ACAM) back-end network [19], for final classification of the extracted feature map through analogue information processing. to implement specifically the last similarity comparison back- end system, as shown in Fig. 2. A. ACAM Architecture and Background Template matching hardware accelerators can be used as energy efficient classification engines closely integrated with edge sensors.",
    "source": "2502.10089v1_A_Hybrid_Edge_Classifier_Combining_TinyML-Optimise.pdf",
    "length": 2012,
    "tokens": 475
  },
  {
    "text": "5 3.4 Additional Baseline Policies In order to justify our precision assignment policy, we compare with multiple potential baseline methods for selecting which blocks to retain in higher precision (the comparison is provided in Figure 6 in Section 5.3). Our first baseline policy (referred to as Quantization Error ) determines which blocks to retain in higher precision using only the quantization error for each block within a single layer ùëô. The error when a 1D block v of size ùëÅis quantized in low precision ùëùùëôrather than high precision ùëù‚Ñécan be expressed as: ùêº ùëÑùê∏(v) ùëÅ ùëñ 1 (Œîùëù‚Ñé ùëùùëôùë£ùëñ)2 (12) where Œîùëù‚Ñé ùëùùëôgives the increase in error from quantizing ele- ment ùë£ùëñto low precision rather than high precision. As a second baseline, we also include an Output Error policy, which minimizes the error at the layer output for layer ùëôin order to determine which blocks to retain in higher precision for that layer. For this policy, we first calibrate for the average squared input channel magnitude across the other tensor Q. For quantizing a block v in weight matrix W(ùëô), we compute the average squared magnitudes for the corresponding input channels in X(ùëô), and for quantizing a block v in activation matrix X(ùëô) we compute the average squared magnitudes for the corresponding input channels in W(ùëô). We compute the average squared magnitudes for the input channels in X(ùëô) statically using calibration data from the Wikitext- 103 training set.",
    "source": "2504.14152v1_FGMP_Fine-Grained_Mixed-Precision_Weight_and_Activ.pdf",
    "length": 1441,
    "tokens": 387
  },
  {
    "text": "On subsequent passes, the sums for each batch are accumulated in the output sum HILTs. The output sum accumulation mechanism reads the output sum HILTs as described above, and: 8 h00 Œ£ 8 8 8 8 8 8 8 Œ£ 8 8 8 8 Œ£ 8 8 8 8 8 8 8 Œ£ 8 8 8 8 8 8 8 D Q D Q 8 8 Œ£ 8 8 D Q D Q 8 8 Œ£ 8 8 D Q D Q Œ£ 8 8 8 8 8 8 8 8 D Q HILT SLD Œ£ Acc X 4 8 5 8 4 4 4 W Act 8 Œ£ Acc X 4 8 5 8 4 4 4 W Act 8 Œ£ Acc X 4 8 5 8 4 4 4 W Act 8 Œ£ Acc X 4 8 5 8 4 4 4 W Act 8 Œ£ Acc X 4 8 5 8 4 4 4 W Act 8 Œ£ Acc X 4 8 5 8 4 4 4 W Act 8 Œ£ Acc X 4 8 5 8 4 4 4 W Act 8 Œ£ Acc X 4 8 5 8 4 4 4 W Act 8 Œ£ Acc X 4 8 5 8 4 4 4 W Act 8 WD(252-255) ActClk(24576) Activation HILT(24576) Activation broadcast latch tree 512 32 4 8192 131072 WD(248-251) ActClk(24575) Activation HILT(24575) WD(244-247) Activation HILT(24574) Activation broadcast latch tree 512 32 4 8192 131072 1024 64 8 16384 262144 128 8 8 SIPO FIFO Output sum HILT(1) 1024 64 8 16384 262144 128 8 8 SIPO FIFO Output sum HILT(2) 1024 64 8 16384 262144 128 8 8 SIPO FIFO Output sum HILT(3) 1024 64 8 16384 262144 8 8 Output sum HILT(1) CASCADE array adder CREST multiplexer previous-array column segment latch current-array column segment latch output sum read latch output sum write latch output sum recirculating adder Figure 9 The bottom of CASCADE array 384 and some of the 8,192 output sum memories 23 latching the stored value in the output sum read latch; adding the current CASCADE column sum using the output sum recirculating adder; latching the result in output sum write latch; and converting the calculation frequency from the SLD frequency to the HILT frequency using the output sum write SIPO FIFO. For consistency with the remainder of the SLDS, the recirculating sum mechanism can be moved from the SLD to the HILT.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 1747,
    "tokens": 570
  },
  {
    "text": "[4] L. Brown. Accelerate Machine Learning with the cuDNN Deep Neural Network Library, Sept. 2014. [5] T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Yan, H. Shen, M. Cowan, L. Wang, Y. Hu, L. Ceze, C. Guestrin, and A. Krishnamurthy. TVM: An Automated End-to-End Optimizing Compiler for Deep Learning. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), pages 578 594, 2018. [6] Confiance AI. Un collectif franc ais d envergure in edite pour concevoir et industrialiser des syst emes a base d intelligence artificielle de confiance, 2023. Accessed: 2023-05-01. [7] I. Dimitrovski, I. Kitanovski, D. Kocev, and N. Simidjievski. Current trends in deep learning for Earth Observation: An open-source bench- mark arena for image classification. ISPRS Journal of Photogrammetry and Remote Sensing, 197:18 35, Mar. 2023. [8] X. Feng, Y. Jiang, X. Yang, M. Du, and X. Li. Computer vision algo- rithms and hardware implementations: A survey. Integration, 69:309 320, Nov. 2019. [9] K. Guo, S. Zeng, J. Yu, Y. Wang, and H. Yang. [DL] A Survey of FPGA-based Neural Network Inference Accelerators. ACM Transactions on Reconfigurable Technology and Systems, 12(1):2:1 2:26, Mar. 2019. [10] D. Karapetyan, S. Mitrovic Minic, K. T. Malladi, and A. P. Punnen. Satellite downlink scheduling problem: A case study. Omega, 53:115 123, June 2015. [11] D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization, Jan. 2017. arXiv:1412.6980 [cs]. [12] B. B. K ovari and E. Ebeid.",
    "source": "2503.08700v1_Real-Time_Semantic_Segmentation_of_Aerial_Images_U.pdf",
    "length": 1492,
    "tokens": 495
  },
  {
    "text": "The results in Figure 8 and Table 5 show that SFCI-NDP suffers a performance drop compared to SFCI, showing that this prefix provides an effective guidance for model s decision making. 6.2. SFCI without Component-Type Tokens in Output To validate the design choice of keeping component-type tokens in the output of SFCI, we compare SFCI against a variant SFCI-NCT that removes them from the output as 8 LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog Topology Generation VIN GND VOUT S G D D S G B B VDD PMOS NMOS LM input: {Performance specification} sep VDD VIN GND PMOS 0 S PMOS 0 G PMOS 0 D PMOS 0 B NMOS 1 S NMOS 1 G NMOS 1 D NMOS 1 B sep LM output: {parameters to be set} sep VDD PMOS 0 S PMOS 0 B , VIN PMOS 0 G NMOS 1 G , GND NMOS 1 S NMOS 1 B , VOUT PMOS 0 D NMOS 1 D sep Figure 9. The example of extending SFCI into a transistor-based inverter that contains an NMOS device and a PMOS device, each has four distinct device pins: drain (D), gate (G), source (S), and body (B). We view an NMOS with the identifier 0 as four nodes: NMOS 0 D, NMOS 0 G, NMOS 0 S and NMOS 0 B. In addition, a PMOS device with the identifier 1 is viewed as four nodes: PMOS 0 D, PMOS 0 G, PMOS 0 S and PMOS 0 B. Then, we base on all nodes to construct our inverter. shown in Figure 4. SFCI-NCT yields worse performance according to both Figure 8 and Table 5. These results con- firm that including component-type tokens helps the model better capture the circuit structure, leading to more accurate generation despite a slightly longer output. 6.3. Computation Efficiency Analysis We analyze the computational efficiency of our SFCI by comparing it with the SFM. Specifically, the training for SFM saturates at 8,943 steps, whereas SFCI converges sig- nificantly faster at 6,886 steps, representing a 23.0 re- duction in required training steps.",
    "source": "2506.10235v1_LaMAGIC2_Advanced_Circuit_Formulations_for_Languag.pdf",
    "length": 1855,
    "tokens": 498
  },
  {
    "text": "9), fully utilizing the capabilities of the NPU. This precomputed mask establishes node connections beforehand, significantly reducing irregular memory accesses and improving memory latency and energy efficiency, all without requiring extensive hardware modifications. For Graph Attention and GraphSAGE, GraNNite leverages precomputed masks an at- tention mask for efficient attention score calculation and a sampled adjacency matrix for reuse during inference (see Fig. 9. StaGr: execution of GNNs on a static graph structure with dynamic node features. Fig. 10. One of the challenges to efficiently enable GNNs on NPUs: Dynamic input graph (An example of on-device knowledge graph). Fig. 9). This methodology achieves highly efficient inference, minimizing computational overhead and latency while optimiz- ing NPU performance under fixed-structure conditions. GrAd NodePad: To handle dynamic input graphs (refer Fig. 10), GraNNite proposes a new approach (GrAd) that uses a mask as input rather than a precomputed weight, allowing dynamic updates to edges without the need to recompile the model. Real-time graphs often undergo structural changes with nodes and edges dynamically added or removed (Fig. 10). However, NPUs typically support static input shapes, as DNN models are precompiled for fixed input shapes, with optimiza- tions such as tiling based on corresponding input configuration. This limitation requires recompilation when the input graph shape changes. Compiling the model for a static input shape and using mini-batches for inference may seem viable, but risks information loss by excluding edges connecting nodes outside the subgraph. Additionally, selecting an optimal batch size is challenging and may lead to underutilized NPU resources. Our approach introduces a node-padding technique (NodePad) that compiles the entire model with a higher node capacity than immediately needed for the whole input graph. For smaller graphs, embeddings for unused nodes are zero-padded, while absent edges are represented by zeroes in the adjacency matrix, following the conventional interpretation of 0 as no edge and 1 as an active connection. This node padding strategy min- imizes the need for frequent recompilation and eliminates the need to store multiple precompiled model versions for different graph sizes. Fig. 11 illustrates how a GNN with GraphConv Fig. 11.",
    "source": "2502.06921v2_GraNNite_Enabling_High-Performance_Execution_of_Gr.pdf",
    "length": 2380,
    "tokens": 479
  },
  {
    "text": "Second, the heterogeneous neural and symbolic kernels lead to low compute array utilization and efficiency of ML accelerators. Third, heavy memory transactions exhibited in both components can cause large communication latency, which is even more challenging in FPGA deployment. DAG perfectly addresses the above challenges by first, identifying data dependencies through dataflow graph to fully exploit parallelism opportunities among NN and symbolic operations with featured HW architecture; second, balancing NN and symbolic operations on our AdArray , empowered by both design-level flexibility and kernel-level flexibility to achieve maximum utilization; third, configuring memory units adaptively to best-fit workload s memory usage, thus eliminating unnecessary transactions and stalls. B. Data Dependency Identification graph(): ... Neuro Operation - CNN (Resnet18) relu_1[16,64,160,160] : call_module[relu](args ( bn1 [16,64,160,160])) maxpool_1[16,64,160,160] : call_module[maxpool](args ( relu_1[16,64,160,160])) conv2d_1[16,64,160,160] : call_module[conv2d](args ( maxpool_1[16,64,160,160])) ... Symbolic Operations Inverse binding of two block codes vectors by blockwise cicular correlation inv_binding_circular_1[1,4,256] : call_function[nvsa. inv_binding_circular](args ( vec_0[1,4,256], vec_1[1,4,256])) inv_binding_circular_2[1,4,256] : call_function[nvsa. inv_binding_circular](args ( vec_3[1,4,256], vec_4[1,4,256])) Compute similarity between two block codes vectors match_prob_1[1] : call_function[nvsa.match_prob](args ( inv_binding_circular_1[1,4,256], vec_2 [1,4,256])) Compute similarity between a dictionary and a batch of query vectors match_prob_multi_batched_1[1]: call_function[nvsa.",
    "source": "2504.19323v2_NSFlow_An_End-to-End_FPGA_Framework_with_Scalable_.pdf",
    "length": 1713,
    "tokens": 481
  },
  {
    "text": "Conf. on Robotics and Automation (ICRA). [54] Liang Shi, Beichen Li, Changil Kim, Petr Kellnhofer, and Wojciech Matusik. 2021. Towards Real-time Photorealistic 3D Holography with Deep Neural Networks. Nature 592 (2021). [55] Tomoyoshi Shimobaba, Jiantong Weng, Takahiro Sakurai, Naohisa Okada, Takashi Nishitsuji, Naoki Takada, Atsushi Shiraki, Nobuyuki Masuda, and To- moyoshi Ito. 2012. Computational Wave Optics Library for C : CWO Library. Computer Physics Communications (2012), 1124 1138. [56] Jeffrey H. Shuhaiber. 2004. Augmented Reality in Surgery. Archives of Surgery (2004), 170 174. [57] Randall Shumaker and Lackey Stephanie. 2014. Virtual, Augmented and Mixed Reality: Designing and Developing Augmented and Virtual Environments: 6th International Conference, VAMR 2014, Held as Part of HCI International 2014, Heraklion, Crete, Greece, June 22-27, 2014, Proceedings, Part I. Vol. 8525. Springer. [58] Julian Steil, Inken Hagestedt, Michael Xuelin Huang, and Andreas Bulling. 2019. Privacy-Aware Eye Tracking Using Differential Privacy. In Proc. ACM Interna- tional Symposium on Eye Tracking Research and Applications (ETRA). 1 9. [59] Stereolabs. 2020. ZED Software Development Kit. \"\". [60] techradar. 2020. Google Pixel 2 Review. \" google-pixel-2-review\". [61] Oren M Tepper, Hayeem L Rudy, Aaron Lefkowitz, Katie A Weimer, Shelby M Marks, Carrie S Stern, and Evan S Garfein. 2017. Mixed Reality with HoloLens: Where Virtual Reality Meets Augmented Reality in the Operating Room. Plastic and reconstructive surgery (2017), 1066 1070. [62] Lingjie Wei and Yuji Sakamoto. 2019. Fast Calculation Method with Foveated Rendering for Computer-generated Holograms Using an Angle-changeable Ray- tracing Method. Appl. Opt.",
    "source": "HoloAR.pdf",
    "length": 1731,
    "tokens": 486
  },
  {
    "text": "Available: [19] B. Perozzi et al., Let your graph do the talking: Encoding structured data for llms, 2024. [Online]. Available: 05862 [20] K. Guu et al., Realm: Retrieval-augmented language model pre- training, 2020. [Online]. Available: [21] Z. Luo et al., Graphinstruct: Empowering large language models with graph understanding and reasoning capability, 2024. [Online]. Available: [22] Z. Chai et al., Graphllm: Boosting graph reasoning ability of large language model, 2023. [Online]. Available: 05845 [23] X. He et al., G-retriever: Retrieval-augmented generation for textual graph understanding and question answering, 2024. [Online]. Available: [24] B. Fatemi et al., Talk like a graph: Encoding graphs for large language models, 2023. [Online]. Available: [25] Y. Zhao et al., Codev: Empowering llms for verilog generation through multi-level summarization, 2024. [Online]. Available: https: arxiv.org abs 2407.10424 [26] M. Liu et al., Chipnemo: Domain-adapted llms for chip design, 2024. [Online]. Available: [27] K. Chang et al., Data is all you need: Finetuning llms for chip design via an automated design-data augmentation framework, in Proceedings of the 61st ACM IEEE Design Automation Conference, ser. DAC 24. New York, NY, USA: Association for Computing Machinery, 2024. [Online]. Available: [28] H. Wu et al., Chateda: A large language model powered autonomous agent for eda, IEEE Transactions on Computer-Aided Design of Inte- grated Circuits and Systems, 2024. [29] Y. Zhao et al., Mage: A multi-agent engine for automated rtl code generation, 2024. [Online]. Available: [30] Q. Cai et al., Codegraph: Enhancing graph reasoning of llms with code, 2024. [Online]. Available: [31] L. Luo et al., Reasoning on graphs: Faithful and interpretable large language model reasoning, 2024. [Online].",
    "source": "2505.13479v1_RTL_Graph-enhanced_LLM_for_RTL_Code_Generation.pdf",
    "length": 1810,
    "tokens": 492
  },
  {
    "text": "Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Mean 0.0 2.5 5.0 7.5 Speedup AlexNet one-sided SCNN one-sided SparTen FlexiSAGA 8x8 Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 7 Layer 8 Layer 9 Layer 10 Layer 11 Layer 12 Mean 0.0 2.5 5.0 7.5 Speedup VGG16 Layer 3a.0 Layer 3a.1 Layer 3a.2 Layer 3a.3 Layer 3a.4 Layer 3a.5 Layer 5a.0 Layer 5a.1 Layer 5a.2 Layer 5a.3 Layer 5a.4 Layer 5a.5 Mean 100 101 102 Speedup (log) GoogLeNet Fig. 10. Operator-wise sparse-over-dense speedup comparison between one-sided SCNN, one-sided SparTen, and FlexiSAGA 8 8 for AlexNet, VGG16, and GoogLeNet convolution operators. where some operators have such a high sparsity that they can almost be skipped entirely, leading to a sparse-over-dense speedup of up to 100. For the mean sparse-over-dense speedup FlexiSAGA shows better results for all three DNNs. 6.4 Design Space Exploration Lastly, we present a design space exploration (DSE) in Fig. 11 for two AlexNet operators processed on different FlexiSAGA instances, all comprised of 72 PEs coupled with different dataflows and pruning parameters. The DSE heatmap shows the runtime in clock cycles. For the CONV operator on the left-hand side, the best runtime (local optimum) is achieved for a FlexiSAGA instance of size 6 12 using the sOS dataflow with pruned column vectors of length n 6. For the FC operator on the right-hand side, the best runtime (local optimum) is reached with a SA of size 12 6 using the csOS dataflow and pruned column vectors of length n 12. We conducted this DSE for all AlexNet CONV and FC operators and selected the architecture and pruning method which has the lowest whole DNN inference runtime. This resulted in a SA of size 4 18 for which the DNN is pruned using column vectors with n 4. For the presented operators in Fig.",
    "source": "2506.01566v1_FlexiSAGA_A_Flexible_Systolic_Array_GEMM_Accelerat.pdf",
    "length": 1801,
    "tokens": 492
  },
  {
    "text": "The construction of a reservoir representation is critically important because it significantly affects both the accuracy of the DFR and hardware resource usage. We discuss the implementation of the reservoir representation in the next section. The construct and function of the output layer are similar to those of ESN. The weight of the output layer, ùëäout is learned in advance, as in ESN. 3 RESERVOIR REPRESENTATION The reservoir maps the input signal to a higher dimensional space [13]. Let the elements of ùíô(ùëò) (i.e., the features) be [ùë•(ùëò)1,ùë•(ùëò)2, . . . ,ùë•(ùëò)ùëÅùë•]. The reservoir then yields a total of ùëá ùëÅùë•features that are denoted by ùë•(ùëò)ùëõ(ùëò 1, 2, . . . ,ùëá; ùëõ 1, 2, . . . , ùëÅùë•). As explained in the previous sections, because the length of the input series ùëámay vary, so does the number of features. In this situation, the features cannot be processed with an output layer of a fixed size. Consequently, conversion to reservoir representation with a fixed length is required. The following subsections review the existing reservoir representations. 3.1 Last Reservoir State (LRS) In [12], LRS after all the input series were entered was used as the reservoir representation. The reservoir representation is then expressed as: ùíì ùíô(ùëá). (7) The underlying assumption of this method is that the reservoir cumulatively stores information about past inputs. Therefore, the LRS is considered to contain all information for the entire input. The number of dimensions available with this representation is ùëÅùë•and it does not depend on the series lengthùëá. However, ùëÅùë•is relatively small (otherwise, the hardware cost increases). As a result, the prediction of reservoirs using this reservoir representation is less accurate than that using existing neural-network-based classifiers, particularly for multivariate time-series classification tasks [5]. ACM Trans. Embedd. Comput. Syst., Vol. 00, No.",
    "source": "2504.11981v1_Hardware-Friendly_Delayed-Feedback_Reservoir_for_M.pdf",
    "length": 1892,
    "tokens": 498
  },
  {
    "text": "Figure 14(c) shows LightNobel achieved 5.62-6.73 , 5.32-6.49 lower latency with the chunk op- tion and 1.47-2.42 , 1.19-2.19 lower latency without the chunk option compared to A100 and H100 in this experiment. We also conduct experiments on proteins that GPUs cannot process without the chunk option to evaluate the performance of LightNobel on proteins with long sequence lengths. Figure 14(d) shows LightNobel achieves 2.34-3.30 , 1.94-2.97 lower latency with the chunk option Baseline PPM (w o Chunk) Baseline PPM (w o Chunk) Baseline PPM (w Chunk) Baseline PPM (w Chunk) LightNobel LightNobel Baseline PPM (w o Chunk) Baseline PPM (w Chunk) LightNobel Baseline PPM (w o Chunk) Baseline PPM (w Chunk) LightNobel (a) (b) Peak Mem Requirement (GB, Log Scale) CASP16 CAMEO 1 2 3 4 Dataset 4,957.94 CASP14 CASP15 11.52 7.79 6.14 11.52 7.79 6.14 308.80 36.67 11.19 308.80 36.67 11.19 597.35 54.36 14.28 597.35 54.36 14.28 208.87 41.29 208.87 41.29 Peak Mem Requirement (GB, Log Scale) CASP16 CAMEO 1 2 3 4 Dataset 4,957.94 CASP14 CASP15 11.52 7.79 6.14 308.80 36.67 11.19 597.35 54.36 14.28 208.87 41.29 1 2 3 4 Sequence Length (K) 10 1 2 3 4 5 6 7 8 9 9,945 3,360 1,660 80GB Peak Mem Requirement (GB, Log Scale) 15,121GB 1 2 3 4 Sequence Length (K) 10 1 2 3 4 5 6 7 8 9 9,945 3,360 1,660 80GB Peak Mem Requirement (GB, Log Scale) 15,121GB Baseline PPM (w o Chunk) Baseline PPM (w Chunk) LightNobel (a) (b) Peak Mem Requirement (GB, Log Scale) CASP16 CAMEO 1 2 3 4 Dataset 4,957.94 CASP14 CASP15 11.52 7.79 6.14 308.80 36.67 11.19 597.35 54.36 14.28 208.87 41.29 1 2 3 4 Sequence Length (K) 10 1 2 3 4 5 6 7 8 9 9,945 3,360 1,660 80GB Peak Mem Requirement (GB, Log Scale) 15,121GB Figure 15: Peak memory requirement of PPM across (a) datasets and (b) various sequence lengths.",
    "source": "2505.05893v1_LightNobel_Improving_Sequence_Length_Limitation_in.pdf",
    "length": 1774,
    "tokens": 643
  },
  {
    "text": "For larger Transformer models (e.g., with hidden sizes of 1024 or 4096), advanced memory management strategies such as double- buffered streaming and ping-pong buffering will be essen- tial to hide latency and effectively manage data flow. Reducing Data Transfer Overhead: To unlock the FPGA s full potential, future work should further minimize data movement between the CPU and FPGA. Leveraging more persistent on-chip memory for activations and implementing DMA could significantly reduce transfer latency. Improving CPU-FPGA Execution Pipelining: Maximiz- ing performance requires overlapping CPU and FPGA exe- cution. While the FPGA effectively accelerates the QKV pro- jections, concurrent processing of subsequent stages (e.g., attention softmax or FFN layers) on the CPU is essential for further throughput gains. Techniques such as double buffering can facilitate simultaneous data transfer and com- putation, thereby reducing idle time. Addressing the unignor- able transmission delays associated with the current PYNQ overlay and custom forward pass represents a promising direction for future work. Optimizing FPGA Utilization and Scaling: Future im- provements might include parallelizing the Q, K, and V pro- jections on separate systolic arrays, as well as keeping the FPGA accelerator active across multiple layers rather than resetting registers and flushing memory for each new com- putation. Such strategies would reduce overhead and further improve efficiency. 9 CONCLUSION We have presented the design and implementation of an FPGA- Based Tiled Matrix Multiplication Accelerator for Transformer Self- Attention on the Xilinx KV260 SoM. Our work demonstrates that through loop tiling, on-chip buffering, and parallel unrolling, even a resource-constrained FPGA can significantly accelerate the ma- trix multiplication operations that underpin Transformer attention mechanisms. Standalone GEMM benchmarks and the acceleration Tiled MatMul Accelerator for Transformer Self-Attention Conference 17, July 2017, Washington, DC, USA of Q, K, and V projections achieved a notable 7 speedup over PyTorch CPU execution and marked improvements in energy effi- ciency.",
    "source": "2503.16731v3_Design_and_Implementation_of_an_FPGA-Based_Hardwar.pdf",
    "length": 2177,
    "tokens": 467
  },
  {
    "text": "5. It mainly consists of a Cortex-M4 CPU as the SoC host processor and 80 Akida Neural Processing Units (NPUs) as the SNN processors cores. Four NPUs form a single node, hence forming 20 nodes. Each NPU mainly consists of 8 Neural Processing Engines (NPEs) as the compute units for executing synaptic and neuronal operations (e.g., event-based convolutions), and 100KB SRAM buffer as the local memory for storing weights (40KB) and data spikes (60KB) [23]. Akida NSoC has a direct memory access (DMA) controller, a power management unit (PMU), several data interfaces (i.e., USB 3.0, PCIe 2.1, I2S, I3C, UART, and JTAG), two memory interfaces (i.e., SPI Flash and LPDDR4), and an interface for multi-chip expansion. For data encoding, Akida employs the data pixel- spike converter. Furthermore, to facilitate SNN developments and their Akida implementations, BrainChip provides MetaTF framework [17], which accommodates features like ANN-to- SNN conversion, SNN mapping, and on-chip learning. USB 3.0 PCIe 2.1 I2S I3C UART JTAG Cortex M4 CPU and DSP Akida Neural Processing Units (NPUs) DMA Engine Data Pixel -Spike Converter SPI LPDDR4 Weights Input Packet Power Management Unit (PMU) Neural Processing Engine (NPE) Threshold Event NPU Multi-Chip Expansion Neural Processing Engine (NPE) Threshold Event Fig. 5. Overview of the Akida architecture; adapted from [7] [23]. III. OUR DESIGN METHODOLOGY We propose a design methodology to address the targeted problem and related challenges, whose key steps are shown in Fig. 6, and discussed in Sections III-A until III-C. For an overview, we describe the flow of our methodology as follows. 1) It starts with the network development using the existing environment (e.g., TensorFlow Keras) to provide offline- trained NN models. 2) The offline-trained NN models and the processor configu- ration are leveraged to perform the network compatibility analysis by using our analytical model for selecting an NN model that meets the memory and compute budgets.",
    "source": "2504.00957v2_Enabling_Efficient_Processing_of_Spiking_Neural_Ne.pdf",
    "length": 2001,
    "tokens": 497
  },
  {
    "text": "SeerAtten- tion: Learning Intrinsic Sparse Attention in Your LLMs. arXiv:2410.13276 [cs.CL] [27] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab- hishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzm√°n, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind That- tai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasu- den Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsim- poukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur √áelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Va- sic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Ro- han Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, V√≠tor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Gold- schlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ram- chandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowl- ing, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Cag- gioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Han- nah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Has- son, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navy- ata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Niko- lay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyag- ina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Ran- gaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, 13 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robin- son, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024.",
    "source": "2504.17584v1_L3_DIMM-PIM_Integrated_Architecture_and_Coordinati.pdf",
    "length": 8572,
    "tokens": 2910
  },
  {
    "text": "Traditional analytical methods often fall short given the huge search space for optimal implementations on modern CPU architectures. To address this challenge, autotuning has emerged as a powerful approach. Autotuning systematically evaluates multiple imple- mentations of the same workload using mathematical models or ML techniques to guide subsequent selections of implemen- tations [1], [2]. Apache Tensor Virtual Machine (TVM) [3], a well-known ML compiler framework, implements several autotuning strategies [4], [5]. Autotuning typically requires execution on real hardware, which introduces non-determinism due to factors such as the system load [6], cache collisions [7], thermal throttling [8], and frequency and voltage scaling [9]. To mitigate these issues, each benchmark is executed multi- ple times, outliers are removed, cooldown periods are inserted, and caches are flushed before each repetition. Consequently, benchmarking a single implementation takes significantly longer than its actual run time. This is time-consuming, espe- cially when only limited hardware (HW) devices are available. This paper presents the following two main contributions: Contribution I : Simulator Interface - We present an interface allowing autotuning workloads to be executed on simulators rather than real hardware. Figure 1 I illustrates Fig. 1: The proposed simulator interface I and the score predictor approach using instruction-accurate simulators II this approach. We extract the TVM tasks and provide them as executables for the target architecture through a simulator interface. The number of parallel tasks is configurable, each running in a separate instance of the simulator. Potential scenarios that profit from our simulator interface are: The HW is not yet available, e.g., for pre-silicon software (SW) development. The embedded HW is only available in limited quanti- ties, making parallel execution of simulations on high- performance computers faster than native execution. Other metrics besides run time should be optimized. Contribution II : Score Predictor - We aim to demon- strate that instruction-accurate simulators can be used for per- formance analysis, using autotuning workloads as an example. To the best of our knowledge, we are the first to show how performance analysis can be conducted with fast instruction- accurate simulators.",
    "source": "2505.13357v1_Introducing_Instruction-Accurate_Simulators_for_Pe.pdf",
    "length": 2365,
    "tokens": 466
  },
  {
    "text": "As the model converges while training, the frequency of feedback to clauses reduces [38]. DTM skips loading of all TA slices from BRAM and updation when there is no clause-level feedback to a particular group of clauses, saving PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 10 Fig. 7: Training time per epoch. Fig. 8: The block diagram of the PRNG cluster. precious cycles. Explanation: when the clause matrix slides through the slices (see Fig. 5a), if no feedback is present for the corresponding y clauses, the computation slice moves directly to the next row without traversing the current row s bottom column. This is implemented by adjusting the read write addresses of the TA RAM, as shown in Algorithm 6. With this optimization, the CoTM gets 40 improvement in training time as shown in Fig. 7, while achieving similar test accuracy. The reduction in training time indicates fewer clause updates each epoch. The training time eventually saturates to a constant value, which approximately equals the inference time plus the weight update time for both target and negated class updating. C. Pseudo Random Number Generator (PRNG): To gener- ate a large number of random numbers for parameter updating in real time, the accelerator is integrated with a master-slave architecture PRNG cluster developed from [42]. In the PRNG cluster, the master PRNG is used to generate the seeds for the slave PRNGs. For each slave PRNG, if the random number generation finishes one PRNG cycle period, it will request a new seed from the master PRNG; this is called seed refresh. Fig. 8 shows the architecture of the PRNG cluster. The authors in [42] map the PRNG cluster to DSP blocks; however, the slave PRNG used in this design is LFSR based due to DSP block limitations in the chosen FPGAs. In the LFSR version of the master-slave PRNG cluster, the master PRNG will generate and set the seeds for the slave PRNG after 2LLF SR clock cycles, where LLF SR is the length of LFSR. One DSP slice can only afford 16-bits Mid-square based PRNG, hence in this accelerator, LFSR is used as the slave PRNG in the PRNG cluster.",
    "source": "2504.19797v1_Dynamic_Tsetlin_Machine_Accelerators_for_On-Chip_T.pdf",
    "length": 2144,
    "tokens": 489
  },
  {
    "text": "The weights are stored directly in the CASCADE array in the SLD. Table 11 shows the support logic, HILT arrays, and FIFOs feeding the CASCADE arrays with activations and weights and collecting output sums. The activation HILTs feed into the centers of the broadcast latch trees of the CASCADE rows, and are positioned in the centers of CASCADE arrays to minimize 12 GHz wire lengths. The output sums HILTs are connected to the final CASCADE array and are large enough to need to be distributed across the chip. The clock frequency of the output sum hilts can readily be reduced with negligible effect on system performance by increasing write parallelism from 128 to 256 bits. 17 ZettaLith Data Fabric Table 12 shows the number of lanes and bandwidth of ZettaLith TRIMERA data fabric links. The ZettaLith data fabric is a 2D asymmetric mesh with 39 TB s chip-to-chip bandwidth in the vertical direction, and 11 TB s chip-to-chip bandwidth in the horizontal direction, As ZettaLith is not a general purpose machine, there is no attempt to generalize the data fabric to an any-to-any configuration that maximizes flexibility. Instead, the data fabric is configured for the maximum usefulness for transformer inference within the constraints of the WSSCB, the TRIMERA chips, and UCIe 2.0 connections. The vertical connections between TRIMERA chips is chosen to be the higher bandwidth connection because the horizontal connections are interrupted by the HBM4 links, and these horizontal data fabric connections need to be routed around the TRIMERA-HBM4 links in the WSSCB. The vertical connections are not interrupted by the HBM interface. For simplicity, they are identical parallel 1.4 mm USR wires. UCIE 2.0 has a data transfer rate of 32 GT s lane. To achieve the 39 TB s chip-to-chip bandwidth, 9,750 lanes are required. As each lane requires 4 wires, there are 39,000 wires between vertically adjacent TRIMERA stacks. As the TRIMERA stacks are 13 mm wide, the wire density of the vertical fabric links is 3 wires per Œºm. The number of RDL layers required in the WSSCB depends on the WSSCB wiring pitch. For example, if the pitch is 1 Œºm, then a minimum of 3 RDL layers are required.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2185,
    "tokens": 498
  },
  {
    "text": "To set a stricter criterion, we define the robustness threshold between the minimum drop and the first quartile, ensuring that only the most resilient architectures are selected. On the other hand, For analog drift, our goal is to identify architectures that remain non-robust even with HWT. Since most architectures recover with adaptation, we set the robustness threshold 14 Table 3: Statistical summary of drift metrics Metric ( ) Mean Std Min 25 50 75 Max Threshold Noisy Drift Drop 60s 14.33 6.72 1.90 9.54 13.05 17.76 50.75 5 Noisy Drift Drop 1h 24.63 7.96 6.67 18.84 23.66 29.59 69.02 10 Noisy Drift Drop 1d 35.76 8.66 12.97 29.63 35.52 41.85 70.20 16 Noisy Drift Drop 30d 47.54 7.87 21.26 42.18 48.26 53.36 70.92 25 Analog Drift Drop 60s 1.59 1.39 -1.34 0.70 1.20 2.07 17.61 2.5 Analog Drift Drop 1h 2.32 2.02 -0.61 1.05 1.74 2.94 32.01 3.5 Analog Drift Drop 1d 3.44 2.96 -0.46 1.59 2.53 4.29 36.95 4.5 Analog Drift Drop 30d 5.27 4.40 -0.13 2.49 3.86 6.49 44.89 7 between the third quartile and the maximum drop, capturing those that still degrade significantly despite adaptation. F Neural Graph Features Analysis To better understand their structure, neural network architectures can be analyzed using graph- based features in terms of node connectivity, path relationships, and the presence of specific operations (Kadlecov√° et al., 2024). These features can be grouped into several categories: operation count (op_count), which measures the frequency of specific operations; minimum path length (min_path_len), which captures the shortest distance between nodes; maximum operations on a path (max_op_on_path), which identifies the most frequent operations along a path; and node degree metrics (node_degree), which include in-degree, out-degree, and average degree measures that describe how connected specific operations are within the architecture.",
    "source": "2506.18495v1_AnalogNAS-Bench_A_NAS_Benchmark_for_Analog_In-Memo.pdf",
    "length": 1862,
    "tokens": 491
  },
  {
    "text": "This systematic knob-turning could ensure that the final ZettaLith implementation meets its zettaFLOPS, power- efficiency, thermal-reliability and yield targets with well- quantified margins. 50 30 ZettaLith Risk Mitigation ZettaLith includes multiple risk mitigation strategies to address technical, manufacturing, and operational challenges, summarized in Table 23. Table 23: ZettaLith risk mitigation Risk Mitigation Effect of mitigation Overall risk too high Start with ExaLith Delays datacenter version Not interested in datacenter version Use ExaLith or NEXAI Scaled performance and cost 5 trillion weights not enough Use maximum memory 59 more expensive 20 trillion weights not enough Use combination HBM, HBF Increased software complexity ZettaLith compute not enough Connect units via 800 GbE Complex, decreased efficiency Processing speed HBM speed Use large batch sizes Already incorporated in ZettaLith TRIMERA HILT insufficient Extreme bandwidth fabric Already incorporated in ZettaLith Partial sum transfer bottleneck Eliminate using CASCADE Already incorporated in ZettaLith Timing closure can't be achieved Reduce clock speed Proportionally fewer PFLOPS Don't want to reduce clock speed Use dataflow architecture Complex simulation TRIMERA clock distribution Clock domain 0.367 mm 2 Already incorporated in ZettaLith Transistor cushion exceeded Increase transistor cushion 0.19 transistor fewer PFLOPS EUVL not available Use 7 nm for SLD Approx 78 fewer PFLOPS Thermal expansion of WSSCB MEMS silicon springs Already incorporated in ZettaLith Warpage of WSSCB MEMS silicon springs Already incorporated in ZettaLith Crack propagation in WSSCB MEMS silicon springs Already incorporated in ZettaLith Fragile handling of WSSCB MEMS silicon springs Already incorporated in ZettaLith Unclean environment for WSSCB MEMS silicon springs Already incorporated in ZettaLith Hardware faults in CASCADE arrays CREST fault tolerance Already incorporated in ZettaLith Marginal CASCADE arrays CREST fault tolerance Already incorporated in ZettaLith Errors in transformer inference CREST fault tolerance Already incorporated in ZettaLith Arrays fail, but are correctable Ignore None More than correctable arrays fail Graceful degradation 0.0001 fewer PFLOPS TRIMERA fails Fail in place 0.78 per TRIMERA fewer PFLOPS CPU SLD fails Fail in place 6.25 per CPU fewer PFLOPS Power supply unit fails Fail in place 1.2 per PSU fewer PFLOPS Power density too high Reduce clock speed Proportionally fewer PFLOPS Electromigration in SLD SHAPE design Already incorporated in ZettaLith Electromigration in BID, HILT Careful design, simulation Normal design process Activation broadcast SSN Decouple CASCADE columns Already incorporated in ZettaLith HBM4 unobtainable Use HBM3E 39 fewer weights TSMC A16, A14 unavailable Use TSMC N2 30 fewer PFLOPS Poor TRIMERA or HBM attach yield Set acceptance threshold 0.78 per TRIMERA fewer PFLOPS Hybrid bond pitch too fine Consolidate power and ground More simulation required Poor hybrid bond yield CREST fault tolerance Already incorporated in ZettaLith Processors not replaceable Fail in place Not repairable Power supplies not replaceable Fail in place Not field repairable Not field repairable Swap, return to factory Downtime unless local spare Potential low yield of TRIMERAs SHAPE, CREST, small die Already incorporated in ZettaLith SOTA CMOS design complexity SHAPE design Already incorporated in ZettaLith Complex software stack Develop focused subset None (ZettaLith is not a GPU) 51 31 Glossary of New Terms This glossary explains terms that are new to the ZettaLith technology.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 3617,
    "tokens": 780
  },
  {
    "text": "It is inspired by general recurrences and functional programming constructs, such as scan, fold, map, and zip. During each iteration, the inputs to the operator consist of the current values of its state variables and the specific element of the input tensor being processed. Its Scan Op : Body LSTM computation executed in each iteration (h_init, c_init) X1 X2 X3 Scan Inputs : Initial hidden and cell states processing inputs X tanh X X tanh ct-1 ht-1 ct ht c1 c2 c3 h1 h2 h3 Scan Outputs : Concatenated hidden states Last hidden state Sequence size 3 Inp-1 (t 0) Inp-2 (t 1) Inp-3 (t 2) h1 - t 1 h2 - t 2 c1 - t 1 c2 - t 2 Fig. 4: The figure illustrates how the Scan operator replicates the recurrent hidden state computations in an LSTM layer. The yellow, red, and blue arrows highlight the pathways for input data processing and state updates at each time step for every input in the sequence. outputs comprise the updated state variables and one or more output tensors. The computation performed in each iteration is defined by the operator s body attribute that encodes a computational graph. The operator has unique properties that differentiate it from other ONNX compute nodes: It allows the state variable to be updated after each iteration, enabling the updated state to be used in the processing of the next input. It processes inputs sequentially, either row by row or column by column, continuously updating the hidden state for each input while storing the hidden states at each step. We utilise this container operator to encapsulate the LSTM compute graph within its body, effectively modelling the recurrent computational structure of the LSTM layer. The quantised ONNX implementation of the layer is constructed using the QuantizeLinear, Clip, and DequantizeLinear oper- ators, forming a QCDQ representation that exactly follows the QuantLSTM implementation in Brevitas (a framework for training quantised neural networks [24]). To verify its functional correctness, we compare our quantised LSTM layer implementation against this QuantLSTM layer. Figure 4 illus- trates the internal workings of the Scan operator in the context of the LSTM compute graph over three time steps.",
    "source": "2506.20810v1_FINN-GL_Generalized_Mixed-Precision_Extensions_for.pdf",
    "length": 2197,
    "tokens": 497
  },
  {
    "text": "The optimization process can thus be operated taking into account intermediate sequence lengths (i.e., n N). III. RCNET HARDWARE CONSTRAINTS MODELING To properly formulate the optimization problem, a set of hardware-related constraints and requirements has been identified, as detailed in this section and summarized in Table I. 7 cat ùë•ùëõ; catùëù 1,ùêæ ùúÅùëù[ùëõ] ùúâùëù[ùëõ] ùúÅùëù [ùëõ] ùúâùëù [ùëõ] ùíôùëò ùëõ ùúÅùëò[ùëõ] ùúâùëò[ùëõ] ùúÅùëò [ùëõ 1] ùúâùëò [ùëõ 1] 0.5 sign( ) ùëæùëò. ùíöùëò ùëõ 1 ùíöùëò 1 ùëõ ùíöùëò ùëõ ùõÄùëò ‚Ñéùë¶. ùíÉùëò ‚Ñé decoder cell ùêæ encoder cell ùêæ 0.5 sign( ) Fig. 4: RCNet - Encoder-Decoder cell description. A. Fidelity term optimization and stability regularization A multi-objective loss F is defined to both consider the regression problem aiming at mini- mizing the error between the input signal (xa) and its quantized counterpart (xq), so as managing stability and saturation issues. Our log-domain fidelity loss FLSE whose goal is to match outputs with input signals (respectively xq RS and xa RS, with S a number of training samples, equation (6)) mimics and relaxes the behavior of a max distance (i.e., a Linf norm) operator by computing the log-sum-exp of the conversion error. The additional regularization term FDR (equation (7)) adds a penalization in case of internal signals voltage excursions beyond given bounds ( Œ¥k). This regularization caps the internal dynamic range to avoid saturation issues and limit signals distortion.",
    "source": "2506.16903v1_RCNet_ŒîŒ£_IADCs_as_Recurrent_AutoEncoders.pdf",
    "length": 1385,
    "tokens": 484
  },
  {
    "text": "ùê∂'! ùê∂'\" ùê∂(\" ùê∂(! C x Q bits Chunk Weights (W) Unique Chunks Unique Chunk IDs (a) Optimization 1: Indexing and Encoding Weight Matrix x (c) Optimization 3: Frequency-aware Reindexing x (b) Optimization 2: Packet-specific Encoding Precision Encoded W 2 4 1 3 0 4 1 3 3 3 3 0 4 3 4 4 Encoding Precision log2(5) 3 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 1 1 0 8 clock Cycles 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 7 clock Cycles Na√Øve Packing Packet-specific Encoded Precision 1 1 0 0 0 1 1 0 1 0 0 0 0 Mode Bit√† More data packed A B C D E F G H A B C D E F G 4 1 3 0 2 1 3 0 0 0 0 2 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 6 Clock Cycles 2 4 1 3 0 4 1 3 3 3 3 0 4 3 4 4 0 1 1 1 1 1 Mode Bits Original Encoded W Original Unique Matrix 0 1 2 3 4 Reindexed Unique Matrix 0 1 2 3 4 Re-arrange chunk IDs based on frequency Modified Encoded W After Frequency-aware Reindexing OPT-1.3B OPT-125M More Data Packed Figure 4. Figure showing (a) process of generating the unique matrix and the trends in the reduction ratios for OPT-125M and OPT-1.3B LLM models across different layers in the decoder. Reduction ratios are averaged across all the decoder layers. (b) packet-specific encoding precision and (c) frequency-aware reindexing to further optimize the DRAM bandwidth. depicted in Fig.",
    "source": "2503.11663v1_MEADOW_Memory-efficient_Dataflow_and_Data_Packing_.pdf",
    "length": 1400,
    "tokens": 468
  },
  {
    "text": "D. Personalized Inference-Time Optimizations As discussed in Sec. I, the lack of personalized optimiza- tions restricts the potential for maximizing both speed and quality in model inference. Thus, we evaluate the impact of two test-time optimizations: multi-token generation for accel- eration and customized sampling for quality improvement. We show that by tailoring the multi-token generation configuration and sampling strategies to the client s compute budget, the optimization achieves 2.3 speedup ratio and up to 46 syntax accuracy improvement over default greedy decoding. In Fig. 5, we evaluate the speedup ratio with respect to the tree size, a parameter reflecting the token parallelism. While the acceptance ratio increases with larger tree sizes, the speedup ratio peaks at a tree size of 60 due to limited idle compute resources. This aligns with previous research [6], which shows that tree size must be optimized per client to achieve maximum speedup. In Fig. 5, we examine how different sampling strate- gies affect syntax accuracy. Beam search performs best with sample sizes under 3, while combined sampling outperforms others for larger sample sizes. Our findings emphasize the need 60 75 90 105 120 Tree Size 2.5 2.6 2.7 2.8 2.9 Acceptance Ratio Acceptance Ratio 1.9 2.0 2.1 2.2 2.3 2.4 Speedup Ratio Speedup Ratio 1 2 3 4 5 Sample Number 20 30 40 50 60 70 Syntax Accuracy ( ) Temperature Sampling Combined Sampling Top-k Sampling Greedy Sampling Top-p Sampling Beam Search Fig. 5: Personalized Test-Time Optimization. Left: multi- token generation. Right: Customized sampling for HLS mod- els. Combined Sampling uses both top-k and top-p filtering. 1 2 3 4 5 6 7 8 9 10 Sample Number 35 40 45 50 55 60 ( ) Sampling Latency (full GPU Capacity) Sampling Latency (20 GPU Capacity) 13 15 17 19 21 23 25 27 29 31 Latency (s) 1 2 3 4 5 6 7 8 9 10 Sample Number 1.5 2.0 2.5 3.0 3.5 4.0 4.5 Trueput (design s) 4.08 2.91 full GPU capacity 20 GPU capacity Fig. 6: Trueput Evaluation. Left: and latency for different sample sizes.",
    "source": "2506.00002v1_Advancing_AI-assisted_Hardware_Design_with_Hierarc.pdf",
    "length": 2042,
    "tokens": 495
  },
  {
    "text": "1 Introduction Quantization plays a critical role in the deployment of large language models (LLMs), offering a means of bridging the gap between full-precision reference models and their low-precision counterparts. The primary objective of quantiza- tion is to minimize inference costs while maintaining model quality by reducing the bit width requirements of weights and activations. To this end, recent advancements in post-training quantization (PTQ) have enabled the practical use of 4-bit weights and activations during inference, effectively addressing challenges that are particularly pronounced in LLMs, such as mitigating the impact of outliers [1, 2] and managing non-uniform error sensitivity [3, 4]. Traditionally, the quantization design space has been navigated with a Ô¨Åxed set of strategies; namely, the selection of bit width and data format [5, 6, 7], the addition or removal of zero-points [8, 9], and the granularity or optimization of scaling factors [10, 11]. Interestingly, parameter volume (i.e., model size bit width) is commonly viewed in one dimension; volume is primarily reduced via bit width reductions, and model size is rarely considered outside of scaling analyses [12, 13, 14]. However, in real-world applications where maintaining model quality is paramount, a modest model size increase of 5-10 is often an acceptable trade-off. For example, when deploying on hardware accelerators restricted to power-of-2 bit widths (e.g., GPUs), failing to meet an accuracy requirement at 4 bits necessitates reverting to 8-bit precision, a signiÔ¨Åcant step size increase in parameter volume. While increasing model size typically requires either Ô¨Çexible supernetworks [15] or end-to-end re-training [14], we present post-training model expansion as an emerging strategy to reduce the gap between full-precision reference models and their quantized counterparts. Our proposal aligns with existing literature, which consistently demonstrates that increasing model size is generally an effective strategy for improving accuracy [16, 17, 18]. Moreover, a new trend is emerging; as further discussed in Section 2, recent studies have already introduced post-training quantization techniques that incidentally increase model size while still decreasing parameter volume. We argue that post-training model expansion offers a promising path to balancing the trade-off between model cost and quality.",
    "source": "2503.17513v1_Improving_Quantization_with_Post-Training_Model_Ex.pdf",
    "length": 2413,
    "tokens": 494
  },
  {
    "text": "arXiv:2505.02181v1 [cs.LG] 4 May 2025 PREPRINT - Accepted at IEEE International Symposium on Asynchronous Circuits and Systems (ASYNC) 2025 Efficient FPGA Implementation of Time-Domain Popcount for Low-Complexity Machine Learning Shengyu Duan , Marcos L. L. Sartori , Rishad Shafik , Alex Yakovlev , Emre Ozer Microsystems Research Group, Newcastle University, Newcastle upon Tyne, UK Pragmatic Semiconductor, Cambridge, UK {shengyu.duan, marcos.sartori, rishad.shafik, alex.yakovlev}.newcastle.ac.uk Abstract Population count (popcount) is a crucial operation for many low-complexity machine learning (ML) algorithms, in- cluding Tsetlin Machine (TM)-a promising new ML method, par- ticularly well-suited for solving classification tasks. The inference mechanism in TM consists of propositional logic-based structures within each class, followed by a majority voting scheme, which makes the classification decision. In TM, the voters are the outputs of Boolean clauses. The voting mechanism comprises two operations: popcount for each class and determining the class with the maximum vote by means of an argmax operation. While TMs offer a lightweight ML alternative, their per- formance is often limited by the high computational cost of popcount and comparison required to produce the argmax result. In this paper, we propose an innovative approach to accelerate and optimize these operations by performing them in the time domain. Our time-domain implementation uses programmable delay lines (PDLs) and arbiters to efficiently manage these tasks through delay-based mechanisms. We also present an FPGA design flow for practical implementation of the time-domain popcount, addressing delay skew and ensuring that the behavior matches that of the model s intended functionality. By leveraging the natural compatibility of the proposed popcount with asyn- chronous architectures, we demonstrate significant improvements in an asynchronous TM, including up to 38 reduction in latency, 43.1 reduction in dynamic power, and 15 savings in resource utilization, compared to synchronous TMs using adder- based popcount. Index Terms Popcount, Machine Learning, Tsetlin Machine, Programmable Delay Line, FPGA. I.",
    "source": "2505.02181v1_Efficient_FPGA_Implementation_of_Time-Domain_Popco.pdf",
    "length": 2205,
    "tokens": 490
  },
  {
    "text": "arXiv:2506.03474v1 [cs.LG] 4 Jun 2025 CORE: Constraint-Aware One-Step Reinforcement Learning for Simulation-Guided Neural Network Accelerator Design Yifeng Xiao Yurong Xu Ning Yan Masood Mortazavi Pierluigi Nuzzo Abstract Simulation-based design space exploration (DSE) aims to efficiently optimize high-dimensional structured designs under complex constraints and expensive eval- uation costs. Existing approaches, including heuristic and multi-step reinforcement learning (RL) methods, struggle to balance sampling efficiency and constraint satisfaction due to sparse, delayed feedback, and large hybrid action spaces. In this paper, we introduce CORE, a constraint-aware, one-step RL method for simulation- guided DSE. In CORE, the policy agent learns to sample design configurations by defining a structured distribution over them, incorporating dependencies via a scaling-graph-based decoder, and by reward shaping to penalize invalid designs based on the feedback obtained from simulation. CORE updates the policy using a surrogate objective that compares the rewards of designs within a sampled batch, without learning a value function. This critic-free formulation enables efficient learning by encouraging the selection of higher-reward designs. We instantiate CORE for hardware-mapping co-design of neural network accelerators, demonstrat- ing that it significantly improves sample efficiency and achieves better accelerator configurations compared to state-of-the-art baselines. Our approach is general and applicable to a broad class of discrete-continuous constrained design problems. 1 Introduction Simulation-based design space exploration (DSE) plays a critical role in automated hardware-software co-design, compiler tuning, and system-level optimization. Such optimization tasks typically involve complex, hybrid discrete-continuous design spaces, expensive evaluations through black-box sim- ulations, and strict design constraints. Traditional approaches, such as genetic algorithms [1] and Bayesian optimization [2], face significant challenges due to sparse and delayed feedback, scalability issues, and limited mechanisms for enforcing structural constraints. Methods based on reinforcement learning (RL) [3, 4] typically frame DSE as a sequential Markov decision process (MDP) problem [5, 6], requiring long rollout-based exploration or value function approximations that are often impractical for expensive simulation environments.",
    "source": "2506.03474v1_CORE_Constraint-Aware_One-Step_Reinforcement_Learn.pdf",
    "length": 2456,
    "tokens": 479
  },
  {
    "text": "Since the dequantization engine does not require the entire KV cache for its operation, we designed it to function in a streaming manner. This design allows the dequantization engine to maintain low latency, while efficiently processing all past KV cache. Memory management unit. Figure 10 illustrates the operations of memory management unit, which manages the reading and writing of quantized KV cache. We design the MMU unit to handle dense- and-sparse matrices in a page-based manner, optimizing bandwidth utilization. It supports multiple memory accesses in burst mode and streamlined operations to hide latency of memory and quantiza- tion dequantization operations. Since Oaken s MMU units share a common address space, MMU operates in each compute core inde- pendently, preventing interference. Without this specialized MMU, processing variable-sized sparse matrices would require additional overhead for indexing, reshaping, and subsequent operations. There are two major challenges associated with the design of MMU unit: g idx s idx s s g idx Memory Layout ... Page ... Page idx s idx s ... Address Xfer Size 0x0000 0x04 0x0004 0x04 0x0008 0x04 0x000C 0x04 ... Address Xfer Size 0x1000 0x02 0x1002 0x02 0x1004 0x01 0x1005 0x01 ... Dense Management Table Sparse Management Table g g g g g Sequence Length Token 1 Layer 1 Head 1 Token 2 Layer 1 Head 1 idx idx idx idx idx ... s s s s s g g Sequence Length g idx s idx s g g Figure 10: Operations of memory management unit (MMU) for handling dense and sparse data. (1) Addresses and transfer size management: Dense matri- ces have predictable sizes that are well-aligned within memory spaces, while sparse matrices vary in size. Thus, management tables are needed for both dense and sparse data to accommodate this variability. These tables contain the virtual-to-physical ad- dress mappings and transfer sizes for the KV cache, considering up to the maximum sequence length per attention head. Physical addresses and transfer sizes are dynamically calculated during inference by checking available pages on demand.",
    "source": "2503.18599v2_Oaken_Fast_and_Efficient_LLM_Serving_with_Online-O.pdf",
    "length": 2073,
    "tokens": 470
  },
  {
    "text": "The effectiveness of our cost model relies on the inclusion of all components, each contributing uniquely to the overall performance. As illus- trated in Figure 7, the exclusion of individual components leads to a noticeable decline in speedups. For example, ex- cluding the input featurizer (IFE) causes a decline from 1.40x to 1.26x. Similarly, omitting the configuration map- per (FM) leads to a further decline to 1.16x, and excluding latent encoder (LE) lowers speedup to 1.01x. This em- phasizes that each component contributes uniquely to the model s high performance, and all need to act synergistically to maximize the benefits of knowledge transfer. 8 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning 1.0 1.2 1.4 1.6 Geomean Speedup 1.01 1.04 1.31 1.40 1.55 VAE FA PCA COGNATE Optimal Figure 9: Selection of auto- encoders for COGNATE. 1.0 1.2 1.4 1.6 Geomean Speedup 1.29 1.38 1.43 1.40 1.55 NT 5 NT 100 NT 1000 COGNATE Optimal Figure 10: Data overhead w o transfer learning. Selection of MLP Predictor. As shown in Figure 3, the MLP predictor from WACO s base cost model was retained in our enhanced design. Figure 8 provides a comparative analysis of alternative predictors, including LSTM, GRU, and Transformer (TF). The results demonstrate that our proposed cost model design outperforms the alternatives, with the TF predictor achieving the next best performance with 1.36 speedup. These findings highlight that an MLP predictor is sufficient to deliver robust performance with limited data. In contrast, the suboptimal performance of the TF predictor can be attributed to the limited dataset, as the high simulation costs associated with emerging hardware make it challenging to collect larger datasets for fine-tuning. Selection of Auto-Encoders. Figure 9 shows our inves- tigation into various methods for handling the heteroge- neous components of program configurations. We evaluated choices ranging from conventional feature augmentation (FA) to principal component analysis (PCA), auto-encoders, and variational auto-encoders (VAE).",
    "source": "2506.00424v2_COGNATE_Acceleration_of_Sparse_Tensor_Programs_on_.pdf",
    "length": 2099,
    "tokens": 496
  },
  {
    "text": "This is because the delay of the half-adder is not dependent on the operand bit-width (shown in Table V). To solve QII in bit-serial computation, we employ encoding and half-accumulation strategies (Figure 2(E)). In step ‚ù∂, encoders are used to replace the original Skip Zero Unit. The encoding can utilize modified Booth encoding (MBE) [23] or EN-T [45] encoding. Subsequently, sparse encoding is performed on the encoded numbers, which differs from other bit-slice-based calculations. While other schemes per- form sparse encoding on the 0 bit-slice of the multiplicand, we conduct sparse encoding on the encoded number. This is because the encoded number is directly utilized to generate the PPs (Figure 1(A) step ‚ù∂). For comparison purposes, two computational examples are presented in Figure 2(B) and (E), where 114, 15, and 124 are multiplied by three multipliers B0, B1 and Bn. Bit-serial computation generates 4, 4, and 5 PPs respectively, requiring corresponding cycles to complete accumulation. In contrast, our proposed method only requires 3, 2, and 2 PPs for these operands. This allows for skipping both zeros and consecutive 1 bit-slices in the multiplicand. The second improvement involves the use of compressors to replace accumulators in step ‚ù∏of Figure 2(B)(E). This is aimed at optimizing area and timing. In terms of optimization for bit-interleaved methods (Figure 2(F)), this paper employs sparse encoding for the encoded bits of A. The sparsely encoded indices are used to select the encoded bits, while also utilizing the indices to prefetch B. Subsequently, PPs are generated by using the encoded bit segments of non- zero multiplicand A and multiplier B. These PPs are then accumulated using 3-2 compressor to optimize computational efficiency. In summary, our core contributions are as follows: 1) We propose a finer-grained TPE notation and introduce new methods and ideas for designing and optimizing PE microarchitecture in specific applications.",
    "source": "2503.06342v1_Exploring_the_Performance_Improvement_of_Tensor_Pr.pdf",
    "length": 1977,
    "tokens": 442
  },
  {
    "text": "IEEE Journal on Selected Areas in Communications 37, 6 (2019), 1205 1221. doi:10.1109 JSAC.2019.2904348 [158] Wei Wang, Vincent W. Zheng, Han Yu, and Chunyan Miao. 2019. A Survey of Zero-Shot Learning: Settings, Methods, and Applications. ACM Trans. Intell. Syst. Technol. 10, 2, Article 13 (jan 2019), 37 pages. doi:10.1145 3293318 [159] Xin Wang, Fisher Yu, Zi-Yi Dou, and Joseph E. Gonzalez. 2017. SkipNet: Learning Dynamic Routing in Convolutional Networks. CoRR abs 1711.09485 (2017). arXiv:1711.09485 [160] Yulin Wang, Rui Huang, Shiji Song, Zeyi Huang, and Gao Huang. 2021. Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with Adaptive Sequence Length. CoRR abs 2105.15075 (2021). arXiv:2105.15075 [161] Christopher J. C. H. Watkins and Peter Dayan. 1992. Q-learning. Machine Learning 8, 3 (01 May 1992), 279 292. doi:10.1007 BF00992698 [162] Kang Wei, Jun Li, Chuan Ma, Ming Ding, Feng Shu, Haitao Zhao, Wen Chen, and Hongbo Zhu. 2024. Gradient sparsification for efficient wireless federated learning with differential privacy. Science China Information Sciences 67, 4 (March 2024). doi:10.1007 s11432-023- 3918-9 [163] Olivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre-Alvise Rebuffi, Ira Ktena, Krishnamurthy Dvijotham, and A. Taylan Cemgil. 2021. A Fine-Grained Analysis on Distribution Shift. CoRR abs 2110.11328 (2021). arXiv:2110.11328 [164] R. J. Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8 (1992), 229 256.",
    "source": "2505.12523v1_Energy-Aware_Deep_Learning_on_Resource-Constrained.pdf",
    "length": 1527,
    "tokens": 465
  },
  {
    "text": "Table 4. MSEs of voltage conversion ratio and efficiency evaluated on models finetuned with 500, 1k, and 2k 6-component circuits. 500 1000 2000 MSE Voltage Eff Voltage Eff Voltage Eff CF 0.1843 0.1970 0.1684 0.1844 0.1459 0.1661 PM 0.1661 0.1705 0.1494 0.1565 0.1334 0.1315 FM 0.1324 0.1156 0.1341 0.1325 0.1014 0.0865 SFM 0.1570 0.1543 0.1188 0.1109 0.0941 0.1009 SFCI 0.1102 0.0899 0.0475 0.0719 0.0580 0.0418 5.3. Transferability Evaluation on 6-Component Circuit In real world scenario, large amount of data for circuits with a large node number could be hard to collect. A six- component circuit requires 30 seconds of simulation to ob- tain the voltage and efficiency. Similar to LaMAGIC, we extend models trained with 3, 4, 5-components circuits to be finetuned with only 500, 1k, and 2k 6-component circuits randomly selected from our dataset. Then, we evaluate each model on the testing set (9k data points) and run simulation on each generated circuit to get its real performance. The results are shown in Figure 7 and Table 4. Similar to the results of 3, 4, 5-component circuits, SFM has higher suc- cess rates in low tolerance ranges (0.01 to 0.03) compared to FM. This shows that our succinct formulation also enhances low precision sensitivity in transferability evaluation. Models trained with SFCI demonstrate best transferability. Notably, the success rate of the model finetund with 2000 6-component circuits improves to 0.84 compared to FM s 0.76 under tolerance 0.1. In MSE, SFCI also performs the best compared to all other formulations with up to 58.5 improvement. This result shows that the sparse graph rep- resentation is more suitable for complex circuits due to its short token length. Additionally, our component-type tokens let model to better learn different component s representa- tions to boost the generalizability.",
    "source": "2506.10235v1_LaMAGIC2_Advanced_Circuit_Formulations_for_Languag.pdf",
    "length": 1850,
    "tokens": 485
  },
  {
    "text": "Let the total sequence length (prompt already generated tokens) be M. The query q is now a 1 ÀÜ N vector corresponding to the new token. The key (Kcache) and value (Vcache) matrices contain the cached representations of all M previous tokens and are of dimensions M d. The core attention computation involves: 1. Calculating attention scores: q ÀÜ KT cache (a 1 ÀÜ N vector multiplied by a N ÀÜ M matrix, resulting in a 1 ÀÜ M score vector). 2. Applying softmax to the scores. Multiplying the resulting 1 ÀÜ M vector by Vcache (an M ÀÜ N matrix) to get the 1 ÀÜ N output vector. The computation involves primarily matrix-vector and vector-vector operations. The computational load per step (OpNdq) is significantly lower than the total prefill computation. However, this phase requires fetching the large Kcache and Vcache matrices from memory (e.g., off-chip DRAM) in every step. Consequently, the decoding phase is often memory-bandwidth bound, especially as the sequence length M grows. Since computation is less intensive and latency is dominated by memory access (fetching the KV cache), massive parallelism is inefficient and wastes resources. A more sequential or lower-parallelism computation unit is sufficient. This approach significantly reduces the required on- chip hardware resources (e.g., number of PEs, buffer sizes) compared to the prefill unit. The profiling result in Figure 8 clearly shows that the attention in the decoding phase is memory-bounded, while the prefill phase is compute-bounded. This demonstrates the necessity of lightweight decoding im- plementation to save on-chip resources for the Prefill phase. Fig. 8: Characterization of Attention Module during Pre- fill Decoding Phase 1) Reuse of Decoding Attention for LM Head: Following the processing thr-ough N transformer blocks in typical LLM architectures like LLaMA, the final step before token genera- tion involves the LM Head. This component performs a crucial linear projection, mapping the final hidden state output from the last transformer block to a vector of logits representing the probability distribution over the entire vocabulary.",
    "source": "2504.16266v2_TeLLMe_An_Energy-Efficient_Ternary_LLM_Accelerator.pdf",
    "length": 2123,
    "tokens": 468
  },
  {
    "text": "Define the energy budget Eb for a single quanta and for the entire inference. Initialize the loop iteration parameters l. 17 Compute the activations a and apply the dropout mask: adropout i ai mi Compute the loss L(Y, ÀÜY) where Y is the output of the network and ÀÜY is the target output. Calculate the gradients and Hessians of the loss with respect to the weights: L Wij , 2L W 2 ij For each layer L and loop i within the layer, estimate the energy Ei required for the current quanta size li: Ei DynAgent.estimateEnergy(L, i, li) If Ei Eb, fuse tasks to reduce the overhead: FuseTasks(L, i, li, Eb) Update Ei after task fusion: Ei DynAgent.estimateEnergy(L, i, li) Update the dropout mask m based on the sensitivities: pi Œ≤ P j 2L W 2 ij (Wij)2 max P j 2L W 2 ij (Wij)2 œµ mi 0 if Bernoulli(1 pi) 0 1 otherwise Perform the backward pass to update the network weights, considering the dropout mask: W W Œ∑ L W m where Œ∑ is the learning rate and denotes element-wise multiplication. Inference with Optimal Brain Damage Dropout and QuantaTask Optimization: Check the available energy using DynAgent. If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget. Otherwise, maintain or reduce the dropout rate to improve accuracy. Perform the forward pass with the updated dropout mask to obtain the output Y. This approach ensures that the network is robust to varying energy conditions by incorporating dynamic dropout influenced by the sensitivity of the weights, along with the QuantaTask optimization to handle energy constraints. D.3 Feature Map Reconstruction Error Dropout with QuantaTask Optimization Feature Map Reconstruction Error Dropout leverages the reconstruction error of feature maps to adjust dropout rates, combined with the QuantaTask optimization to handle energy constraints in intermittent systems. Mathematical Formulation: Let W be the weight matrix of a layer and F be the feature maps produced by the layer.",
    "source": "NexUME.pdf",
    "length": 1993,
    "tokens": 473
  },
  {
    "text": "Therefore, the number of PEs is determined to be 20, which is their least common multiple. However, this introduces two key challenges. First, scaling fac- tors must be considered before accumulation, as inlier multipli- cation results require scaling before being combined with outlier multiplication results. Second, 4-to-1 and 5-to-1 adder trees are incompatible, necessitating a dynamic approach to accumulation. To address these challenges, DAL dynamically manages varying numbers of PE Lane outputs while ensuring correct scaling factor application. Figure 9(e) shows the architecture of DAL. Specifically, for computations requiring 4 PE Lanes, scale factors are applied after accumulation. In contrast, for computations requiring 5 PE Lanes, inlier values are first accumulated and scaled before being combined with the outlier results. To achieve this, one adder tree in DAL is disabled, and the outputs of the remaining four adder trees are accumulated with the result of the fifth PE Lane. The Arbiter is employed at the front of DAL to rearrange PE Lane outputs before entering DAL, ensuring efficient computation. At the highest level, as shown in Figure 9(d), the RMPU Engine comprises four PE Clusters. To accommodate various workloads, the RMPU Engine produces multiple types of output results. The sum of the 2 PE results corresponds to the output required for MHA with a head dimension of 32, as described earlier. The sums of 4 and 5 PE Lane results serve as computational outputs for quantized tokens processed within the DAL, and the sums of 8 or 16 PE Lane results serve as computational outputs for non-quantized tokens. Additionally, the engine generates the sum of 80 PE results, enabling scalability for larger computations that require multiple RMPUs. As the last layer, it also performs ReLU operations. Ultimately, a single RMPU Engine supports up to 20 tokens simultaneously, achieving a high level of token parallelism. 5.3 Versatile Vector Processing Unit The Versatile Vector Processing Unit (VVPU) is designed to sup- port all vector operations required for PPM, including LayerNorm, Softmax, residual connections, and the new operations required for AAQ. By having a unified structure, the VVPU eliminates the need for separate dedicated components for each operation, achieving both high resource utilization and operational flexibility. Figure 10 shows the microarchitecture of VVPU.",
    "source": "2505.05893v1_LightNobel_Improving_Sequence_Length_Limitation_in.pdf",
    "length": 2420,
    "tokens": 496
  },
  {
    "text": "TABLE IV TIMING ANALYSIS ON OPEN-SOURCE DESIGNS COMPARED TO NGSPICE SIMULATION Open-Source Stats Delay error (ps) Slew error (ps) Designs Area (¬µm2) Sinks Mesh Size Buffers GAT-Mesh [1] GAT-Mesh [1] gcd 1082.88 35 2x2 4 1.76 79.63 0.31 N A aes 52785.04 562 6x6 31 2.65 157.34 2.69 N A ibex 57585.27 1932 15x15 168 2.91 203.39 7.48 N A dynamic node 54950.01 2257 15x15 180 3.58 199.78 3.18 N A tiny rocket 193770.36 4035 18x18 268 3.67 278.45 7.74 N A jpeg 198612.62 4383 19x19 287 3.46 266.13 5.10 N A swerv 387381.12 11180 34x34 799 7.99 197.13 7.87 N A swerv wrapper 1056663.92 11214 35x35 538 16.16 219.71 13.50 N A Average Error 5.27 200.19 5.98 N A TABLE V RUNTIME ANALYSIS OF GATMESH, FIRST-ORDER MODEL [1], AND NGSPICE Open-Source Runtime (s) Designs GATMesh [1] Ngspice gcd 0.0038 0.0003 0.2094 aes 0.0039 0.0527 2.5333 ibex 0.0041 0.0637 14.4586 dynamic node 0.0041 0.6546 16.8264 tiny rocket 0.0044 2.4565 41.7799 jpeg 0.0046 3.5786 49.8608 swerv 0.0088 29.5014 851.3769 swerv wrapper 0.0087 28.3169 795.6460 Average Runtime 0.0047 8.0780 221.5864 Average Speedup 1 1718 47146 The other models still utilize the same RC mesh and features, except that the second model adds the auxiliary connections but no JK connections, and the third model adds only JK connections but no auxiliary connections. The last is our GATMesh model with both auxiliary connections and JK connections. All the other parameters are the same as described in Table III.",
    "source": "2507.05681v1_GATMesh_Clock_Mesh_Timing_Analysis_using_Graph_Neu.pdf",
    "length": 1453,
    "tokens": 522
  },
  {
    "text": "TABLE III: The average NumPPs of each multiplicand in different encoding based on the normal distribution matrix. can be expressed as four PPs: 91B (B 6) (2B 4) (- B 2) (-B). The candidate PPs only need to compute {-2B, -B, 0, B, 2B} in MBE for selection by the encoded numbers, and the shifter is responsible for shifting the generated PPs by the corresponding bits weight. However, not all numbers will produce 4 non-zero PPs (Figure 3(B) 124 can be encoded as {2, 0, -1, 0}, so 124B (2B 6) (-B 2)). We counted the NumPPs generated by the two Radix-4 encoders and Radix-2 bit-serial within the INT8 range (Table II). Under MBE, 175 ((108 54 12 1) 256 68.4 ) numbers generate 3 or fewer non-zero PPs during multiplication. Under EN-T, 184 ((108 60 15 1) 256 71.9 ) numbers generate 3 or fewer non-zero PPs. Similarly, Radix-2 bit-serial complement multiplication can be viewed as generating PPs without encoding. Only 93 ((84 9) 256 36.3 ) numbers generate 3 or fewer non-zero PPs during multiplication. To assess the overall operational cost of batch data, we use the average NumPPs as a metric. Fewer PPs lead to faster computation and lower power consumption. In Table III (matrices size 1024 1024), the average NumPPs for EN- T and MBE range from 2.22 to 2.45. Therefore, for large- scale matrix multiplication, we break down the vector dot product operation into two key steps: ‚ù∂Generating non-zero partial products. ‚ù∑Reduction of partial products. For parallel MAC, the multiplicand is encoded during the computation of the vector dot product, and the partial products are expanded spatially as an implicit dimension for parallel processing and reduction. This process ignores the scenario where some of the generated partial products are zero. Thus, by decomposing the multiplication operation into sequential partial product reduction combined with a non-zero partial product generator, the number of operations in matrix multiplication can be significantly reduced. III.",
    "source": "2503.06342v1_Exploring_the_Performance_Improvement_of_Tensor_Pr.pdf",
    "length": 1980,
    "tokens": 477
  },
  {
    "text": "Tests can be repeated at voltage margins by reprogramming the power supplies, and at temperature margins by including patterns of data designed to increase or decrease the power consumption of the PEs. 20.7 Graceful degradation ZettaLith experiences no performance degradation until the spare columns in an TRIMERA are consumed. At that point, Figure 10a Figure 10b Figure 10c Figure 10d Figure 10e Figure 10f Figure 10g spare column data in column column under test defective CASCADE column CASCADE columns carrying incorrect data column with defect 5 1 2 4 6 7 8 9 10 11 12 13 14 - 1 2 4 5 5 5 5 6 7 8 9 10 11 12 13 14 - 1 2 4 6 7 8 9 10 11 12 13 14 - 1 2 3 3 3 3 4 Compare Faulty 6 7 8 9 10 11 12 13 14 - - 5 1 2 4 6 7 8 9 10 11 12 13 14 - 5 3 - - - - - - - - - - - - - - - - - - 1 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 5 6 7 8 9 10 11 12 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 14 - - - - - - - - - - - - - - - - - - 1 2 3 4 5 6 7 8 9 10 11 12 13 13 14 1 2 4 5 6 7 8 9 10 11 12 13 14 1 2 4 5 6 7 8 9 10 11 12 13 14 1 2 4 5 6 7 8 9 10 11 12 13 14 - - 9 10 11 12 13 14 1 2 4 5 6 7 8 9 10 11 12 13 14 1 2 4 5 6 4 5 6 4 5 6 4 5 6 4 5 6 7 8 1 2 7 8 9 10 11 12 13 14 1 2 7 8 9 10 11 12 13 14 1 2 6 7 8 9 10 11 12 13 7 8 9 10 11 12 13 14 1 2 7 8 9 10 11 12 13 14 1 2 4 5 6 7 8 9 10 11 12 13 14 1 2 4 5 6 7 8 9 10 11 12 13 14 - - 1 2 2 4 5 6 7 8 9 10 11 12 14 - 1 4 5 6 7 8 9 10 11 12 14 - - 1 2 4 5 6 7 8 9 10 11 12 14 5 1 2 3 4 6 7 8 9 10 11 12 13 14 - 1 2 3 4 6 7 8 9 10 11 12 13 14 - 1 2 3 4 X X X X X X X X X X X X X X 6 7 8 9 10 11 12 13 14 - 1 2 3 4 4 4 4 4 Compare OK 6 7 8 9 10 11 12 13 14 - - 5 1 2 3 4 6 7 8 9 10 11 12 13 14 - 4 - - - - 5 1 2 3 4 6 7 8 9 10 11 12 13 14 - 1 2 3 4 5 5 5 5 5 5 5 6 7 8 9 10 11 12 13 14 - 1 2 3 4 6 7 8 9 10 11 12 13 14 - 1 2 3 4 Compare OK 6 7 8 9 10 11 12 13 14 - - 5 1 2 3 4 6 7 8 9 10 11 12 13 14 - 5 - 5 5 1 2 3 4 6 7 8 9 10 11 12 13 14 1 2 3 4 5 5 5 5 5 5 5 6 7 8 9 10 11 12 13 14 - 1 2 3 4 6 7 8 9 10 11 12 13 14 - 1 2 3 4 Compare OK 6 7 8 9 10 11 12 13 14 - - 5 1 2 3 4 6 7 8 9 10 11 12 13 14 - 5 - 5 1 2 3 4 6 7 8 9 10 11 12 13 14 1 2 3 4 5 5 5 5 5 6 7 8 9 10 11 12 13 14 - 1 2 3 4 6 7 8 9 10 11 12 13 14 - 1 2 3 4 Compare Faulty 6 7 8 9 10 11 12 13 14 - - 5 1 2 3 4 6 7 8 9 10 11 12 13 14 - 5 - - 5 5 5 5 1 2 3 4 6 7 8 9 10 11 12 13 14 - - 1 2 3 4 5 5 1 2 3 4 6 7 8 9 10 11 12 13 14 - - 3 5 5 5 5 6 7 8 9 10 11 12 13 14 - 1 2 3 4 6 7 8 9 10 11 12 13 14 - 1 2 3 4 Compare OK 6 7 8 9 10 11 12 13 14 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Compare Faulty CRow(1) CRow(2) CRow(3) CRow(4) CRow(5) CRow(1) CRow(2) CRow(3) CRow(4) CRow(5) CRow(1) CRow(2) CRow(3) CRow(4) CRow(5) CRow(1) CRow(2) CRow(3) CRow(4) CRow(5) CRow(1) CRow(2) CRow(3) CRow(4) CRow(5) CRow(1) CRow(2) CRow(3) CRow(4) CRow(5) CRow(1) CRow(2) CRow(3) CRow(4) CRow(5) CRow(6) CRow(7) CRow(8) CRow(9) CRow(10) CRow(11) CRow(12) CRow(13) CRow(14) CRow(15) CRow(16) comparison mechanism 33 the TRIMERA can still operate, but not at full capacity, given the software drivers are written to accommodate imperfect arrays.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 3056,
    "tokens": 1208
  },
  {
    "text": "These tables contain the virtual-to-physical ad- dress mappings and transfer sizes for the KV cache, considering up to the maximum sequence length per attention head. Physical addresses and transfer sizes are dynamically calculated during inference by checking available pages on demand. (2) Read-write granularity and order determination: To max- imize memory bandwidth utilization, burst access should be lever- aged whenever possible to reduce the total number of memory transactions. Writing KV cache involves relatively small sizes, as it only includes the key-value for the current token, whereas reading requires retrieving the KV cache for all previous tokens. To address this, Oaken organizes KV cache for the current token in a layout that facilitates burst reads in subsequent operations. Key-value vectors generated in the current layer are divided by attention head and written to distinct pages, as explained in Sec- tion 5.2. When the KV cache for the next token is generated, it is divided similarly and written sequentially, immediately follow- ing the previous tokens KV cache. This sequential arrangement allows that the KV cache for all previous tokens can be read in burst mode, allowing Oaken to efficiently utilize bandwidth. 5.3 Token-level Batch Scheduling Efficient scheduling is crucial for Oaken to efficiently serve LLM inference. Each compute core in Oaken is optimized to process a single token efficiently. During the prefill phase, input tokens from each request are scheduled for parallel processing across multiple cores. However, in the generation phase, each core handles a single output token from one request, which reduce hardware utiliza- tion. For larger batches, Oaken improves overall core utilization by processing multiple requests in parallel. Although the overhead for KV cache quantization and dequan- tization is minimal, Oaken further minimizes this by overlapping them with other operations. In batched inference, KV cache cannot be shared across cores because each core processes distinct requests, forcing each core to monopolize the memory bandwidth. Oaken employs a scheduling strategy that hides latency by overlapping KV quantization and dequantization with DMA reads and attention computations from other requests. ISCA 25, June 21 25, 2025, Tokyo, Japan Kim, Hong, et al. Table 1: Hardware specification of NVIDIA A100 GPU and Oaken equipped with either HBM or LPDDR memory.",
    "source": "2503.18599v2_Oaken_Fast_and_Efficient_LLM_Serving_with_Online-O.pdf",
    "length": 2434,
    "tokens": 481
  },
  {
    "text": "611 626. [2] G.-I. Yu, J. S. Jeong, G.-W. Kim, S. Kim, and B.-G. Chun, Orca: A distributed serving system for {Transformer-Based} generative models, in 16th USENIX Symposium on Operating Systems Design and Imple- mentation (OSDI 22), 2022, pp. 521 538. [3] NVIDIA, NVIDIA Announces Financial Results for Fourth Quarter and Fiscal 2024, details 2024 NVIDIA-Announces-Financial-Results-for-Fourth-Quarter- and-Fiscal-2024 , Feb. 2024, [Online; accessed 09-01-2025]. [4] A. Agrawal, N. Kedia, A. Panwar, J. Mohan, N. Kwatra, B. S. Gulavani, A. Tumanov, and R. Ramjee, Taming throughput-latency tradeoff in llm inference with sarathi-serve, arXiv preprint arXiv:2403.02310, 2024. [5] B. Sun, Z. Huang, H. Zhao, W. Xiao, X. Zhang, Y. Li, and W. Lin, Llumnix: Dynamic scheduling for large language model serving, arXiv preprint arXiv:2406.03243, 2024. [6] W. Lee, J. Lee, J. Seo, and J. Sim, {InfiniGen}: Efficient generative inference of large language models with dynamic {KV} cache manage- ment, in 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), 2024, pp. 155 172. [7] B. Wu, R. Zhu, Z. Zhang, P. Sun, X. Liu, and X. Jin, {dLoRA}: Dynamically orchestrating requests and adapters for {LoRA}{LLM} serving, in 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), 2024, pp. 911 927.",
    "source": "2502.15763v1_Hybrid_Offline-online_Scheduling_Method_for_Large_.pdf",
    "length": 1335,
    "tokens": 441
  },
  {
    "text": "PCIe 5.0 PCIe 6.0 version NA PCIe links 36 16 links 0.44 PCIe bandwidth 2,304 2,048 GB s 0.89 Total active PEs 217 31,407 million 145 Power efficiency 12 17,882 TF W 1490 PE power 86 72 kW 0.83 CPU and other power 34 13 kW 0.38 Total power consumption 120 84 kW 0.70 Cooling Water 2-PIC type NA 45 27 ExaLith: ZettaLith Chips on the Desktop While the full ZettaLith architecture targets the large scale and performance demands of hyperscale data center, smaller desktop systems are also useful. Users might include small-to-medium businesses (SMBs), research institutions, AI developers, and creative professionals who require substantial local AI inference capabilities but lack the budget and infrastructure for multi-rack GPU clusters or dedicated data center solutions. ExaLith is conceived as a direct application of the core ZettaLith chips and technologies to provide exascale-class FP4 inference performance within the familiar form factor and power envelope of a high-end workstation or desktop PC component. There are several feasible formats using ZettaLith silicon in desktop, workstation, or departmental environments: PCIe card: for integration into standard workstations and servers. AI Workstation: a complete, pre-integrated desktop tower system built around one or more ExaLith accelerators. Network Attached AI Accelerator (NAA): a standalone, network-accessible box containing a single ExaLith accelerator. Multi-Accelerator Appliance: a dedicated chassis housing multiple (e.g., 2-8) ExaLith accelerators for shared, high- throughput network access. Server Blade Module: Integrating the ExaLith accelerator onto a standard blade form factor for denser rack deployments. This format is particularly suited for private clouds which don't require full ZettaLith performance. The power consumption of a ZettaLith TRIMERA stack is too high to be used in a notebook computer. For this application, new silicon would be required, and the NEXAI concept is more appropriate.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 1986,
    "tokens": 442
  },
  {
    "text": "This dataset serves as a benchmark for evaluating both arithmetic and log- ical reasoning skills, essential attributes for advanced language models. Each problem in the dataset is intricately constructed to simulate real-world situations involving numerical rela- tionships, necessitating that models comprehend the problem contextually and perform accurate calculations to derive the correct solution. The GSM8K dataset comprises 1,319 unique problems as input requests, with an average input length of 68.43 tokens and a standard deviation of 25.04 tokens. For our experiments, we selected the LLaMA-65B language model due to its open- source nature and wide accessibility, making it a suitable candidate for academic and reproducible research. In our tests, the LLaMA-65B model generated responses averaging 344.83 tokens, with a standard deviation of 187.99 tokens. To ensure consistency and focus on quality responses, we constrained the maximum output length to 512 tokens during testing. Our computational setup is characterized by a robust hard- ware configuration, consisting of eight Ascend processing units, each equipped with a maximum memory capacity of 64 GB. This formidable hardware infrastructure is essential for facilitating the efficient processing and testing necessary for our experiments. Additionally, we have assessed the KV cache usage for input in this experiment, establishing baseline settings that are also utilized in practical applications. The current hardware, along with the LLM employed, imposes a memory constraint of 1024 blocks of KV cache. Each block can accommodate a maximum of 128 tokens. For the GSM-8k benchmark, the combined maximum input and output for each request requires five blocks. Consequently, this configuration limits us to a maximum of approximately 200 clients running concurrently, calculated by the expression 1024 5 200. In our experimental setup, we conduct an estimation of the operation time required for prefill and decode stages using over 400 data groups. We find that both prefill and decode times exhibit a linear relationship with the number of tokens involved. Specifically, the prefill time can be calculated as 0.13 milliseconds per token, plus a fixed overhead of 25 millisec- onds. For the decode process, the time required for each batch 0 25000 50000 75000 100000 125000 150000 175000 200000 Time(ms) 0 25 50 75 100 125 150 175 200 Client Total Inference Time prefill decode Utilization rate: 80.2 . Total inference time: 201.00 seconds. Fig.",
    "source": "2502.15763v1_Hybrid_Offline-online_Scheduling_Method_for_Large_.pdf",
    "length": 2520,
    "tokens": 491
  },
  {
    "text": "In this work, we seek to minimize the target domain data requirement during fine-tuning (Shen et al., 2021), by tar- geting heterogeneous input feature spaces present between general-purpose hardware and emerging sparse accelerators (data-collection objective) (detailed in Appendix A). 3. Our Methodology: COGNATE Autoencoder Latent Encoder Configuration Mapper Sparse Convolution Input Featurizer (3) Sparse Latent Config Sparse Matrix Program Configurations Homogenous Heterogenous Predictor WACO Program Embedder Sparse Convolution Input Featurizer Sparse Config Predictor Program Configurations (a) (b) (4) Runtime Cost (1) (2) COGNATE Figure 3: A comparative overview of the enhanced cost model design in COGNATE (b) alongside WACO s cost model design (a), highlighting key differences. Here, we present COGNATE, a novel framework to design data-frugal learned cost models to accelerate the execution of sparse tensor programs on emerging hardware. The fol- lowing subsections outline our contributions toward achiev- ing the objectives set forth in Section 2.3; maximizing the accuracy while minimizing the data collection overhead. 3.1. Enhancements to Enable Transfer Learning We build upon WACO considering it as our base model by refining its architecture to better facilitate transfer learning across diverse hardware platforms. These enhancements represent contributions that are orthogonal to WACO s orig- inal scope. Our improved cost model design (Figure 3(b)) is structured around four key components: configuration mapper, input featurizer, latent encoder, and predictor. The configuration mapper (Figure 3(b)(1)) and latent encoder (Figure 3(b)(2)) replace the program embedder in WACO, while the input featurizer (Figure 3(b)(3)) has been modi- fied to more effectively capture low-level information from sparsity patterns. Both configuration mapper and input fea- turizer remain consistent across hardware platforms, serving as the components that enable efficient knowledge transfer. Configuration Mapper (FM). The configuration mapper captures homogeneity across hardware platforms by pro- cessing program configurations (cj) and their parameters to identify similarities in code optimizations across various 4 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning platforms.",
    "source": "2506.00424v2_COGNATE_Acceleration_of_Sparse_Tensor_Programs_on_.pdf",
    "length": 2337,
    "tokens": 498
  },
  {
    "text": "IV. EXPERIMENT RESULTS A. Experiment Setup All experiments are conducted on a Linux-based platform equipped with 2.8GHz 96-core Intel Xeon Gold 6342 CPUs and 1.5TB memory. Unless specified otherwise, all multipliers and MACs adopt AND-based PPG and default CPA instantiated with s a b style RTL. We compare the multipliers and MACs generated by DO- MAC with several baseline approaches, including: Wallace trees [2], Dadda trees [3], and commercial IPs from Synopsys DesignWare Library [21]. The commercial IPs are instantiated with r a b and r a b c style RTL. Additionally, the multipliers are compared with those produced by GOMIL [9] (ILP-based) and ArithmeticTree [13] (RL-based) leveraging their open-sourced implementation. All de- signs are synthesized by Synopsys Design Compiler (Version R- 2020.09-SP2) [22] using Nangate 45nm Open Cell Library [17] and the compile_ultra command. To illustrate the trade-off between delay and area, we sweep the target delay constraints from 0ns to 2ns. B. Results and Analysis Fig. 4 illustrates the synthesized results of multipliers, displaying the Pareto frontiers with respect to delay and area. In most cases, DOMAC achieves superior performance over all baseline methods. Our framework identifies designs that Pareto-dominate those of the commercial IPs, delivering up to 6.5 reduction in delay and 25 300 400 500 600 Area (um 2) 0.8 1.0 1.2 1.4 Delay (ns) 8b Multiplier 1500 1750 2000 Area (um 2) 1.2 1.4 1.6 Delay (ns) 16b Multiplier 6000 7000 8000 Area (um 2) 1.4 1.6 1.8 2.0 Delay (ns) 32b Multiplier Wallace Dadda GOMIL ArithmeticTree DesignWare DOMAC Fig. 4. Pareto frontiers of the synthesized results on multipliers.",
    "source": "2503.23943v1_DOMAC_Differentiable_Optimization_for_High-Speed_M.pdf",
    "length": 1676,
    "tokens": 447
  },
  {
    "text": "[54] A. Hao, Y. Min, and X. Chen, Self-mutual distillation learning for continuous sign language recognition, in Proc. ICCV, 2021, pp. 11 303 11 312. [55] S. Sun et al., Logit standardization in knowledge distillation, in Proc. CVPR, 2024, pp. 15 731 15 740. [56] M. Alwani, H. Chen, M. Ferdman, and P. Milder, Fused-layer cnn accelerators, in Proc. MICRO, 2016, pp. 1 12. [57] T. Abtahi, C. Shea, A. Kulkarni, and T. Mohsenin, Accelerating convolutional neural network with fft on embedded hardware, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 26, no. 9, pp. 1737 1749, 2018. [58] A. Syafeeza et al., Convolutional neural networks with fused layers applied to face recognition, International Journal of Computational Intelligence and Applications, vol. 14, no. 03, p. 1550014, 2015. [59] S. Liu, H. Fan, and W. Luk, Accelerating fully spectral cnns with adaptive activation functions on fpga, in Proc. DATE, 2021, pp. 1530 1535. [60] M. Mathieu, M. Henaff, and Y. LeCun, Fast training of convolutional networks through ffts, arXiv preprint arXiv:1312.5851, 2013. [61] O. Rippel, J. Snoek, and R. P. Adams, Spectral representations for convolutional neural networks, Advances in neural information processing systems, vol. 28, 2015. [62] H. Zeng et al., A framework for generating high throughput cnn implementations on fpgas, in Proc. FPGA, 2018, pp. 117 126. [63] S. Liu, H. Fan, and W. Luk, Design of fully spectral cnns for efficient fpga-based acceleration, IEEE Transactions on Neural Networks and Learning Systems, 2022.",
    "source": "2505.13461v1_FPGA-based_Acceleration_for_Convolutional_Neural_N.pdf",
    "length": 1554,
    "tokens": 470
  },
  {
    "text": "We also plan to make RTL a fully open-source model with its 100K instruction tuning dataset to support collaboration in EDA and chip design community. II. RELATED WORK A. LLM for RTL Code Generation While recent advancements in hardware design automation have shown the effectiveness of adapting LLMs for specialized tasks like EDA automation and optimization, e.g., scripting [26], error interpretation [27], assistant chatbot for design flow [28], etc., numerous efforts have been made to fine-tune and pre-train models for RTL (Verilog) code generation: 4While RTL code (text) is for syntactical and semantical perspective, its CFG and DFG is for structural perspective of the circuit. This is conceptually a cross-modality fusion, in which the model integrate insights from both the RTL code (syntax and semantic) and its graph structures (structure). 5Structs include (but not limited to) module, port, wire, reg, procedural blocks (e.g., always and initial), control flow constructs (e.g., if-else and case), instantiation, FSM, parameters and constants, generate constructs, arrays and memories, etc. (i) VeriGen [15] is an early attempt that compiled Verilog files from GitHub and textbooks for training dataset. De- spite assembling a substantial dataset, the lack of proper pre-processing and organization led to inconsistencies which causes the fine-tuned CodeGen model to often generate non- functional Verilog code with syntax errors. (ii) RTLCoder [10] is another early endeavor that has lever- aged GPT-3.5 for synthesizing code-instruction pairs by ex- tracting RTL-specific keywords (to overcome dataset scarsity and code generation quality). However, its dependence on GPT-3.5 s embedded knowledge limited code diversity. This limitation prompted subsequent efforts to enrich data diversity through augmentation techniques. (iii) OriGen [8] advanced RTL code generation by introducing code-to-code augmentation and a self-reflection mechanism. The former diversifies the dataset with semantically equivalent but syntactically varied Verilog code, while the latter uses compiler feedback to iteratively correct errors, addressing VeriGen s inconsistencies and improving code quality.",
    "source": "2505.13479v1_RTL_Graph-enhanced_LLM_for_RTL_Code_Generation.pdf",
    "length": 2200,
    "tokens": 481
  },
  {
    "text": "Evaluation. We evaluate Stage 3 on 9,500 test instances (500 per topology) using our gradient-based optimization pipeline. A design is considered converged if it meets both: (i) a predicted mean relative error below 10 , and (ii) a layout area under a topology-specific bound 1 mm2 for most circuits and 1.5 mm2 for DLNA, DohPA, and ClassBPA. The 10 error threshold reflects the forward model s 9 average prediction error (Section 5). A design is deemed successful if its final Cadence- simulated performance deviates from the target by less than 20 , confirming real-world viability. Our method achieves a success rate of 78.5 and a mean relative error of 17.7 across converged designs, with average inference time under 1 second on a MacBook CPU. Notably, success rate is coupled with the convergence threshold: tighter error bounds yield higher accuracy but require more iterations critical for large-scale design tasks. To illustrate the effectiveness of our pipeline, Figure 7 shows a representative result for the DohPA topology: the synthesized schematic is shown on the left, and the corresponding layout is on the right. These results confirm that the recovered parameters are both functionally accurate and physically realizable. Together, they demonstrate that FALCON enables layout-aware inverse design within a single differentiable pipeline a capability not supported by existing analog design frameworks. 7 Conclusion and Future Work We presented FALCON, a modular framework for end-to-end analog and RF circuit design that unifies topology selection, performance prediction, and layout-aware parameter optimization. Trained on over one million Cadence-simulated mm-wave circuits, FALCON combines a lightweight MLP, a generalizable GNN, and differentiable gradient reasoning to synthesize circuits from specification to layout-constrained parameters. FALCON achieves 99 topology selection accuracy, 10 predic- tion error, and efficient inverse design all within sub-second inference. The GNN forward model generalizes to unseen topologies with minimal fine-tuning, supporting broad practical deployment. In future work, we aim to expand the topology library and support hierarchical macroblocks for scalable design beyond the cell level. We also plan to extend the layout-aware optimization with learned parasitic models and EM-informed constraints for more accurate post-layout estimation.",
    "source": "2505.21923v1_FALCON_An_ML_Framework_for_Fully_Automated_Layout-.pdf",
    "length": 2405,
    "tokens": 481
  },
  {
    "text": "The DLC lookup code loads embedding elements through mem- ory streams. The iteration space for these memory streams is con- tained in index streams, which are generated by traversal operators and can be transformed with integer ALU streams. Formally: Ember: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures PyTorch Structured Imperative (MLIR scf, memref, arith) Structured Lookup-Compute (SLC) IR Imperative Compute IR Access Unit (e.g. TMU) Code Execute Unit (e.g. CPU) Code Optimization (Sec. 7) Lowering (Sec. 6.2) torch-mlir, MPACT Lowering (Sec. 6.3) Codegen (Sec. 6.3) Ember (Sec. 5) Sec. 6.1 Decoupled Lookup-Compute (DLC) IR (Sec 4) Dataflow Lookup IR Figure 11: Overview of Ember. Yellow represents compiler inputs outputs and blue represents new contributions. loop_tr(lb,ub,stride): traverses the iteration space i lb; i ub;i stride, where lb and ub are streams or immedi- ate values and stride is an immediate value. The stream loop_tr.0 contains the induction variable. mem_str(base,idx): loads into a stream the values of the base memory locations indexed by the idx stream. alu_str(op,op1,op2): computes an integer binary opera- tion op { , , , } on operands op1 and op2, which can either be streams or immediate values. Lines 1-12 in Figure 10c represent lines 2-6 in Figure 10b. Compute code consists of callbacks that the access unit triggers while traversing and loading tensor operands. In Figure 10b, we can wrap the FMA operation in line 7 into a callback that the access unit can trigger at each iteration of its parent loop. To trigger such callback, the access unit marshals its corresponding token (the inner-loop iteration token (ùëíùëñ)) in the control queue and its operands (result coordinatesùëèand ùëíand value ùë£) in the data queue.",
    "source": "2504.09870v1_Ember_A_Compiler_for_Efficient_Embedding_Operation.pdf",
    "length": 1796,
    "tokens": 481
  },
  {
    "text": "We trained and compared different score predictors for the x86, ARM, and RISC-V architectures. Our results show that the tuned predictors can identify optimal implementations within the top 3 of predictions. In the case of limited availability of the target HW, our approach can even be used to accelerate autotuning. In the best case, 3 parallel simulations on the used x86 machine were sufficient to replace one RISC-V board. Our research lays the groundwork for using instruction-accurate simulations for performance estimation. Future work will focus on benchmarking a broader range of CPUs to train more generalized predictors. These generalized predictors can then be applied to previously untested CPUs, enhancing our methodology s appeal for pre-silicon software development. or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. including reprinting republishing this material for advertising or promotional purposes, collecting new collected works for resale 2025 IEEE: Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, PREPRINT - Accepted for publication at the 62th Design Automation Conference (DAC), June 22-25, 2025, in San Francisco. REFERENCES [1] K. Datta et al., Stencil computation optimization and auto-tuning on state-of-the-art multicore architectures, in SC 08: Proceedings of the 2008 ACM IEEE conference on Supercomputing. IEEE, 2008, pp. 1 12. [2] L. Lin and M. Gen, Auto-tuning strategy for evolutionary algo- rithms: balancing between exploration and exploitation, Soft Comput- ing, vol. 13, pp. 157 168, 2009. [3] T. Chen et al., TVM: An automated End-to-End optimizing compiler for deep learning, in 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18), 2018, pp. 578 594. [4] L. Zheng et al., Ansor: Generating High-Performance tensor programs for deep learning, in 14th USENIX symposium on operating systems design and implementation (OSDI 20), 2020, pp. 863 879.",
    "source": "2505.13357v1_Introducing_Instruction-Accurate_Simulators_for_Pe.pdf",
    "length": 2026,
    "tokens": 460
  },
  {
    "text": "Sun, Deep residual learning for image recognition, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770 778. [16] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, Rethinking the inception architecture for computer vision, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2818 2826. [17] K. Simonyan and A. Zisserman, Very deep convolutional networks for large-scale image recognition, arXiv preprint arXiv:1409.1556, 2014. [18] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, You only look once: Unified, real-time object detection, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 779 788. [19] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. Keutzer, Squeezenet: Alexnet-level accuracy with 50x fewer parameters and 0.5 mb model size, arXiv preprint arXiv:1602.07360, 2016. [20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, ≈Å. Kaiser, and I. Polosukhin, Attention is all you need, Advances in neural information processing systems, vol. 30, 2017. [21] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., Language mod- els are few-shot learners, Advances in neural information processing systems, vol. 33, pp. 1877 1901, 2020.",
    "source": "2507.03522v1_A_Flexible_Instruction_Set_Architecture_for_Effici.pdf",
    "length": 1393,
    "tokens": 450
  },
  {
    "text": "E. Runtime Analysis In TABLE VI, we present NetTAG s runtime across various benchmarks. Most runtime is spent on preprocessing (i.e., converting netlists into TAG format) and node-level inference using ExprLLM. Despite these steps, NetTAG demonstrates around a 10x speedup over traditional physical design workflows using commercial EDA tools. There is significant potential for further runtime improvements. For example, symbolic expression extraction for each gate can be highly parallelized, reducing preprocessing time. Additionally, the ExprLLM inference time could be largely reduced by scaling up GPU resources, as this runtime depends on GPU performance and quantity. IV. DISCUSSION A. Scalability: Performance Scaling with Model and Data Size In Fig. 7, we study how the downstream task performance of NetTAG scales with both model size and pre-training data size. TABLE VI: Runtime (minutes) Comparision. Source EDA Tool Ours (Avg.) P R (Avg.) Pre Infer Total ExprLLM TAGFormer ITC99 164 2 5 0 7 OpenCores 288 18 12 1 31 Chipyard 251 15 10 1 26 VexRiscv 207 8 5 2 15 GNNRE 4 2 0 6 Preprocessing (chunking into cones and converting netlist into TAG). The plot shows the performance of each task after fine-tuning. Scaling ExprLLM backbone from 110M parameters (BERT) to larger models like Meta Llama 3.1 with 1.3B and 8B parameters results in significant performance improvements across all four tasks. Similarly, expanding data size from 25 to 100 of the dataset consistently en- hances performance. These results demonstrate the scalability of Net- TAG, suggesting that further increases in model and data size could lead to even greater improvements in accuracy and generalization. 88 96 97 79 83 86 75 85 95 Accuracy ( ) 26 23 15 24 22 12 10 20 30 110M 1.3B 8B MAPE ( ) Task 1 Task 2 Task 3 Task 4 95 96 97 80 84 86 80 90 100 Accuracy ( ) 19 16 15 15 13 12 10 20 25 50 100 MAPE ( ) (a) Increasing model size (b) Increasing data size Fig. 7: Performance scaling with model and data size. B. Demo: Reasoning Netlist s Arithmetic Function with NetTAG Fig. 8 shows how NetTAG improves functional reasoning for arithmetic netlists.",
    "source": "2504.09260v1_NetTAG_A_Multimodal_RTL-and-Layout-Aligned_Netlist.pdf",
    "length": 2139,
    "tokens": 498
  },
  {
    "text": "These models enable accurate hardware performance estimation at advanced technology nodes. Using traces from the software component, the estimator calculates detailed energy consumption for each array. The bit-serial input scheme is explicitly modeled - each input value is processed one bit at a time, with the framework tracking energy consumption for each operation. Weight values from the traces are converted to corresponding conductance or ca- pacitance states based on parameters specified in the hardware configuration file. In NeuroSim V1.5, we extended the hardware analyzer to support charge-based computation with non-volatile capacitive 8 memories. This required the addition of new circuit models for nvCap arrays, charge-to-voltage conversion using an amplifier [61], and control circuits for the unique charge-based compu- tation. The hardware estimator uses these models along with device parameters specified in the configuration file to accurately evaluate PPA metrics for nvCap-based accelerators while maintaining compatibility with the existing trace-based inter- face between behavioral simulation and hardware analysis. E. Evaluation Setup Our experimental evaluation consists of three main com- ponents: design space exploration of hardware configura- tions, inference accuracy analysis under various device non- idealities, and runtime performance comparison across open- source platforms. The DNN algorithms and proposed Neu- roSim framework are implemented using PyTorch, with all simulations performed on a single NVIDIA RTX A6000 GPU with 48GB GDDR6. 1) Design Space Exploration: For the design space explo- ration, we examine the trade-offs between hardware efficiency and inference accuracy across a range of architectural param- eters. Table I summarizes the key parameters and their ranges used in the design space exploration study. The memory device characteristics are based on Intel s 22nm RRAM platform [6]. We process inputs using the bit-serial approach (1b per cycle) to maintain consistency with the hardware estimator and activate all rows in parallel to maximize throughput. The ADC precision requirements are determined by the dynamic range of array outputs, which depends on three key factors: memory cell precision (bcell), DAC precision (PDAC), and array rows (R).",
    "source": "2505.02314v1_NeuroSim_V15_Improved_Software_Backbone_for_Benchm.pdf",
    "length": 2313,
    "tokens": 437
  },
  {
    "text": "In each case, we configured such tools to maintain model struc- ture equivalence while applying platform-appropriate op- timizations. Note that in our model compilation workflow, template inference code is often generated along with a com- piled model. However, this doesn t include model-specific pre post-processing steps, which should be implemented manually by the developer, who can update the template code as needed. Fig 3 below details our model compilation framework for converting a base (Torch ONNX TFLM) model into various platform-specific formats. Our framework will be released as open-source, and we hope its use can ease the process of cross-platform model compilation and benchmarking. 3.3 Evaluation Metrics We measure latency, power, energy-efficiency in terms of number of inference operations per mJ and memory usage across each benchmark platform and model. The impacts of various platform-specific model optimizations or compila- tion workflows on model accuracy are out of scope for our study. Latency can be considered proportional to throughput, since batching and other amortization techniques are not practical on ùúáNPU platforms due to memory constraints. Millar et al. Export to ONNX Export to MLIR with Sophgo Compiler Generate Calibration table Export to CVI PTQ (INT8) Export to TFLM PTQ (INT8) GAPFlow NNTOOL Compiler AutoTiler Compiler GAPFlow Code Gen Export to optimized TFLM with eIQ Neutron Compiler eIQ Code Gen Export to optimized TFLM with Arm Vela Compiler Vela Code Gen Quantize to AI8X PTQ format Export to AI8X model with YAML network layout AI8X Code Gen Figure 3: an overview of our model compilation workflow. 16090 16100 16110 16120 16130 16140 16150 Time (ms) 80 85 90 95 100 105 110 Peak Power (mW) Max Power (mW) NPU mem configured Inference NPU de-init Idle Figure 4: power trace of YoloV1 inference on HX-WE2. Latency: Latency is measured using each platform s internal timer. Notably, all MCUs, bar the MILK-V, are configured to run at 100 MHz. The MILK-V does not support manual frequency scaling, only DVFS.",
    "source": "2503.22567v2_Benchmarking_Ultra-Low-Power_ŒºNPUs.pdf",
    "length": 2065,
    "tokens": 492
  },
  {
    "text": "4.3 Training Dynamics Analysis Figure 2 illustrates the training dynamics of our VeriReason models across three different parameter scales (1.5B, 3B, and 7B). We track both the mean reward values (top row) and their standard deviations (bottom row) throughout the reinforcement learning process. 4.3.1 Reward Progression The reward curves reveal distinct learning patterns across model sizes. The 1.5B model demonstrates steady, monotonic improvement in reward values from approximately 0.5 to 0.8 over 800 training steps, suggesting a consistent optimization path. In contrast, the 3B model exhibits higher variance in its learning trajectory, with reward values fluctuating between 0.6 and 0.8, before ultimately converging to above 0.8 by step 400. The 7B model shows the most pronounced oscillatory behavior, with rewards ranging between 0.50 and 0.65, reflecting the increased complexity of optimizing larger parameter spaces. 0 200 400 600 800 0.5 0.6 0.7 0.8 Reward 1.5B Rewards 0 100 200 300 400 0.6 0.8 3B Rewards 0 25 50 75 100 0.50 0.55 0.60 7B Rewards 0 200 400 600 800 Training Steps 0.1 0.2 0.3 Standard Deviation Reward STD 0 100 200 300 400 Training Steps 0.7 0.8 0.9 Reward STD 0 25 50 75 100 Training Steps 0.10 0.12 0.14 Reward STD Figure 2: Reward line and std line of VeriReason 8 4.3.2 Reward Stability The standard deviation plots (bottom row) provide critical insights into training stability. The 1.5B model demonstrates exceptional convergence properties, with reward variability consistently decreasing from 0.3 to below 0.1 throughout training. This smooth reduction in standard deviation correlates with the steady increase in mean rewards, indicating robust learning. The 3B model presents a more complex pattern, with initial rapid variability reduction followed by fluctuations between 0.7 and 0.8, suggesting periodic exploration-exploitation transitions. The 7B model s standard deviation exhibits the most dynamic behavior, oscillating between 0.09 and 0.14, which aligns with its more variable reward progression. Interestingly, despite having fewer parameters, the 1.5B model achieves the most stable convergence pattern, with monotonically decreasing standard deviation.",
    "source": "2505.11849v1_VeriReason_Reinforcement_Learning_with_Testbench_F.pdf",
    "length": 2208,
    "tokens": 483
  },
  {
    "text": "6 Concorde: Fast and Accurate CPU Performance Modeling with Compositional Analytical-ML Fusion Table 2: Workload space with 5486B instructions from 29 programs Type Name Traces Instructions (M) Proprietary Compression (P1) 4 1845 Search1 (P2) 168 17854 Search4 (P3) 170 23188 Disk (P4) 168 23441 Video (P5) 268 26981 NoSQL Database1 (P6) 168 30283 Search2 (P7) 84 52989 MapReduce1 (P8) 84 56677 Search3 (P9) 1334 69277 Logs (P10) 191 75845 NoSQL Database2 (P11) 84 91274 MapReduce2 (P12) 84 104750 Query Engine Database (P13) 790 1195128 Cloud Benchmark Memcached (C1) 8 2791 MySQL (C2) 84 9283 Open Benchmark Dhrystone (O1) 1 174 CoreMark (O2) 1 335 MMU (O3) 132 18475 CPUtest (O4) 138 95215 SPEC2017 505.mcf_r (S1) 19 197232 520.omnetpp_r (S2) 20 214749 523.xalancbmk_r (S3) 20 214749 541.leela_r (S4) 20 214749 548.exchange2_r (S5) 20 214749 531.deepsjeng_r (S6) 20 214749 557.xz_r (S7) 38 408022 500.perlbench_r (S8) 41 440235 525.x264_r (S9) 44 472447 502.gcc_r (S10) 94 999282 Latency distributions. We augment our primary throughput based features from 3.2 with three instruction-level latency distributions collected from the ROB model. Specifically, we provide the distribu- tionofthetimethatinstructionsspendintheissue(ùë†ùëñ ùëéùëñ),execution (ùëìùëñ ùë†ùëñ), and commit (ùëêùëñ ùëìùëñ) stages of the ROB model (Equations (1) to (4)) for ROB {1,2,4,8,...,1024}.4 These latency distributions pro- vide additional context that can be useful for understanding certain nuances of the performance dynamics.",
    "source": "2503.23076v1_Concorde_Fast_and_Accurate_CPU_Performance_Modelin.pdf",
    "length": 1488,
    "tokens": 539
  },
  {
    "text": "Table 8: Example Transformer calculation Llama 3.1 405B Core Model Parameters Size Symbol Model dimension 16,384 d Number of attention heads 128 h Vocabulary size 128,000 V Number of layers 80 N Feedforward dimension 65,536 4d Max context length 128,000 max L Batch size used in this example 1,024 B Context length (average used in example) 2,000 L Table 9: Example Transformer Inference Calculations Llama 3.1 405B Table 10: Weights loading Inference - FLOPS Calculation Order OPs of total Weight Order Weights of total Token embedding lookup O(B x L x d) 3.36E 10 0.000005 O(V x d) 2.10E 09 0.60 Positional encoding (RoPE) O(B x L x d) 3.36E 10 0.000005 O(1) - - Layer normalization (pre-attention) O(B x N x L x d) 2.68E 12 0.00038 O(N x d) 1.31E 06 0.00038 Multi-head attention (Q, K, V projections) O(B x N x L x 3 x d x d) 1.32E 17 18.61 O(N x 3 x d x d) 6.44E 10 18.52 Attention score O(B x N x h x L x L) 4.19E 13 0.006 O(1) - - SoftMax of attention scores O(B x N x h x L x L) 4.19E 13 0.006 O(1) - - Value weighting and concatenation O(B x N x h x L x L x (d h)) 5.37E 15 0.76 O(1) - - Output projection O(B x N x L x d x d) 4.40E 16 6.20 O(N x d x d) 2.15E 10 6.17 Residual connection (post attention) O(B x N x L x d) 2.68E 12 0.00038 O(1) - - Layer normalization (pre-FFN) O(B x N x L x d) 2.68E 12 0.00038 O(N x d) 1.31E 06 0.00038 Up projection FFN O(B x N x L x d x 4d) 1.76E 17 24.81 O(N x d x 4d) 8.59E 10 24.70 Gate projection FFN O(B x N x L x d x 4d) 1.76E 17 24.81 O(N x d x 4d) 8.59E 10 24.70 SwiGLU Nonlinearity O(B x N x L x 4d) 1.07E 13 0.0015 O(1) - - Down projection FFN O(B x N x L x 4d x d) 1.76E 17 24.81 O(N x 4d x d) 8.59E 10 24.70 Residual connection (post FFN) O(B x N x L x d) 2.68E 12 0.00038 O(1) - - Final layer normalization O(B x L x d) 3.36E 10 0.000005 O(d) 1.64E 04 0.000005 Output projection (autoregression) O(B x 1 x d x V) 2.15E 12 0.00030 O(V x d) 2.10E 09 0.60 Total for a batch of 1024 inferences 7.09E 17 100 Total weights 3.48E 11 100 ZettaLith peak performance 1,507,534 PFLOPS HBM bandwidth 2.56E 14 Bytes s Target percentage of peak PFLOPS 80 Weight data 1.74E 11 Bytes Time to compute a batch of 1024 inferences 0.00059 Seconds Weights from HBM 0.00068 Seconds 27 15 The Base Interface Die (BID) Figure 6c illustrates the basic contents of the BID, which integrates multiple interface blocks and memory elements in a mainstream process node. This is not a floor plan of the chip, but an approximate use of chip area per function, and approximate arrangement of microbonds to the SCB. The die includes: 1.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2561,
    "tokens": 923
  },
  {
    "text": "The Load and Store queues bound the number of issued memory instructions that have yet to be committed (in order). We briefly discuss the Load queue model (Store queue is analogous). It is identical to the ROB model, with two differences: (i) the calcu- lations are performed exclusively for Load instructions, (ii) there are no dependency constraints: a Load is eligible to start as soon as it obtains a slot in the queue. After computing the commit cycle for each Load, we derive the throughput for each ùëò-instruction window similarly to Equation (5). In these calculations, non-Load operations are assumed to be free and incur no additional latency. Static bandwidth resources. These resources impose limits on the number of instructions (of a certain type) that can be serviced in a single clock cycle. For example, Commit, Fetch, Decode, and Rename widths constrain the throughput of all instructions. The through- put bound imposed by these resources is trivially their respective width. In contrast, issue queues restrict the throughput for a specific group of instructions, e.g., ALU, Floating-point, and Load-Store issue widths in our reference architecture. To compute the throughput bound imposed by such resources, we compute the processing time of the instructions that are constrained by that resource and assume non-affected instructions incur no additional latency. For instance, the throughput bound induced by the ALU issue width in the ùëó-th window of ùëòconsecutive instructions is given by: thrùëó ALU ùëò ùëõùëó ALU ALU issue width, (6) where ùëõùëó ALU is the number of ALU instructions in window ùëó. Dynamic constraints. Some resources impose constraints on a dynamic set of instructions determined at runtime based on the mi- croarchitectural state. Analyzing such resources is more challenging. Two strategies that we have found to be helpful are to use simplified performance bounds or basic discrete-event simulation. We briefly discuss these strategies using two examples. Load Load-Store Pipes. Finite Load and Load-Store pipes limit the number of memory instructions that can be issued per cycle. Store in- structions exclusively use Load-Store pipes, while Load instructions can utilize both Load pipes and Load-Store pipes.",
    "source": "2503.23076v1_Concorde_Fast_and_Accurate_CPU_Performance_Modelin.pdf",
    "length": 2240,
    "tokens": 488
  },
  {
    "text": "It can be observed that the analog model inference performance remains mostly the same despite the increase in the value of the mean Figure 4: Relationship between the model inference accuracy and the symmetry point skew for Swin-ViT analog TL model for the 2-class and 5-class tasks trained using the c-TTv2 algorithm. pulse device-to-device (DtoD) variability and the pulse update noise. The same observation can be made for the 2-class and the 5-class fine-tuning tasks, confirming the robustness of the c-TTv2 training algorithm to the noise in the pulse response. Figure 5: Relationship between the model inference accuracy, pulse update noise (left), and mean pulse device-to-device variation for Swin-ViT analog TL model for the 2-class and 5-class tasks trained using the c-TTv2 algorithm. IV. CONCLUSION In this paper, we assessed the performance of the c-TTv2 algorithm on an Analog Swin-ViT model for an analog TL task. We performed additional simulations to investigate the robustness of the c-TTv2 training algorithm to changes in the material specifications. Results demonstrated that c-TTv2 is suitable for training the analog TL models as it achieves better results than analog training from scratch and digital training from scratch, as expected. We also demonstrated that the c- TTv2 algorithm achieves competitive results with digital TL. Critically, the c-TTv2 algorithm was very tolerant to changes in device material specification, such as weight transfer noise, symmetry point variability, symmetry point skew, pulse update noise, and mean pulse DtoD variability. In future work, we aim to assess the performance of the c-TTv2 algorithm using more complex models and tasks such as LLM models, diffusion models, etc. Furthermore, there is a need to benchmark the performance of the training algorithm against the AGAD algorithm [20]. Lastly, there is a need to propose solutions that help narrow the inference performance gap between analog TL and digital TL models. Our intention for this paper is to lay the initial groundwork required for these future research directions.",
    "source": "2505.11067v1_Assessing_the_Performance_of_Analog_Training_for_T.pdf",
    "length": 2097,
    "tokens": 440
  },
  {
    "text": "The goal is to predict specific outcomes based on given circuit data. Figure 5 summarizes our covered encoder works based on their proposed pre-training techniques. Circuit Encoder Pretext Tasks Self-supervised Supervised Unlabeled Circuit Unlabeled Circuit [C] [C] Masked Unlabeled Circuit Predictor Graph Pretext Task labeled Circuit Predictor Circuit-related Supervisions Circuit Encoder Task- Specific Circuit Data EDA Downstream Tasks HARP ICCAD 23 CircuitFusion ICLR 25 LLM-HD DAC 24 MGVGA ICLR 25 SNS v2 MICRO 23 FGNN DAC 22 CircuitFusion ICLR 25 CircuitEncoder ASP-DAC 25 ProgSG MLCAD 24 Design2Vec NeurIPS 21 DeepGate Family DAC ICCAD ICLR 22-25 HOGA DAC 24 PolarGate ICCAD 24 TAG ICCAD 22 DeepSeq ASP-DAC 25 Circuit GNN NeurIPS 22 Text Unlabeled Circuit Data Unlabeled Circuit [C] Min ‚Ñícontrastive ‚Ñímask ‚Ñísupervision Ground-Truth Ground-Truth ‚Ñítask Pre-training Fine-tuning (a) Contrastive Learning (b) Mask-Reconstruction (c) Circuit Pre-training Supervisions RTL Testing coverage Test vector generation Netlist Signal probability toggle logic synthesis Equivalence checking SAT solving Gate function reasoning Functional ECO Functionality Design Quality HLS Design space exploration RTL Netlist PPA QoR prediction Layout Coguestion prediction Wirelength prediction Capacitance prediction Hotspot detection Predictor NetTAG DAC 25 NetTAG DAC 25 DeepCell Feb 2025 Design2Vec NeurIPS 21 TAG ICCAD 22 ProgSG MLCAD 24 CircuitFusion ICLR 25 NetTAG DAC 25 (d) Multimodal Circuit Fusion Graph Text Graph Encoder Text Encoder Fusion Pretext Tasks transfer Augment Circuit Encoder Circuit Encoder Circuit Encoder GAMORA DAC 23 Fig. 5. Summary of pre-training techniques used in circuit encoders, covered in Section 4. Representative pre- training techniques include (a) self-supervised contrastive learning, (b) self-supervised mask-reconstruction, (c) circuit-related supervisions, and (d) multimodal circuit fusion.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 1919,
    "tokens": 485
  },
  {
    "text": "This is the flow of positive holes the electrons flow the other way. Most of the voltage drop and power dissipation is in the PSU power and ground rails, as these are much longer than any other part of the interface. Table 14: ZettaLith power supply units (PSU) Aspect Value Unit ZettaLith TRIMERAs supplied 2 TRIMERAs Number of PSUs 86 PSUs Power per PSU 980 Watts Current per PSU 1,323 Amps Interface width 48 mm Interface height 11 mm Interface area 528 mm2 CGA spacing 1 mm CGA columns 528 columns CGA Ground columns 264 columns CGA Power columns 260 columns CGA Signal columns 4 columns XDPE132G5C Multiphase controllers 2 chips Multiphase controller phases 16 phases Min.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 677,
    "tokens": 162
  },
  {
    "text": "arXiv:2507.02226v1 [cs.PL] 3 Jul 2025 DecoRTL: A Run-time Decoding Framework for RTL Code Generation with LLMs Mohammad Akyash, Kimia Azar, Hadi Kamali Department of Electrical and Computer Engineering (ECE), University of Central Florida, Orlando, FL 32816, USA {mohammad.akyash, azar, Abstract As one of their many applications, large language models (LLMs) have recently shown promise in automating register transfer level (RTL) code generation. However, conven- tional LLM decoding strategies, originally designed for natural language, often fail to meet the structural and semantic demands of RTL, leading to hallucinated, repetitive, or invalid code outputs. In this paper, we first investigate the root causes of these decoding failures through an empirical analysis of token- level entropy during RTL generation. Our findings reveal that LLMs exhibit low confidence in regions of structural ambi- guity or semantic complexity, showing that standard decoding strategies fail to differentiate between regions requiring deter- minism (syntax-critical regions) and those that benefit from creative exploratory variability (design-critical regions). Then, to overcome this, we introduce DecoRTL, a novel run-time decoding strategy, that is both syntax-aware and contrastive for RTL code generation. DecoRTL integrates two complemen- tary components: (i) self-consistency sampling, which generates multiple candidates and re-ranks them based on token-level agreement to promote correctness while maintaining diversity; and (ii) syntax-aware temperature adaptation, which classifies tokens by their syntactical and functional roles and adjusts the sampling temperature accordingly, enforcing low temperature for syntax-critical tokens and higher temperature for exploratory ones. Our approach operates entirely at inference time without requiring any additional model fine-tuning. Through evaluations on multiple open-source LLMs using the VerilogEval benchmark, we demonstrate significant improvements in syntactic validity, functional correctness, and output diversity, while the execution overhead (performance overhead) is imperceptible1. Index Terms LLMs, RTL Code Generation, Decoding Strat- egy, Self-Consistency Sampling, Temperature Adaptation I.",
    "source": "2507.02226v1_DecoRTL_A_Run-time_Decoding_Framework_for_RTL_Code.pdf",
    "length": 2256,
    "tokens": 469
  },
  {
    "text": "arXiv:2506.06787v1 [cs.LG] 7 Jun 2025 3 FuncGNN: Learning Functional Semantics of Logic Circuits with Graph Neural Networks QIYUN ZHAO As integrated circuit scale grows and design complexity rises, effective circuit representation helps support logic synthesis, formal verification, and other automated processes in electronic design automation. And- Inverter Graphs (AIGs), as a compact and canonical structure, are widely adopted for representing Boolean logic in these workflows. However, the increasing complexity and integration density of modern circuits introduce structural heterogeneity and global logic information loss in AIGs, posing significant challenges to accurate circuit modeling. To address these issues, we propose FuncGNN, which integrates hybrid feature aggregation to extract multi-granularity topological patterns, thereby mitigating structural heterogeneity and enhancing logic circuit representations. FuncGNN further introduces gate-aware normalization that adapts to circuit-specific gate distributions, improving robustness to structural heterogeneity. Finally, FuncGNN employs multi-layer integration to merge intermediate features across layers, effectively synthesizing local and global semantic information for comprehensive logic representations. Experimental results on two logic-level analysis tasks (i.e., signal probability prediction and truth-table distance prediction) demonstrate that FuncGNN outperforms existing state-of-the-art methods, achieving improvements of 2.06 and 18.71 , respectively, while reducing training time by approximately 50.6 and GPU memory usage by about 32.8 . Additional Key Words and Phrases: Electronic Design Automation, Circuit Representation, And-Inverter Graphs, Graph Neural Network ACM Reference Format: Qiyun Zhao. 2025. FuncGNN: Learning Functional Semantics of Logic Circuits with Graph Neural Networks. J. ACM 1, 2, Article 3 (May 2025), 21 pages. 1 Introduction As the scale of integrated circuits continues to expand and the complexity of designs keeps in- creasing, researching how to efficiently and accurately model and analyze circuits can help the field of Electronic Design Automation (EDA) achieve circuit design optimization, verification, and the optimization of automated processes [34]. In the field of EDA, modeling complex logic circuits requires efficient and scalable representations, typically provided by Boolean networks. To address this need, AIGs simplify these networks into a compact, canonical form using two-input AND gates and inverters.",
    "source": "2506.06787v1_FuncGNN_Learning_Functional_Semantics_of_Logic_Cir.pdf",
    "length": 2543,
    "tokens": 494
  },
  {
    "text": "Moreover, we will extend the agent s applicability to autonomous software migration, functional adaptation, and performance optimization for more foundational software. By realizing and continuously improving the capabilities of QiMeng, we aim to address increasingly diverse scenario demands, driving the entire processor chip domain toward intelligence and automation. REFERENCES [1] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan et al., Deepseek-v3 technical report, arXiv preprint arXiv:2412.19437, 2024. [2] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [3] Qwen3: Think Deeper, Act Faster. [Online]. Available: https: qwenlm.github.io blog qwen3 [4] OpenAI, Hello GPT-4o, 2024. [5] Gemini 2.5: Our most intelligent AI model. [On- line]. Available: gemini-model-thinking-updates-march-2025 gemini-2-5-thinking [6] Q. Zhang, K. Ding, T. Lv, X. Wang, Q. Yin, Y. Zhang, J. Yu, Y. Wang, X. Li, Z. Xiang et al., Scientific large language models: A survey on biological chemical domains, ACM Computing Surveys, vol. 57, no. 6, pp. 1 38, 2025. [7] K. M. Jablonka, Q. Ai, A. Al-Feghali, S. Badhwar, J. D. Bocarsly, A. M. Bran, S. Bringuier, L. C. Brinson, K. Choudhary, D. Circi et al., 14 examples of how llms can transform materials science and chemistry: a reflection on a large language model hackathon, Digital discovery, vol. 2, no. 5, pp. 1233 1250, 2023.",
    "source": "2506.05007v1_QiMeng_Fully_Automated_Hardware_and_Software_Desig.pdf",
    "length": 1565,
    "tokens": 500
  },
  {
    "text": "These computations are optimized to efficiently process only the non-zero values. We describe two operations frequently used in these computations below. Sparse Matrix-Matrix Multiplication (SpMM) is the op- eration of multiplying a sparse matrix A RM K with a dense matrix B RK N, resulting in an output matrix D RM N. The SpMM operation can be expressed as Di,k P j Ai,j Bj,k, where Ai,j 0. Sampled Dense-Dense Matrix Multiplication (SDDMM) is an operation that involves the multiplication of two dense matrices, followed by an elementwise multiplica- tion with a sparse matrix. Given a sparse matrix A RM N, a sparse output matrix D RM N, and two dense matrices B RM K and C RK N, SD- DMM operation can be expressed as Di,k Ai,k P j (Bi,j Cj,k) , where Ai,k 0. 2.2. Sparse Tensor Programming Systems Table 1: Program configuration parameters (Config Params) available across CPU, GPU, and SPADE. Config Params CPU GPU SPADE Type Loop Strip-mining Numerical Loop Reordering Categorical Format Reordering Categorical Loop Binding Categorical Loop Unrolling Categorical Tiling Numerical Barrier Binary Cache Bypassing Binary Matrix Reordering Binary A sparse tensor programming system supports a range of code optimizations that modify the structure of the code to enhance performance. The effectiveness of these code optimizations depends on assigning specific values to the parameters of the program configuration. By tuning these parameters, we can significantly impact the runtime perfor- mance of sparse operations. Table 1 outlines the parameters available for program configurations across different hard- ware platforms explored in this work (code optimizations are detailed in Appendix B). The execution strategy for sparse tensor programs depends on both the hardware platform and the corresponding programming system used. In this 3 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning work, for CPU execution (source platform), we use TACO (Kjolstad et al., 2017), a domain-specific language and a compiler designed for sparse tensor algebra. Considering our target accelerators, SPADE has its own tile-based open instruction set architecture (ISA) to leverage different vari- ations of SpMM and SDDMM operations.",
    "source": "2506.00424v2_COGNATE_Acceleration_of_Sparse_Tensor_Programs_on_.pdf",
    "length": 2265,
    "tokens": 496
  },
  {
    "text": "As a result, scaling up supernode capacity is critical not only to meet raw resource requirements but also to sustain the fine-grained locality and performance characteristics demanded by next-generation LLMs. 2) Improved Resource Allocation Efficiency: Scaling up supernode size also enhances system- wide resource utilization in real-world heterogeneous workload conditions. Based on real production traces, we simulate future NPU request patterns by modeling each AI task as a set of tightly-coupled blocks, where each block is a contiguous group of NPUs that must be provisioned within a single supernode to meet intra-job bandwidth and latency constraints. As shown in Figure 24, larger supernodes consistently achieve higher NPU allocation rates across a broad range of average block sizes. For instance, at an average block size of 10.08, a 384-NPU supernode achieves over 94 allocation, while a 224-NPU supernode drops below 91 . This improvement stems from reduced fragmentation and better statistical multiplexing larger resource pools offer greater placement flexibility for non-uniform job sizes. Conversely, for a fixed supernode size, increasing block size leads to lower allocation efficiency due to packing difficulty. When the average block size reaches 11.28, the allocation rate of the 224-NPU supernode drops below 85 . These results highlight that scaling supernode size significantly improves system throughput and efficiency under realistic workload distributions. 3) Nearly Constant Amortized Network Cost: Scaling up the size of a supernode does not inherently lead to higher per-NPU network costs. Given the same network architecture, e.g., a 2-tier Clos-like switching topology, the amortized cost of network infrastructure per NPU remains nearly constant across different supernode sizes as long as the configuration achieves full switch port utilization. As shown in Table 10, configurations with 192, 288, or 384 NPUs all achieve 100 switch utilization with the same per-NPU amortized switch cost. Intermediate configurations, such as 256 or 352 NPUs, suffer from underutilized switches, slightly increasing per-node costs. These results suggest that scaling supernode size to the upper end of a given switching tier does not introduce additional cost overhead, making it a cost-effective strategy from a networking perspective. 4) Accommodating Increased Resource Heterogeneity: Future AI workloads will require increasingly diverse hardware support within the same execution context.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2515,
    "tokens": 493
  },
  {
    "text": "2023. Masterrtl: A pre-synthesis ppa estimation framework for any rtl design. In 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD), pages 1 9. IEEE. Google. 2021. Skywater pdk. Accessed: 2025-02-11. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. 2024. Deepseek-coder: When the large language model meets programming the rise of code intelligence. arXiv preprint arXiv:2401.14196. Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Os- trow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing Wang, and Fuzhen Zhuang. 2023. Scaling sentence embeddings with large language models. arXiv preprint arXiv:2307.16645. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2024. Nv-embed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428. Yibin Lei, Di Wu, Tianyi Zhou, Tao Shen, Yu Cao, Chongyang Tao, and Andrew Yates. 2024. Meta-task prompting elicits embedding from large language models. arXiv preprint arXiv:2402.18458. Zeju Li, Changran Xu, Zhengyuan Shi, Zedong Peng, Yi Liu, Yunhao Zhou, Lingfeng Zhou, Chengyu Ma, Jianyuan Zhong, Xi Wang, et al. 2025. Deepcircuitx: A comprehensive repository-level dataset for rtl code understanding, generation, and ppa analysis. arXiv preprint arXiv:2502.18297. Chin-Yew Lin.",
    "source": "2506.15697v1_DeepRTL2_A_Versatile_Model_for_RTL-Related_Tasks.pdf",
    "length": 1551,
    "tokens": 492
  },
  {
    "text": "FPGAs are ideal for this application due to their parallel processing capabilities and the ability to handle multiple data streams concurrently. 9 Algorithm 2 Training Auto-Encoder with Motion Vectors and Stacked Compression while freezing the inference model. 1: Input: Set of training video sequences V 2: Initialize: MobileNet M Weights frozen 3: Initialize: Autoencoder A Trainable 4: Initialize: Motion Vector Extractor V 5: procedure EXTRACTMOTIONVECTORS(framecurrent, frameprevious) 6: motion_vectors V (framecurrent, frameprevious) 7: return motion_vectors 8: end procedure 9: procedure FORWARDPASS(video) 10: previous_features null 11: previous_compressed null 12: for each frame frame in video do 13: features M(frame) Extract features using frozen MobileNet 14: compressed A.encode(features) Compress features 15: if previous_compressed null then 16: motion_vectors EXTRACTMOTIONVECTORS(frame, previous_frame) 17: stacked_input concatenate (compressed, previous_compressed, motion_vectors) 18: compressed A.reencode(stacked_input) Stacked compression 19: end if 20: reconstructed A.decode(compressed) Decompress to reconstruct 21: Calculate reconstruction loss between frame and reconstructed 22: previous_frame frame 23: previous_compressed compressed 24: previous_features features 25: end for 26: Backpropagate loss and update weights of A only 27: end procedure 28: while not converged do 29: for each video in V do FORWARDPASS(video) 30: end for 31: end while The implementation of layered codecs involve the following components.1. Motion Estimation: Utilize dedicated hardware blocks for calculating motion vectors between consecutive frames. This step can leverage FPGA s DSP slices for fast cross-correlation or block matching algorithms. 2. Prediction and Residual Calculation: Implement pipelined architectures for the predict( ) function and subsequent residual calculation to minimize latency. 3. Layered Encoding: Each layer of the neural codec can be implemented using parallel processing units in FPGA, allowing simultaneous processing of different frame parts or different quality layers.",
    "source": "SaLT.pdf",
    "length": 2116,
    "tokens": 483
  },
  {
    "text": "Figure 8: Cost savings of Cocktail compared to three schemes. Const1 Const2 Const3 Const4 Query 0 5 10 Models Clipper Clipper-X Cocktail (a) Average number of models used in the ensemble. IRV2 DNet201 NASMob DNet121 Xcep MNet Incep MNetV2 RNet50V2 RNet50 Model 0 50 100 Importance( ) (b) Distribution of requests served by each individual model. Figure 9: BeneÔ¨Åts of dynamic model selection policy. Note that, changing the target accuracy to tolerate a 0.5 loss, increases the percentage of requests that meet accuracy to 81 for Cocktail, when compared to 61 for InFaas. The requests meeting accuracy are generally higher for the Relaxed workload because the target accuracy is much lower. Overall, Cocktail was able to deliver an accuracy of 83 and 79.5 on average for the Strict and Relaxed workloads, respectively. This translates to 1.5 and 1 better accuracy than Clipper and InFaas. We do not plot the results for Clipper-X, which achieves similar accuracy to Cocktail, but uses more models as explained in Section 6.2.1. Cost Comparison: Figure 8 plots the cost savings of Cocktail when compared to InFaas, Clipper and Clipper-X policies. It can be seen that, Cocktail is up to 1.45 more cost effective than InFaas for Strict workload. In addition, Cocktail reduces cost by 1.35 and 1.27 compared to Clipper and Clipper-X policies, owing to its dynamic model selection policy, which minimizes the resource footprint of ensembling. On the other hand, Clipper uses all models in ensemble and the Clipper-X policy does not right size the models as aggressively as Clipper, hence they are more expensive. Note that, all the schemes incur higher cost for twitter trace (Figure 8b) compared to wiki trace (Figure 8a). This is because the twitter workload is bursty, thereby leading to intermittent over-provisioned VMs. 6.2 Key Sources of Improvements The major improvements in terms of cost, latency, and accu- racy in Cocktail are explained below. For brevity in explana- tion, the results are averaged across Wiki and Twitter traces for strict workload.",
    "source": "cocktail.pdf",
    "length": 2056,
    "tokens": 487
  },
  {
    "text": "The datapath contains FP4, FP8, and mixed FP4 8 dot-product units. In a single cycle, the active dot-product unit in each lane (determined by inspecting the metadata bit associated with each weight and activation block) computes ùêµùëÜ-wide dot products across ùêøparallel lanes and adds this FP32 partial sum to the previously accumulated partial sum, thereby performing 2 ùêµùëÜ ùêøoperations per cycle. The dot-product units with at least one FP4 input also perform the scale factor multiplication before adding the partial sum to the accumulated value. The remaining three dot-product units in each lane are inactive and are clock-gated or data-gated appropriately to minimize switching power. When computing a matrix multiplication between a weight tile A and an activation tile B, we assume a weight-stationary dataflow in which A is held sta- tionary and blocks in B are streamed in one per cycle and broadcast across all lanes. Note that we maintain the same math throughput per cycle regardless of the precision of the weights and inputs being consumed, which improves energy efficiency by maximizing tem- poral reuse of A and spatial reuse of B while avoiding complicated control logic. Our proposed design contains independent FP4 FP8 and FP8 FP4 dot-product units in each lane so that the ùê¥input can be consistently held constant during the innermost temporal loop, improving energy efficiency at the cost of area overhead. Note that datapath components could be shared to reduce area; however, this would come at the cost of increased energy consumption. Using shared hardware for all four dot-product combinations would obvi- ate any energy gains of FGMP. Sharing hardware between two or three dot-product formats could still realize some (reduced) energy savings. 4.2 Post-Processing Activation Quantization Unit Figure 4 provides a diagram of our mixed-precision activation quan- tization unit, which is a post-processing unit that quantizes output activation blocks before writing them out to memory. In a typi- cal single-precision datapath, values are locally accumulated in high precision (e.g., FP32) before being sent to a PPU for scaling and quantization to the (single) activation data format.",
    "source": "2504.14152v1_FGMP_Fine-Grained_Mixed-Precision_Weight_and_Activ.pdf",
    "length": 2205,
    "tokens": 480
  },
  {
    "text": "In the implementation, we keep a hash table that caches the frequency of all two-term subexpressions to speed up the optimization. While [3, 9] show that selecting the two-term subexpression that minimally conflicts with the other subexpressions could result in lower final adder usage, this approach requires a one-step look-ahead operation when selecting the two-term common subexpression in each update step, which involves a complexity of O( ùêøùëñùëöùëùùëô 2). Due to practical considerations for larger matrices, we instead select the most common subexpression to implement for each update step, which can be performed with a complexity of O( ùêøùëñùëöùëùùëô ). As ùêøùëñùëöùëùùëô is of the same order of magnitude as the total number of non-zero digits in ùëÄùëíùë•ùëùùëü, the additional time complexity for performing the look-ahead operation would be substantial. As suggested in [3], because this only improves the final resource utilization by less than 2 as measured in adder count, we decided that the additional complexity is not worth the runtime overhead. In contrast to [15], we also take into account the operands quantized intervals (i.e., the bitwidths and shift) when choosing the subexpression for each update step. As implied by the cost function in (1), it is preferred to have the two operands with similar bitwidths and shifts. However, directly weighting the frequency of the two-term subexpressions by the total cost is not ideal: it would also count the half-adders used, which are \"overheads\" as they may unnecessarily increase the accumulator width downstream. Instead, we weight the frequency by the number of overlapping bits between the two operands. This weighting reduces to a constant factor when the input bitwidths are uniform and significantly larger than the constant matrix s bitwidths. We show a minimal example of the second stage in Figure 3 with a constant matrix from the H.264 integer trans- form [42]. For presentation purposes, the matrix shown is a transposed matrix (i.e., ùë¶ ùëÄ ùë•instead of ùë¶T ùë•TùëÄ).",
    "source": "2507.04535v1_da4ml_Distributed_Arithmetic_for_Real-time_Neural_.pdf",
    "length": 2009,
    "tokens": 501
  },
  {
    "text": "[34] How to run NGC deep learning containers with singularity. [35] Nepal electricity prices, March 2024 GlobalPetrolPrices.com globalpetrolprices.com. globalpetrolprices.com Nepal electricity_prices . [Accessed 29-11-2024]. [36] NVIDIA Tesla T4 GPUs now available in beta Google Cloud Blog cloud.google.com. nvidia-tesla-t4-gpus-now-available-in-beta. [Accessed 29-11-2024]. 10",
    "source": "2503.11246v1_Cost-effective_Deep_Learning_Infrastructure_with_N.pdf",
    "length": 378,
    "tokens": 114
  },
  {
    "text": "982 995. [22] X. Sun, H. Wan, Q. Li, C.-L. Yang, T.-W. Kuo, and C. J. Xue, Rm- ssd: In-storage computing for large-scale recommendation inference, in 2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 2022, pp. 1056 1070. [23] M. Kim and S. Lee, Reducing tail latency of dnn-based recommender systems using in-storage processing, in Proceedings of the 11th ACM SIGOPS Asia-Pacific Workshop on Systems, 2020, pp. 90 97. [24] M. Wilkening, U. Gupta, S. Hsia, C. Trippel, C.-J. Wu, D. Brooks, and G.-Y. Wei, Recssd: near data processing for solid state drive based recommendation inference, in Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, 2021, pp. 717 729. [25] H. Wan, X. Sun, Y. Cui, C.-L. Yang, T.-W. Kuo, and C. J. Xue, Flashembedding: storing embedding tables in ssd for large-scale recom- mender systems, in Proceedings of the 12th ACM SIGOPS Asia-Pacific Workshop on Systems, 2021, pp. 9 16. [26] Z. Wang, Y. Wei, M. Lee, M. Langer, F. Yu, J. Liu, S. Liu, D. G. Abel, X. Guo, J. Dong et al., Merlin hugectr: Gpu-accelerated recommender system training and inference, in Proceedings of the 16th ACM Con- ference on Recommender Systems, 2022, pp. 534 537. [27] Y. Xiao, S. Zhao, Z. Zhou, Z. Huan, L. Ju, X. Zhang, L. Wang, and J. Zhou, G-meta: Distributed meta learning in gpu clusters for large-scale recommender systems, in Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, 2023, pp. 4365 4369.",
    "source": "2504.00520v1_SCRec_A_Scalable_Computational_Storage_System_with.pdf",
    "length": 1564,
    "tokens": 483
  },
  {
    "text": "We offload the management of DRAM refresh to the rank PU, which is responsible for the refresh of all DRAM chips in the rank. The need for refresh is checked with each head computation completion, while the rank PU can still receive fol- lowing transactions. According to the protocol [1], the interval between REF commands can also support dynamically adjusting, with up to 8 REF commands can be delayed, which provides flex- ibility for the rank PU to manage the refresh when token length varies dynamically. Furthermore, since computation time is nearly linearly correlated with token length, the rank PU can dynamically determine the optimal timing for issuing the next REF command. Parameter settings. The number of arithmetic units (e.g., mul- tipliers, adders, exponential units) in bank PUs and rank PU is primarily determined by the bandwidth. For instance, since the bank row buffer provides 64-bit data for one RD (read) command, we implement four multipliers supporting two 16-bit inputs in each physical bank PU, while the shared buffer also broadcasts 64-bit data to all bank PUs in each computation phase. The size of on-chip buffer in the rank PU determines the number of tokens that can be processed in parallel. Following work [70], a 256KB buffer is lever- aged, which can support about a token size of 128K. For requests with larger lengths, repeated data fetching is required. 5 L3 System This section describes L3 system, a PIM-powered LLM inference system with hardware-software co-design to achieve scalability. The L3 system primarily addresses two major challenges: 1) mini- mizing the additional overhead caused by communication between the GPU and host memory, and 2) balancing the computation be- tween GPU and DIMM-PIM with scheduling techniques to reduce idle bubbles on both sides for high resource utilization. H0 H1 H3 H2 Channel 0 Channel 1 Channel N Req 0 H0 Layer 0 H0 Layer 1 Layer 0 Req 1 Rankset 0 Rankset 1 H0 H0 H0 H0 H0 H0 Layer 1 Req 2 Req 3 H0 H0 H0 H0 H0 H0 H0 H0 H0 Ch0 H1 Ch1 H2 Ch2 H3 Ch3 (a) Communication and computation with the rankset. (b) The data mapping on ranksets for load balancing.",
    "source": "2504.17584v1_L3_DIMM-PIM_Integrated_Architecture_and_Coordinati.pdf",
    "length": 2142,
    "tokens": 502
  },
  {
    "text": "Serverless com- puting (FaaS) has recently emerged as a first-class platform to deploy latency-critical user facing applications as it miti- gates resource management overheads for developers while simultaneously offering instantaneous scalability. However, deploying complex microservice-based applications on FaaS has unique challenges owing to its design limitations. First, due to the stateless nature of FaaS, individual mi- croservices have to be designed as functions and explicitly chained together using tools to compose the entire appli- cation, thus forming a Directed Acyclic Graph (DAG) [33]. Second, the state management between dependent functions has to be explicitly handled using a predefined state ma- chine and made available to the cloud provider [6, 23]. Third, the presence of conditional branches in some DAGs can lead to uncertainties in determining which functions will 153 SoCC 21, November 1 4, 2021, Seattle, WA, USA V. Bhasi, J.R. Gunasekaran et al. be invoked by different requests to the same application. For instance, in a train-ticket application [40], actions like make_reservation can trigger different paths workflows (sub- set of functions) within the application. These design chal- lenges, when combined with the scheduling and container provisioning policies of current serverless platforms, result in crucial inefficiencies with respect to application performance and provider-side resource utilization. Two such inefficien- cies are described below: The majority of serverless platforms [32, 44, 46, 50] assume that DAGs in applications are static, implying that all com- posite functions will be invoked by a single request to the application. This assumption leads to the spawning of equal number of containers for all functions in proportion to the application load, resulting in container over-provisioning. Dynamic DAGs, where only a subset of functions within each DAG are invoked per request type, necessitate the ap- portioning of containers to each function. Recent frame- works like Xanadu [27], predict the most likely functions to be used in the DAG. This results in container provisioning along a single function chain. However, not proportionately allocating containers to all functions in the application can lead to under-provisioning containers for some functions when requests deviate from the predicted path.",
    "source": "kraken.pdf",
    "length": 2371,
    "tokens": 491
  },
  {
    "text": "arXiv:2505.22868v1 [cs.ET] 28 May 2025 CrossNAS: A Cross-Layer Neural Architecture Search Framework for PIM Systems Md Hasibul Amin University of South Carolina Columbia, SC, USA Mohammadreza Mohammadi University of South Carolina Columbia, SC, USA Jason D. Bakos University of South Carolina Columbia, SC, USA Ramtin Zand University of South Carolina Columbia, SC, USA Abstract In this paper, we propose the CrossNAS framework, an automated approach for exploring a vast, multidimensional search space that spans various design abstraction layers circuits, architecture, and systems to optimize the deployment of machine learning work- loads on analog processing-in-memory (PIM) systems. CrossNAS leverages the single-path one-shot weight-sharing strategy com- bined with the evolutionary search for the first time in the context of PIM system mapping and optimization. CrossNAS sets a new benchmark for PIM neural architecture search (NAS), outperform- ing previous methods in both accuracy and energy efficiency while maintaining comparable or shorter search times. Keywords processing-in-memory, neural architecture search, weight-sharing, evolutionary algorithm, mixed-precision quantization 1 Introduction Processing-in-memory (PIM) architectures have emerged as promis- ing alternatives to conventional von Neumann-based machine learn- ing (ML) hardware [15]. These architectures exploit features such as massive parallelism, analog computation, and the ability to perform computations directly where data is stored, leading to significant performance improvements [4, 8, 10, 24]. The foundation of most PIM architectures is based on memristive crossbar arrays, which uti- lize resistive memory technologies such as resistive random-access memory (RRAM) and magnetoresistive random-access memory (MRAM) [32]. These arrays enable matrix-vector multiplication (MVM) in the analog domain using basic circuit laws [9, 13]. Despite the progress in PIM architectures, previous research indicates that deploying pre-trained ML models, designed and opti- mized for digital von Neumann architectures, does not consistently achieve comparable performance on analog PIM architectures [2]. This is caused by several factors, including the limited numerical precision of memristive devices [20], and circuit imperfections such as interconnect parasitics [1], and device variations [16].",
    "source": "2505.22868v1_CrossNAS_A_Cross-Layer_Neural_Architecture_Search_.pdf",
    "length": 2380,
    "tokens": 492
  },
  {
    "text": "Figure 2 high- lights in different colors the parts that can be removed for sparse processing (red) versus what must be kept to keep the data-flow regular (blue). B. Separating Processing from Data-Flow Elements Inspired by the fact that no compute element is actually needed when the weight is zero, we decompose the PE into two components: the MAC unit (i.e., processing components), and the pipeline stages (data-flow components). The resulting two blocks are shown in Figure 3. Sparse Processing Element (SPE): A block containing only pipeline registers, required for sparse as well as non- sparse processing MAC unit: A block that can be optionally connected to an SPE, required only for non-zero weights. left_in_reg top_in_reg 0 accumulat or_reg X left_in top_in bottom_out right_out Fig. 2. Example architecture of a PE. Parts marked in red can be removed when the stationary weight is zero. left_in_reg accumulat or_reg left_in acc_in acc_out right_out weight_reg X left_in weight_in weight_out acc_in acc_out SPE MAC Fig. 3. Decomposing the PE to a data-flow and a compute block. C. Virtual Upscaled Systolic Array (VUSA) As MAC units are not required for zero weights, they can be instantiated in a lower dimension than the maximum virtual systolic array size. Based on that, a sparse N M systolic array with N A MAC units is proposed, where N is the number of rows, M is the number of columns, and A is the number of MAC units per row. Thus, each row has M SPEs but only A number of MAC units where 1 A M. If a weight is zero, then only an SPE is sufficient in that position. Otherwise, a MAC unit is connected to the SPE in that position for the needed arithmetic computation. If the number of non-zero weights per row is less than or equal to the number of available MAC units per row (A), then the sparse systolic array virtually grows to an N M systolic array.",
    "source": "2506.01166v1_VUSA_Virtually_Upscaled_Systolic_Array_Architectur.pdf",
    "length": 1876,
    "tokens": 460
  },
  {
    "text": "While ResBench is not explicitly designed to emphasize combi- national logic and arithmetic operations, the current problem set naturally includes a high proportion of such designs. Future work will expand the benchmark to include more sequential designs, such as pipelined architectures and state-driven circuits. Additionally, although the current evaluation focuses on Verilog, our framework is designed to support multiple HDLs. Future efforts will extend support to VHDL and high-level synthesis (HLS) tools. Acknowledgments We thank the anonymous reviewers for their valuable feedback and suggestions. The support of the United Kingdom EPSRC (grant number UKRI256, EP V028251 1, EP N031768 1, EP S030069 1, and EP X036006 1), Intel, and AMD is gratefully acknowledged. AI Usage Statement: This work involves the use of generative AI in multiple aspects. The methodology presented in this paper focuses on evaluating the ability of AI models to generate HDL code. As such, all experimental results are based on Verilog designs produced by LLMs. For writing, ChatGPT-4 and Llama 3.1 were used to refine phrasing, improve clarity, and proofread the text. HEART 25, May 26 28, 2025, Kumamoto, Japan C. Guo and T. Zhao References [1] Abdelatty, M., Ma, J., and Reda, S. MetRex: A benchmark for Verilog code metric reasoning using llms. arXiv preprint arXiv:2411.03471 (2024). [2] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [3] Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical report. arXiv preprint arXiv:2309.16609 (2023).",
    "source": "2503.08823v2_ResBench_Benchmarking_LLM-Generated_FPGA_Designs_w.pdf",
    "length": 1748,
    "tokens": 482
  },
  {
    "text": "It was originally developed by Google to predict the faces' identities [18]. It extracts high-quality features from the face and predicts a 128-elements vector representation called face embedding. Face embeddings are then mapped to generate a compact Euclidean space, where L2 distances are calculated to measure face similarity [21]. Similarly to FaceDetect, its backbone network consists of a cascade of convolutional, pooling and fully connected layers. An important design aspect for accelerating the system throughput is the policy to allocate the FaceDetect and FaceNet models onto the GPU and DLA hardware engines. For instance, to select the most efficient hardware allocation, it is important to map supported layers of the models on DLA while minimizing the data transfer between the hardware engines. As (c) Time Init Display DLA GPU Display DLA GPU (b) Time Init Display Display GPU DLA DLA GPU Init Reduced time due to tracker integration Init GPU Display GPU Init Display Time (a) Init Reduced time due to hardware allocations Figure 3: Example of hardware allocation (a) without using pipelining and tracker (b) using the DLA GPU cores in the pipelining (c) using pipelining and tracker (b) (a) (b) Figure 4: Model Structure for (a) FaceDetect-ResNet18 (b) FaceNet 5 was mentioned earlier, DLA supports several layers of CNN models under some conditions. Specifically, in the case of the FaceNet model, compiling the model to run on DLA through TensorRT results in the creation of several shuffle layers. The purpose of a shuffle layer is to transform the format of the output of a given layer to make it compatible with the DLA hardware architecture. Although DLA supports the shuffle layer, generated shuffle layers did not follow the shuffle layers requirements, and therefore, they were allocated to run on GPU. For any unsupported layer that falls back to the GPU, its output must be transferred to the DLA s local memory after being executed in the GPU core. If a non-connected supported layer repeats several times or a model has many separated unsupported layers, the model partitioning will increase device- to-device memory copy (memcpy).",
    "source": "2505.04524v1_Edge-GPU_Based_Face_Tracking_for_Face_Detection_an.pdf",
    "length": 2164,
    "tokens": 451
  },
  {
    "text": "[41] S. Palnitkar, in Verilog HDL: a guide to digital design and synthesis, 2003. [42] D. Harris and N. Weste, in CMOS VLSI Design: A Circuits and Systems Perspective, 2010. [43] Llama Team, The Llama 3 herd of models, in arXiv, 2024. [44] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., Evaluating large language models trained on code, in arXiv, 2021. [45] S. Ouyang, J. M. Zhang, M. Harman, and M. Wang, An empirical study of the non-determinism of chatgpt in code generation, in TOSEM, 2025. [46] J. Li, G. Li, Y. Li, and Z. Jin, Structured chain-of-thought prompting for code generation, in TOSEM, 2025. [47] S. Williams, in The Icarus Verilog Compilation System, 2024. [48] Synopsys, Inc, in Synopsys VCS, 2023. [49] Cadence Design Systems, Inc, in Xcelium, 2023. [50] S. Takamaeda-Yamazak, Pyverilog: A python-based hardware de- sign processing toolkit for Verilog HDL, in ARC, 2015. [51] W. Snyder, Verilator and SystemPerl, in DAC, 2004. [52] N. Pinckney, C. Batten, M. Liu, H. Ren, and B. Khailany, Revisiting verilogeval: A year of improvements in large-language models for hardware code generation, in TODAES, 2025. [53] J. Blocklove, S. Garg, R. Karri, and H. Pearce, Chip-Chat: Challenges and opportunities in conversational hardware design, in MLCAD Work- shop, 2023. [54] Silicon Integration Initiative (Si2), NanGate open cell library and free PDK libraries, 2024, accessed: 2024-10-29.",
    "source": "2503.11662v2_Lorecast_Layout-Aware_Performance_and_Power_Foreca.pdf",
    "length": 1475,
    "tokens": 489
  },
  {
    "text": "Critically, many of the cur- rently existing benchmarks for both Verilog generation and Verilog understanding (Chang et al., 2024; Liu et al., 2025) do not include designs exceeding 2048 tokens in length. 4.4. Training Learning Challenges As we previously discuss in section 2.1 HDLs, such as Ver- ilog, are fundamentally different from traditional imperative programming languages, as they are used to describe the structure of digital circuits, rather than defining sequential behavior. This poses unique challenges for LLMs, includ- ing those optimized for coding (Rozi ere et al., 2024), as 6 ML For Hardware Design Interpretability: Challenges and Opportunities these models have been primarily trained on sequential pro- gramming languages. Such a model may misinterpret a design s concurrency, assuming two components execute sequentially in the order they appear in the code. RTL code also involves underlying digital design concepts that are rarely explicitly documented within the code itself but can be essential for understanding the design s perfor- mance and functionality. Critical path timing, circuit area, and the effects of propagation delays are crucial to accurate design analysis but are not explicitly represented in Verilog. In hardware design, different logic components exhibit vary- ing delay characteristics. For instance, an adder generally has higher delay than a basic AND gate due to the addi- tional gates involved in carry propagation. Now consider a scenario where a model trained on traditional software- oriented code is asked to document the critical path in a Verilog design. Without an understanding of component- specific delays, such a model might naively identify the longest dependency chain as the critical path, focusing solely on operation count. In doing so, it could miss a shorter path that includes higher-delay components, such as adders, which ultimately defines the actual critical path. Figure 4. We asked GPT4o-mini to evaluate the critical path of simple Verilog modules. This figure shows how the model will give incorrect or inconsistent answers. For this example, the path leading to f will have a longer critical path due to carry logic necessary for the adders. Figure 4 shows how GPT 4o-mini can make this exact mis- take when asked to identify the critical path in a simple Verilog design.",
    "source": "2504.08852v1_ML_For_Hardware_Design_Interpretability_Challenges.pdf",
    "length": 2353,
    "tokens": 468
  },
  {
    "text": "Moreover, the input sequences for TCAMs in the same column must remain consistent to minimize input pre-processing overhead for query packing. Given these constraints, we propose ODR to enhance memory utilization for resource-constrained environments that prioritize energy efficiency. As outlined in Algorithm 1, ODR first sorts conditions by occurrence frequency in descending 8 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS Algorithm 1 Occurrence-Based Double Reordering (ODR) Require: pool root to leaf paths Require: conditions pool.union() Ensure: condition order, path order 1: paths {} 2: sorted conditions conditions.sort() descending freq. 3: for c in sorted conditions.reverse() do 4: for path in pool do 5: if path.contains(c) then 6: paths.append(path) 7: pool.remove(path) 8: return sorted conditions, paths order, similar to FR. Instead of eliminating partial sequences when an entire TCAM row is irrelevant to a given path, which introduces sequential computational overhead as seen in FR, ODR optimizes path ordering based on condition usage. Paths that contain rare conditions are placed at the top, concentrating most of the X-state cells in the bottom-right TCAMs. TCAMs that are entirely filled with X-state cells can then be removed, reducing redundancy without distributing a single path across multiple rows, thereby avoiding additional computational overhead. Despite its effectiveness, ODR still leaves considerable redundancy, as shown in Fig. 4. For example, in the top- right TCAM after applying ODR, only a single meaningful cell remains, yet the entire TCAM is required for correct functionality. To further improve memory efficiency, the next section introduces a more aggressive data placement strategy that reduces CAM capacity requirement at the cost of higher energy consumption. However, its computational overhead is comparable to that of naive unified mapping, and is, in fact, even lower in practice. 4) Similarity-Based Path Clustering (SPC): Since memory redundancy arises from unencountered conditions, clustering paths with high similarity (i.e., those that share many con- ditions) can substantially improve space efficiency.",
    "source": "2506.05994v1_RETENTION_Resource-Efficient_Tree-Based_Ensemble_M.pdf",
    "length": 2202,
    "tokens": 464
  },
  {
    "text": "ResBench features 56 problems spanning 12 categories. The benchmark problems are designed to expose LLMs ResBench: A Resource-Aware Benchmark for LLM-Generated FPGA Designs HEART 25, May 26 28, 2025, Kumamoto, Japan Table 5: LUTmin for each LLM across categories GPT-3.5 GPT-4 GPT-4o GPT-o1 Llama3.1 Qwen-max Qwen-plus Qwen2.5-coder Codestral turbo mini 405B 32B fsm 3state 1 0 0 0 0 0 0 0 0 traffic light 1 1 2 0 0 2 3 2 elevator controller 3 3 2 2 2 2 2 2 2 vending machine 1 1 2 1 2 1 1 2 1 int sqrt 68 177 64 229 173 fibonacci 56 1 56 56 56 mod exp 4466 4669 1911 1678 power 79 93 93 93 93 93 log2 int 10 20 12 abs diff 12 12 14 12 12 12 12 12 modulo op 82 82 82 82 111 left shift 10 10 10 10 10 12 12 10 10 pipelined adder 0 16 0 0 15 pipelined multiplier 77 70 56 70 pipelined max finder 0 24 0 24 24 24 24 24 ùë•3 3ùë•2 3ùë• 1 49 49 0 91 0 91 0 91 49 (ùë• 2)2 (ùë• 2)2 (ùë• 2)2 64 33 96 11 108 108 26 18 33 (ùëé ùëè)2 (ùëé ùëè)2 0 213 59 16 213 16 16 16 relu 8 8 8 8 8 16 8 8 16 mse loss 216 64 64 216 64 216 64 64 compound interest 13060 10135 10135 52950 9247 10135 52950 currency converter 0 0 25 0 free fall distance 6 6 64 6 6 64 67 64 6 kinetic energy 70 70 54 54 54 54 54 54 54 potential energy 6 6 84 0 6 6 6 6 6 carbon footprint 174 121 110 92 121 121 110 110 110 heat index 16 16 201 16 195 16 124 201 201 air quality index 128 104 104 116 128 128 Number of wins 7 12 10 19 11 10 9 7 8 ability to generate Verilog designs optimized for FPGA resource usage. While ResBench is not explicitly designed to emphasize combi- national logic and arithmetic operations, the current problem set naturally includes a high proportion of such designs.",
    "source": "2503.08823v2_ResBench_Benchmarking_LLM-Generated_FPGA_Designs_w.pdf",
    "length": 1635,
    "tokens": 507
  },
  {
    "text": "Index Terms LLM, transformer, flashattention, softmax, neu- ral network acceleration, exponential function, RISC-V I. INTRODUCTION Transformer-based models such as the GPT family [2] and the LLaMa family [3], have emerged as a cornerstone of ma- chine learning, demonstrating state-of-the-art performance in diverse domains, including natural language processing (NLP), computer vision, and audio processing. These models leverage pre-trained representations on large-scale unlabeled datasets, enabling remarkable accuracy improvements in fine-tuned downstream tasks such as sentence classification and question answering. At the core of their success is the Transformer architecture [4], which utilizes the self-attention mechanism to model complex relationships within input sequences. Despite the interest in deploying Transformer-based models on mobile and edge devices, their substantial computational and memory requirements present challenges in meeting the resource and energy constraints of these devices. In encoders and the prefill stage of decoders, the computational complexity of attention layers scales quadratically with the input sequence length, leading to memory and computational overheads that necessitate mitigation by means of dedicated acceleration. This work was supported by the NeuroSoC project, funded under the European Union s Horizon Europe research and innovation programme (Grant Agreement No. 101070634). Base Optim 128 Base Optim 256 Base Optim 512 Base Optim 1024 Base Optim 2048 Sequence Length 0 20 40 60 80 100 Runtime [ ] GPT-3 Forward Pass Runtime (FP16) FlashAttention Softmax Linear Others Fig. 1. Runtime breakdown for GPT-3 on a RISC-V multi-cluster platform [5]. For each sequence length, the left bar shows unoptimized GEMM results, while the right bar reflects optimized GEMM results. Although many architectures utilize General Matrix-Matrix Multiplication (GEMM) acceleration to alleviate the com- putational burden, performance bottlenecks are increasingly shifting toward non-linear operations, especially the Softmax function within the attention layers. Accelerating Softmax poses challenges due to its non-linear, non-pointwise nature and its reliance on a transcendental function, i.e. the exponentiation. The low arithmetic intensity of Softmax constrains parallelism and processing efficiency, a limitation that becomes more pronounced as GEMM latency decreases with acceleration.",
    "source": "2504.11227v1_VEXP_A_Low-Cost_RISC-V_ISA_Extension_for_Accelerat.pdf",
    "length": 2438,
    "tokens": 491
  },
  {
    "text": "2023. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning. PMLR, 38087 38099. [33] Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas Moshovos. 2020. Gobo: Quantizing attention-based nlp models for low latency and energy ef- ficient inference. In 2020 53rd Annual IEEE ACM International Symposium on Microarchitecture (MICRO). IEEE, 811 824. [34] Ben Zandonati, Adrian Alan Pol, Maurizio Pierini, Olya Sirkin, and Tal Kopetz. 2022. Fit: A metric for model sensitivity. arXiv preprint arXiv:2210.08502 (2022). [35] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. 2024. Atom: Low- bit quantization for efficient and accurate llm serving. Proceedings of Machine Learning and Systems 6 (2024), 196 209. 12",
    "source": "2504.14152v1_FGMP_Fine-Grained_Mixed-Precision_Weight_and_Activ.pdf",
    "length": 880,
    "tokens": 254
  },
  {
    "text": "TABLE VI IMPACT OF LORECAST PROMPTING IN COMPARISON WITH OTHER PROMPTING TECHNIQUES WITH THE SAME VERILOG CODE TO POWER TNS PREDICTION TECHNIQUE. Design Ground Truth Lorecast Forecast with RTLLM [12] Prompting Forecast with VerilogEval [11] Prompting Power TNS Syntax Func Power TNS Syntax Func Power TNS Syntax Func Power TNS right shifter 992 0.069 748 0.0689 773 0.0690 - - adder bcd 8 0 24 0 37 0.0005 - - signal generator 1508 0.225 1547 0.2135 1346 0.2130 4774 0.0433 multiply 35 0 37 0.0001 48 0.0002 - - accu 4042 0.306 3996 0.3062 3717 0.2822 - - LIFObuffer 29 0 33 0.0001 - - - - asyn fifo 100 0 160 0.0003 - - - - sobel filter 49475 0.995 47813 0.9541 - - - - matmul22 307 0 326 0 - - - - mux256to1v 79 0 45 0.0002 45 0.0001 36 0.0001 pe 32bit 84818 1.139 86442 1.1432 87456 1.1463 2003 0.0421 izigzag 221131 0.05 217893 0.0501 - - - - huffmandecode 37461 0.877 37102 0.8751 - - - - pe 64bit 334799 1.584 329173 1.5870 332914 1.6127 - - fft 16bit 466462 0.9 458226 0.9019 - - - - Conditional Error (1 ) (1 ) (48 ) (48 ) (98 ) (99 ) - Forecast can not be performed due to syntax errors in Verilog code generated by LLMs. Power unit is ¬µW and TNS unit is ns. Direct LLM-based forecasting without Verilog code generation. One may ask: why not let an LLM directly forecast performance and power without generating Verilog code.",
    "source": "2503.11662v2_Lorecast_Layout-Aware_Performance_and_Power_Foreca.pdf",
    "length": 1334,
    "tokens": 478
  },
  {
    "text": "3.2 Setting the Threshold for Precision Assignment In order to determine which blocks can be quantized to reduced precision, we need to determine the threshold for the importance score ùêº L(v), above which the block must be retained in higher precision. One option would be to compute the threshold for a target ratio of high-precision to low-precision blocks dynamically for each layer. However, sweeping over the activation tensor for layer ùëôonline during inference to compute ùêº L(v) for each block and then computing the target threshold for that layer would lead to unacceptable inference overhead (since we would need to write out each output block from the previous operation in both precisions before we knew which precision was going to be used). It is there- fore preferable to calibrate offline to determine the threshold for activations for a target mixed-precision ratio. Namely, for a target mixed-precision ratio ùëÖ, we can assign ùëÖ of blocks to be in higher precision by setting the threshold ùëáùëôùëúùëêùëéùëôto be the ùëÖ-th percentile of the impact score ùêº L(v), computed over all ùêΩùëôblocks v in a single weight or activation tensor W(ùëô) or X(ùëô): ùëáùëôùëúùëêùëéùëô ùëÉùëÖ({ùêº L(v(ùëó))}ùêΩùëô ùëó 1) (9) where ùëÉùëÖ() denotes the ùëÖ-th percentile. Additionally, different layers in the network exhibit differing sensitivities in terms of their impacts on the model output. In order to allow our policy to adapt to varying sensitivities at different layers, instead of calibrating for a separate threshold for each layer, we instead set a single global threshold across the entire model (one for weights and one for activations). Since our sensitivity-weighted policy for precision assignment estimates the impact that quantizing the block will have on the final model output, it is already normalized across different layers (meaning that we can use the same threshold across different layers in the network even if they have different average magnitudes).",
    "source": "2504.14152v1_FGMP_Fine-Grained_Mixed-Precision_Weight_and_Activ.pdf",
    "length": 1930,
    "tokens": 490
  },
  {
    "text": "(b) Dataflow of Protein Folding Block (1 Block). 2.3 Protein Structure Prediction Model (PPM) Protein Structure Prediction Model (PPM) aims to predict the three- dimensional folding structure of a protein from its amino acid sequence. Recent state-of-the-art PPMs [3, 5, 33, 39] demonstrate exceptional performance through the use of attention mechanisms. Figure 2(a) shows the overall process of PPM, including Input Embedding, Protein Folding Block, and the Structure Module. In the Input Embedding stage, the amino acid sequence of a protein is taken as input and converted into two types of biological informa- tion. Pair Representation contains information about interactions between pairs of amino acids in the protein sequence. It undergoes iterative updates to capture the protein s distogram patterns, reflect- ing positional relationships. Pair Representation has a dimension of (Ns, Ns, Hz), where Ns is the length of the protein sequence and Hz is the hidden dimension of Pair Representation, typically set to 128, which is a significantly smaller value compared to other attention- based models [58]. Here, a token in PPM means a vector in the Hz direction with a dimension of (1, 1, Hz), similar to attention- based models [64]. Sequence Representation contains information derived from other organisms similar to the input protein. It has a dimension of (Ns, Hm), where Hm is the hidden dimension of Sequence Representation, typically set to 1024. Some models, such as AlphaFold2 [33], use Multiple Sequence Alignment (MSA) as the Sequence Representation, combining information from multiple organisms. In the Protein Folding Block stage, the core attention mechanism is applied. At this stage, both Pair and Sequence Representations are used together to capture relationships between all sequence positions in the amino acid sequence, forming the information required for protein structure prediction. The Evoformer in Al- phaFold2 [33] and the Folding trunk in ESMFold [39] are examples of this block. Figure 2(b) shows the dataflow of a Protein Folding Block, especially a Folding trunk in ESMFold. A key part of this stage is the Pair Representation dataflow, which effectively captures the interactions between amino acid positions.",
    "source": "2505.05893v1_LightNobel_Improving_Sequence_Length_Limitation_in.pdf",
    "length": 2252,
    "tokens": 481
  },
  {
    "text": "Adaptive load balancing in distributed machine learning systems. Journal of Parallel and Distributed Computing, 145:45 58, 2020. [59] Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A Smith, and Luke Zettlemoyer. Demix layers: Disentangling domains for modular language modeling. arXiv preprint arXiv:2108.05036, 2021. [60] Suchin Gururangan, Margaret Li, Mike Lewis, Weijia Shi, Tim Althoff, Noah A Smith, and Luke Zettlemoyer. Scaling expert language models with unsupervised domain discovery. arXiv preprint arXiv:2303.14177, 2023. [61] Linley Gwennap. Groq rocks neural networks. Microprocessor Report, Tech. Rep., jan, 2020. [62] Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, Lucy Sun, Alex Wardle-Solano, Hannah Szabo, Eka- terina Zubova, Matthew Burtell, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Alexander R. Fabbri, Wojciech Kryscinski, Semih Yavuz, Ye Liu, Xi Victoria Lin, Shafiq Joty, Yingbo Zhou, Caiming Xiong, Rex Ying, Arman Cohan, and Dragomir Radev. Folio: Natural language reasoning with first-order logic. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2024. [63] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. Journal of Ma- chine Learning Research, 22(241):1 124, 2021.",
    "source": "NSF_LLM_Medium_Proposal.pdf",
    "length": 1529,
    "tokens": 448
  },
  {
    "text": "[22] G. M. Danciu and A. Dinu, Coverage fulfillment automation in hardware functional verification using genetic algorithms, Applied Sciences, vol. 12, 2022. [23] S. Das, H. Patel, C. Karfa, K. Bellamkonda, R. Reddy, D. Puri, A. Jain, A. Sur, and P. Prajapati, Rtl simulation acceleration with machine learning models, in 2024 25th International Symposium on Quality Electronic Design (ISQED), 2024, pp. 1 7. [24] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, Imagenet: A large-scale hierarchical image database, in 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 248 255. [25] G. Dimitrakopoulos, E. Kallitsounakis, Z. Takakis, A. Stefanidis, and C. Nicopoulos, Multi-armed bandits for autonomous test application in risc-v processor verification, in 2023 12th International Conference on Modern Circuits and Systems Technologies (MOCAST), 2023, pp. 1 5. [26] A. Dinu, G. M. Danciu, and S, tefan Gheorghe, Level up in verification: learning from functional snapshots, in 2021 16th International Conference on Engineering of Modern Electric Systems (EMES), 2021, pp. 1 4. [27] M. Elver and V. Nagarajan, Mcversi: A test generation framework for fast memory consistency verification in simulation, in 2016 IEEE International Symposium on High Performance Computer 35 40 Architecture (HPCA), 2016, pp. 618 630. [28] M. Fajcik, P. Smrz, and M. Zachariasova, Automation of processor verification using recurrent neural networks, in 2017 18th International Workshop on Microprocessor and SOC Test and Verification (MTV), 2017, pp. 15 20. [29] M. Farkash, B. Hickerson, and M. Behm, Coverage learned targeted validation for incremental hw changes, in 2014 51st ACM EDAC IEEE Design Automation Conference (DAC), 2014, pp. 1 6.",
    "source": "2503.11687v1_Review_of_Machine_Learning_for_Micro-Electronic_De.pdf",
    "length": 1762,
    "tokens": 490
  },
  {
    "text": "This technique enables the development of a model that maintains high accuracy while significantly reducing computational requirements and memory foot- print. 2) Pruning: Following distillation, an iterative pruning strategy is applied to further reduce the model size. This step involves identifying and removing less important weights and connections, resulting in a sparser network structure. 3) quantisation: We implement a quantisation scheme that reduces the precision of weights and activations. 4) ACAM optimisations: Utilising the compressed model, we generate templates suitable for pattern matching, designed to utilise the analogue computation capabilities of ACAM hardware. A. Knowledge distillation Knowledge distillation forms a crucial component of the methodology for creating smaller, efficient models [11]. Our implementation follows a teacher-student framework where knowledge is transferred from a high-capacity teacher network to a more compact student network. The knowledge distillation process is controlled by two key parameters: Œ± and temperature T. The Œ± parameter balances the importance between learning from the teacher s soft predictions and learning from the ground truth labels. The temperature parameter T controls the softness of probability distributions during knowledge transfer, where higher tempera- tures produce softer distributions that reveal more fine-grained knowledge about the relationships between the classes that the teacher has learned. Traditional supervised learning typically uses hard labels for training. However, these hard labels don t capture the rich information about similarities between classes that the teacher model has learned. Knowledge distillation preserves this in- formation by using the teacher s soft probability distributions as training input for the student model. The knowledge distillation process is guided by a composite loss function that combines two components: L Œ±LKD(zs, zt) (1 Œ±)LCE(zs, y) (1) where LKD represents the knowledge distillation loss between the student outputs zs and teacher outputs zt, LCE denotes the standard cross-entropy loss between student predictions and ground truth labels y, and Œ± is the balancing parameter.",
    "source": "2502.10089v1_A_Hybrid_Edge_Classifier_Combining_TinyML-Optimise.pdf",
    "length": 2222,
    "tokens": 405
  },
  {
    "text": "Input encoding method planes (M) VGG-Small MUXORNet-11 FP Bin. FP Bin. Baseline gamma-inversed 32-b 82.29 79.44 83.03 78.44 Fixed Linear Thermometer [11] 8 76.51 73.40 78.39 74.99 16 77.99 75.09 80.81 76.98 32 78.86 75.86 80.85 77.30 Ours GLT 8 78.24 75.69 80.90 77.45 16 79.54 76.46 81.78 78.05 32 80.35 77.88 82.06 78.51 B. Validation of the block pruning on fully-binarized model In this part, we consider the MUXORNet-11 model with GLT and M 32, trained on gamma-inversed dataset (78.51 acc. ), to conduct experiment. The pre-trained model after the first stage in V-A is used as teacher in our KD scheme. We set the groups g as 1, 2, 8 for three blocks, respectively, since the last layers are less sensitive to model s performance. We compare our gradual block pruning method with three competitors, involving 1) baseline: the pruned model but trained from scratch; 2) depth: depth pruning [15] using our LWCs as auxiliary model but trained in one shot without the KD loss Ldistr; 3) magnitude: channel-pruning based on the magnitude of the latent weights (inspired by [16]) in which the pruning ratio is computed to have the same model size as the block-pruned model. The pruned models are trained during 300 epochs with learning rate initialized at 10 3 and reduced to 10 10 using cosine decay scheduler. Figure 5 shows the trade-off of model size BOPs-accuracy loss of the pruned models. At each pruning point, our method always offers more than 3.6 higher accuracy compared to other methods. In particular, when pruning the third block, we successfully reduce the model size by 70 and the number of BOPs [26] by 16 , this nearly without accuracy drop. Even if we seek for a model of under 0.5Mb and 1GBOPs, our method still reaches 73 accuracy while other methods cannot exceed 68 .",
    "source": "2505.13462v1_End-to-end_fully-binarized_network_design_from_Gen.pdf",
    "length": 1792,
    "tokens": 487
  },
  {
    "text": "Meanwhile, the optimal transformation passes are identified via hierarchical auto-tuning based on the performance optimization feedback loop. Specifically, we combine inter-pass Monte Carlo Tree Search [27] for optimal transformation sequencing and intra- 14 TABLE VI PERFORMANCE COMPARISON OF MATRIX MULTIPLICATION METHODS ACROSS DIFFERENT HARDWARE PLATFORMS C910(GFLOPS), NVIDIA RTX4070(TFLOPS), NVIDIA A100(TFLOPS). SPEEDUP RATIOS FOR QIMENG-GEMM ARE CALCULATED AGAINST OPENBLAS (C910) AND CUBLAS (RTX 4070, A100). THE A100 AND RTX4070 GPU UTILIZES CUDA CORES. Hardware Method Dimension (M K N) 1024 2048 4096 GPT-4o [4] 0.14 0.10 0.09 C910 Claude 3.5 Sonnet [75] 2.64 1.56 0.74 (RISC-V) OpenBLAS [76] 5.01 5.11 4.85 QiMeng-GEMM 9.91(1.98 ) 10.08(1.97 ) 10.23(2.11 ) GPT-4o 1.77 1.78 1.65 RTX 4070 Claude 3.5 Sonnet 1.71 1.79 1.61 (NVIDIA) cuBLAS [62] 10.79 12.77 12.78 QiMeng-GEMM 11.47(1.06 ) 13.31(1.04 ) 14.16(1.11 ) GPT-4o 4.19 4.27 4.71 A100 Claude 3.5 Sonnet 4.64 5.33 5.27 (NVIDIA) cuBLAS 16.26 17.20 18.97 QiMeng-GEMM 12.61(0.77 ) 16.17(0.94 ) 18.27(0.96 ) pass constraint-based auto-tuning of critical tuning parameters, such as memory tiling configurations. Ultimately, our solution enables an automated tensor program transcompiler across var- ious processors like Nvidia GPUs [71], Cambricon MLU [72], AMD MI accelerators [73], Intel DLBoost [74], and program- ming models like SIMT, SIMD.",
    "source": "2506.05007v1_QiMeng_Fully_Automated_Hardware_and_Software_Desig.pdf",
    "length": 1405,
    "tokens": 499
  },
  {
    "text": "Therefore, for a fair comparison, we exclude the proteins that cannot be processed on GPUs without the chunk option and conduct experiments on the remaining datasets. The CAMEO dataset is excluded because it can already be fully pro- cessed without the chunk option. Figure 14(c) shows LightNobel achieved 5.62-6.73 , 5.32-6.49 lower latency with the chunk op- tion and 1.47-2.42 , 1.19-2.19 lower latency without the chunk option compared to A100 and H100 in this experiment. We also conduct experiments on proteins that GPUs cannot process without the chunk option to evaluate the performance of LightNobel on proteins with long sequence lengths.",
    "source": "2505.05893v1_LightNobel_Improving_Sequence_Length_Limitation_in.pdf",
    "length": 648,
    "tokens": 145
  },
  {
    "text": "2011. Introduction to Remote Sensing. Guilford press. [15] Arezoo Aghaei Chadegani, Hadi Salehi, Melor Md Yunus, Hadi Farhadi, Masood Fooladi, Maryam Farhadi, and Nader Ale Ebrahim. 2013. A Comparison between Two Main Academic Literature Collections: Web of Science and Scopus Databases. Asian Social Science 9, 5 (April 2013), p18. [16] C Chellaswamy, MM Manjula, B Ramasubramanian, and A Sriram. 2024. FPGA-based Remote Target Classification in Hyperspectral Imaging Using Multi-Graph Neural Network. MICROPROCESSORS AND MICROSYSTEMS 105 (March 2024). [17] Hanting Chen, Yunhe Wang, Chunjing Xu, Boxin Shi, Chao Xu, Qi Tian, and Chang Xu. 2020. AdderNet: Do We Really Need Multiplications in Deep Learning?. In Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition. 1468 1477. [18] L Chen, X Wei, WC Liu, H Chen, and L Chen. 2020. Hardware Implementation of Convolutional Neural Network-Based Remote Sensing Image Classification Method. In Beijing Institute of Technology, Q Liang, X Liu, Z Na, W Wang, J Mu, and B Zhang (Eds. ), Vol. 516. 140 148. [19] Yu-Hsin Chen, Tien-Ju Yang, Joel Emer, and Vivienne Sze. 2019. Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices. arXiv:1807.07928 [cs] [20] Gong Cheng, Junwei Han, and Xiaoqiang Lu. 2017. Remote Sensing Image Scene Classification: Benchmark and State of the Art. Proc. IEEE 105, 10 (Oct. 2017), 1865 1883. [21] Gong Cheng, Junwei Han, Peicheng Zhou, and Lei Guo. 2014. Multi-Class Geospatial Object Detection and Geographic Image Classification Based on Collection of Part Detectors. ISPRS Journal of Photogrammetry and Remote Sensing 98 (Dec. 2014), 119 132.",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 1681,
    "tokens": 479
  },
  {
    "text": "Ad- ditionally, the incurred overheads to the system perfor- mance and the hardware area are shown to be minimal. II. BACKGROUND: SPARSE SYSTOLIC TENSOR ARRAYS A sparse systolic tensor array computes the matrix product C A W of a dense input matrix A and a structured-sparse weight matrix W. It adopts the same systolic array architecture as a conventional dense array, but, instead of using a scalar Processing Element (PE) as its building block, it employs a more complex Tensor PE, aptly called TPE [5]. Scalar PEs in a Weight-Stationary (WS) dataflow accept a single input value and use a single local (pre-loaded) weight value to perform a Multiply-Accumulate (MAC) operation per cycle. In contrast, Tensor PEs take a block of M consecutive input values and use up to N local weight values to perform up to N multiplications in each MAC operation. This micro-architecture supports N:M structured block sparsity. The architecture of a two- row sparse tensor array is illustrated in Fig. 2. This array is configurable and can support both 2:4 and 1:4 structured sparsities. As shown in the figure, each TPE consists of two weight registers storing the (up to) two non-zero weight elements identified within every four elements of the pruned weight matrix. These weight registers are loaded with the appropriate weights during a distinct weight-loading phase, as dictated by the WS dataflow. During the computation phase, four consecutive input values from the same row of matrix A are fed into every TPE from the West side. To perform matrix multiplication, up to two of the four input elements are selected (using multiplexers) and multiplied with the locally- stored weights. The selection of the suitable input value(s) is facilitated by two 4-to-1 multiplexers that are controlled by the column indexes of the stored weights. The resultant products in each TPE are accumulated downwards across each column of the tensor array. When structured sparsity of 1:4 is enabled, only one of the two multiplexers and multipliers will be activated in each TPE.",
    "source": "2504.18628v1_Periodic_Online_Testing_for_Sparse_Systolic_Tensor.pdf",
    "length": 2057,
    "tokens": 468
  },
  {
    "text": "and Area (PPA) metrics. Additionally, CraftRTL [22] enriched model understanding by integrating auxiliary design artifacts, e.g., state diagrams and waveforms into the training process. While these methods have significantly advanced LLM- based RTL code generation, they share a common underlying limitation: they inherit natural language-oriented decoding strategies, which are not suited to the strict syntactic, struc- tural, and semantic constraints of RTL code. Specifically, they may produce hallucinated logic, incomplete modules, or structurally invalid outputs, especially when the model encounters complex or uncommon design scenarios. Moreover, even in fine-tuned models, decoding is typically performed using greedy search, beam search, or top-k sampling, which is not optimized for the unique demands of RTL. To date, no prior study has explored decoding-time adaptations as a means of improving RTL generation with LLMs. This is a critical gap in the field: while prompt engineering and fine-tuning rely on extensive human effort or large-scale dataset curation, decoding-time strategies offer a lightweight, generalizable, and model-agnostic alternative capable of significantly enhancing code quality without additional data collection or training. B. Decoding Strategies in LLMs LLMs generate text through an autoregressive decoding process, producing one token at a time based on the prob- ability distribution conditioned on the previously generated context [26]. While the model computes probabilities over the vocabulary, the actual output depends on the decoding strategy, which governs how tokens are selected. These strategies are broadly divided into the following categories: (1) Deterministic Decoding: Deterministic methods prioritize coherence and confidence by always selecting the most proba- ble tokens. For instance, greedy decoding chooses the highest- probability token at each step, making it simple and fast but of- ten resulting in repetitive or generic outputs [27]. Beam search [15] improves upon this by maintaining multiple candidate sequences to optimize the overall sequence-level likelihood. However, it still tends to favor low-diversity completions due to its emphasis on probability maximization. (2) Stochastic Probabilistic Decoding: Stochastic methods introduces controlled randomness to increase diversity and re- duce repetition. Temperature sampling [28] adjusts the sharp- ness of the probability distribution: lower temperatures make the model more confident (favoring top tokens), while higher values flatten the distribution to allow more exploration.",
    "source": "2507.02226v1_DecoRTL_A_Run-time_Decoding_Framework_for_RTL_Code.pdf",
    "length": 2609,
    "tokens": 500
  },
  {
    "text": "Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [2] Fan Cui, Chenyang Yin, Kexing Zhou, Youwei Xiao, Guangyu Sun, Qiang Xu, Qipeng Guo, Yun Liang, Xingcheng Zhang, Demin Song, et al. Origen: Enhancing rtl code generation with code-to-code augmentation and self-reflection. In Proceedings of the 43rd IEEE ACM International Conference on Computer-Aided Design, pages 1 9, 2024. [3] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL [4] DeepSeek-AI. Deepseek-v3 technical report, 2025. URL 19437. [5] Mingzhe Gao, Jieru Zhao, Zhe Lin, Wenchao Ding, Xiaofeng Hou, Yu Feng, Chao Li, and Minyi Guo. Autovcoder: A systematic framework for automated verilog code generation using llms. In 2024 IEEE 42nd International Conference on Computer Design (ICCD), pages 162 169. IEEE, 2024. [6] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021. [7] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model, 2025. URL [8] Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. O1 replication journey part 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson?, 2024. URL https: arxiv.org abs 2411.16489.",
    "source": "2505.24183v2_CodeV-R1_Reasoning-Enhanced_Verilog_Generation.pdf",
    "length": 1599,
    "tokens": 488
  },
  {
    "text": "A marks improvement, while a denotes a decline compare with the original design. Two comparison scenarios are shown: without compiler optimization (upper improvement) and with compiler optimization (lower improvement). Model Method sppm redundancy subexpression elim adder architecture vending fft Power (mW) Time (ns) Area (¬µm 2) Power (mW) Time (ns) Area (¬µm 2) Power (mW) Time (ns) Area (¬µm 2) Power (mW) Time (ns) Area (¬µm 2) Power (mW) Time (ns) Area (¬µm 2) Original 2.863 7.41 40102.6 5.265 11.09 10989.1 0.418 2.78 1023.5 7.61 227.86 176982.98 58.23 8.26 2255264.75 GPT-3.5-Turbo 2.863 7.41 40102.6 5.265 11.09 10989.1 0.418 2.78 1023.5 7.61 227.86 176982.98 58.23 8.26 2255264.75 GPT4o-mini 2.863 7.41 40102.6 5.265 11.09 10989.1 0.418 2.78 1023.5 7.61 227.86 176982.98 58.23 8.26 2255264.75 GPT-4-Turbo 2.863 7.41 40102.6 3.936 11.09 7783.03 0.392 2.74 1023.5 7.50 227.86 176982.98 58.23 8.26 2255264.75 GPT-4o 2.863 7.41 40102.6 5.296 11.09 8984.61 0.392 2.74 1023.5 7.61 227.86 176982.98 58.23 8.26 2255264.75 RTLCoder-DS 2.863 7.41 40102.6 5.265 11.09 10989.1 0.418 2.78 1023.5 7.61 227.86 176982.98 58.23 8.26 2255264.75 SymRTLO 1.762 7.29 29606.18 3.018 2.87 7358.8 0.328 1.97 762.6 6.97 227.86 164831.1 31.71 8.09 1726125.71 Improvement( ) 38.46 1.62 26.17 42.68 74.12 33.04 21.53 29.14 25.49 8.41 0 6.87 45.54 2.06 23.46 Original Compiler Opt.",
    "source": "2504.10369v1_SymRTLO_Enhancing_RTL_Code_Optimization_with_LLMs_.pdf",
    "length": 1359,
    "tokens": 590
  },
  {
    "text": "SPARAMX: ACCELERATING COMPRESSED LLMS TOKEN GENERATION ON AMX-POWERED CPUS Ahmed F. AbouElhamayed 1 Jordan Dotzel 1 Yash Akhauri 1 Chi-Chih Chang 1 Sameh Gobriel 2 J. Pablo Mu noz 2 Vui Seng Chua 2 Nilesh Jain 2 Mohamed S. Abdelfattah 1 ABSTRACT Large language models have high compute, latency, and memory requirements. While specialized accelerators such as GPUs and TPUs typically run these workloads, CPUs are more widely available and consume less energy. Accelerating LLMs with CPUs enables broader AI access at a lower cost and power consumption. This acceleration potential for CPUs is especially relevant during the memory-bound decoding stage of LLM inference, which processes one token at a time and is becoming increasingly utilized with reasoning models. We utilize Advanced Matrix Extensions (AMX) support on the latest Intel CPUs together with unstructured sparsity to achieve a 1.42 reduction in end-to-end latency compared to the current PyTorch implementation by applying our technique in linear layers. We provide a set of open-source customized sparse kernels that can speed up any PyTorch model by automatically replacing all linear layers with our custom sparse implementation. Furthermore, we demonstrate for the first time the use of unstructured sparsity in the attention computation achieving a 1.14 speedup over the current systems without compromising accuracy. Code: Aware-Automated-Machine-Learning tree main SparAMX 1 INTRODUCTION The usage of large language models (LLMs) has grown exponentially over the past few years and is expected to con- tinue its unprecedented growth. This has enabled many AI- driven applications, yet significant hardware and power re- sources are required, which have motivated recent attempts at model compression and acceleration. One such compres- sion method is unstructured pruning, which removes some model parameters without structural constraints like contigu- ous nonzero values. Although such a method can achieve high sparsity while maintaining accuracy, achieving actual speedup on current hardware, such as GPUs and TPUs, is challenging. This high-cost specialized hardware makes LLMs inacces- sible to many people and limits their use. CPUs, on the other hand, are more ubiquitous and therefore can be used to accelerate LLM-driven applications for a wider audience.",
    "source": "2502.12444v1_SparAMX_Accelerating_Compressed_LLMs_Token_Generat.pdf",
    "length": 2339,
    "tokens": 500
  },
  {
    "text": "Most prior work on LLM quantization focuses on weight and or activation quantization [17, 40, 42, 60, 69, 70] to reduce inference computation costs. RPTQ [78], SpinQuant [44], and QuaRot [5] introduce transformation matrices for weight and activation quantization. SmoothQuant [71] mitigates the quantiza- tion difficulty by transferring activation outlier scales to weights. SqueezeLLM [30] applies dense-and-sparse quantization for stor- ing weight outliers in full precision. However, many existing ap- proaches overlook the KV cache, whose size scales with sequence length and batch size, often becoming a major bottleneck for la- tency and throughput in batched LLM inference. Oaken overcomes this issue by employing an offline-online KV cache quantization algorithm with a customized hardware module, achieving high throughput with minimal accuracy degradation. LLM inference accelerator. DFX [21] is one of the first LLM ac- celerators, which is designed to accelerate the entire GPT2 model using HBM and FPGA. CXL-PNM [53] introduces a LLM accelerator largely leveraging the DFX design, while employing LPDDR for striking a sweet spot in the bandwidth-capacity tradeoff space for large-scale LLM serving. LPU [48] is another LLM accelerator that differs from prior work in optimizing its design for minimal latency. Table 4: Area overhead analysis of compression and decom- pression engines on TSMC 28nm. Module Area (mm2) Area ratio ( ) Matrix processing unit 0.908 22.86 Vector processing unit 0.239 6.03 Quantization engine 0.074 1.86 Dequantization engine 0.252 6.35 Compute core 3.971 100 TransPIM [88] proposes a PIM accelerator targeted for encoder- based transformer models such as BERT. AttAcc [52], IANUS [59], and NeuPIMS [20] employ PIM technologies for decoder-based transformer LLM serving, while they impose capacity pressure on the large-batch long-sequence serving scenarios. Unlike these prior works, this work devises Oaken, which jointly employs KV quan- tization and LPDDR for unleashing larger capacity and increased bandwidth to enable fast and efficient LLM serving.",
    "source": "2503.18599v2_Oaken_Fast_and_Efficient_LLM_Serving_with_Online-O.pdf",
    "length": 2098,
    "tokens": 496
  },
  {
    "text": "Dur- ing profiling, Qscore is collected using an off-the- shelf reward model to evaluate output quality. For NVIDIA GPUs and Intel CPUs, power is measured every 200ms using NVIDIA (pynvml) and Intel (psutil) APIs for energy modeling, respectively. 3.4 Step 4: Carbon Modeling Unlike prior work that profiles performance and energy without considering serving constraints, FUEL defines and calculates carbon emission per FU (CFU), measuring the emissions of FUs that meet certain serving constraints. Formally, CFU Total carbon emissions for all tokens Nf , Nf PN i 1 I(Qi Œ±) I(TTFTi Œ≤) I(TPOTi Œ≥), where N is the total number of output tokens, Nf is the total number of tokens considered FUs, Q is the Qscore, Œ±, Œ≤, and Œ≥ are the constraints for Qscore, TTFT, and TPOT, respectively. Note that we consider a token to meet the Qscore requirement if its corresponding response does, as Qscore is defined at the response level. Next, we describe how to calculate carbon emissions. Carbon emission calculation. Following prior work (Nguyen et al., 2024; Li et al., 2024d; Shi et al., 2024; Ding and Shi, 2024), total carbon emis- sions in LLM serving include operational carbon emission Cop and embodied carbon emissions Cem. We now describe how to calculate each. Operational carbon is calculated as the product of the energy consumed, Eop, and the carbon intensity of the energy source (CI). Carbon inten- sity is the amount of CO2eq emitted per kilowatt- hour (kWh) of electricity used (Maji et al., 2022; Li et al., 2024a; Yan et al., 2025). The operational carbon emission is thus given by: Cop Eop CI (1) Embodied carbon of a hardware device is deter- mined by factors such as processor chip area and memory capacity (Gupta et al., 2022; Faiz et al., 2024). The detail of modeling the total embodied carbon of a hardware device is in Appendix B. The embodied carbon emission of an LLM exe- cution over time t is calculated by amortizing the hardware s total embodied carbon Cem,total over its lifetime (LT), typically 5 to 7 years (Ostrou- chov et al., 2020).",
    "source": "2502.11256v2_Unveiling_Environmental_Impacts_of_Large_Language_.pdf",
    "length": 2061,
    "tokens": 500
  },
  {
    "text": "Since the simulation of generative AI (especially training monolithic LLMs) can take extremely long running times and may not even be feasible in some cases, we will also build, as the third component of our evaluation framework, using data from our actual machine experiments and simulations, analytical models that provide fast evaluation of EoE systems and LLMs with reasonable accuracy, such as [40, 169]. Our approach will em- body the best of both the worlds taking the deeper system level insights from real hardware simulators and extending it to large scale by analytically modeling their behavior as a complex system. Custom Accelerator Chip Systolic Array Cache Hierarchy DRAM HBM Chiplet Interconnect network Cacti gem5 ScaleSimv2 Ramulator Rapid Chiplet MQSim FlashSim gem5 GARNET GPGPU Sim Hardware stats Training dataset Analytical models Estimation for: Time H W choice Power Cost Accuracy Latency stats Accuracy stats User Input EoE Software Runtime HARDWARE SIMULATION ENVIRONMENT INFERENCE TRAINING Figure 7: An end-to-end evaluation environment Task-4.2: Methodology We plan to compare our proposed optimizations against state-of-the-art MoEs [38, 42, 57, 74, 83, 89], CoEs [60, 67, 131, 151] and monolithic LLMs [107, 192] on state-of-the-art GPUs, CPUs, and custom acceler- ators (e.g., Groq, Cerebras, SambaNova, Habana Gaudi, and GraphCore). In our evaluation, we will employ both architecture-level and LLM application-level met- rics. The former includes execution cycles, energy consumption and carbon footprint (by extending GA4HPC [11]). In addition to these, we will also use LLM application-level metrics such as LLMOps (an extension of MLOps [84] tailored for LLMs). To evaluate the effectiveness of our optimizations and compare them against state- of-the-art, in addition to the standalone LLM EoE systems, we will use various applications that employ LLMs (e.g., text generation, summarization, chatbot, language translation, and sentiment analysis) as well as emerging benchmarks like HellaSwag [195], TruthfulQA [97], GLUE [159], and MMLU [191].",
    "source": "NSF_LLM_Medium_Proposal.pdf",
    "length": 2082,
    "tokens": 500
  },
  {
    "text": "The runtime is primarily influenced by the precision param- eters that determine computation complexity through Equation TABLE IV COMPARISON OF DNN ACIM SIMULATORS DNN CIM Simulator CrossSim [37] AIHWKit [63] NeuroSim V1.4 [54] NeuroSim V1.5 Supported devices PCM, RRAM, EcRAM, DRAM RRAM, PCM, Flash SRAM, RRAM, FeFET SRAM, RRAM, FeFET, nvCap Supported network types Linear, Conv Linear, Conv, Recurrent, Transformer Linear, Conv, Recurrent Linear, Conv, Recurrent, Transformer Custom Compute Circuit Support No No No Yes Bit Slicing Support Yes No Yes Yes PPA Support No No Yes Yes PPA evaluation excluded. 3: DAC precision affects the number of input cycles, cell bit- precision (MLC) determines the number of weight compo- nents, and noise modeling adds statistical sampling overhead. With 8b 8b None configuration (DAC MLC Noise) shown in Table V, V1.5 achieves a baseline runtime of 0.95 ms image for VGG8 and CIFAR-10. When using 1b cells to represent 8b weights, runtime increases 4 to 4.1 ms image due to additional bit-slicing operations. Adding device noise results in negligible runtime increase and adding per-MAC output noise increases runtime to only 5.1 ms image. NeuroSim V1.5 achieves this dramatic improvement in simulation efficiency through three key advances: a PyTorch- optimized ACIM behavioral simulator, noise modeling with pre-characterized statistical noise modeling and TensorRT in- tegration for efficient quantization. As shown in Fig. 9, these optimizations deliver a 6.51 speedup over V1.4 for CIFAR- 10 inference with 1-bit cells. VGG8 CIFAR-10 VGG8 CIFAR-10 ResNet18 CIFAR-100 ResNet50 ImageNet Swin_t ImageNet 0.00 0.05 0.10 0.15 0.20 0.25 0.30 1b cell 2b cell 4b cell Runtime (Seconds Image) V1.4 V1.5 6.51x Fig. 9.",
    "source": "2505.02314v1_NeuroSim_V15_Improved_Software_Backbone_for_Benchm.pdf",
    "length": 1751,
    "tokens": 478
  },
  {
    "text": "Figure 3b shows that ensemble-spot can re- duce the cost by up to 3.3 when compared to ensemble-OD. For certain baselines like IRV2, ensemble-spot is also 1.5 cheaper than single-OD. However, the crucial downside of using transient VMs is that they can be unilaterally preempted by the cloud provider at any given point due to reasons like in- crease in bid-price or provider-induced random interruptions. As we will discuss further, Cocktail is resilient to instance failures owing to the fault-tolerance of ensembling by com- puting multiple inferences for a single request. Key takeaway: The cost-effectiveness of transient instances, is naturally suitable for hosting ensemble models. Fast Cache MobileNet NasNet ResNet50 DenseNet121 Dynamic Model Selection . . . Aggregator Master VM User Requests Queries Cost aware Procurement Importance Sampling Model-1 Model-2 Model-3 Model-4 Model-n output Heterogeneity Prediction Policy Autoscaler Resource Controller Load Balancer argmax O1 (latency) argmin O2 (accuracy) CPU GPU CPU GPU Objectives 1a 3 4b 1b 2 4 4a 4b 1 6 6b 6a w1 w2 w3 wk w4 3 5 Bin-Packing Weight Matrix L N Figure 5: High-level overview of Cocktail design. 4 Overall Design of Cocktail Motivated by our observations, we design a novel model- serving framework, Cocktail, that can deliver high-accuracy and low-latency predictions at reduced cost. Figure 5 depicts the high-level design of Cocktail. Users submit requests to a master VM, which runs a model selection algorithm, 1a to decide the models to participate in the ensemble. The participating models are made available in a model cache 1b for faster access and avoid re-computation for requests having similar constraints. Then, individual queries are dispatched to instances pools 2 dedicated for each model. The results from the workers are ensembled using an weighted majority voting aggregator 3 to agree upon a correct prediction. To efÔ¨Åciently address the resource management and scalability challenges, Cocktail applies multiple strategies. First, it maintains dedicated instance pools to serve indi- vidual models which simpliÔ¨Åes the management and load balancing overheads for every model.",
    "source": "cocktail.pdf",
    "length": 2175,
    "tokens": 499
  },
  {
    "text": "This outcome can be attributed to the structural similarity between the Verilog code generated by Lorecast and its functionally correct counterparts, as discussed in Section IV-D. TABLE VII EFFECT OF FUNCTIONALLY INCORRECT VERILOG CODE GENERATED BY LORECAST ON LOGIC LAYOUT SYNTHESIS (NOT FORECAST) RESULTS. Design Functionally Correct Functionally Incorrect Manual GPT4 Gemini 1.5 Pro GPT4o DeepSeek V3 Power TNS Power TNS Power TNS Power TNS Power TNS signal generator 1508 0.225 1346 0.279 1467 0.253 1367 0.263 asyn fifo 100 0 125 0 sobel filter 49475 0.995 48737 0.981 50537 1.029 47415 1.011 46734 1.026 : functionally correct and thus out of the scope of this experiment. : syntax correct but not synthesizable. Power unit is ¬µW and TNS unit is ns. E. Ablation Studies To assess the general applicability of Lorecast, we additionally conduct ablation studies on the RTLLM benchmark [12], a set of design tasks introduced in prior work, as a complementary evaluation beyond our primary testcases. Impact of different templates in prompting correctness without I-PREF. Although template-based structured prompting has been proposed in previous works [11] [12], different template styles also matter. In Table VIII, we show that our template style can improve the syntax correctness rate by 33 43 . Please note that I-PREF is not performed in these cases. TABLE VIII IMPACT OF DIFFERENT TEMPLATES ON SYNTAX AND FUNCTIONAL CORRECTNESS ACROSS LLMS (WITHOUT I-PREF). Prompt Technology Correctness Rate( ) GPT4 GPT4o Gemini 1.5 Pro DeepSeek V3 Syntax Func Syntax Func Syntax Func Syntax Func VerilogEval [11] 66.7 21.4 50 21.4 42.9 17.9 55.2 25 RTLLM [12] 86.2 39.3 82.8 42.9 71.4 42.9 79.3 53.6 Lorecast 100 57 93.1 50 86.2 57 93.1 57 Effect of functional correctness across LLMs.",
    "source": "2503.11662v2_Lorecast_Layout-Aware_Performance_and_Power_Foreca.pdf",
    "length": 1781,
    "tokens": 496
  },
  {
    "text": "[12] R. Child, S. Gray, A. Radford, and I. Sutskever, Generating long sequences with sparse transformers, arXiv preprint arXiv:1904.10509, 2019. [13] G. Bai, Y. Li, C. Ling, K. Kim, and L. Zhao, Sparsellm: Towards global pruning of pre-trained language models, in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [14] G. Hinton, Distilling the knowledge in a neural network, arXiv preprint arXiv:1503.02531, 2015. [15] N. Shazeer, Fast transformer decoding: One write-head is all you need, arXiv preprint arXiv:1911.02150, 2019. [16] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebr on, and S. Sanghai, Gqa: Training generalized multi-query transformer models from multi-head checkpoints, arXiv preprint arXiv:2305.13245, 2023. [17] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro et al., Efficient large-scale language model training on gpu clusters using megatron-lm, in Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 2021, pp. 1 15. [18] Y. Huang, Y. Cheng, A. Bapna, O. Firat, M. X. Chen, D. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu et al., Gpipe: Easy scaling with micro-batch pipeline parallelism, proceeding of Computer Science Computer Vision and Pattern Recognition, 2019.",
    "source": "2502.15763v1_Hybrid_Offline-online_Scheduling_Method_for_Large_.pdf",
    "length": 1378,
    "tokens": 444
  },
  {
    "text": "First the verilog is transformed to Symbolic Representation, then the Symbolic system applied minimization algorithm to optimize the FSM. 1 module example( 2 input wire clk, 3 input wire reset, 4 input wire [1:0] input_signal, 5 output reg output_signal); 6 parameter S0 3 b000,S1 3 b001,S2 3 b010,S3 3 b011,S4 3 b100,S5 3 b101; 7 reg [2:0] current_state, next_state; 8 always (current_state) begin 9 output_signal 0; 10 case (current_state) 11 S0: output_signal 1; 12 S2: output_signal 1; 13 S4: output_signal 1; 14 default: output_signal 0; 15 endcase 16 end 17 always (posedge clk or posedge reset) begin 18 if (reset) begin 19 current_state S0; Reset to state S0 20 end else begin 21 current_state next_state; 22 end 23 end 24 always ( ) begin 25 next_state current_state; 26 case (current_state) 27 S0: case (input_signal) 28 2 b00: next_state S0; 29 2 b01: next_state S1; 30 2 b10: next_state S2; 31 2 b11: next_state S3; 32 endcase 33 S1: case (input_signal) 34 2 b00: next_state S0; 35 2 b01: next_state S3; 36 2 b11: next_state S5; 37 endcase 38 S2: case (input_signal) 39 2 b00: next_state S1; 40 2 b01: next_state S3; 41 2 b10: next_state S2; 42 2 b11: next_state S4; 43 endcase 44 S3: case (input_signal) 45 2 b00: next_state S1; 46 2 b01: next_state S0; 47 2 b10: next_state S4; 48 2 b11: next_state S5; 49 endcase 50 S4: case (input_signal) 51 2 b00: next_state S0; 52 2 b01: next_state S1; 53 2 b10: next_state S2; 54 2 b11: next_state S5; 55 endcase 56 S5: case (input_signal) 57 2 b00: next_state S1; 58 2 b01: next_state S4; 59 2 b10: next_state S0; 60 endcase 61 endcase 62 end 63 endmodule Listing 1: Example Test Case: example1 state. 12 SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired Symbolic Reasoning 1 states : [ S0 , S1 , S2 , S3 , S4 , S5 ], 2 transitions : { 3 S0 : { 4 input_signal 00 : { next_state : S0 }, 5 input_signal 01 : { next_state : S1 }, 6 input_signal 10 : { next_state : S2 }, 7 input_signal 11 : { next_state : S3 } 8 }, 9 S1 : { 10 input_signal 00 : { next_state : S0 }, 11 input_signal 01 : { next_state : S3 }, 12 input_signal 11 : { next_state : S5 } 13 }, 14 S2 : { 15 input_signal 00 : { next_state : S1 }, 16 input_signal 01 : { next_state : S3 }, 17 input_signal 10 : { next_state : S2 }, 18 input_signal 11 : { next_state : S4 } 19 }, 20 S3 : { 21 input_signal 00 : { next_state : S1 }, 22 input_signal 01 : { next_state : S0 }, 23 input_signal 10 : { next_state : S4 }, 24 input_signal 11 : { next_state : S5 } 25 }, 26 S4 : { 27 input_signal 00 : { next_state : S0 }, 28 input_signal 01 : { next_state : S1 }, 29 input_signal 10 : { next_state : S2 }, 30 input_signal 11 : { next_state : S5 } 31 }, 32 S5 : { 33 input_signal 00 : { next_state : S1 }, 34 input_signal 01 : { next_state : S4 }, 35 input_signal 10 : { next_state : S0 } 36 } 37 }, 38 outputs : { S0 : { output_signal : 1}, 39 S1 : { output_signal : 0}, 40 S2 : { output_signal : 1}, 41 S3 : { output_signal : 0}, 42 S4 : { output_signal : 1}, 43 S5 : { output_signal : 0} } Listing 2: LLM-generated symbolic representation for example1 state.",
    "source": "2504.10369v1_SymRTLO_Enhancing_RTL_Code_Optimization_with_LLMs_.pdf",
    "length": 3084,
    "tokens": 1112
  },
  {
    "text": "9(a) and 9(b) show their end-to-end throughput on performance- and cost-optimized HSS, respectively. 0 2 4 6 8 CDE K-SVM RNN-HSS Sibyl Sibyl K-SVM Sibyl RNN-HSS CDE RL-Migr SAPM Harmonia Oracle 0 30 60 90 120 SYSTOR17 RocksDB YCSB MLPerf AVG Normalized Avg. Request Latency (a) Performance-Optimized HSS (b) Cost-Optimized HSS Figure 8: Performance of Harmonia and baselines on performance-optimized (top) and cost-optimized (bottom) HSS configurations, shown as average request latency nor- malized to Fast-Only. Lower is better. We make four key observations. First, Harmonia consistently out- performs all baselines in both HSS configurations. In performance- (cost-) optimized HSS, Harmonia improves performance by 49.5 (31.7 ) and end-to-end throughput by 49.4 (156.2 ) over the best- performing prior approach, Sibyl, on average. Second, Harmonia bridges the performance gap between Sibyl and Oracle by 64.2 (64.3 ) and achieves 71.7 (62.2 ) of Oracle s throughput on average in performance- (cost-) optimized HSS. 0.0 0.2 0.4 0.6 0.8 1.0 CDE K-SVM RNN-HSS Sibyl Sibyl K-SVM Sibyl RNN-HSS CDE RL-Migr SAPM Harmonia Oracle 0.00 0.02 0.04 0.06 0.08 0.10 SYSTOR17 RocksDB YCSB MLPerf AVG Normalized End-to-End Throughput (a) Performance-Optimized HSS (b) Cost-Optimized HSS 0.20 0.48 0.16 Figure 9: End-to-end throughput of Harmonia and baselines on performance-optimized (top) and cost-optimized (bottom) HSS. Throughput values (in IOPS) are normalized to Fast- Only policy. Higher is better.",
    "source": "2503.20507v2_Harmonia_A_Multi-Agent_Reinforcement_Learning_Appr.pdf",
    "length": 1496,
    "tokens": 475
  },
  {
    "text": "These optimizations are detailed as follows. Serving Large Language Models on Huawei CloudMatrix384 21 Rank 0 Rank N-1 Gating Top-K All-to-All Dynamic Quant FFN Local Combine Add Norm Original Dispatch Original Combine Gating Top-K Dynamic Quant FFN Local Combine Add Norm All-to-All All-to-All Local Dispatch Local Dispatch (a) A basic MoE computation flow with all-to-all communications. Fused Dispatch Fused Combine Gating Top-K Data-Sending Pipe FFN Add Norm Send Flag Count Wait Local Data Copy Data-Sending Pipe Send Flag Wait Local Combine Gating Top-K Data-Sending Pipe FFN Add Norm Send Flag Count Wait Local Data Copy Data-Sending Pipe Send Flag Wait Local Combine Rank 0 Rank N-1 (b) Our proposed MoE computation flow with FusedDsipath and FusedCombine. Fig. 10. Comparison between basic MoE computation flow with all-to-all communications and our proposed MoE computation flow with fused communication operators. 1 AIV-Direct Communication across NPUs: The conventional all-to-all communication among NPUs typically relies on communication firmware such as a system direct memory access (SDMA) engine to transfer data (red line in Figure 11). However, SDMA introduces considerable startup overhead, which becomes a critical performance bottleneck in ultra-low-latency scenarios, particularly during decode. To overcome this bottleneck, we design a new communication mecha- nism, which we refer to as AIV-Direct. AIV-Direct enables AI vector (AIV) cores to directly write data into the memory of remote NPUs via the UB interconnect, completely bypassing the latency-prone SDMA path (blue line in Figure 11). By eliminating SDMA s startup overhead, AIV-Direct provides a fast and lightweight pathway for peer-to-peer communication. This sharply reduces transfer initiation latency and accelerates inter-NPU data exchange, significantly improving performance in latency-sensitive operations such as decode. 2 Early Quantization: In the original MoE computation flow, as shown in Figure 10a, BF16 token data is transmitted during token dispatch, resulting in high communication volume. To mitigate this, we introduce early quantization by performing INT8 quantization before sending token data within FusedDispatch.",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 2223,
    "tokens": 480
  },
  {
    "text": "By addressing key limitations in quantized LoRA, LowRA leverages fine-grained precision assignment, adaptive quantization mappings, and optimized CUDA kernels to minimize memory overhead while preserving performance. Extensive evaluations show that LowRA achieves a superior performance precision trade-off above 2 bits and remains accurate at even 1.15 bits per parameter, reducing memory usage by up to 50 . This enables fine-tuning in ultra-resource-constrained environments, making LLMs more accessible for real-world applications. Looking ahead, LowRA paves the way for ultra-low-bit fine-tuning and deployment. We hope it inspires further research and brings efficient LLM fine-tuning and inference to mobile devices, embedded systems, and beyond. References [1] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. [2] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin King. Binarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701, 2020. [3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [4] Yuji Chai, Mujin Kwen, David Brooks, and Gu-Yeon Wei. Flexquant: Elastic quantization framework for locally hosted llm on edge devices, 2025. [5] Lequn Chen, Zihao Ye, Yongji Wu, Danyang Zhuo, Luis Ceze, and Arvind Krishnamurthy. Punica: Multi-tenant lora serving. Proceedings of Machine Learning and Systems, 6:1 13, 2024. [6] NVIDIA Corporation. Tesla P100 GPU Accelerator. data-center tesla-p100 , 2016.",
    "source": "2502.08141v1_LowRA_Accurate_and_Efficient_LoRA_Fine-Tuning_of_L.pdf",
    "length": 1892,
    "tokens": 496
  },
  {
    "text": "YOLOv3 i8 - 8.5 - - - - - - 7.0e-3 - - [86] KU040 AlexNetLite i16 5.71G -? 31 39 200 - - - - - KU040 MobileNetv1Lite i16 50.0M -? 31 39 200 - - - - - ML S [13] ZU7EV Deep Belief Network f32 - - 10 0 250 - - - 5.4e-3 - GNN F [138] U280 GNN f32 - - 43 37 260 400 38.0 10.5 2.7e-2 759 [137] ZU7EV GNN f32 2.13M 1.0 74 91 125 - 6.3 - 1.1e-4 9500 Automatic FINN CNN S [46] 7Z020 CNN b - - - - - - - - 3.3e-1 - [84] 7Z045 BNN b - - - - - - - - 2.1e-2 - [95] 7Z020 CloudSatNet-1 Q2 b - 1.4 14 - 100 - 2.5 - - 15.5 7Z020 CloudSatNet-1 Q4 i4 - 3.1 14 - 100 - 2.6 - - 15.5 Vitis AI CNN F [53] ZU19EG ETAUS i8 - - - - 106 329.9 1.6 199.9 1.1e-1 9.3 [71] ZU7EV RFA-YOLO i8 - 24.8 - - 200 - 15.8 - - 28 [87] ZU7EV YOLOv4 i8 105 G 245.0 - - 100 - 30.1 - 1.1e-1 17.9 ZU7EV YOLOv4-tiny 3L i8 15.3G 24.1 - - 100 - 26.4 - 1.9e-2 125 [104] ZU3EG ENet i8 4.06G 0.4 70 75 300 - 3.4 - - 25.2 ZU3EG ESPNet i8 3.71G 0.3 70 75 300 - 3.1 - - 11.7 ZU3EG FPN i8 17.3G 5.8 70 75 300 - 4.0 - - 14.1 ZU3EG U-Net i8 96.6G 7.4 70 75 300 - 3.4 - - 2.7 [119] ZU9EG ResNet-50 i8 - - - - - - - - 1.1e-2 93 ZU9EG SICNet i8 - - - - - - - - 1.8e-3 530.7 [136] ZU7EV Impr.",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 1131,
    "tokens": 599
  },
  {
    "text": "While hdl2v succeeds in improving LLM Verilog generation via fine-tuning, its real strength is the novel Verilog data it provides. Unlike prior work which focuses on augmentation of existing Verilog, we create multiple datasets of entirely new Verilog, which are both 1) unseen in LLM pre-training corpora and 2) not generated by LLMs themselves. We demonstrate the value of this approach by combining hdl2v VHDL and Chisel data with the existing Verilog corpus to boost the performance of data augmentation-based fine-tuning by 63 . In future work, we would like to combine our dataset with other data augmentation methods, reasoning models, and agentic flows to push Verilog generation performance of open-weight models to even higher levels. REFERENCES [1] [Online]. Available: [2] A. Amid, D. Biancolin, A. Gonzalez, D. Grubb, S. Karandikar, H. Liew, A. Magyar, H. Mao, A. Ou, N. Pemberton, P. Rigge, C. Schmidt, J. Wright, J. Zhao, Y. S. Shao, K. Asanovi c, and B. Nikoli c, Chipyard: Integrated design, simulation, and implementation framework for custom socs, IEEE Micro, vol. 40, no. 4, pp. 10 21, 2020. [3] J. Bachrach, H. Vo, B. Richards, Y. Lee, A. Waterman, R. AviÀázienis, J. Wawrzynek, and K. Asanovi c, Chisel: constructing hardware in a scala embedded language, in Proceedings of the 49th Annual Design Automation Conference, ser. DAC 12. New York, NY, USA: Association for Computing Machinery, 2012, p. 1216 1225. [Online]. Available: 6 [4] C. Batten, N. Pinckney, M. Liu, H. Ren, and B. Khailany, Pyhdl-eval: An llm evaluation framework for hardware design using python- embedded dsls, in Proceedings of the 2024 ACM IEEE International Symposium on Machine Learning for CAD, ser. MLCAD 24.",
    "source": "2506.04544v2_hdl2v_A_Code_Translation_Dataset_for_Enhanced_LLM_.pdf",
    "length": 1706,
    "tokens": 491
  },
  {
    "text": "We report SCC for STSB, MCC for CoLA, F1 score for QQP and MRPC, Accuracy for the remaining GLUE tasks. Models COLA SST-2 MRPC QQP MNLI-m MNLI-mm QNLI RTE WNLI STSB Score BERT(2MB) 3,93 75,20 63,88 81,03 64,95 63,65 62,75 49,38 70,68 6,74 54,22 MAMBA(2MB) 7,22 80,60 60,72 80,66 64,27 64,44 59,78 51,76 80,00 4,58 55,40 Embedder 9,65 78,90 62,25 83,28 62,06 62,17 65,40 52,73 77,20 15,58 56,92 Embedder Conv 9,25 79,10 60,50 82,98 61,98 60,93 62,08 52,00 79,16 16,10 56,41 BERT NE 5,22 77,34 63,18 81,12 64,14 64,53 66,76 51,14 85,62 4,72 56,38 BERT NE EA 2,66 78,68 61,90 83,16 62,65 61,63 62,36 48,38 85,62 8,94 55,60 EmbBERT 5,32 78,50 62,54 82,58 63,82 65,78 63,68 51,26 87,30 9,76 57,05 Table 12: Evaluation of pretrained and finetuned models on the GLUE benchmark (Embedder and Embedder Conv are directly trained on the donwstream datasets). We report SCC for STSB, MCC for CoLA, F1 score for QQP and MRPC, Accuracy for the remaining GLUE tasks.",
    "source": "2502.10001v1_EmbBERT-Q_Breaking_Memory_Barriers_in_Embedded_NLP.pdf",
    "length": 951,
    "tokens": 394
  },
  {
    "text": "Real-Time Semantic Segmentation of Aerial Images Using an Embedded U-Net: A Comparison of CPU, GPU, and FPGA Workflows Julien Posso , Hugo Kieffer , Nicolas Menga , Omar Hlimi , S ebastien Tarris , Hubert Guerard , Guy Bois , Matthieu Couderc , Eric Jenn Ecole Polytechnique de Montr eal IRT Saint Exup ery Viveris Technologies Airbus Defence and Space Space Codesign Systems Abstract This study introduces a lightweight U-Net model optimized for real-time semantic segmentation of aerial images, targeting the efficient utilization of Commercial Off-The-Shelf (COTS) embedded computing platforms. We maintain the accu- racy of the U-Net on a real-world dataset while significantly re- ducing the model s parameters and Multiply-Accumulate (MAC) operations by a factor of 16. Our comprehensive analysis covers three hardware platforms (CPU, GPU, and FPGA) and five different toolchains (TVM, FINN, Vitis AI, TensorFlow GPU, and cuDNN), assessing each on metrics such as latency, power consumption, memory footprint, energy efficiency, and FPGA resource usage. The results highlight the trade-offs between these platforms and toolchains, with a particular focus on the practical deployment challenges in real-world applications. Our findings demonstrate that while the FPGA with Vitis AI emerges as the superior choice due to its performance, energy efficiency, and maturity, it requires specialized hardware knowledge, empha- sizing the need for a balanced approach in selecting embedded computing solutions for semantic segmentation tasks. Index Terms Deep Learning, Neural Networks, Computer Vision, Semantic Segmentation, Inference, Embedded Systems, Aerospace, CPU, GPU, FPGA, MPSoC I. INTRODUCTION The advent of deep neural networks, especially Convolu- tional Neural Networks (CNNs), has revolutionized computer vision [13], introducing advanced capabilities for embedded systems in areas such as autonomous navigation [29] and earth observation [7], [16], [20]. Efficient hardware acceleration is vital for leveraging this technology, involving CPUs, GPUs, ASICs, FPGAs [27], and neural network compilers that bridge the gap between high-level Python libraries and hardware accelerators [5].",
    "source": "2503.08700v1_Real-Time_Semantic_Segmentation_of_Aerial_Images_U.pdf",
    "length": 2198,
    "tokens": 490
  },
  {
    "text": "The RAG pipeline typically introduces three components before the prefill stage: query embedding, context retrieval, and re-ranking. These components can either be co-located on the same system or disaggregated across different clients. Additionally, each component has distinct hardware requirements: the embedding stage is compute-intensive (similar to prefill), whereas the retrieval stage is memory-bound due to large-scale database lookups. In this section, we study: i) The impact of co-locating all three stages on the same hardware versus disaggregating the embedding model and the retrieval re-ranking module; and ii) The importance of bandwidth between the re-ranker client and the prefill client. This helps us evaluate whether high-bandwidth CPU-GPU links (e.g., Grace Hopper) offer significant benefits, or whether lower-bandwidth links like PCIe are sufficient. We evaluate three hardware configurations: 1. Large CPU(Grace-inspired)[82] with 14.2 TFLOPs Single Precision compute units, LPDDR5X with memory of size 1 TB and delivering 768 GB s bandwidth for both Embedding and Retrieval, 2. Small CPU(Sapphire Rapids-inspired)[83] with 6.27 TFLOPs worth of compute cores, and DDR5 memory with 8-channel totaling 4TB storage delivering 307.2 GB s memory for Embedding and Retrieval, 3. A100 GPU for Embedding Large CPU for Retrieval. In all setups, prefill and decode are executed on a single H100 GPU running LLaMA-3.1-8B. The prefill client connects to the retrieval client via PCIe 4.0 x4 (32GB s). We evaluate two embedding models: E5-Base [31], [30], [29], [84], and Mistral-7B. The retrieval uses the IVF-PQ algorithm with: 4M centroids, 50 probes, 5K points per probe.",
    "source": "2504.09775v3_Understanding_and_Optimizing_Multi-Stage_AI_Infere.pdf",
    "length": 1688,
    "tokens": 419
  },
  {
    "text": "For the EKF predict step, we employ a non-linear first- order motion model to fuse vehicle velocity and IMU mea- surements. Then we used the popular iterative closest point (ICP) scan matching to update the kalman filter with new measurements. Additionally, erroneous measurements were excluded using the œá2 anomaly detector (e.g., [45]), with an empirically determined probability of valid data of 0.95. To construct maps of each test environment, we utilized the popular hector mapping [46] algorithm to generate a 2D map using the lidar sensor. Additionally, we measured the ground truth pose P for each experiment using a lidar- inertial localization stack which utilized an EKF to fuse IMU, wheel encoders, and lidar ICP measurements. This was benchmarked against a VICON motion capture system showing that the lidar ground truth was always within 20 cm of the VICON ground truth. Finally, we use the standard absolute trajectory error (ATE) and relative trajectory error (RTE) metrics [47] to assess the localization performance of each method. Here, for the i-th frame, the error metrics for translation (tr) and heading (hd) trajectory errors are defined as ATEtr(i) ÀÜpi pi 2 RTEtr(i) (ÀÜpi ÀÜpi 1) (pi pi 1) 2, (2) TABLE I: Comparison of average inference time where RaGNNarok achieves significant reductions compared to existing works. Platform RaGNNarok RadCloud [5] RadarHD [3] Raspberry Pi 5 7.3 ms 178.5 ms 290.7 ms Desktop (GPU) 1.3 ms 8.2 ms 33.3 ms ATEhd(i) ÀÜŒ®i Œ®i RTEhd(i) (ÀÜŒ®i ÀÜŒ®i 1) (Œ®i Œ®i 1) . (3) E. Real-time Full Stack Case Studies To round out our evaluation, we demonstrated the real- time feasibility of the RaGNNarok framework, by per- forming real-time simultaneous localization and mapping (SLAM) and navigation case studies. We highlight that the RaGNNarok framework was run alongside industry standard SLAM, localization, and navigation stacks in real-time on the Raspberry Pi 5.",
    "source": "2507.00937v1_RaGNNarok_A_Light-Weight_Graph_Neural_Network_for_.pdf",
    "length": 1909,
    "tokens": 499
  },
  {
    "text": "The impact of neuron i on the loss function L can be approximated using the first-order Taylor expansion: Li L ai ai where ai is the activation of neuron i, and L ai is the gradient of the loss with respect to the activation. Define the dropout probability pi for neuron i based on the Taylor expansion approximation of its impact on the loss: pi Œª L ai ai œµ where Œª is a scaling factor to adjust the overall dropout rate, and œµ is a small constant to avoid division by zero. Define a binary dropout mask m [m1, m2, . . . , mn] where mi {0, 1}. Each element of the mask is determined by sampling from a Bernoulli distribution with probability 1 pi: mi Bernoulli(1 pi) Apply the dropout mask during the forward pass. Let ai denote the activation of neuron i: adropout i ai mi Training with Taylor Expansion Dropout and QuantaTask Optimization: Initialize the network parameters W, dropout mask m, and scaling factor Œª. Define the energy budget Eb for a single quanta and for the entire inference. Initialize the loop iteration parameters l. Compute the activations a and apply the dropout mask: adropout i ai mi Compute the loss L(Y, ÀÜY) where Y is the output of the network and ÀÜY is the target output. 22 Calculate the gradients of the loss with respect to the activations: L ai For each layer L and loop i within the layer, estimate the energy Ei required for the current quanta size li: Ei DynAgent.estimateEnergy(L, i, li) If Ei Eb, fuse tasks to reduce the overhead: FuseTasks(L, i, li, Eb) Update Ei after task fusion: Ei DynAgent.estimateEnergy(L, i, li) Update the dropout mask m based on the Taylor expansion approximation: pi Œª L ai ai œµ mi 0 if Bernoulli(1 pi) 0 1 otherwise Perform the backward pass to update the network weights, considering the dropout mask: W W Œ∑ L W m where Œ∑ is the learning rate and denotes element-wise multiplication.",
    "source": "NexUME.pdf",
    "length": 1854,
    "tokens": 485
  },
  {
    "text": "In doing so, we ensured con- sistent scaling by using symmetric quantization with a fixed scale factor and zero-point for the weights and activations so that the int8 results could be dequantized to match the FP32 baseline. We verified that the FPGA outputs matched CPU computation exactly for small test matrices and remained within quantization error for end-to-end model outputs. Tiled MatMul Accelerator for Transformer Self-Attention Conference 17, July 2017, Washington, DC, USA zynq_ultra_ps_e_0 Zynq UltraScale MPSoC M_AXI_HPM0_FPD M_AXI_HPM1_FPD S_AXI_HP0_FPD S_AXI_HP1_FPD S_AXI_HP2_FPD maxihpm0_fpd_aclk maxihpm1_fpd_aclk saxihp0_fpd_aclk saxihp1_fpd_aclk saxihp2_fpd_aclk pl_ps_irq0[0:0] pl_resetn0 pl_clk0 pl_clk1 mmult_accel_0 Mmult_accel (Pre-Production) s_axi_control m_axi_gmemA m_axi_gmemB m_axi_gmemC ap_clk ap_rst_n interrupt rst_ps8_0_99M Processor System Reset slowest_sync_clk ext_reset_in aux_reset_in mb_debug_sys_rst dcm_locked mb_reset bus_struct_reset[0:0] peripheral_reset[0:0] interconnect_aresetn[0:0] peripheral_aresetn[0:0] axi_smc_1 AXI SmartConnect S00_AXI S01_AXI S02_AXI M00_AXI M01_AXI M02_AXI aclk aresetn axi_smc AXI SmartConnect S00_AXI S01_AXI M00_AXI aclk aresetn Figure 3: Vivado Block Design 6 PERFORMANCE EVALUATION 6.1 Experimental Setup We benchmarked the accelerator on the KV260 board running Ubuntu with PYNQ support.",
    "source": "2503.16731v3_Design_and_Implementation_of_an_FPGA-Based_Hardwar.pdf",
    "length": 1368,
    "tokens": 517
  },
  {
    "text": "6 Published at ICLR 2025 Workshop (SCOPE) REFERENCES Steven Abreu, Jens E. Pedersen, Kade M. Heckel, and Alessandro Pierro. Q-S5: Towards Quantized State Space Models, June 2024. URL arXiv:2406.09477 [cs]. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: Reason- ing about Physical Commonsense in Natural Language. Proceedings of the AAAI Confer- ence on Artificial Intelligence, 34(05):7432 7439, April 2020. ISSN 2374-3468. doi: 10. 1609 aaai.v34i05.6239. URL view 6239. Number: 05. Hung-Yueh Chiang, Chi-Chih Chang, Natalia Frumkin, Kai-Chiang Wu, and Diana Marculescu. Quamba: A Post-Training Quantization Recipe for Selective State Space Models, October 2024. URL arXiv:2410.13229. Kyunghyun Cho, Bart van Merri enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning Phrase Representations using RNN En- coder Decoder for Statistical Machine Translation. In Alessandro Moschitti, Bo Pang, and Walter Daelemans (eds. ), Proceedings of the 2014 Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP), pp. 1724 1734, Doha, Qatar, October 2014. Association for Com- putational Linguistics. doi: 10.3115 v1 D14-1179. URL D14-1179. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge, March 2018. URL arXiv:1803.05457.",
    "source": "2503.18002v2_Neuromorphic_Principles_for_Efficient_Large_Langua.pdf",
    "length": 1452,
    "tokens": 469
  },
  {
    "text": "A precision 3D-printed JETSTREAM manifold manages the flow of 2-PIC coolant to and from all 172 WSSCB locations for TRIMERA stacks and CPUs. The JETSTREAM manifold is manufactured using additive manufacturing of metal (e.g. laser melting of titanium powder) that has a very high precision and rigidity, and minimum interaction with 2-PIC coolant. The complex internal geometry of the JETSTREAM manifold incorporates flow distribution channels and 3D printed baffles. These are designed and optimized using computational multiphysics simulation in ANSYS or other suitable engineering simulation software to ensure uniform 2-PIC coolant delivery jetted to each TRIMERA stack. This optimization process integrates thermal, mechanical, and fluidic simulations to achieve optimal flow distribution across all chip locations, with individually optimized baffle and or nozzle structures for each SLD position on the WSSCB to ensure the appropriate 2-PIC coolant flow. The CPU logic stacks will consume a different amount of power than the CASCADE arrays, and this difference can be accommodated in the JETSTREAM manifold design. Table 18: 2-PIC PCHE heat exchanger Aspect Value Units ZettaLith heat to be removed 84,305 Watts PSU heat to be removed 11,496 Watts Total heat to be removed (Q) 95,801 Watts Condensation heat transfer coefficient (h) 50,000 W (m2 K) Opteon 2P50 boiling point 49 C Average condenser temperature 30 C Opteon temperature difference (ŒîT) 19 C 2-PIC heat exchange area (A Q (h ŒîT)) 0.1 m2 Water inlet temperature 25 C Water outlet temperature 35 C Water temperature difference (ŒîT) 10 C Water heat transfer coefficient (U) 2,000 W (m2 K) Water heat exchange area (A Q (U ŒîT)) 4.8 m2 Maximum of water and Opteon PCHE area 4.8 m2 Channel surface area density 3,000 m2 m3 PCHE volume 0.00160 m3 Cylindrical PCHE diameter 380 mm Cylindrical PCHE minimum height 14 mm 40 The JETSTREAM cooling system has redundant pumps circulating 2-PIC coolant through the PCHE and JETSTREAM manifold.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 1999,
    "tokens": 486
  },
  {
    "text": "Arghadip Das (corresponding author, e-mail: and Vijay Raghunathan are with the Elmore Family School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA. Shamik Kundu (e-mail: Arnab Raha (e-mail: Soumendu Ghosh (e-mail: and Deepak Mathaikutty (e-mail: are with the Advanced Architecture Research Team, NPU IP, CGAI (CCG), Intel Corporation, Santa Clara, CA, USA. Personal Assistant Question-answering Enhanced search Recommendations Event-driven vision tasks Theft detection Automatic lock Proximity alarm Speech- to-text AI- backend Model library Storage LLMs GNNs RAG Knowledge Graph (KG) Event Stream Subsampling Graph Generation GNNs Prediction GNN tasks run frequently in the background Map on NPU for faster response with lower power Link prediction KG alignment Node classification KG reasoning Image classification Object detection Image segmentation Pose estimation Face recognition Fig. 1. Applications of GNNs on Client PCs: showcasing GNN-driven tasks like recommendations and event-driven vision, mapped onto Intel Core Ultra processors for faster response and lower power. cations in areas like network analysis, recommendation sys- tems [1], and speech analytics [2]. Their ability to capture com- plex relationships through graph topology distinguishes them from traditional neural networks such as CNNs and LLMs. Recently, the inclusion of R-GAT, a prominent GNN model, in MLPerf s inference benchmarks emphasizes their growing importance in real-world applications. GNNs are particularly important compared to LLMs and newer architectures like State Space Models (SSMs) due to their ability to explicitly model relational and structural information, which is critical for tasks involving interconnected data such as social networks, molecular structures, and knowledge graphs [3]. While LLMs excel in sequential data processing and SSMs offer efficiency in modeling long-range dependencies [4], GNNs uniquely capture complex relationships through graph topology, making them indispensable for tasks where data is inherently non-Euclidean. Running GNNs on edge devices, including laptops and client PCs, has significant advantages. Edge-based inference ensures real-time processing, enhances data privacy, and reduces depen- dency on cloud infrastructure.",
    "source": "2502.06921v2_GraNNite_Enabling_High-Performance_Execution_of_Gr.pdf",
    "length": 2302,
    "tokens": 484
  },
  {
    "text": "In 2013 IEEE 19th International Sym- posium on High Performance Computer Architecture (HPCA), pages 1 12. IEEE. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, and 1 others. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. CloudPanel. 2023. What are arm-based servers? comparison with x86, benefits and drawbacks. Accessed: 2024-10-31. Matthew Connatser. 2023. Intel s ceo says moore s law is slowing to a three-year cadence, but it s not dead yet. Tom s Hardware. Chris Cummins, Volker Seeker, Dejan Grubisic, Baptiste Roziere, Jonas Gehring, Gabriel Synnaeve, and Hugh Leather. 2024. Meta large language model compiler: Foundation models of compiler optimization. arXiv preprint arXiv:2407.02524. Anderson Faustino Da Silva, Bruno Conde Kind, Jos√© Wesley de Souza Magalh√£es, Jer√¥nimo Nunes Rocha, Breno Campos Ferreira Guimaraes, and Fernando Magno Quin√£o Pereira. 2021. Anghabench: A suite with one million compilable c benchmarks for code-size reduction. In 2021 IEEE ACM International Symposium on Code Generation and Optimization (CGO), pages 378 390. IEEE. Tri Dao. 2023. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691. Robert H Dennard, Fritz H Gaensslen, Hwa-Nien Yu, V Leo Rideout, Ernest Bassous, and Andre R LeBlanc. 1974. Design of ion-implanted mosfet s with very small physical dimensions. IEEE Journal of solid-state circuits, 9(5):256 268. DeepSeek-AI et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948.",
    "source": "2506.14606v1_Guaranteed_Guess_A_Language_Modeling_Approach_for_.pdf",
    "length": 1709,
    "tokens": 481
  },
  {
    "text": "no information. When inserting the Ô¨Årst P0 point into the bounding box and the octree, two actions are taken: (1) expanding the bounding box with a step-size of 2n, where n 1,2,3, , until P0 is wrapped inside the bounding box. In this case, the side length of the bounding box cube becomes 2, and now P0 is located inside the bounding box. And (2), inserting the new point into the current octree. In this case, P0 is located in the 7th child of the root node, and the root node now stores the occupy information, which is 00000001 (the right-most 1 indicates a child in 7th leaf node). Similarly, P1 is located inside the current bounding box and inserted into the octree as the 6th child of the root. Interestingly, in order to include P2, the current bounding box has to expand its side length by 4 , i.e., enlarging from 2 to 8, and now the octree also contains more levels with all three points being in its leaf level. Obviously, both the bounding box and the octree are updated point-by-point, which forces the pipeline to be sequential. In our proposal shown in the lower Ô¨Ågure, instead of constructing the octree in a point-by-point fashion, we process all three points as one batch in the Morton Code Generation step in parallel, and output the Ô¨Ånal bounding box cuboid with side lengths 4 3 3 (x-axis: 3-(-1) 4, y-axis: 3-0 3, and z-axis 3-0 3). With the Morton code in place, next we invoke the parallel octree construction technique (for more details, please refer to [31], [64]) to construct the octree. Note that this step is also amenable to parallelism. 2) How to Apply to PCC? : As discussed in Sec. IV-A2, the octree Construction step returns several arrays containing the relationship among octree nodes; e.g., the code array contains the Morton codes for all the nodes, while the parent array contains the index of the current node s parent in the code array (e.g., in Fig.",
    "source": "PCcompress.pdf",
    "length": 1894,
    "tokens": 481
  },
  {
    "text": "Detector Method SAM-B SAM-L SAM-H FP32: (B L H) W4A4 W5A5 W6A6 W4A4 W5A5 W6A6 W4A4 W5A5 W6A6 Faster R-CNN BRECQ 0.2 16.7 28.0 5.0 31.8 35.2 17.5 31.3 35.8 QDrop 2.3 12.9 26.2 0.8 31.9 35.0 6.0 32.6 36.0 33.1 36.0 36.8 PTQ4SAM 2.7 14.4 26.8 2.4 33.0 35.5 6.7 33.3 36.2 AHCPTQ 11.7 27.5 31.6 27.4 34.8 35.6 31.4 35.7 36.3 YOLOX BRECQ 0.2 19.0 31.9 6.3 35.3 39.4 19.7 34.7 39.7 QDrop 2.6 15.6 30.3 1.0 36.2 39.4 6.8 36.0 40.1 37.2 40.4 41.0 PTQ4SAM 3.8 18.4 30.9 2.4 37.1 39.9 7.4 37.1 40.3 AHCPTQ 13.4 31.8 35.4 31.0 39.1 40.0 35.2 40.0 40.4 H-DETR BRECQ 0.3 11.2 32.0 5.2 36.1 40.4 19.1 35.3 40.6 QDrop 2.0 13.1 30.5 1.3 37.0 40.3 6.9 37.0 41.1 38.2 41.5 42.0 PTQ4SAM 2.8 16.9 30.7 2.6 38.1 40.9 7.1 38.0 41.4 AHCPTQ 14.1 32.1 36.6 32.3 40.3 41.0 35.6 40.9 41.5 DINO BRECQ 0.2 13.5 34.8 3.6 41.4 47.0 20.7 40.3 47.2 QDrop 1.9 13.4 34.5 1.0 42.7 47.0 7.0 42.4 47.9 44.5 48.6 49.1 PTQ4SAM 1.9 17.6 35.1 2.3 44.1 47.8 8.9 43.8 48.2 AHCPTQ 16.8 36.7 41.9 36.6 46.8 47.9 41.2 47.6 48.3 Note: This paper reports the corrected performance of PTQ4SAM and QDrop after fixing a bug (refer to code repo.) in the PTQ4SAM [29] framework. sults demonstrate that each technique plays a vital role in enhancing SAM s performance, with the highest gains achieved when both CAG and HLUQ are applied together.",
    "source": "2503.03088v2_AHCPTQ_Accurate_and_Hardware-Compatible_Post-Train.pdf",
    "length": 1289,
    "tokens": 661
  },
  {
    "text": "NIPS 22. Red Hook, NY, USA: Curran Associates Inc., 2022. [21] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding, 2019. [Online]. Available: [22] Z. Liu, G. Li, and J. Cheng, Hardware acceleration of fully quan- tized bert for efficient natural language processing, in 2021 Design, Automation Test in Europe Conference Exhibition (DATE), 2021, pp. 513 516. [23] Z. Wang, G. Wang, H. Jiang, N. Xu, and G. He, Cosa:co-operative systolic arrays for multi-head attention mechanism in neural network using hybrid data reuse and fusion methodologies, in 2023 60th ACM IEEE Design Automation Conference (DAC), 2023, pp. 1 6. [24] H. Wang, Y. Bai, J. Yu, and K. Wang, Transfru: Efficient deployment of transformers on fpga with full resource utilization, in 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC), 2024, pp. 521 526. [25] Wikipedia, Hamming weight wikipedia, the free encyclopedia. [Online]. Available: weight",
    "source": "2504.16269v2_COBRA_Algorithm-Architecture_Co-optimized_Binary_T.pdf",
    "length": 1037,
    "tokens": 293
  },
  {
    "text": "Additionally, the first row of the weight tile is loaded into the registers of the left PE column. In steps 1 to 4, the weight tile row is horizontally propagated through the SA and the first output tile row is accumulated in the bottom PE row and then forwarded to the main memory. FlexiSAGA 9 a d c b e 0 0 0 0 0 0 0 e a b d c 0 A B C D E F G H A B C D E F G H a A a A 0 a B a B c A c A 0 0 c B c B -1 Step 2 a A a A 0 a B a B c A c A 0 0 c B c B A -1 0 B Step 3 b A a A 3 a B a B d A c A 1 c B c B e A 3 B Step 4 b G a A b G 3 b H a B b H d A c A 1 d B c B e A 3 e B Step 5 b G a A b G 3 b H a B b H d G c A 1 3 d H c B e A 3 e B Step 6 b G a A b G 3 b H a B b H d G c A 1 3 d H c B e G e G 3 3 e H e H Step 7 b C a A b G 3 b D a B b H d C c A 1 d D c B e G e G 3 e H e H Step 8 b C a A b G 3 b D a B b H d C c A d C 1 d D c B d D e C e G 3 1 e D e H Step 9 a A a A 0 a B a B c 0 c -1 Step 1 a 0 c 0 -1 Step 0 col idx weight input tmp idx partial sum 0 0 -1 3 1 3 2 a c b d e (d) Step by step processing of sparse tiled GEMM for the output stationary dataflow (csOS) using the CSB format. (a) GEMM tiles CSB comp. weight tile input tile weights inputs non-zero values num. of columns column index array (b) CSB format representation (c) controller and PE register contents PE finished Fig. 6.",
    "source": "2506.01566v1_FlexiSAGA_A_Flexible_Systolic_Array_GEMM_Accelerat.pdf",
    "length": 1295,
    "tokens": 461
  },
  {
    "text": "2023. Mix-GEMM: An effi- cient HW-SW Architecture for Mixed-Precision Quantized Deep Neural Net- works Inference on Edge Devices. In 2023 IEEE International Symposium on High- Performance Computer Architecture (HPCA). 1085 1098. doi:10.1109 HPCA56546. 2023.10071076 [57] Minsoo Rhu, Mike O Connor, Niladrish Chatterjee, Jeff Pool, Youngeun Kwon, and Stephen W Keckler. 2018. Compressing DMA engine: Leveraging activation sparsity for training deep neural networks. In 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE, 78 91. [58] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. WinoGrande: An Adversarial Winograd Schema Challenge at Scale. arXiv preprint arXiv:1907.10641 (2019). [59] Minseok Seo, Xuan Truong Nguyen, Seok Joong Hwang, Yongkee Kwon, Guhyun Kim, Chanwook Park, Ilkon Kim, Jaehan Park, Jeongbin Kim, Woojae Shin, Jongsoon Won, Haerang Choi, Kyuyoung Kim, Daehan Kwon, Chunseok Jeong, Sangheon Lee, Yongseok Choi, Wooseok Byun, Seungcheol Baek, Hyuk-Jae Lee, and John Kim. 2024. IANUS: Integrated Accelerator based on NPU-PIM Unified Memory System. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3 (La Jolla, CA, USA) (ASPLOS 24). Association for Computing Machinery, New York, NY, USA, 545 560. doi:10.1145 3620666.3651324 [60] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. 2023. Omniquant: Omni- directionally calibrated quantization for large language models.",
    "source": "2503.18599v2_Oaken_Fast_and_Efficient_LLM_Serving_with_Online-O.pdf",
    "length": 1614,
    "tokens": 490
  },
  {
    "text": "[19] A. Mojsilovic. Introducing AI Explainability 360, Aug. 2019. [20] K. Muhammad, S. Khan, M. Elhoseny, S. Hassan Ahmed, and S. Wook Baik. Efficient Fire Detection for Uncertain Surveillance Environment. IEEE Transactions on Industrial Informatics, 15(5):3113 3122, May 2019. Conference Name: IEEE Transactions on Industrial Informatics. [21] H. Nori, S. Jenkins, P. Koch, and R. Caruana. InterpretML: A Unified Framework for Machine Learning Interpretability, Sept. 2019. arXiv:1909.09223 [cs, stat]. [22] E. Nurvitadhi, G. Venkatesh, J. Sim, D. Marr, R. Huang, J. Ong Gee Hock, Y. T. Liew, K. Srivatsan, D. Moss, S. Subhaschandra, and G. Boudoukh. Can FPGAs Beat GPUs in Accelerating Next-Generation Deep Neural Networks? In Proceedings of the 2017 ACM SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA 17, pages 5 14, New York, NY, USA, Feb. 2017. Association for Computing Machinery. [23] NVIDIA Corporation. Nvidia tensorrt. TensorRT, 2023. [24] A. P erez, A. Rodr ƒ±guez, A. Otero, D. G. Arjona, A. Jim enez-Peralo, M. A. Verdugo, and E. De La Torre. Run-Time Reconfigurable MPSoC- Based On-Board Processor for Vision-Based Space Navigation. IEEE Access, 8:59891 59905, 2020. Conference Name: IEEE Access. [25] A. Pappalardo. Xilinx brevitas, June 2021. original-date: 2018-07- 10T22:37:01Z. [26] B. Peccerillo, M. Mannino, A. Mondelli, and S. Bartolini. A survey on hardware accelerators: Taxonomy, trends, challenges, and perspectives. Journal of Systems Architecture, 129:102561, Aug. 2022.",
    "source": "2503.08700v1_Real-Time_Semantic_Segmentation_of_Aerial_Images_U.pdf",
    "length": 1520,
    "tokens": 494
  },
  {
    "text": ". . , stepj j J (18) The above six equations define constraints for determin- ing which rows of EMB j are fetched into BRAM in the compressed TT-format and the size of those rows before compression. The variable x tt is a binary variable used to calculate the percentage of rows from EMB j stored in BRAM, as described in Equation 15, while x ptr tt is a binary variable used to calculate the size of the EMB rows allocated to BRAM. Unlike Equations 9 12, the EMB rows stored in TT-format are positioned between the rows stored in DRAM and SSD. Thus, the starting and ending row indices of the EMB stored in TT-format can be derived using the x dram and x ptr tt binary variables via Equations 14 and 15. Additionally, the size of EMB j before compression into TT-format, denoted as mem ttj, is calculated using the ICDF in Equation 13, and this variable serves as a constraint on the BRAM capacity of the SmartSSD. JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 7 X i x ptr ttji ICDFj X i x dramji ICDFj X i x row ttji i stepj j J (19) X i x row ttji 1 j J (20) x row ttji {0, 1} i 0, . . . , stepj j J (21) The above three equations define constraints for calculating the x row tt binary variable, which is necessary to determine the size of EMB j when represented across multiple TT-cores. Using x row tt, the cost model calculate the size of the EMBs compressed into TT-format, enabling the formulation of BRAM capacity constraints. pct dramj pct ttj hot thrj j J (22) Equation 22 imposes constraints to prevent excessive TT- format compression, ensuring that less frequently accessed embedding vectors are stored in the SSD. The hot thrj is assigned a value for each EMB. A value of 1 is used when the EMB row size is relatively small, whereas a value less than 1 is applied for significantly larger row sizes, ensuring that EMB parameters from larger tables are fetched from the SSD.",
    "source": "2504.00520v1_SCRec_A_Scalable_Computational_Storage_System_with.pdf",
    "length": 1897,
    "tokens": 487
  },
  {
    "text": "Left: SFT dataset; Middle: Correct samples in RL dataset; Right: Incorrect samples in RL dataset. during RL, while CodeV-R1-7B s tendency to generate longer responses increases overlong instances on the SFT dataset. D Case Study Comparison Between CodeV-R1-7B and DeepSeek-R1: In this section, we first present a case study on a specific Verilog problem in VerilogEval-v2 to illustrate the advantage of CodeV-R1-7B over DeepSeek-R1 in this problem. The problem, reasoning chains, and results are shown in the blocks below. Both Deepseek-R1 and our model comprehended the problem and engaged in reasoning and reflection, but only our model accurately implemented the logical function defined by the Karnaugh map, while R1 s answer executed an incorrect logical function. The key distinction lies in our model s use of the declaration input [4:1] x, which directly corresponds to the notation of the Karnaugh map in the problem description. In contrast, R1 employed input [3:0] x, leading to confusion in subsequent reasoning. Although our model was initially challenged by this unconventional declaration, it ultimately chose the correct declaration through reflection, avoiding potential index mapping ambiguities. This resulted in significantly clearer code that is less prone to errors. Problem Description Question: Question: I would like you to implement a module named TopModule with the following interface. All input and output ports are one bit unless otherwise specified. - input x (4 bits) - output f The module should implement the function f shown in the Karnaugh map below. d is don t-care, which means you may choose to output whatever value is convenient. x[1]x[2] x[3]x[4] 00 01 11 10 00 d 0 d d 01 0 d 1 0 11 1 1 d d 10 1 1 0 d Answer: 19 Reasoning Chain (DeepSeek-R1) Okay, so I need to write a Verilog module called TopModule that takes a 4-bit input x and outputs a single bit f based on the given Karnaugh map. Let me start by understanding the problem step by step. ... Wait, the K-map s rows are x3x4, columns x1x2. Let s see. For example, the first row is x3x4 00.",
    "source": "2505.24183v2_CodeV-R1_Reasoning-Enhanced_Verilog_Generation.pdf",
    "length": 2088,
    "tokens": 500
  },
  {
    "text": "Figure 3b illustrates that the pseudo-golden dataset consistently exhibits notably lower reward throughout the training process compared to our original RL dataset. This underscores the critical role of golden code accuracy during reinforcement learning. 3.3.3 Acceleration via Adaptive DAPO To quantitatively demonstrate the acceleration achieved by our adaptive DAPO algorithm, we provide a comparison of time usage in Figure 4a. The plots reveal a notable increase in the time per RL step in baseline DAPO training around step 150. This performance degradation in the baseline is attributed to its fixed generation batch size, which becomes insufficient to yield enough samples for a 8 complete training batch as training progresses. In contrast, our adaptive DAPO effectively mitigates this issue. It dynamically adjusts and increases the generation batch size across steps. In addition, when a generation attempt does not produce sufficient valid samples for a training batch, the algorithm recalculates the required remaining batch size. In Figure 4b, we provide the average speedup of adaptive DAPO, along with a breakdown of performance before and after step 150. Notably, the time reduction after step 150 is significantly more pronounced the speedup factor reaches 1.44 after step 150, compared to 1.04 before step 150. This disparity highlights the critical benefit of eliminating sampling more than once. After applying adaptive DAPO, the final speedup factor reaches 1.25 . (a) (b) Figure 4: Time comparison between adaptive DAPO and baseline DAPO. (a): Comparison of RL training time per step. (b): Acceleration ratio between adaptive DAPO and baseline DAPO, breakdown by step (whether before 150). 3.3.4 Testbench Performance Evaluation We evaluate our auto-testbench generation framework against a DeepSeek-V3-generated testbench, both taking the Verilog code from GitHub as the golden reference. We conduct two key tests: Correctness classification test. First, we assess whether the testbenches might misclassify correct code as incorrect. To do this, we use both testbenches to validate \"golden vs. golden\" inputs (i.e., comparing the golden code against itself). The expected outcome is 100 correct classification. The results show that our equivalence checker incorrectly flags 2.7 of golden code as wrong, while the LLM(DeepSeek-V3)-generated testbench misclassifies 7.6 of cases.",
    "source": "2505.24183v2_CodeV-R1_Reasoning-Enhanced_Verilog_Generation.pdf",
    "length": 2402,
    "tokens": 498
  },
  {
    "text": "Thus, both radars are configured to have a range resolution (dres) of 7 cm, maximum sensing range (dmax) of 8.56 m, azimuth angular resolution (Œ∏res) of 14.3 , and velocity resolutions (vres) 0.01 m s. Here, we minimize the processing load by using the TI mmWave SDK [38] to implement a standard radar processing pipeline directly on the radars. Radar Point Clouds Ground Truth RaGNNarok Model Probabilistic Occupancy Grid 1 16 4 Input Nodes GNN Model Prediction Multipath Detections ReLu SAGEConv Fig. 2: Overview of the RaGNNarok model architecture Additionally, we filter ground detections by removing all detections within a range of 1.5 m. The final combined point cloud typically features 50 points. Step 2 : Dynamic object detection. In dynamic environ- ments, a significant number of radar detections can corre- spond to moving objects (e.g., people). Thus, for each new frame of radar data, we identify detections corresponding to dynamic objects by utilizing the UGV s translational velocity (v) and the velocity component of each detection (dv). For a detection with 2D coordinates q [dx; dy], the relative velocity measured by the radar for a static point can be expressed as ÀÜdv q q , v Thus, we define dynamic detections as any detection where ÀÜdv dv 0.05 m s. Once separated, static detections are then used to generate a probabilistic occupancy grid map while dynamic detections can then be used by other downstream tasks (e.g. ; tracking) Step 3 : Probabilistic occupancy grid Given the sparsity of the radar point clouds, we additionally employ a proba- bilistic occupancy grid to provide recent temporal history to the model. Empirically, we found that an occupancy grid with a cell resolution of 20 cm, range of [-5 m,5 m], and temporal history leveraging the previous 20 radar frames (i.e. 1 s of previous sensing) best balanced between increasing point cloud accuracy while minimizing the required sensing duration. Here, we utilize a UGV s pose (i.e. ; position and orientation) estimate to continuously align the occupancy grid with the most recently recorded radar frame.",
    "source": "2507.00937v1_RaGNNarok_A_Light-Weight_Graph_Neural_Network_for_.pdf",
    "length": 2096,
    "tokens": 486
  },
  {
    "text": "Cock- tail comprises of two key components: (i) a dynamic model selection framework, which reduces the number of models in the ensemble, while satisfying the accuracy and latency requirements; (ii) an adaptive resource management (RM) framework that employs a distributed proactive autoscaling policy, to efÔ¨Åciently allocate resources for the models. The RM framework leverages transient virtual machine (VM) in- stances to reduce the deployment cost in a public cloud. A prototype implementation of Cocktail on the AWS EC2 plat- form and exhaustive evaluations using a variety of workloads demonstrate that Cocktail can reduce deployment cost by 1.45 , while providing 2 reduction in latency and satisfy- ing the target accuracy for up to 96 of the requests, when compared to state-of-the-art model-serving frameworks. 1 Introduction Machine Learning (ML) has revolutionized user experience in various cloud-based application domains such as product recommendations [70], personalized advertisements [44], and computer vision [13, 43]. For instance, Facebook [44, 82] serves trillions of inference requests for user-interactive ap- plications like ranking new-feeds, classifying photos, etc. It is imperative for these applications to deliver accurate predic- tions at sub-millisecond latencies [27,34,35,39,44,83] as they critically impact the user experience. This trend is expected to perpetuate as a number of applications adopt a variety of ML models to augment their services. These ML models are typically trained and hosted on cloud platforms as service end- points, also known as model-serving framework [6, 28, 60]. From the myriad of ML Ô¨Çavours, Deep Neural Networks (DNNs) [54] due to their multi-faceted nature, and highly gen- eralized and accurate learning patterns [45,73] are dominating the landscape by making these model-serving frameworks accessible to developers. However, their high variance due to the Ô¨Çuctuations in training data along with compute and mem- ory intensiveness [59,65,84] has been a major impediment in designing models with high accuracy and low latency. Prior model-serving frameworks like InFaas [83] are conÔ¨Åned by the accuracy and latency offered by such individual models.",
    "source": "cocktail.pdf",
    "length": 2218,
    "tokens": 479
  },
  {
    "text": "More critically, they failed to include the necessary HIP header, rendering the output uncompilable. This example highlights how CASS-7B goes beyond syntactic accuracy to produce code that is both faithful to the original structure and immediately usable in a real compilation setting. 19 CASS-7B include hip hip_runtime.h include iostream ... add (N 255) 256, 256 (d_a, d_b, d_c, N); Claude, Qwen-Coder include iostream ... hipLaunchKernelGGL(add, (N 255) 256, 256, 0, 0, d_a, d_b, d_c, N); Lastly, when verifying numerical correctness, CASS-7B preserved the original logging behavior by correctly emitting output to std::cout, as in the source code. This choice maintains consistency with the original program s semantics, especially in distinguishing between standard output and error streams; important in contexts where output may be redirected or parsed. In contrast, GPT-4o unnecessarily altered the output stream to std::cerr, which, while syntactically valid, changes the runtime behavior of the program. Such a change could lead to unexpected side effects in downstream tools or logging pipelines. This example further demonstrates CASS-7B s attention to both structural and behavioral fidelity in its translations. CASS-7B std::cout \"Error at element \" i \": \" h_output[I] \" vs. expected \" h_reference[i] std::endl; GPT4o std::cerr \"Error at element \" i \": \" h_output[i] \" vs expected \" h_reference[i] std::endl; 20",
    "source": "2505.16968v3_CASS_Nvidia_to_AMD_Transpilation_with_Data_Models_.pdf",
    "length": 1425,
    "tokens": 357
  },
  {
    "text": "Figure 17: Leak rates for Memory ordering measured in Rap- tor Lake. Figure 18: Leak rates for SMC measured in Raptor Lake. Instruction Set Count Instruction Set Count ADOX_ADCX 8 AES 12 AVX 695 AVX2 286 AVX2GATHER 16 AVX512F_512 2192 AVX512F_128 1816 AVX512F_256 1940 AVX512F_SCALAR 584 AVX512DQ_128 247 AVX512DQ_256 281 AVX512DQ_512 357 AVX512BW_128 467 AVX512BW_256 467 AVX512BW_512 467 AVX512F_128N 23 AVX512DQ_SCALAR 44 AVX512CD_512 38 AVX512CD_128 38 AVX512CD_256 38 AVX512BW_128N 8 AVX512DQ_128N 8 AVX512DQ_KOP 18 AVX512BW_KOP 34 AVX512F_KOP 15 AVXAES 12 I86 809 I386 196 I486REAL 37 CMOV 96 PENTIUMREAL 5 I186 124 LONGMODE 24 LAHF 2 I286PROTECTED 26 I286REAL 10 FAT_NOP 3 RDPMC 1 PPRO 2 BMI1 26 BMI2 32 CET 2 F16C 8 FMA 192 INVPCID 1 CMPXCHG16B 2 LZCNT 6 PENTIUMMMX 129 SSE 97 MOVBE 6 PCLMULQDQ 2 RDRAND 3 RDSEED 3 RDTSCP 1 RDWRFSGS 8 FXSAVE 2 FXSAVE64 2 SSEMXCSR 2 SSE2 264 SSE2MMX 6 SSE3 20 SSE3X87 2 SSE4 96 SSE42 25 POPCNT 6 SSSE3MMX 32 SSSE3 32 X87 119 FCMOV 8 FCOMI 4 XSAVE 6 XSAVEC 2 XSAVEOPT 2 XSAVES 4 Table 3: Number of instructions per set used in the action space for Skylake-X. 19",
    "source": "2502.14307v1_ŒºRL_Discovering_Transient_Execution_Vulnerabilitie.pdf",
    "length": 1101,
    "tokens": 459
  },
  {
    "text": "To adapt the images for TCN pro- cessing, we flatten each image pixelwise (see Fig. 14, bottom), effectively creating a sequential Omniglot representation. FSL results Using a 116k-parameter TCN with 14 layers, we report in Table I the classification accuracies on the Omniglot test set across 5,20-way 1,5-shot as well as 32-way 1-shot scenarios found in the state of the art. It can be seen that Chameleon outperforms existing works [7] [9] by up to 16 accuracy points, setting new records across all scenarios, even though both FSL CIM designs [7], [8] use FP32 off- chip embedders. Chameleon consumes 11.6 mW for end-to- end FSL on Omniglot at 100 MHz and 1.0 V. At 100 kHz and 0.625 V, it consumes 12.9 ¬µW. The latencies for learning one shot of a new class are 0.59 ms and 0.54 s respectively, yielding an energy per shot of 6.84 ¬µJ and 6.97 ¬µJ, including the embedding phase. Since prototypical parameter extraction only takes a few clock cycles, 0.04 of the embedding computation time on Omniglot, learning and inference have effectively identical total energy, power and latency. This contrasts sharply with other on-chip training methods like CHIMERA [41], which employs low-rank training with 8-bit signed parameters and requires 103 104 RRAM update steps for learning. Similarly, Park et al. [42] use FP8 for full on-chip SGD-based training and hence could host MAML-like setups. However, their lowest reported power is already 13 higher than Chameleon s peak end-to-end power, demonstrating its incompatibility with extreme-edge operation. Alternatively, instead of learning new classes, Cioflan et al. [43] use embed- dings to adapt to the speech characteristics of end users for KWS. A microcontroller updates an embedding layer using few-shot spoken keywords, inducing a 35 FLOPS overhead compared to embedding computation alone. CL results We evaluate CL on Chameleon using the same TCN model as for FSL.",
    "source": "2505.24852v2_Chameleon_A_MatMul-Free_Temporal_Convolutional_Net.pdf",
    "length": 1921,
    "tokens": 476
  },
  {
    "text": "Mag., vol. 39, no. 2, pp. 28 44, 2022. [29] C. Zhang, O. B. Demirel, and M. Akc akaya, Cycle-consistent self- supervised learning for improved highly-accelerated MRI reconstruc- tion, in IEEE Int. Symp. Biomed. Imag., 2024, pp. 1 5. [30] Y. U. Alc alar, M. G ulle, and M. Akc akaya, A convex compressibility- inspired unsupervised loss function for physics-driven deep learning reconstruction, in Proc. IEEE Int. Symp. Biomed. Imag., 2024, pp. 1 5. [31] Y. U. Alc alar and M. Akc akaya, Sparsity-driven parallel imaging consistency for improved self-supervised MRI reconstruction, in IEEE Int. Conf. Image Process., 2025. [32] H. Gu, B. Yaman, S. Moeller, J. Ellermann, K. U gurbil, and M. Akc akaya, Revisiting ‚Ñì1-wavelet compressed-sensing MRI in the era of deep learning, Proc. Natl. Acad. Sci., vol. 119, no. 33, 2022, Art. no. e2201062119. [33] O. B. Demirel, B. Yaman, C. Shenoy, S. Moeller, S. Weing artner, and M. Akc akaya, Signal intensity informed multi-coil encoding operator for physics-guided deep learning reconstruction of highly accelerated myocardial perfusion CMR, Magn. Reson. Med., vol. 89, no. 1, pp. 308 321, Jan. 2023. [34] R. Timofte, E. Agustsson, L. V. Gool, M.-H. Yang, and L. Zhang, NTIRE 2017 challenge on single image super-resolution: Methods and results, in Proc. IEEE Conf. Comput. Vis. Pattern Recog. Worksh., 2017, pp. 114 125.",
    "source": "2506.03183v1_Edge_Computing_for_Physics-Driven_AI_in_Computatio.pdf",
    "length": 1363,
    "tokens": 445
  },
  {
    "text": "[33] M. Courbariaux, Y. Bengio, and J.-P. David, Binaryconnect: Training deep neural networks with binary weights during propagations, in Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2, NIPS 15, pp. 3123 3131, 2015. [34] M. Alwani, H. Chen, M. Ferdman, and P. Milder, Fused-layer cnn accelerators, in 2016 49th Annual IEEE ACM International Symposium on Microarchitecture (MICRO), pp. 1 12, 2016. [35] M. D. Lam, E. E. Rothberg, and M. E. Wolf, The cache performance and optimizations of blocked algorithms, in Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), pp. 63 74, 1991. [36] K. Ma, X. Li, S. R. Srinivasa, Y. Liu, J. Sampson, Y. Xie, and V. Narayanan, Spendthrift: Machine learning based resource and frequency scaling for ambient energy harvesting nonvolatile processors, in 2017 22nd Asia and South PaciÔ¨Åc Design Automation Conference (ASP-DAC), pp. 678 683, 2017. [37] N. Binkert, B. Beckmann, G. Black, S. K. Reinhardt, A. Saidi, A. Basu, J. Hestness, D. R. Hower, T. Krishna, S. Sardashti, R. Sen, K. Sewell, M. Shoaib, N. Vaish, M. D. Hill, and D. A. Wood, The Gem5 Simulator, SIGARCH Comput. Archit. News, vol. 39, pp. 1 7, Aug. 2011. [38] Synopsis, HSPICE. Ô¨Åcation ams- veriÔ¨Åcation hspice.html .",
    "source": "ResiRCA.pdf",
    "length": 1349,
    "tokens": 428
  },
  {
    "text": "At that point, Figure 10a Figure 10b Figure 10c Figure 10d Figure 10e Figure 10f Figure 10g spare column data in column column under test defective CASCADE column CASCADE columns carrying incorrect data column with defect 5 1 2 4 6 7 8 9 10 11 12 13 14 - 1 2 4 5 5 5 5 6 7 8 9 10 11 12 13 14 - 1 2 4 6 7 8 9 10 11 12 13 14 - 1 2 3 3 3 3 4 Compare Faulty 6 7 8 9 10 11 12 13 14 - - 5 1 2 4 6 7 8 9 10 11 12 13 14 - 5 3 - - - - - - - - - - - - - - - - - - 1 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 5 6 7 8 9 10 11 12 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13 14 - - - - - - - - - - - - - - - - - - 1 2 3 4 5 6 7 8 9 10 11 12 13 13 14 1 2 4 5 6 7 8 9 10 11 12 13 14 1 2 4 5 6 7 8 9 10 11 12 13 14 1 2 4 5 6 7 8 9 10 11 12 13 14 - - 9 10 11 12 13 14 1 2 4 5 6 7 8 9 10 11 12 13 14 1 2 4 5 6 4 5 6 4 5 6 4 5 6 4 5 6 7 8 1 2 7 8 9 10 11 12 13 14 1 2 7 8 9 10 11 12 13 14 1 2 6 7 8 9 10 11 12 13 7 8 9 10 11 12 13 14 1 2 7 8 9 10 11 12 13 14 1 2 4 5 6 7 8 9 10 11 12 13 14 1 2 4 5 6 7 8 9 10 11 12 13 14 - - 1 2 2 4 5 6 7 8 9 10 11 12 14 - 1 4 5 6 7 8 9 10 11 12 14 - - 1 2 4 5 6 7 8 9 10 11 12 14 5 1 2 3 4 6 7 8 9 10 11 12 13 14 - 1 2 3 4 6 7 8 9 10 11 12 13 14 - 1 2 3 4 X X X X X X X X X X X X X X 6 7 8 9 10 11 12 13 14 - 1 2 3 4 4 4 4 4 Compare OK 6 7 8 9 10 11 12 13 14 - - 5 1 2 3 4 6 7 8 9 10 11 12 13 14 - 4 - - - - 5 1 2 3 4 6 7 8 9 10 11 12 13 14 - 1 2 3 4 5 5 5 5 5 5 5 6 7 8 9 10 11 12 13 14 - 1 2 3 4 6 7 8 9 10 11 12 13 14 - 1 2 3 4 Compare OK 6 7 8 9 10 11 12 13 14 - - 5 1 2 3 4 6 7 8 9 10 11 12 13 14 - 5 - 5 5 1 2 3 4 6 7 8 9 10 11 12 13 14 1 2 3 4 5 5 5 5 5 5 5 6 7 8 9 10 11 12 13 14 - 1 2 3 4 6 7 8 9 10 11 12 13 14 - 1 2 3 4 Compare OK 6 7 8 9 10 11 12 13 14 - - 5 1 2 3 4 6 7 8 9 10 11 12 13 14 - 5 - 5 1 2 3 4 6 7 8 9 10 11 12 13 14 1 2 3 4 5 5 5 5 5 6 7 8 9 10 11 12 13 14 - 1 2 3 4 6 7 8 9 10 11 12 13 14 - 1 2 3 4 Compare Faulty 6 7 8 9 10 11 12 13 14 - - 5 1 2 3 4 6 7 8 9 10 11 12 13 14 - 5 - - 5 5 5 5 1 2 3 4 6 7 8 9 10 11 12 13 14 - - 1 2 3 4 5 5 1 2 3 4 6 7 8 9 10 11 12 13 14 - - 3 5 5 5 5 6 7 8 9 10 11 12 13 14 - 1 2 3 4 6 7 8 9 10 11 12 13 14 - 1 2 3 4 Compare OK 6 7 8 9 10 11 12 13 14 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Compare Faulty CRow(1) CRow(2) CRow(3) CRow(4) CRow(5) CRow(1) CRow(2) CRow(3) CRow(4) CRow(5) CRow(1) CRow(2) CRow(3) CRow(4) CRow(5) CRow(1) CRow(2) CRow(3) CRow(4) CRow(5) CRow(1) CRow(2) CRow(3) CRow(4) CRow(5) CRow(1) CRow(2) CRow(3) CRow(4) CRow(5) CRow(1) CRow(2) CRow(3) CRow(4) CRow(5) CRow(6) CRow(7) CRow(8) CRow(9) CRow(10) CRow(11) CRow(12) CRow(13) CRow(14) CRow(15) CRow(16) comparison mechanism 33 the TRIMERA can still operate, but not at full capacity, given the software drivers are written to accommodate imperfect arrays. For each column exceeding the allowance of spare columns in an TRIMERA, ZettaLith loses 0.0001 of its performance. Once the TRIMERA loses enough arrays that it is deemed more of a computational problem than an asset, the TRIMERA is mapped out of active use.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2985,
    "tokens": 1203
  },
  {
    "text": "INTRODUCTION The rampant growth, and anticipated sustained expansion of data collection and consumption are currently driving data-driven analytics using trained inference models, with signiÔ¨Åcant economic impact. Amidst the myriad of data-driven domains, urban mobility, smart cities, autonomous driving, and the Internet of Things (IoT) emerge as some of the most rapidly expanding Ô¨Åelds contributing to the global economy, amounting to more than 4 trillion US dollars [1], [54], [76], [98]. These statistics underscore the profound signiÔ¨Åcance and transformative potential of these data-driven realms, delineat- ing their pivotal role in shaping the landscape of computing technology, from algorithms to architecture. What distinguishes these data is their diverse origin, span- ning from IoT devices to wearables, and their acquisition from challenging environments, including autonomous driving and urban mobility scenarios. Consequently, they frequently ex- hibit a phenomenon known as data drift , where the incoming data deviates from the distribution of the originally trained model, leading to degradation in inference accuracy. Mitigating Data Drift: Dealing with data drift in edge com- pute nodes presents a signiÔ¨Åcant challenge. While larger models with more parameters may exhibit limited data drift due to their increased capacity to generalize, deploying such large models on edge compute nodes can be difÔ¨Åcult due to inherent limitations in form factor, energy efÔ¨Åciency, thermal constraints, and compute resources. To accommodate these constraints, it is a common practice to employ compressed Deep Neural Network (DNN) models, that are quantized, distilled, or otherwise reduced in size. However, while com- pressed models are essential for meeting resource limitations, they are more sensitive to data drift because they may not generalize as effectively. Traditionally, data drift has been handled by cloud-based periodic re-training using continuous learning algorithms [20], [74]. However, there are challenges in resources, privacy, and sustainability to utilize existing techniques at envisioned scales.",
    "source": "Usas.pdf",
    "length": 2128,
    "tokens": 424
  },
  {
    "text": "For CodeT5p, our method delivers a 2.66 speedup, outperforming the model trained with Medusa, which achieves a 1.16 speedup. To TABLE I EVALUATION RESULTS FOR THE QUALITY OF GENERATED VERILOG CODE Test Model Data Size Benchmark ( ) ( ) ( ) Pass Rate ( ) Ours Medusa NTP Ours Medusa NTP Ours Medusa NTP Ours Medusa NTP Function CodeLlama 34K RTLLM 16.21 4.66 16.72 26.87 10.96 27.89 33.85 13.07 32.62 41.38 13.79 37.93 VGen 30.59 27.06 29.12 49.17 34.48 47.65 56.78 38.04 55.16 64.71 41.18 58.82 68K RTLLM 18.28 13.28 18.79 30.55 16.78 29.34 35.52 19.87 34.24 41.38 24.14 37.93 VGen 32.06 25.59 24.12 47.01 32.88 44.12 53.61 36.59 51.81 58.82 41.18 58.82 102K RTLLM 20.52 13.10 17.07 37.48 19.88 31.58 46.29 23.50 38.25 55.17 27.59 41.38 VGen 31.18 26.76 32.35 53.42 34.36 53.82 63.17 36.81 61.72 70.59 41.18 64.71 136K RTLLM 21.55 13.79 12.24 32.25 20.42 30.63 38.56 25.04 37.55 44.83 34.48 37.93 VGen 34.12 22.35 31.76 55.47 32.78 51.64 65.51 35.01 63.21 76.47 35.29 76.47 CodeT5p 32K RTLLM 1.21 0.34 0.00 3.87 1.54 0.00 5.15 2.63 0.00 6.90 3.45 0.00 VGen 14.41 0.29 1.76 19.79 1.47 6.48 19.97 2.94 11.15 23.53 5.88 17.65 64K RTLLM 0.86 1.90 0.52 3.95 4.89 2.07 6.99 6.08 3.09 10.34 6.90 3.45 VGen 15.88 10.59 7.06 21.09 13.23 13.54 22.91 14.71 16.22 23.53 17.65 17.65 96K RTLLM 5.00 0.69 0.69 11.91 2.93 3.09 15.46 5.17 5.26 17.24 10.34 6.90 VGen 14.71 9.71 7.65 23.24 11.76 12.47 29.47 11.76 14.67 35.29 11.76 17.65 128K RTLLM 5.52 0.52 0.34 14.58 2.40 1.54 19.82 4.36 2.63 27.59 6.90 3.45 VGen 15.29 9.71 9.41 21.57 11.76 11.74 23.34 11.76 11.76 23.53 11.76 11.76 Syntax CodeLlama 34K RTLLM 60.52 14.31 40.69 80.61 30.67 70.47 84.52 39.28 77.85 86.21 44.83 82.76 VGen 86.76 69.71 88.82 99.14 80.61 99.48 99.97 82.10 99.97 100.00 88.24 100.00 68K RTLLM 60.69 26.90 53.45 79.48 51.12 75.87 84.87 62.85 81.10 89.66 68.97 82.76 VGen 97.65 71.76 60.88 100.00 82.40 89.90 100.00 86.48 93.73 100.00 88.24 94.12 102K RTLLM 66.55 36.72 45.52 84.10 62.56 72.08 88.82 72.48 78.80 89.66 79.31 82.76 VGen 96.47 66.18 75.59 100.00 78.82 97.14 100.00 81.64 99.80 100.00 88.24 100.00 136K RTLLM 66.38 39.48 33.28 80.97 61.11 64.90 84.46 67.88 74.84 86.21 72.41 79.31 VGen 99.12 67.65 73.53 100.00 79.89 96.94 100.00 82.65 99.78 100.00 88.24 100.00 CodeT5p 32K RTLLM 12.59 9.66 10.86 33.97 22.50 32.60 46.19 28.52 45.12 58.62 34.48 51.72 VGen 63.53 17.06 34.12 85.74 47.09 71.55 90.86 59.36 83.75 94.12 70.59 94.12 64K RTLLM 28.28 16.21 15.17 52.27 35.71 38.19 61.96 43.35 51.73 68.97 51.72 65.52 VGen 74.12 46.76 26.47 86.85 77.14 64.50 88.14 84.70 79.40 88.24 88.24 88.24 96K RTLLM 33.28 12.76 24.83 59.49 29.05 46.58 67.72 38.94 59.44 72.41 48.28 68.97 VGen 82.35 42.94 52.65 97.79 72.81 83.97 99.93 81.68 92.84 100.00 88.24 94.12 128K RTLLM 44.83 10.69 19.48 62.67 24.93 37.30 72.54 33.22 46.31 79.31 41.38 55.17 VGen 77.35 43.82 35.88 98.00 75.25 71.91 99.89 84.38 82.89 100.00 94.12 88.24 TABLE II EVALUATION RESULTS FOR THE SPEED OF GENERATING VERILOG CODE Method CodeLlama CodeT5p Speed (tokens s) Speedup Speed (tokens s) Speedup Ours 420.13 5.05 243.70 2.66 Medusa 294.99 3.55 106.33 1.16 NTP 83.13 1 91.65 1 further illustrate the effectiveness of our method, Fig. 5 compares the decoding process of our approach to those of Medusa and NTP for a specific example.",
    "source": "2503.14153v1_Speculative_Decoding_for_Verilog_Speed_and_Quality.pdf",
    "length": 3261,
    "tokens": 1495
  },
  {
    "text": "The process begins by identifying the Target Node Type, such as Always, Instance, Assign or Module Instantiation. For each node of the specified type, we apply Œ¶ to decide whether it requires rewriting. Once the target nodes are identified, the Transformation Rule is applied as follows: œÑ : { a A Œ¶(a) true} A, (5) where œÑ replaces the matched node with an optimized AST subtree (e.g., merging nested if-else statements, folding constants, or simplifying expressions; code example see Appendix B). To ensure functional correctness, the trans- formed design undergoes an equivalence check using LLM- generated testbenches. If the template passes verification, it is stored in the RAG system as reusable content. RTL Design Pyverilog AST Construction A[0:7] AND B[0:7] NOT MUX SHR C1[0] O[0:7] Y[0:7] C2[0] Subexpression removal Templates Critical Path reduction Dead code elimination LLM Template selection Selected Reconstruct AST based Optimized code RAG system Feedback loop: Re-select on failed symbolic case Figure 2. SymRTLO AST template optimization workflow. As shown in Figure 2, the RTL design is initially interpreted as an AST representation. The RAG system provides the LLM with multiple template options. Due to the varying op- timization goals and scenarios, the system avoids relying on a fixed sequence of templates. Instead, the LLM determines which templates to apply and in what order, tailoring the optimization process to the design s specific requirements. To further prevent conflicts between templates or failures in the symbolic system, we introduce a feedback loop. This loop allows the LLM to re-select templates and adjust its strategy based on prior failures, ensuring robustness and adaptability in the optimization process. 3.3 Control Flow Optimization Control Flow, unlike Data Flow s focus on how information is processed and propagated, defines the execution paths and sequencing of operations in RTL designs through finite- state machines (FSMs) that capture states, transitions, and outputs. These FSMs are tightly coupled with design con- straints (i.e., partial specifications, clock gating, and reset logic), making generic symbolic systems fragile or incom- plete. Addressing these challenges requires deeper semantic analysis beyond simple pattern matching or generic AST templates.",
    "source": "2504.10369v1_SymRTLO_Enhancing_RTL_Code_Optimization_with_LLMs_.pdf",
    "length": 2325,
    "tokens": 493
  },
  {
    "text": "Codebert: A pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155, 2020. [33] Mikel Galar, Alberto Fernandez, Edurne Barrenechea, Humberto Bustince, and Francisco Herrera. A review on ensembles for the class imbalance problem: Bagging-, boosting-, and hybrid-based ap- proaches. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 42(4):463 484, 2012. [34] Arpan Gujarati, Sameh Elnikety, Yuxiong He, Kathryn S. McKinley, and Bj√∂rn B. Brandenburg. Swayam: Distributed Autoscaling to Meet SLAs of Machine Learning Inference Services with Resource EfÔ¨Åciency. In USENIX Middleware Conference, 2017. [35] Arpan Gujarati, Reza Karimi, Safya Alzayat, Antoine Kaufmann, Ymir Vigfusson, and Jonathan Mace. Serving dnns like clockwork: Perfor- mance predictability from the bottom up. In 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), Banff, Alberta, November 2020. USENIX Association. [36] Jashwant Raj Gunasekaran, Prashanth Thinakaran, Nachiappan C.Nachiappan, Mahmut Taylan Kandemir, and Chita R. Das. Fifer: Tackling Resource Underutilization in the Serverless Era. In USENIX Middleware Conference, 2020. [37] Jashwant Raj Gunasekaran, Prashanth Thinakaran, Mahmut Taylan Kandemir, Bhuvan Urgaonkar, George Kesidis, and Chita Das. Spock: Exploiting serverless functions for slo and cost aware resource procure- ment in public cloud. In IEEE CLOUD, 2019. [38] Jashwant Raj Gunasekaran, Prashanth Thinakaran, Cyan Subhra Mishra, Mahmut Taylan Kandemir, and Chita R. Das. Towards designing a self-managed machine learning inference serving system inpublic cloud, 2020.",
    "source": "cocktail.pdf",
    "length": 1664,
    "tokens": 481
  },
  {
    "text": "III-B, we can rearrange the order of the NP and the KP (Figure 8(A) line 9 line 15), and move the encode and sparse to the outer level of NP dimension (here, we omit the primitive representation of the SIMD core.). Only the map and the half reduce remain in the innermost loop. Essentially, as operand A is broadcast across the PE columns, PEs in each column can share the (A) Extracted and shared encoder architecture A1[i 1:i] A4[i 1:i] Encoder Encoder Sparse Encoder Mux CE CPPG Mux 3-2 Compress Tree Prefetch B PE PE PE Bank 0 Bank 1 Bank i Network On Chip Bank 2 PE PE Network On Chip Bank 0 Bank 1 Bank i Bank 2 PE PE PE PE A B Move Out of PE and shared logic In PE Sparse Encoder Sparse Encoder Sparse Encoder Encoder Encoder Encoder Prefetch B A (B) Modified PE Array (C) OPT4C 2 10 Bank 0 Bank 1 Bank i Network On Chip Bank 2 Bank 0 Bank 1 Bank i Bank 2 A B Sparse Encoder Sparse Encoder Sparse Encoder Encoder Encoder Encoder Prefetch B A (D) Modified PE Array with shared DFF (E) OPT4E sel PE1 PE4 4 sel NP MP PE PE PE sel 8 8 2 2 8 B4 CPPG Mux B1 CPPG Mux PE Group 6-2 Compressor Tree DFF PE PE PE Network On Chip 0.40ns tpd 0.40ns tpd 6-2 Compressor Tree 0.29ns tpd 0.29ns tpd The encoder and sparse logic is moved outside of the NP, since all NP share the operand A. Only 1 encoder and sparse encoder are needed in each MP dimension Figure 8: The proposed optimization architecture 4 (OPT4). same encoder and sparse encoder (Figure 8(B)). By placing the shared encoder outside the PE array, the duplication area of the encoder is reduced in each PE, which also reduces the bandwidth requirement of operand A. Additionally, with the sparse encoder located outside the PE array, the memory can recognize the sparsity of encoded operand A, and prefetch operand B by non-zero indices.",
    "source": "2503.06342v1_Exploring_the_Performance_Improvement_of_Tensor_Pr.pdf",
    "length": 1794,
    "tokens": 490
  },
  {
    "text": "5, we propose solutions to overcome these challenges and enhance processing efficiency, which emulates the convolution process in bit-scalable CIM architectures. In CIM arrays, quantized weights break down into smaller segments, bit-split weights, to fit the number of capable bits per memory cell. When replicating bit-scalable operations, it is necessary to enable access to each bit-split of the weight. To facilitate this, we duplicate the original weight, W, according to the number of bit-splits, s, allowing independent processing of each bit-split during the quantization and convolution stages. Subsequently, these duplicated weights are quantized to Wq at layer-wise, array-wise, or column-wise granularity. Each quantized weight then undergoes array-wise MAC operation, facilitated by weight mapping and tiling. In the conventional im2col method, time-consuming linear operations are used to compute MAC results, creating a noticeable bottleneck. To improve processing efficiency, we propose a novel tiling method that transforms the linear operation into a convolution. By strategically adjusting the tiling stride, we ensure stretched kernels remain intact in each array, as described in Fig. 5, and reshape it into a 4-dimensional convolutional weight. Moreover, sequential array-wise convolution introduces crit- ical overhead, as it requires indexing arrays one by one. To mitigate this challenge, we utilize group convolution, matching the number of groups to the number of arrays, as illustrated in Fig. 5. This removes sequential indexing delays and simplifies access to array-wise partial-sums, resulting in faster convolution and subsequent partial-sum quantization. Following the array-wise convolution, the generated partial- sums are quantized and accumulated. Importantly, weight and partial-sum granularities are independently selected from layer- wise, array-wise, or column-wise quantization, enabling tai- lored optimization based on the specific requirements of the model and hardware configuration. Finally, the convolution outputs from each bit-split weight are shifted and accumulated to produce one convolutional layer output. D. Efficient One-stage QAT via Granularity Alignment In [9], a two-stage QAT approach was employed to manage the granularity mismatch between weights and partial-sums, with partial-sums only quantized in the second stage of training to reduce training costs.",
    "source": "2502.07842v2_Column-wise_Quantization_of_Weights_and_Partial_Su.pdf",
    "length": 2419,
    "tokens": 502
  },
  {
    "text": "31 39 200 - - - - - KU040 MobileNetv1Lite i16 50.0M -? 31 39 200 - - - - - ML S [13] ZU7EV Deep Belief Network f32 - - 10 0 250 - - - 5.4e-3 - GNN F [138] U280 GNN f32 - - 43 37 260 400 38.0 10.5 2.7e-2 759 [137] ZU7EV GNN f32 2.13M 1.0 74 91 125 - 6.3 - 1.1e-4 9500 Automatic FINN CNN S [46] 7Z020 CNN b - - - - - - - - 3.3e-1 - [84] 7Z045 BNN b - - - - - - - - 2.1e-2 - [95] 7Z020 CloudSatNet-1 Q2 b - 1.4 14 - 100 - 2.5 - - 15.5 7Z020 CloudSatNet-1 Q4 i4 - 3.1 14 - 100 - 2.6 - - 15.5 Vitis AI CNN F [53] ZU19EG ETAUS i8 - - - - 106 329.9 1.6 199.9 1.1e-1 9.3 [71] ZU7EV RFA-YOLO i8 - 24.8 - - 200 - 15.8 - - 28 [87] ZU7EV YOLOv4 i8 105 G 245.0 - - 100 - 30.1 - 1.1e-1 17.9 ZU7EV YOLOv4-tiny 3L i8 15.3G 24.1 - - 100 - 26.4 - 1.9e-2 125 [104] ZU3EG ENet i8 4.06G 0.4 70 75 300 - 3.4 - - 25.2 ZU3EG ESPNet i8 3.71G 0.3 70 75 300 - 3.1 - - 11.7 ZU3EG FPN i8 17.3G 5.8 70 75 300 - 4.0 - - 14.1 ZU3EG U-Net i8 96.6G 7.4 70 75 300 - 3.4 - - 2.7 [119] ZU9EG ResNet-50 i8 - - - - - - - - 1.1e-2 93 ZU9EG SICNet i8 - - - - - - - - 1.8e-3 530.7 [136] ZU7EV Impr. YOLOv4-tiny i8 - - - - 300 - 20.0 - - 295.9 [143] K26 YOLOv4-MobileNetv3 i8 - 5.7 59 - 200 - 7.2 - - 48.1 MATLAB CNN S [31] ZU9EG ResNet-18 YOLOv2 i8 - - - - 250 - - - - 46 ML S [42] 7A35T CAG-SC2S i32 - -?",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 1263,
    "tokens": 664
  },
  {
    "text": "NAND2 Electrical similarity Rise delay Which NOR2 arc is closest to arc(INVx1, Y, A) arc(NOR2x1,Y,B) Determine the answer by evaluating the Euclidean distance between delay transition power-specific cell arc vectors and report top-K accuracy Which NAND2 arc is closest to arc(INVxp33,Y,A) arc(NAND2xp33,Y,B) Fall delay Which NOR2 arc is closest to arc(A2O1A1Ixp33,Y,A1) arc(NOR2xp67,Y,A) Which BUF arc is closest to arc(AO211x2,Y,A1) arc(BUFx8,Y,A) Rise transition Which NAND2 arc is closest to arc(INVx1,Y,A) arc(NAND2x1,Y,B) Which NAND2 arc is closest to arc(INVx2,Y,A) arc(NAND2x2,Y,B) Fall transition Which BUF arc is closest to arc(AO211x2,Y,A1) arc(BUFx2,Y,A) Which BUF ar is closest to arc(AO211x2,Y,A2) arc(BUFx4,Y,A) Rise internal power Which NOR2 arc is closest to arc(INVx1,Y,A) arc(NOR2x1,Y,A) Which NOR2 arc is closest to arc(INVx2,Y,A) arc(NOR2x2,Y,A) Fall internal power Which BUF arc is closest to arc(AO211x2,Y,A1) arc(BUFx2,Y,A) Which BUF arc is closest to arc(AO211x2,Y,A2) arc(BUFx2,Y,A) Fig. 3. Accuracy comparison in (a) inverting functionality, (b) functional similarity and (c) electrical similarity test sets between random guess and Lib2Vec with various embedding sizes.",
    "source": "2503.22900v1_Learning_Library_Cell_Representations_in_Vector_Sp.pdf",
    "length": 1196,
    "tokens": 456
  },
  {
    "text": "Model Layers Nl Heads Nh Embedding De Tensor Para. Data Para. OPT-66B 64 72 9216 2 4 GPT-89B 48 96 12288 4 2 GPT-175B 96 96 12288 8 1 Precision: ùëÅùëùùëüùëí 2 (16-bit floating point); Head Embedding: ùê∑‚Ñé ùê∑ùëí ùëÅ‚Ñé 128. Table 4: Real-world traces. Trace Avg. Lin Std. Lin Avg. Lout Std. Lout OpenR1-Math-220k [5] 96.0 75.1 12684.1 8464.6 Dolphin-r1 [4] 201.9 563.0 3926.2 4216.0 OpenThoughts-114k-math [6] 89.4 66.7 6366.7 4662.9 LongBench [12] 7703.9 4285.5 89.8 213.7 6 Evaluation 6.1 Experimental Setup L3 implementation. L3 is built upon the DGX-A100 system [3]. On the GPU side, 8 NVIDIA A100 GPUs, each with 5 HBM2e of 80 GB capacity, are integrated, providing a total of 156 TFLOPs on FP16. The GPUs are connected via NVLink [82]. On the CPU side, there are 16 channels with 2 DIMMs of total 2TB capacity, which are further equipped with the proposed PIM capability. We develop a simulator based on DRAMSim3 [66] and AttAcc [72] to evaluate the performance of L3, including both GPU and DIMM- PIM components. The hardware specifications are listed in Table 2. For hardware validation, we implement the logic of the bank PU and the rank PU with SystemVerilog. The prototype is synthesized with Synopsys Design Compiler under the TSMC 28nm CMOS technology to estimate the area and power consumption. Baseline systems. We compare L3 with four baseline systems: 1) GPU-only, which executes LLM inference exclusively on GPUs. 2) GPU with HBM-PIM, which represents NeuPIMs [33] and At- tAcc [72]. We assume double buffers in each bank for concurrent GPU and HBM-PIM execution.",
    "source": "2504.17584v1_L3_DIMM-PIM_Integrated_Architecture_and_Coordinati.pdf",
    "length": 1564,
    "tokens": 499
  },
  {
    "text": "Restrictions apply. W1 W4 W7 W2 W5 W8 W3 W6 W9 A3 A2 A1 A5 A4 0 A7 0 0 W1A1 W4 W7 W2 W5 W8 W3 W6 W9 A4 A3 A2 A6 A5 A4 A8 A7 0 C3 C2 C1 C5 C4 B6 C7 B9 B8 D1 C3 C2 C6 C5 C4 C8 C7 B9 t 0 t 1 t 7 t 6 W1 W4 W7 W8 W5 Placing I-Txn-Var in the SA Grid Tile Level Acclerator Level Global Scratchpad Banked 1 - 4 T12 T13 T14 T15 T8 T9 T10 T11 T4 T5 T6 T7 T0 T1 T2 T3 Weights Output Stream Banked 1 - 4 Input Stream Input Stream Input Stream Input Stream Banked 1 - 4 T12 T13 T14 T15 T8 T9 T10 T11 T4 T5 T6 T7 T0 T1 T2 T3 Weights Output Stream Banked 1 - 1 Input Stream Input Stream Input Stream Input Stream Global Scratchpad Banked 1 - 1 Banked 1 - 1 Power Level -1 Power Level -2 Power Level -3 Tile n W1 W4 W7 X2 W5 W8 X3 X6 X9 A3 A2 A1 A5 A4 0 A7 0 0 X1 X4 X7 X8 X5 W1 W4 W7 W2 W5 W8 W3 W6 W8 A3 A2 A1 A5 A4 0 A7 0 0 W1 W4 W7 W8 W5 Tile 2 W1 W4 W7 V2 W5 W8 V3 V6 V9 A3 A2 A1 A5 A4 0 A7 0 0 V1 V4 V7 V8 V5 Tile 1 A3 A2 A1 I-Txn-00 A6 A5 A4 I-Txn-01 A9 A8 A7 I-Txn-02 Tile 0 W1 W4 W7 U2 W5 W8 U3 U6 U9 A3 A2 A1 A5 A4 0 A7 0 0 U1 U4 U7 U8 U5 Input Stream Global Scratchpad Banked 1 - 16 Banked 1 - 16 T12 T13 T14 T15 T8 T9 T10 T11 T4 T5 T6 T7 T0 T1 T2 T3 Weights Output Stream Banked 1 - 16 Input Stream Input Stream Input Stream Input Stream C1 C2 C3 C4 A5 A6 C7 A8 A9 B1 B2 B3 B4 A5 A6 B7 A8 A9 A1 A2 A3 A4 A5 A6 A7 A8 A9 W1 W2 W3 W4 W5 W6 W7 W8 W9 Weight Stationary Convolution Toy Example PE Level Fig. 6: Weight stationary compute mapping.",
    "source": "Usas.pdf",
    "length": 1435,
    "tokens": 656
  },
  {
    "text": "During deployment, only the non-zero weight values are stored in the RAMAN parameter memory. The same seed and LFSR structure employed during training are used to generate vi an identical PRS sequence during inference. The non-zero weight values are then read from memory and matched with the indices generated by the LFSR. This matching ensures that each non-zero weight value is correctly multiplied by its corresponding activation value. Ultimately, the indices generated by the LFSR guarantee that all non-zero weights stored in the RAMAN memory are covered, facilitating the computation of the final output. Additionally, the balanced pruning strategy ensures a uni- form distribution of non-zero weights across weight tiles, effectively mitigating workload imbalances. Without this strat- egy, non-zero weights would be unevenly distributed among various weight tiles processed by different processing elements (PEs), resulting in workload discrepancies and performance limitations imposed by the PE handling the heaviest load. The operation of the balanced pruning strategy is illustrated in Fig. 6. Initially, N filters with an input channel dimension of M are represented as a weight matrix of size M N. Subsequently, the weight matrix is divided into tiles of size 1 n, where n is determined by the depth of the PE register-file (RF), set to 16 in our design. Each 1 n tile is then pruned based on the required sparsity level to have Œò non-zeros, where Œò is a function of the pruning percentage. For example, for 25 , 50 , and 75 pruning percentages, Œò is set to 12, 8, and 4, respectively. The weight values with indices not covered by LFSR-generated PRS are pruned. Furthermore, the LFSR- generated indices ensure that the same number of weights are pruned in each tile, resulting in structured sparsity that can be efficiently leveraged in RAMAN. Following balanced pruning, each tile contains an equal number of non-zero weights pro- cessed by different PE columns, achieving uniform workload distribution across the PE array. Each processing element (PE) contains four multiply-accumulate (MAC) units, executing four MAC operations in a single cycle and necessitating four non-zero weights per cycle. Thus, we employ four 4-bit LFSRs to generate four random indices simultaneously.",
    "source": "2504.06996v1_Neural_Signal_Compression_using_RAMAN_tinyML_Accel.pdf",
    "length": 2296,
    "tokens": 479
  },
  {
    "text": "Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 785 794. Chrabaszcz, P., Loshchilov, I., and Hutter, F. (2017). A downsampled variant of imagenet as an alternative to the CIFAR datasets. CoRR, abs 1707.08819. Dong, X. and Yang, Y. (2020). Nas-bench-201: Extending the scope of reproducible neural architec- ture search. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Elsken, T., Metzen, J. H., and Hutter, F. (2019). Neural architecture search: A survey. Journal of Machine Learning Research, 20(55):1 21. Gallo, M. L., Lammie, C., B√ºchel, J., Carta, F., Fagbohungbe, O., Mackin, C., Tsai, H., Narayanan, V., Sebastian, A., Maghraoui, K. E., and Rasch, M. J. (2023). Using the IBM analog in-memory hardware acceleration kit for neural network training and inference. CoRR, abs 2307.09357. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A. G., Adam, H., and Kalenichenko, D. (2018). Quantization and training of neural networks for efficient integer-arithmetic-only inference. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 2704 2713. Computer Vision Foundation IEEE Computer Society. Jiang, W., Lou, Q., Yan, Z., Yang, L., Hu, J., Hu, X. S., and Shi, Y. (2021). Device-circuit-architecture co-exploration for computing-in-memory neural accelerators. IEEE Trans. Computers, 70(4):595 605.",
    "source": "2506.18495v1_AnalogNAS-Bench_A_NAS_Benchmark_for_Analog_In-Memo.pdf",
    "length": 1567,
    "tokens": 475
  },
  {
    "text": "Here, the inference for increasing sequence length corresponds to more rows in the input matrix, maintaining the same computation flow and EMA analysis for linear projection. Table 2 shows that varying sequence lengths influence the optimal stationary scheme choice, indicating that TAS s adaptive mechanism compensates for limitations in fixed schemes. TABLE IV MEASUREMENT OF COMPUTING ENERGY COST FOR BERT-BASE WITH NAIVE IMPLEMENTATION, AYAKA [9] S OPTIMIZATION, AND OURS ACROSS LAYERS Layer ID Na ƒ±ve (A) [9] (B) Ours (C) Reduction A B A A C A 0 65.81 35.76 1.89 48.47 97.17 1 66.30 35.05 1.90 48.86 97.15 2 67.65 37.30 1.94 49.88 97.09 3 67.44 37.13 1.93 49.72 97.10 4 67.40 36.23 1.93 49.69 97.10 5 67.42 35.35 1.93 49.70 97.10 6 67.35 37.40 1.93 49.65 97.10 7 64.46 35.28 1.85 47.40 97.23 8 67.44 33.44 1.93 49.72 97.10 9 67.55 35.12 1.94 49.80 97.09 10 65.04 34.63 1.86 47.86 97.20 11 64.74 34.59 1.85 47.62 97.21 12 66.55 35.61 1.91 49.03 97.14 Table IV shows the energy consumption in the BERT- Base model by appling our method to [9]. The computational energy cost includes both external data transfer and internal chip processing, following the same approach and energy numbers in [9]. Basically, the energy consumed by external data transmission is 10 to 100 times greater than that of internal chip computation. To simplify the effective simulation of computing energy costs, measurements can be efficiently taken by evaluating the EMA ratio across various stationary schemes. The fixed stationary scheme introduced by [9] results in an approximate 48 reduction in energy usage during BERT-Base model inference, on average, when compared to a basic implementation lacking stationary schemes.",
    "source": "2503.19640v1_An_Efficient_Data_Reuse_with_Tile-Based_Adaptive_S.pdf",
    "length": 1706,
    "tokens": 479
  },
  {
    "text": "As this KV cache is not shared across different user requests, the large batch size is directly translated into a large KV cache size, requiring massive memory capacity. Furthermore, as KV cache size scales linearly with the sequence length, the recent trend of supporting very long sequences (e.g., 2 million tokens [76]) places even greater pressure on memory capacity. Consequently, LLM serving systems require both high bandwidth and high capacity to enable fast inferencing. This resource demand aligns with the immense user demand for LLM services, making it challenging for service providers to build cost-effective LLM serving systems. Existing solutions often choose to trade-off one resource for the other, as visualized in Figure 1. Below, we classify the existing solutions into the following three categories: (1) Prioritizing bandwidth over capacity: Currently, using HBM- equipped GPUs is the de-facto standard solution for LLM in- ference processing [28, 45, 49]. While this approach achieves massive bandwidth, it often compromises capacity, forcing sys- tems to scale out, which not only reduces core utilization but also increases the cost of building the system. (2) Leveraging PIM and or PNM: Even with HBMs, LLM infer- encing systems still face bandwidth bottlenecks. To address this challenge, recent works have explored the near-data processing (NDP) paradigm, leveraging PIM [20, 52, 59] and or PNM [53]. While these approaches mitigate the bandwidth bottleneck, their inherent nature requires further reductions in memory capacity, limiting their viability as a fundamental solution. (3) Exploring LLM quantization strategies: One fundamental strategy for jointly addressing the conflicting objectives is to minimize the memory footprint required for LLM inferencing. To achieve this goal, a large body of prior work [10, 17, 22, 30, 31, 33, 40, 41, 43, 75, 78, 79, 86] have recently developed LLM-targeted quantization techniques. While these techniques successfully achieve significant reductions in bitwidth, they often prioritize minimizing bitwidth over effectively translating these reductions into practical inferencing speedups. Alone, none of the solutions is sufficient for building fast and effi- cient LLM serving systems, which motivates us to develop an accel- eration solution, namely Oaken.",
    "source": "2503.18599v2_Oaken_Fast_and_Efficient_LLM_Serving_with_Online-O.pdf",
    "length": 2332,
    "tokens": 488
  },
  {
    "text": "As for solutions monitoring ships, Yang et al. [132] present OSCAR-RT, an end-to-end algorithm hardware codesign framework dedicated to real-time onboard ship detection with SAR images. Their three experiments considerably focus on real-time performance using on-chip implementations. Neris et al. [86] use lightweight CNNs to identify ships and airplanes, deploying them on a Kintex US FPGA, which has a radiation-hardened counterpart, the XQRKU060 also used by Rapuano et al. [98]. Ieracitano et al. [55] also 166U CubeSat, launched 09 2020, 173U CubeSat, 12 2019 - 05 2024, 183U CubeSat educational mission from MIT, expected launch 2025, 196U CubeSat platform from the Korea Aerospace Research Institute (KARI) Manuscript submitted to ACM FPGA-Enabled Machine Learning Applications in Earth Observation: A Systematic Review 25 target ship monitoring with HO-ShipNet, a CNN enhanced with xAI techniques for increased transparency, achieving 95 accuracy. As discussed in Section 3.4, radiation resilience remains a key requirement for space deployment [110]. Sabogal and George [104] study SEEs effects on DL applications, quantifying performance degradation in Xilinx DPU implementations [5]. Such effects are mitigated through circuit-level hardening or software solutions, like Triple Modular Redundancy (TMR). By contrast, UAV platforms operate in less radiation-exposed environments, enabling more flexible trade-offs in model design and hardware selection. AI-Accelerated UAVs Payloads: Autonomous and Real-Time Insights. Spacecraft are not the only edge RS platforms benefiting from ML; numerous studies explore ML-supported pipelines for UAVs. Wang and Qiu [124] deploy a CNN on a Zynq-7000 to detect objects in images from the 2018 DAC System Design Contest, targeting real-time inference under low-power constraints. Suh et al. [113] design three sizes of SSD models to detect drones achieving high energy efficiency on a Zynq US . Addressing a practical use case, Yu et al. [136] develop an Improved YOLOv4-tiny to detect abnormal railway track fasteners.",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 2068,
    "tokens": 476
  },
  {
    "text": "[44] X. Tao, X. Hong, X. Chang, S. Dong, X. Wei, and Y. Gong, Few- shot class-incremental learning, in Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2020. [45] Y. E. Wibowo, C. Cioflan, T. M. Ingolfsson, M. Hersche, L. Zhao, A. Rahimi, and L. Benini, 12 mj per class on-device online few-shot class-incremental learning, in 2024 Des., Automat. Test in Europe Conf. Exhib. (DATE), 2024, pp. 1 6. 14 [46] P. Warden, Speech commands: A dataset for limited-vocabulary speech recognition , 2018. arXiv:1804.03209. [47] O. Rybakov, N. Kononenko, N. Subrahmanya, M. Visontai, and S. Laurenzo, Streaming keyword spotting on mobile devices , 2020. arXiv:2005.06720. [48] Y. Zhang, N. Suda, L. Lai, and V. Chandra, Hello edge: Keyword spotting on microcontrollers , 2017. arXiv:1711.07128. [49] S. Davis and P. Mermelstein, Comparison of parametric represen- tations for monosyllabic word recognition in continuously spoken sentences, IEEE Trans. on acoustics, speech, and signal processing, vol. 28, no. 4, pp. 357 366, 1980. [50] W. Shan, M. Yang, T. Wang, Y. Lu, H. Cai, L. Zhu, J. Xu, C. Wu, L. Shi, and J. Yang, A 510-nw wake-up keyword-spotting chip using serial-fft-based mfcc and binarized depthwise separable cnn in 28-nm cmos, IEEE J. of Solid-State Circuits, vol. 56, no. 1, pp. 151 164, 2021. [51] B. Liu, H. Cai, Z. Wang, Y.",
    "source": "2505.24852v2_Chameleon_A_MatMul-Free_Temporal_Convolutional_Net.pdf",
    "length": 1376,
    "tokens": 474
  },
  {
    "text": "1 module example_raw 2 ( parameter BW 8) 3 ( 4 input [BW-1:0] a, 5 input [BW-1:0] b, 6 input [BW-1:0] c, 7 input [BW-1:0] d, 8 output [BW-1:0] s1, 9 output [BW-1:0] s2, 10 output [BW-1:0] s3, 11 output [BW-1:0] s4, 12 output [BW-1:0] s5, 13 output [BW-1:0] s6 14 ); 15 assign s1 a b; 16 assign s2 a b; 17 assign s3 a \\ b d; 18 assign s4 c d b a; 19 assign s5 a - b; 20 assign s6 (b 1) a d c -b; 21 endmodule Listing 6: Example Test Case: subexpression elimination. 14 SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired Symbolic Reasoning 1 module example 2 ( parameter BW 8) 3 ( input [BW-1:0] a, 4 input [BW-1:0] b, 5 input [BW-1:0] c, 6 input [BW-1:0] d, 7 output [BW-1:0] s1, 8 output [BW-1:0] s2, 9 output [BW-1:0] s3, 10 output [BW-1:0] s4, 11 output [BW-1:0] s5, 12 output [BW-1:0] s6 13 ); 14 assign s1 a b; 15 assign s2 a b; 16 assign s3 a \\ b d; 17 assign s4 c d s2; 18 assign s5 a - b; 19 assign s6 s4 s5; 20 endmodule Listing 7: Example Test Case: subexpression elimination after applying the Common Sub-Expressions Elimination template. The Common Sub-Expressions are reused in the states after it.",
    "source": "2504.10369v1_SymRTLO_Enhancing_RTL_Code_Optimization_with_LLMs_.pdf",
    "length": 1130,
    "tokens": 440
  },
  {
    "text": "A toy example of an adaptive ResNet with two skippable blocks. time-sensitive applications such as autonomous driving, it is imperative to deploy models that have predictable execution times. Our approach enables a fully controllable inference that is adaptable and predictable (unattainable for input-dependent approaches [18, 19]). II. ADAPTIVE SKIPPING WITH STOCHASTIC TRAINING Figure 3 gives an overview of our framework for adaptive inference. It works in two phases. In the first phase, models are trained following the Stochastic Depth approach [9], and the DNN design space is navigated to deliver operating points at the Pareto front for the skipping configurations (i.e., different set of skipped blocks deliver different accuracy-performance profiles). The generation of the Pareto front consists of four steps (1 to 4 in the figure). The second phase happens at runtime after the model has been trained and the Pareto front configurations have been selected. At runtime, the selected operating points (skipping configurations) can be switched at no cost (given this approach s single-model property, there is no need to load a different set of weights) to adapt the inference processing to current conditions. The list of configurations is passed to the runtime algorithm which, based on the profile of the current conditions at the edge, adapts the inference processing (step 5 in the Figure 3). For instance, see the toy example on Figure 4 with a ResNet with only two skippable blocks. There are three possible skipping configurations for adapting the inference processing, skipping block one, block two, or none. As explained earlier, these combinations will show different accuracy, latency, and energy profiles to be exploited dynamically in response to changes in runtime conditions or requirements at the edge. A. Design-Time In this section, we describe the main innovations regarding the design-time phase including the usage of Stochastic Depth for skipping-resilient models, the insertion of the fully-controllable gates, and the selection of skipping configurations for Pareto configurations. 1) Inputs and Training: The design-time phase starts by reading in the user CNN (in PyTorch) and the dataset. In Step 1 of this phase (see Figure 3), the CNN is trained with the Stochastic Depth procedure described in subsection I-A).",
    "source": "2505.17626v1_Leveraging_Stochastic_Depth_Training_for_Adaptive_.pdf",
    "length": 2352,
    "tokens": 483
  },
  {
    "text": "The state-of-the-art PCC pipeline typically utilizes tree structures like Octree [63] or kd-tree [62] for compression, and often, the tree construction becomes a bottleneck due to lack of parallelization. Moreover, the conventional PC typically stores the geometry, while a wide array of applications, especially the ones meant for content consumption, infotainment and gaming, need the attributes to be stored as well, hence making the compres- sion even more complex. For example, TMC13 [56] and CWIPC [48] two state-of-the-art (SOTA) PCC techniques take 4.1s and 4.2s, respectively, to compress one PC frame on an edge platform, which are signiÔ¨Åcantly higher than the real-time requirement ( 100ms [19]), making them even more challenging to employ in emerging edge devices. To address this, we study the SOTA compression pipelines and observe that the main reason behind their performance inefÔ¨Åciencies is their sequential updates to the global result with each intermediate local runtime state in a point-by-point fashion. Moreover, there has been little effort in parallelizing them on the state-of-the-art commercial systems, let alone on any edge mobile devices. Prior works on PCC acceleration [19], [33] only consider the PC with geometry data and or have limited parallelism, and thus, could neither leverage GPU nor beneÔ¨Åt from other types of accelerators. In this context, this paper explores the following three opportunities: 1 The points can be processed in parallel by using Morton codes [30] (which mathematically represent the geometry relationship among points) to identify the spatial-locality1 within one frame for geometry compression. 2 Further, this locality also exists in attributes (RGB pixels), i.e., spatial locality leads to attribute similarities, and hence opening opportunities for fast attribute compression. 3 And, Ô¨Ånally, the locality extends beyond a single frame, i.e., the temporal locality, which can be leveraged by sorting the points in the Morton code order, creating further opportunities to improve the compression efÔ¨Åciency. Motivated by these opportunities, we propose and evaluate a two-pronged compression approach, where the intra-frame approach leverages the opportunities described in 1 and 2 , and the inter-frame approach takes advantage of 3 .",
    "source": "PCcompress.pdf",
    "length": 2299,
    "tokens": 502
  },
  {
    "text": "Second, in both HSS configurations, Harmonia 0 1 2 3 CDE K-SVM RNN-HSS Sibyl Sibyl K-SVM Sibyl RNN-HSS CDE RL-Migr SAPM Harmonia Oracle 0 1 2 3 SYSTOR17 RocksDB YCSB MLPerf AVG Write Amplification (b) Cost-Optimized HSS (a) Performance-Optimized HSS Figure 15: Write amplification of Harmonia and baselines on performance-optimized (top) and cost-optimized (bottom) HSS. Lower is better. has lower WA (1.24) in write-intensive workloads (i.e., SYSTOR17) compared to its WA (1.8) in read-intensive workloads (i.e., RocksDB, MLPerf). In write-intensive workloads, Harmonia learns to move data during update operations and performs fewer migrations. Write Traffic Distribution in Harmonia. Fig. 16 shows the write distribution of Harmonia s migration policy across devices in performance- and cost-optimized HSS. We make two observations. 0 25 50 75 100 SYSTOR17 RocksDB YCSB MLPerf AVG Fast Device Writes Slow Device Writes SYSTOR17 RocksDB YCSB MLPerf AVG (a) Performance- Optimized HSS (b) Cost-Optimized HSS Figure 16: Write traffic due to Harmonia s migrations on performance-optimized (left) and cost-optimized (right) HSS. First, Harmonia migrates more data to slow device as it effectively utilizes fast device for frequently-accessed data. Second, Harmonia has more writes to fast device in read-intensive workloads (i.e., RocksDB, MLPerf) as it proactively prefetches frequently-read data to the fast device even when there are no application updates. We conclude that Harmonia s adaptive policies provide higher performance at slightly higher average WA compared to Sibyl. 7 Discussion Alternate Objectives and Reward Structures. Since Harmonia s focus is to improve HSS performance, we design the RL agents reward structure based on I O latencies. We can design Harmonia for other objectives such as device lifetimes, fairness, quality of service (QoS) and energy efficiency.",
    "source": "2503.20507v2_Harmonia_A_Multi-Agent_Reinforcement_Learning_Appr.pdf",
    "length": 1884,
    "tokens": 488
  },
  {
    "text": "1 out_ptr out 2 while(tkn ctrlQ.pop() ! done){ 3 if(tkn ùëíùëí){ 4 for(e 0; e emb_len; e vlen){ 5 v dataQ.pop vlen x f32 () 6 v_fma(out_ptr e, v) }} 7 else if(tkn ùë†ùëí){ 8 out_ptr emb_len } ee ee se ee se done ABCD IJKL UVWX ctrlQ: dataQ: (d) Queue aligned code. Figure 14: Impact of SLC optimizations on the SLS compute code and output queues. 6.3 Lowering SLC IR to DLC IR After applying optimizations (Section 7), the SLC IR is lowered to the low-level DLC IR (Section 4). The SLC IR is generated by traversing the SLC IR from the outer to the inner SLC for-loop. SLC for-loops and streams are lowered to DLC traversal operators and streams. Callbacks, instead, are moved into the while-loop in the compute code (Figure 9). Multiple callbacks are chained into an if-then-else construct that cases the token IDs popped from the control queue (e.g. Figure 14d). Token s push instructions are generated according to the location of the callback in the SLC IR. Operands push and pop instructions are generated according to the SLC stream-to-value operations. 7 Optimizing Embedding Operations This section describes three key DAE optimizations enabled by the SLC IR. Figure 15 and 14 show how these optimizations transform the SLC IR, compute code, and queues. 7.1 Vectorization Vectorization is one of the most impactful DAE optimizations [61]. As shown in Figure 14b, for each token, vectorization pops a vector of vector length (vlen) elements, improving marshaling and compute efficiency. While auto-vectorizing general code is challenging [27], the SLC IR provides an effective representation to vectorize loading, marshaling, and computing of embedding operations. As shown in Figure 15b, Ember vectorizes code by converting SLC operations into their vectorized SLCV duals.",
    "source": "2504.09870v1_Ember_A_Compiler_for_Efficient_Embedding_Operation.pdf",
    "length": 1772,
    "tokens": 470
  },
  {
    "text": "Comet relies on cudaMemcpyPeerAsync [39], while FasterMoE and Megatron-LM use NCCL exclusively for communication. We also evaluate FlashDMoE on a multi-node environment and discuss our findings in F. 4.1 Desiderata In our experiments, we observe Comet exhibiting anomalously bad performance values at 8 GPUs, so we exclude their results from evaluations at 8 GPUs and only include for results at 4 GPUs. Note we evaluate FlashDMoE using FP32 precision whereas all baselines use FP16. We do so because (1) no baseline supports FP32 and (2) time constraints prevent us from tuning our system to peak performance at FP16. Most importantly, this precision discrepancy disadvantages FlashDMoE by doubling the communication and computation precision, making our results a conservative lower bound. Yet, as we show in the succeeding sections, FlashDMoE outperforms all baselines. 4.2 Forward Latency 4K 8K 16K Number of Tokens 0 5 10 15 20 25 Runtime (ms) 2.1 3.4 6.2 3.0 5.4 10.0 7.9 12.6 22.4 4.9 16.6 17.4 5.1 9.0 17.2 Forward Latency E 32 k 2 4 H100s is better FlashDMoE Comet FasterMoE Megatron-CUTLASS Megatron-TE (a) 4 H100s 4K 8K 16K Number of Tokens 0 10 20 30 40 Runtime (ms) 2.1 3.7 7.0 10.3 21.3 45.3 9.9 18.9 36.9 10.0 18.9 36.9 Forward Latency E 32 k 2 8 H100s is better FlashDMoE FasterMoE Megatron-CUTLASS Megatron-TE (b) 8 H100s Figure 10: Forward Latency as the Number of Tokens per GPU increases. We first measure the forward latency of FlashDMoE across different sequence lengths on both 4 and 8 GPU setups (Figure 10). FlashDMoE consistently outperforms all baselines, with especially notable improvements at longer sequence lengths. On 4 GPUs, it achieves up to 4.6x speedup over Megatron-TE at 16K tokens, and 2.6x over FasterMoE.",
    "source": "2506.04667v2_FlashDMoE_Fast_Distributed_MoE_in_a_Single_Kernel.pdf",
    "length": 1746,
    "tokens": 487
  },
  {
    "text": "7 and 8. This is the previous best known software implementation for these benchmarks. 4. Hardware FSM. Gemmini can generate accelerators with specialized hardware units for two coarse-grained operations: GEMM and convolution. These hardware units, implemented as finite state machines (FSMs), encode control sequences for each of these operations in hardware. If tuned correctly, the FSMs can exceed the theoretical maximum performance of any software-based implementation, as hardware is inherently parallel. However, this is accomplished at the cost of scheduling flexibility, as well as increased area, power, and hardware complexity. We use the hardware FSM as a reference for the highest achievable performance, but do not expect to exceed its performance for these GEMM convolution benchmarks as its compute utilization exceeds 90 for all but one benchmark (82 on 12544x256x64). Despite this, Autocomp-generated code approach hardware FSM performance for GEMM convolution, and as seen in Sec. 4.5, even exceeds hardware FSM performance in end-to-end application performance thanks to Autocomp s greater scheduling flexibility. For GEMM and convolution, we use Exo Unoptimized, which contains statically pre-computed loops, addresses, and indices, as Autocomp s starting point. This simplifies code generation and allows us to directly compare the effectiveness of Autocomp to hand-optimization (i.e., Exo Optimized code). The third workload type, robotics control code, is a multi-kernel application containing sequences of element-wise operations and matrix-vector multiplications. As this is not directly supported by Exo, we compare to an unoptimized software implementation ported to accelerator code by Dong et al. [14], and an extensively hand-optimized hardware FSM-based implementation written by an expert. 4.3 Matrix multiplication (GEMM) We run Autocomp on a set of GEMM benchmarks selected by Ikarashi et al. [26] from ResNet-50 [19] for diversity in size and shape. We run search with beam size B 6, N 6 plans per element in the beam, K 2 code candidates per plan, and T 15 iterations. This takes around 5 hours to run. Fig.",
    "source": "2505.18574v2_Autocomp_LLM-Driven_Code_Optimization_for_Tensor_A.pdf",
    "length": 2144,
    "tokens": 466
  },
  {
    "text": "Therefore, these apparent deficiencies of choosing the appropriate resource type and model type for a given user re- quirement motivates the central question of this work: Does there exist an optimal resource procurement system which can balance the goals of diverse user requirements for accuracy, latency and cost, by efficiently mapping model parameters to heterogeneous resource specifications? Our preliminary results suggest that using a combination of VMs and serverless func- tions could potentially provide a solution to this problem. As opposed to prior works [5, 10], which try to combine serverless functions with VMs to hide the start-up latencies of VMs, our primary interest lies in exploring the different key aspects to address when hosting DNN-based ML pre- diction serving systems in public cloud, as given below: Diverse Models: How to make the users oblivious of model selection from the extensive pool of models, for satis- fying the accuracy, and latency requirements? Heterogeneous Public Cloud Resources: What are the different options available in terms of combining different VM-based cloud services and serverless functions for a given user requirement? Configuring Resources: From the diverse options, how to right-size VMs and appropriately configure the serverless functions to efficiently cater to user specified cost, accuracy and latency constraint? Bring in Tune: Based on the dynamically changing query arrivals over time, what is the right way to combine model diversity along with resource heterogeneity without com- promising the user-specified requirements? By exploring these key aspects, we envision developing a self-managed inference-serving system, which can provide for different diverse needs of applications by leveraging the 0 50 100 150 200 250 300 350 20.00 40.00 60.00 80.00 100.00 MobileNet V1 MobileNEt V2 Inception V3 Resnet50 ResNet50-V2 DenseNet-201 DenseNet-121 Xxception NasNetMobile InceptionResnetV2 vgg16 NasNetLarge Latency (ms) Accuracy Top1-Accuracy Latency Figure 1. Accuracy and Latency of Different Pretrained Models. 0 20 40 60 80 Percentage Model Type (a) Different accuracy for ISO- latency. 0 200 400 600 800 1000 Time(ms) Model Type (b) Different response latencies for ISO-accuracy. Figure 2. Comparison of different models under ISO-latency and ISO-accuracy setup. heterogeneous resource availability from the public cloud.",
    "source": "PubCloudHet.pdf",
    "length": 2398,
    "tokens": 496
  },
  {
    "text": "To enforce graph-level permutation invariance, AnalogGenie adopt the approach used by GraphRNN (You et al., 2018), employing a breadth-first-search (BFS) algo- rithm to assign node orders within each device type, ensuring that each circuit topology has a unique graph representation. To further minimize permutation invariance at the sequence level, AnalogGe- nie manually defines VSS (the ground node universal to all analog circuits) as the starting node for all sequences. These techniques let AnalogGenie to learn a robust and generalizable representation. 3.4 CUSTOMIZING TOKENIZER TO PRE-TRAIN A DOMAIN-SPECIFIC GPT MODEL Built on the above foundations, we customize a tokenizer to encode and decode our sequence that represents the circuit topology to train AnalogGenie. Table 2 in Appendix A.2 shows an example of a look-up table used for tokenization, where each token represent a device pin. The device type and the maximum number of devices in each device type are determined through a data-driven method by scanning the devices types and number in the training data. To allow AnalogGenie to generate topologies with different numbers of devices, we introduce a special Truncate token and use padding to ensure that all sequences have the same length. With this customized tokenizer, we pre-train AnalogGenie to predict the next device pin in the sequence. Unlike traditional pre-training of LLMs, which often involves randomly cropping se- quences from text documents, AnalogGenie s pre-training ensures that each sequence corresponds to a complete circuit topology. Specifically, given an unsupervised corpus of tokens U {u1, . . . , un} that represent one circuit topology, we aim to maximize the standard language modeling objec- tive (Radford et al., 2018) to train AnalogGenie. During generation, AnalogGenie begins with a single context token, VSS (the starting node for all Eulerian circuits) and completes the rest of the sequence, ensuring it represents a valid circuit topology.",
    "source": "2503.00205v1_AnalogGenie_A_Generative_Engine_for_Automatic_Disc.pdf",
    "length": 2000,
    "tokens": 438
  },
  {
    "text": "Four practical CNNs listed in Table VI are evaluated on the Ô¨Åve power traces illustrated in Figure 4 for each of the Ô¨Åve execution strategies from Section V-A3: Naive1; Naive2; Sequential; Pipelining; and ResiSchedule. TABLE III RERAM PARAMETERS ReRAM computing crossbar DAC(150 N 1 title) 5.4 pJ ADC(M 1 1 title) 749 fJ S A(M 1 1 title) 41.6 fJ area (all peripherals included) 2950.47 ¬µm2 ReRAM memory (16KB) bandwidth 128 bit s read energy 37.993 pJ read latency 1.577 ns write energy 95.412 pJ write latency 20.09 ns TABLE IV IOT-PRACTICAL CNN WORKLOADS CNN Layer Kernel RRAM Size Acti. power Input PV [41] Input Conv1 36 8 752.2¬µW Conv2 72 12 1125.6¬µW Conv3 108 16 1526¬µW Conv4 144 10 1114¬µW Conv5 90 6 676.2¬µW FR [42] Input Conv1 25 4 377.4¬µW Conv2 64 16 1433.6¬µW LeNet [43] Input Conv1 25 6 539.7¬µW Conv2 150 16 1614.2¬µW HG [44] Input Conv1 25 6 539.7¬µW Conv2 96 12 1176¬µW For each application on each power trace, we report the throughput and energy efÔ¨Åciency under the Ô¨Åve different execution strategies. We then demonstrate the beneÔ¨Åts from the proposed smooth transition strategy and power prediction. We also study the sensitivity of our proposed approach to available ReRAM hardware resources. A. Throughput Figure 8 shows the throughput comparison of the Ô¨Åve exe- cution strategies. The bars are all normalized to ResiSchedule. The included table gives the absolute values of throughput by ResiSchedule. The results show that ResiRCA and ResiSched- ule combine to achieve an average throughput improvement of 8 compared to a baseline RCA with intermittency-unaware scheduling.",
    "source": "ResiRCA.pdf",
    "length": 1589,
    "tokens": 473
  },
  {
    "text": "We determine the number of clusters required as a function of current energy availability and accuracy trade off of using a lesser number of clusters. However, naively framed, this approach requires knowledge of what activity is being performed in order to encode the data that will be used to perform inference to determine what activity is being performed. To break this circular de- pendency, we take inspiration from prior work in HAR [47], and use the highly stable temporal continuity of human ac- tivity (relative to the tens of milliseconds timescales for HAR 10 0 0.05 0.1 0.15 0.2 k 15 k 12 k 10 k 8 Activity Aware Coresets (HAR) Energy Aware Coresets (Bearing) Fixed Coresets Data Volume normalized to raw data (a) Data volume with dynamic coresets 0.5867 0.4106 0.2837 0.9139 0.8516 0.6076 0.8742 0.8073 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 WiFi Office Wifi Home Piezo Daily Movement Completion in Origin (HAR) Completion in Seeker (HAR) Completion in Seeker (Bearing) N A (b) Fraction of inferences completed with different EH sources 6.18 6.96 7.12 21.76 19.46 12.57 31.27 28.76 22.56 34.16 32.95 35.19 6.63 11.87 22.56 0 10 20 30 40 50 60 70 80 90 100 Wifi Office Wifi Home Piezo Daily Movement Power Trace: Memoization DNN 16bit DNN 12 Bit Clustering Importance Sampling (c) Distribution of compute off-load to different components Figure 11: Accuracy and communication efficiency of Seeker with different data sets and its sensitivity towards various EH sources.",
    "source": "Seeker.pdf",
    "length": 1484,
    "tokens": 402
  },
  {
    "text": "[17] W. L. Hamilton, R. Ying, and J. Leskovec, Inductive representation learning on large graphs, in NeurIPS, 2017. [18] Y. Zhang, H. You, Y. Fu, T. Geng, A. Li, and Y. Lin, G-CoS: GNN- accelerator Co-Search towards both better accuracy and efficiency, in ICCAD, 2021. [19] F. Teichteil-K onigsbuch, G. Pov eda, G. Gonz alez de Garibay Barba, T. Luchterhand, and S. Thi ebaux, Fast and robust resource-constrained scheduling with graph neural networks, in ICAPS, 2023. [20] J. Nunez-Yanez, Accelerating graph neural networks in pytorch with hls and deep dataflows, in ARC, 2023. [21] C. Savard, N. Manganelli, B. Holzman, L. Gray, A. Perloff, K. Pedro, K. Stenson, and K. Ulmer, Optimizing high-throughput inference on graph neural networks at shared computing facilities with the nvidia triton inference server, in Computing and Software for Big Science, 2024. [22] S. Liang, Y. Wang, C. Liu, L. He, H. LI, D. Xu, and X. Li, EnGN: A high-throughput and energy-efficient accelerator for large graph neural networks, in TC, 2021. [23] A. Raha, R. Sung, S. Ghosh, P. K. Gupta, D. A. Mathaikutty, U. I. Cheema, K. Hyland, C. Brick, and V. Raghunathan, Efficient Hardware Acceleration of Emerging Neural Networks for Embedded Machine Learning: An Industry Perspective. Springer, 2024, pp. 121 172. [24] OpenVINO Documentation, Openvino IR format: Operation sets and specifications, r-format operation-sets operation-specs.html, 2024, accessed: Jan. 30, 2025. [Online].",
    "source": "2502.06921v2_GraNNite_Enabling_High-Performance_Execution_of_Gr.pdf",
    "length": 1464,
    "tokens": 460
  },
  {
    "text": "Being 1,047 faster allows 1,047 the batch size with the same latency. Tables 9 and 10 show how a batch size of 1,024 creates a good match between the time to compute a batch of inferences and the time to load the weights of those inferences from HBM, with both operations taking around the same time for this batch size. Table 8: Example Transformer calculation Llama 3.1 405B Core Model Parameters Size Symbol Model dimension 16,384 d Number of attention heads 128 h Vocabulary size 128,000 V Number of layers 80 N Feedforward dimension 65,536 4d Max context length 128,000 max L Batch size used in this example 1,024 B Context length (average used in example) 2,000 L Table 9: Example Transformer Inference Calculations Llama 3.1 405B Table 10: Weights loading Inference - FLOPS Calculation Order OPs of total Weight Order Weights of total Token embedding lookup O(B x L x d) 3.36E 10 0.000005 O(V x d) 2.10E 09 0.60 Positional encoding (RoPE) O(B x L x d) 3.36E 10 0.000005 O(1) - - Layer normalization (pre-attention) O(B x N x L x d) 2.68E 12 0.00038 O(N x d) 1.31E 06 0.00038 Multi-head attention (Q, K, V projections) O(B x N x L x 3 x d x d) 1.32E 17 18.61 O(N x 3 x d x d) 6.44E 10 18.52 Attention score O(B x N x h x L x L) 4.19E 13 0.006 O(1) - - SoftMax of attention scores O(B x N x h x L x L) 4.19E 13 0.006 O(1) - - Value weighting and concatenation O(B x N x h x L x L x (d h)) 5.37E 15 0.76 O(1) - - Output projection O(B x N x L x d x d) 4.40E 16 6.20 O(N x d x d) 2.15E 10 6.17 Residual connection (post attention) O(B x N x L x d) 2.68E 12 0.00038 O(1) - - Layer normalization (pre-FFN) O(B x N x L x d) 2.68E 12 0.00038 O(N x d) 1.31E 06 0.00038 Up projection FFN O(B x N x L x d x 4d) 1.76E 17 24.81 O(N x d x 4d) 8.59E 10 24.70 Gate projection FFN O(B x N x L x d x 4d) 1.76E 17 24.81 O(N x d x 4d) 8.59E 10 24.70 SwiGLU Nonlinearity O(B x N x L x 4d) 1.07E 13 0.0015 O(1) - - Down projection FFN O(B x N x L x 4d x d) 1.76E 17 24.81 O(N x 4d x d) 8.59E 10 24.70 Residual connection (post FFN) O(B x N x L x d) 2.68E 12 0.00038 O(1) - - Final layer normalization O(B x L x d) 3.36E 10 0.000005 O(d) 1.64E 04 0.000005 Output projection (autoregression) O(B x 1 x d x V) 2.15E 12 0.00030 O(V x d) 2.10E 09 0.60 Total for a batch of 1024 inferences 7.09E 17 100 Total weights 3.48E 11 100 ZettaLith peak performance 1,507,534 PFLOPS HBM bandwidth 2.56E 14 Bytes s Target percentage of peak PFLOPS 80 Weight data 1.74E 11 Bytes Time to compute a batch of 1024 inferences 0.00059 Seconds Weights from HBM 0.00068 Seconds 27 15 The Base Interface Die (BID) Figure 6c illustrates the basic contents of the BID, which integrates multiple interface blocks and memory elements in a mainstream process node.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2719,
    "tokens": 957
  },
  {
    "text": "Prior works [34], [48], [68] suggest that selecting the right hyper-parameters (like batch size, learning rate, number of layers to train etc.) have a huge impact on the convergence and accuracy of the models. For each edge servers to handle multiple steams with multiple drifts, we need to jointly optimize the hyper-parameters for maximizing accuracy with minimum power and resource budget. To achieve this, we design a micro-proÔ¨Åler that can look into the drift of the models as well as the power availability and decide the right hyperparameters to train the models. Prior works [12], [38], [68] have designed hyperparameter micro-proÔ¨Ålers. However, they never considered an intermit- tent power source, nor explored jointly optimizing multiple models with power, accuracy and latency constraints. Further- more, each model might contribute differently to the overall accuracy. Observing this, we propose a weighted accuracy metric , where the weight of each of the model is a function of the accuracy, time needed and power availability. Furthermore, we allow some slack to the weighted accuracy so that the optimizer can choose a better set of hyperparameters if we can reach close to the weighted accuracy with much lower resource (power or compute) consumption. Typically, there is an inverse correlation of the convergence of the stochastic gradient descent (SGD) algorithm, the most popular training algorithm for DNNs, over the number of iterations (ni) [68]: l O(1 ni) and l 1 Œ≤0.ni Œ≤1 Œ≤2, where l is the loss of the SGD and Œ≤i is an non-negative real number. Therefore, by running a few iterations of the SGD algorithms with various other hyperparameters, we can easily predict the con- vergence of the models. Note that this needs to be done every time one of the constraints (accuracy, power etc.) changes. The micro-proÔ¨Åler optimizes the weighted accuracy (Aw Wi Ai Wi; i models;Wi f(time,dri ft,compute) with a user-deÔ¨Åned slack value of Œ¥), with respect to available power (Pav): maxAw; s.t.P Pav. Energy Buffering and Power-Predictor: To regulate, manage and ensure a stable power supply to the circuitry, Us.",
    "source": "Usas.pdf",
    "length": 2128,
    "tokens": 493
  },
  {
    "text": "By constructing correctness feedback from automated 1QiMeng is a Chinese term that refers to the process of imparting fundamental knowledge and skills to beginners, serving as the cornerstone for intellectual development and skill enhancement. Named by QiMeng, we expect this system can achieve fully automated processor chip design through learning human knowledge and experience, followed by practicing and self- evolving. 3 functional verification, LPCM is able to autonomously repair erroneous results and ensure the validity of generated outputs, addressing the challenge of ensuring correctness in proces- sor design. Concurrently, leveraging performance feedback from automated performance evaluation, LPCM is capable of decomposing the solution space and pruning the low- performance subspaces. Thus, LPCM can effectively reduce the dimensionality of the solution space and enable efficient exploration of high-performance design solutions, overcoming the challenge of the enormous solution space. Based on LPCM, QiMeng develops two specialized agents, a Hardware Design Agent and a Software Design Agent, dedicated to the automated design of hardware and software for processors and chips. The Hardware Design Agent adopts a dual-loop mechanism, consisting of an outer module de- composition feedback loop based on performance optimization and an inner module generation feedback loop empowered by automated verification and repair. This dual-loop mechanism facilitates end-to-end automated design from functional spec- ifications to physical layouts, unifying conventional disjointed stages such as logic design, circuit design, and physical design. Thus, Hardware Design Agent enables a fully integrated, cross-stage collaborative design paradigm that is expected to surpass conventional human design, potentially achieving superior performance under identical fabrication technology. Meanwhile, the Software Design Agent also employs a dual- loop mechanism, consisting of an outer performance optimiza- tion feedback loop guided by LLM and an inner function adaptation feedback loop based on automated verification and repair. Software Design Agent autonomously achieves seamless functional adaptation and performance optimization of foundational software for target processor chips, addressing the dynamic and escalating demands of modern applications. Leveraging the Hardware Design Agent and Software De- sign Agent, various applications can be developed to ad- dress diverse real-world use cases of processor chip design. For automated hardware design, significant milestones have been accomplished, including automated front-end design and automated HDL generation. In automated software design, achievements include automated OS configuration optimiza- tion, automated compiler tool-chain design, automated tensor program transcompiler, and automated high-performance li- brary generation.",
    "source": "2506.05007v1_QiMeng_Fully_Automated_Hardware_and_Software_Desig.pdf",
    "length": 2907,
    "tokens": 501
  },
  {
    "text": "(1) Next, matrix Qi is first multiplied with K i to compute the attention score Si, which is then multiplied with Vi to produce the attention output Ai for the ith head. This process is expressed as follows, where dk denotes the hidden dimension: Si Softmax(S i) Softmax(QiK i dk ), Ai SiVi. (2) 4 TABLE I THE COMPUTATIONAL COMPLEXITY OF LINEAR OPERATIONS ACROSS DIFFERENT STAGES IN LLMS Linear Operation Formula Prefill Stage Decode Stage Q K V OMHA XW Q, XW K, XW V , AW O 4ld2 4d2 Attention QK , SV l(l 1)d 2(l 1)d FFN œÉ(XW A)W B 2lddFFN 2ddFFN Notes: l represents the input sequence length, d denotes the input feature dimension, and dFFN is the FFN hidden dimension. a Prefill stage b Decode stage Sequence Length Percentage ( ) 256 512 1024 2048 4096 0 20 40 60 80 100 Sequence Length 256 512 1024 2048 4096 Percentage ( ) 0 20 40 60 80 100 Q K V OMHA Q K V OMHA FFN FFN Attention Attention Fig. 4. The computational breakdown of Llama-2-7B [4] inference during (a) the prefill stage and (b) the decode stage across different sequence lengths. Finally, the attention output from all h heads are concatenated and processed through the output linear projection layer W O to produce the final result of the MHA module OMHA: OMHA concat(A1, ..., Ad) W O. (3) As shown in Fig. 3 (b) (down), in the FFN module, which consists of two linear layers (W A and W B) separated by a non-linear activation function (œÉ( )), the computation for a given input X Rl d can be expressed as: OFFN œÉ(X W A) W B. (4) where OFFN is output of the FFN module. In summary, since the prefill stage processes the input prompt in parallel, the computations involved in both MHA (input and output linear projections in Eq. (1) and Eq. (3), respectively, and attention computations in Eq.",
    "source": "2505.03745v1_AccLLM_Accelerating_Long-Context_LLM_Inference_Via.pdf",
    "length": 1762,
    "tokens": 489
  },
  {
    "text": "Similarly to CSR is the compressed 1 (accessed March 13, 2025). 4 M. M. M√ºller and K. L√ºbeck et al. 0.5 0.6 0.7 0.8 0.9 Sparsity 0 100 200 300 400 Memory Footprint (kB) No Compression RLE-4 COO Bitmap CSR Two-Stage-Bitmap CSC CSB (a) 1 3 2 4 6 5 7 1 3 2 4 6 5 7 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 non-zero values column bit array element bit array 0 1 2 3 4 5 (b) 1 6 4 7 5 2 3 8 0 0 0 0 0 0 0 0 0 0 1 5 6 2 4 7 3 8 0 2 0 3 1 1 4 -1 5 3 column index array number of col. after merge non-zero values 0 1 2 3 4 5 (c) Fig. 1. (a) Memory footprint comparison of different sparse matrix formats and no compression for a 128 512 matrix of 32bit values with varying sparsities and uniformly distributed zero elements. (b) Two-stage bitmap format example. (c) Compressed sparse block (CSB) format example. sparse column (CSC) format, but it stores a matrix by columns instead of rows. The non-zero elements are also stored in an array, along with the row indices and column pointers stored in two additional arrays. The coordinate format (COO) stores the non-zero elements of a matrix as a list of (row, column, value) tuples, which is easy to implement but comes with significant storage overhead. The Run-Length Encoded 4bit (RLE-4) format represents a matrix as a sequence of 4bit codes, where each code indicates the length of a run of zeros followed by a non-zero element. This format can be very compact for matrices with long runs of zeros, but it may be less efficient for matrices with more uniform sparse dis- tributions. The bitmap format also contains an array for the non-zero elements, but additionally defines a bit array, where for every element is stored whether it is non-zero or zero indicated by a 1 and 0 respectively.",
    "source": "2506.01566v1_FlexiSAGA_A_Flexible_Systolic_Array_GEMM_Accelerat.pdf",
    "length": 1748,
    "tokens": 479
  },
  {
    "text": "(3) E. Real-time Full Stack Case Studies To round out our evaluation, we demonstrated the real- time feasibility of the RaGNNarok framework, by per- forming real-time simultaneous localization and mapping (SLAM) and navigation case studies. We highlight that the RaGNNarok framework was run alongside industry standard SLAM, localization, and navigation stacks in real-time on the Raspberry Pi 5. Simultaneous Localization and Mapping We used the commonly used slam-toolbox ROS2 package to perform SLAM using the point clouds generated by the RaGNNarok model[9]. Due to the lower resolution nature of the radar sensing, we set the map resolution to 10 cm. Navigation Finally, we used the popular ROS2 Nav2 pack- age to demonstrate real-time localization and navigation through a mapped environment[7], [8]. Here, we adapted the standard Nav2 adaptive monte carlo localization (AMCL) and default navigation configuration to utilize the enhanced point clouds generated from the RaGNNarok framework. Noteably, we also used the maps generated in the previous step to successfully demonstrate how our framework can be used to successfully map and then autonomously navigation through an environment. V. RESULTS We start with an analysis of the computational time re- quired to run each model, followed by the offline evaluations of point cloud quality and localization accuracy. Then, we conclude with a discussion on our real-time full stack case studies for SLAM and navigation. A. Computational Time Analysis To compare the computational time required to make each inference on RaGNNarok versus the other baseline methods, we conducted run-time timing tests on a Raspberry Pi 5 and a Lenovo P360 equipped with a Nvidia T1000 GPU and an Intel i9 CPU. As seen in Table I, RaGNNarok significantly reduces the computational time required to enhance input point clouds. Even on a GPU equipped machine, we achieve an over 6 and 25 reduction in inference time compared to RadCloud and RadarHD, respectively. Additionally, we highlight that RaGNNarok still maintains low inference times on the compute constrained Raspberry Pi 5, enabling real- time operations for our UGV platform. X(m) Y(m) Fig. 5: RaGNNarok generates more accurate point clouds that remove false detections and generalize well to new environments.",
    "source": "2507.00937v1_RaGNNarok_A_Light-Weight_Graph_Neural_Network_for_.pdf",
    "length": 2307,
    "tokens": 487
  },
  {
    "text": "However, starting from glibc v2.2, it is possible to unmask them using feenableexcept functions from fenv.h library. This function allows the FP exceptions to cause a trap and the program to be interrupted. After masked exceptions we run another training session with the same configuration but with the 1 generated_assembly_function: 2 rep N 3 FLD qword [x] 4 endrep 5 FCOMI st0 , st1 6 JE exit: 7 ; cache encoding 8 MOVZX rax , byte [ rdi] 9 SHL rax , 10 10 MOV rax , qword [rsi rax] 11 exit: 12 RET Listing 1: A simplified instruction sequence that triggers masked FP exception due to repeated x87 instruction in line 4. Following cache encoding instructions (line 8-10) get executed speculatively. 1 generated_assembly_function: 2 rep N 3 PSUBQ MM2 , [R15] 4 FCOMIP ST4 5 endrep 6 VCMPPD K3, ZMM1 , ZMM4 , 2 7 JNE exit 8 ; cache encoding 9 MOVZX rax , byte [ rdi] 10 SHL rax , 10 11 MOV rax , qword [rsi rax] 12 exit: 13 RET Listing 2: A simplified version of the RL generated assembly instruction sequence that has MMX (line 3) to x87 (line 4) transition. Following cache encoding instructions (line 9-11) get executed speculatively. feenableexcept function enabling FE_INVALID, FE_DIVBYZERO, FE_OVERFLOW, FE_UNDERFLOW, and FE_INEXACT bits of the excepts argument. With this configuration, the RL agent was still able to gen- erate instruction sequences that cause observable byte leakage through transient execution without generating any ¬µcode as- sists, faults, or interrupts. Listing 2 shows an example of such an instruction sequence. After simplifying the instruction sequence, we observed that the transient execution is caused by an FP exception that is generated by the FCOMIP instruction. However, the MMX instruction before the FCOMIP instruction causes the exception to get lost. We use the feenableexcept function to unmask FP exceptions, yet the exception generated in the processor gets cleared by the PSUBQ instruction.",
    "source": "2502.14307v1_ŒºRL_Discovering_Transient_Execution_Vulnerabilitie.pdf",
    "length": 1940,
    "tokens": 487
  },
  {
    "text": "Figure 9 shows our inves- tigation into various methods for handling the heteroge- neous components of program configurations. We evaluated choices ranging from conventional feature augmentation (FA) to principal component analysis (PCA), auto-encoders, and variational auto-encoders (VAE). Our findings reveal that auto-encoders were the most effective for capturing het- erogeneous optimizations in a latent space. This was evident from the lower validation loss observed during the training of the auto-encoders to learn the latent representations. Data Collection Overhead w o Transfer Learning. Figure 10 shows that without transfer learning, the overhead of data collection becomes significant on emerging hardware due to the high costs of running simulations. For example, models trained exclusively on SPADE would require 20 200 more target data samples (collected using 100 1000 matrices) to match or surpass the speedups achieved through COGNATE via transfer learning. 1.0 1.2 1.4 1.6 Geomean Speedup 1.07 1.21 1.40 1.36 1.19 1.55 CPU 5 CPU 20 COGNATE CPU 500 CPU 1000 Optimal Figure 11: Impact of nega- tive transfer for fine-tuning. 1.0 1.2 1.4 1.6 Geomean Speedup 1.00 1.401.411.421.43 1.55 Baseline COGNATE TL 100 TL 1000 NT 1000 Optimal Figure 12: Impact of number of samples for fine-tuning. Impact of Negative Transfer. Figure 11 shows that using a large dataset to train the source model (e.g., data samples from 1000 matrices) does not necessarily lead to better out- comes. As the size of the training dataset increases, the model becomes overly specialized to the source platform, diminishing its adaptability during fine-tuning. To inves- tigate this effect, we trained source models on datasets of varying sizes (5, 20, 100, 500, and 1,000 matrices) and eval- uated their transferability to our target platform (SPADE) using few-shot fine-tuning on just 5 matrices (Figure 11). Our results show that training on the CPU (source) with data samples from 100 matrices and fine-tuning on SPADE (target) with data samples from 5 matrices produces the best results.",
    "source": "2506.00424v2_COGNATE_Acceleration_of_Sparse_Tensor_Programs_on_.pdf",
    "length": 2082,
    "tokens": 494
  },
  {
    "text": "Quantization enables faster inference and lowers energy consumption while maintaining model performance, making it a practical solution for real-time applications [60 62]. 5 Quantization can be represented as a mapping function CQ CQ,deq CQ,int, where CQ,int : R Z maps real values to integers, and CQ,deq : Z R is an affine function that recovers original values. This process defines a quantization grid, with a step size s and zero point z aligning the quantizer with the input range: CQ(v) CQ,deq(CQ,int(v)) s vint z s . Here, vint is the quantized integer value, with s q p 2nk 1 determining the step size for an nk-bit representation within the range [p, q]. Post-Training Quantization (PTQ). PTQ applies quantization to a trained model without additional retraining, converting FP32 tensors to INT8 while maintaining accuracy [26, 27]. A single scaling parameter adjusts the quantization range per layer or channel, ensuring adaptability to different data distributions. Fine-tuning based on calibration samples further refines accuracy. However, PTQ may struggle to generalize across unseen data distributions, leading to potential precision loss. Online dynamic schemes mitigate this by recalibrating tensor ranges during inference but introduce computational overhead [63]. Quantization-Aware Training (QAT). Unlike PTQ, QAT incorporates quantiza- tion into the training process to enhance performance on quantized hardware. During forward propagation, weights and activations are quantized, simulating deployment conditions [28, 29]. Backpropagation updates FP32 weights, minimizing quantiza- tion errors while training. This dual-format approach improves model robustness but introduces hyperparameter tuning complexities [64]. Mixed Precision Quantization (MPQ). MPQ mitigates accuracy loss in ultra-low precision models by assigning different bit widths to different network components [30]. While traditional INT8 quantization preserves accuracy in critical layers, MPQ improves compression and efficiency for onboard deployment. The challenge lies in optimally selecting bit widths, prompting techniques like OneShot Mixed-Precision Search [30], which improved model correlation by 41 between sampled child models and standalone fine-tuned models on ImageNet.",
    "source": "2505.08793v1_Onboard_Optimization_and_Learning_A_Survey.pdf",
    "length": 2275,
    "tokens": 479
  },
  {
    "text": "We observe that HBM-PIM s sub-batch strategy demonstrates promising efficiency in bubble reduction, though our analysis re- veals certain limitations when processing Dolphin trace on OPT- 66B. It faces challenges in efficiently utilizing GPU resources when handling inherently small batch sizes, potentially leading to GPU- side execution bottlenecks that may result in performance be- low conventional GPU-only implementations. On the contrary, L3 achieves considerable merit with much larger and more balanced sub-batches, resulting in significant throughput improvements. 0 1 2 3 4 5 Bw-only Cap-only Bw-Cap Normalized Throughput 1 2 4 8 (a) GPT-175B. 0 1 2 3 4 5 6 Bw-only Cap-only Bw-Cap Normalized Throughput 1 2 4 8 (b) GPT-89B. Figure 9: Scalability analysis. The results are evaluated using the OpenR1 trace. Scalability analysis. We further evaluate L3 performance with various host memory configurations, showing that performance scalability requires simultaneous scaling in memory capacity and bandwidth. We first establish the base memory configuration as (512 GB, 2 ranksets). Based on this, we scale up the memory in different ways for different baselines: Bw-only indicates that we only scale up the rankset from 2 to 16, while Cap-only means we only scale up the capacity from 512GB to 4TB. Bw-Cap implies that we scale up both the rankset and capacity to (4TB, 16 ranksets). Similarly, we execute 1,000 requests from the trace and calculate the average throughput. We normalize the performance of the smallest memory configuration to 1. Fig. 9 shows the results of GPT-89B and GPT-175B with the OpenR1 trace. Results for other models and traces are similar. It shows that expanding either bandwidth or capacity alone does not effectively improve throughput. For example, when exclusively scaling memory capacity or bandwidth by 8 on the GPT-175B model, the throughput only increases by 1.6 and 1.1 , respectively. In contrast, when both bandwidth and capacity are enlarged, the throughput increases by 5.1 . This demonstrates that L3 effectively leverages the value of both scalability aspects. Ablation study.",
    "source": "2504.17584v1_L3_DIMM-PIM_Integrated_Architecture_and_Coordinati.pdf",
    "length": 2128,
    "tokens": 469
  },
  {
    "text": "requiring the overhead of one scale factor multiplication. On the other hand, if partial-sums are quantized at array-wise level, those from the same output channel within an array are first shift-and-added. Subsequently, each channel undergoes separate dequantization as illustrated in Fig. 4(b), increasing the overhead to narray noc multiplications for a layer, where narray and noc are the number of arrays and output channels in an array. Further granularity refinement is seen in Fig. 4(c), where each column of partial-sums has its unique scale factor, sp1 to sp4. While the layer-wise weight scale factor, sw, is shared across all columns, the system must store and apply the appropriate multiplied scale factor to each column, resulting in per-column overhead of nsplit narray noc multiplications, where nsplit is the number of bit-splits. Our quantization scheme extends the concept of column-wise quantization by applying unique scale factors to each column for both weights, sw1 to sw4, and partial-sums, sp1 to sp4. The multiplied scale factors are applied to each partial-sum that flows out from its corresponding column, as shown in Fig. 4(d). This approach appears to intensify the dequantization complexity due to the fine weight granularity. However, the key discovery is that, without introducing additional dequantization overhead, aligning column-wise quantization for weights and partial-sums enables finer and more precise control over the quantization process, compared to the layer-wise weight with column-wise partial-sum configuration. C. A Convolution Framework for Column-wise Quantization We propose a custom convolution layer framework designed to efficiently implement convolution layers with column-wise weight and partial-sum quantization. Conventionally, handling individual columns introduces challenges of time costs and complexities in data management, often resulting in ineffi- ciencies. We resolve these obstacles by utilizing a unique array tiling method combined with group convolution. As illustrated in Fig. 5, we propose solutions to overcome these challenges and enhance processing efficiency, which emulates the convolution process in bit-scalable CIM architectures. In CIM arrays, quantized weights break down into smaller segments, bit-split weights, to fit the number of capable bits per memory cell.",
    "source": "2502.07842v2_Column-wise_Quantization_of_Weights_and_Partial_Su.pdf",
    "length": 2350,
    "tokens": 488
  },
  {
    "text": "Total inference time: 201.00 seconds. Fig. 6: Result Gantt: Baseline of clients can be estimated as 0.21 milliseconds per token, with an additional fixed overhead of 29 milliseconds. For instance, when processing a parallel batched decode stage involving 200 clients, where each client produces one token per round, the operation would take approximately 200 0.21 29 71 milliseconds. In the case of prefill stages, if a batch consists of inputs totaling 5,000 tokens, the estimated time required would be 5000 0.13 25 675 milliseconds. We present a Gantt chart in Fig. 6, generated from an experiment using real-world data and open source LLM, to illustrate the current state of online inference services with- out the implementation of our proposed method. This chart demonstrates that, in practical scenarios, a significant number of idle periods, or bubbles , occur when no scheduling strategy is employed. Furthermore, in offline scenarios, if the workload among clients is not evenly distributed, substantial machine idle time is observed after the early completion of some client s tasks. Our analysis of this Gantt chart reveals that the overall machine utilization rate is only 80.2 . B. Offline request scheduling result This offline request scheduling model given by Eqs. (26) (30) can be solved using open-source solver SCIP. Due to significantly reduced complexity, optimal solutions can be achieved within 20 minutes comparing to original problem which is not possible to be solved within hours. Although this offline model only addresses workload balancing using estimations of output length, its performance surpasses that of the original version. As illustrated in Fig. 7, the system shows a significant reduction of idle times, and machine utilization is enhanced to 85.5 . Comparing to the baseline method, this method provides a more balanced request assignment across clients and reduce bubbles . The total inference time can be reduced from 201.00 seconds to 197.08 seconds. Since solving the model still takes relatively long time, we list this method as optional and suggest practitioners use the offline model in typical scenarios such as RLHF training. 11 0 25000 50000 75000 100000 125000 150000 175000 Time(ms) 0 25 50 75 100 125 150 175 200 Client Total Inference Time prefill decode Utilization rate: 85.5 . Total inference time: 197.08 seconds. Fig.",
    "source": "2502.15763v1_Hybrid_Offline-online_Scheduling_Method_for_Large_.pdf",
    "length": 2379,
    "tokens": 482
  },
  {
    "text": "Recently, a new direction in the field has emerged, aimed at pushing ML at the extreme edge, i.e. directly within the sensors [5]. This approach is particularly beneficial for TinyML applications, which usually rely on battery-powered devices, as it further reduces power consumption by utilizing sensors equipped with an Intelligent Sensor Processing Unit (ISPU) [6], featuring an ultra-low- power programmable core able to process data within the sensor itself. In the literature, several works employing the ISPU exist [5], [7] [10]. Among these solutions, only [10] exploits Convolu- tional Neural Networks (CNNs), which are highly effective across diverse ML tasks. However, in [10], the inference time and the memory occupation depend on the input size as the CNNs are implemented in a Width-First manner. As the input size increases, it exceeds the ISPU s strict limitations (i.e., These authors contributed equally to this work a maximum clock frequency of 10 MHz and a memory less than 32 kB). Consequently, tasks requiring larger inputs may either surpass the memory limits, making them infeasible, or necessitate a reduced inference frequency for execution. To address this issue, this paper introduces a novel method- ology to design CNNs capable of leveraging the power of ISPUs taking into account their strict memory and computa- tional limits. Specifically, we present three key strategies for achieving this goal: 1) Depth-First convolutions: The convolutions within the ISPU are computed using an incremental approach (similar to those explored in the works of [11], [12], and [13]). Specifically, the convolution output, typically computed over multiple samples collected within a temporal window, is updated each time a new sample becomes available. This approach removes the need to store all the samples within the window, as the effect of each new sample is computed immediately, further reducing the memory requirements. 2) Partitioned Computation: The CNN computation is partitioned between the ISPU and the microcontroller unit (MCU). This allows us to respect the memory limits of the ISPU, reduce the computational load of the MCU, and reduce the data throughput between the sensor and the MCU. For instance, the ISPU can handle the initial (feature extraction) part of the CNN, while the MCU executes the final part of the CNN.",
    "source": "2503.16939v1_On-Sensor_Convolutional_Neural_Networks_with_Early.pdf",
    "length": 2356,
    "tokens": 490
  },
  {
    "text": "Distributed On-Sensor Compute System for AR VR Devices: A Semi-Analytical Simulation Framework for Power Estimation. arXiv preprint arXiv:2203.07474 (2022). [21] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz, and William J. Dally. 2016. EIE: Efficient Inference Engine on Compressed Deep Neural Network. ACM SIGARCH Computer Architecture News 44, 3 (June 2016), 243 254. [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep Residual Learning for Image Recognition. [23] Nicholas J. Higham. 1993. The Accuracy of Floating Point Summation. SIAM Journal on Scientific Computing 14, 4 (1993), 783 799. 0914050 arXiv: [24] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Wei- jun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. Mo- bileNets: Efficient Convolutional Neural Networks for Mobile Vision Applica- tions. arXiv:1704.04861 [cs.CV] [25] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, An- drew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). [26] W. Kahan. 1965. Pracniques: further remarks on reducing truncation errors. Commun. ACM 8, 1 (Jan. 1965), 40. [27] Alex Krizhevsky. [n. d.]. CIFAR-10 and CIFAR-100 Datasets. kriz cifar.html. [28] H. T. Kung, Bradley McDanel, and Sai Qian Zhang. 2020. Term quantization: furthering quantization at run time. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (Atlanta, Georgia) (SC 20). IEEE Press, Article 96, 14 pages.",
    "source": "2504.09072v1_MGS_Markov_Greedy_Sums_for_Accurate_Low-Bitwidth_F.pdf",
    "length": 1715,
    "tokens": 492
  },
  {
    "text": "Our experiments shows that DNN inference using Origin, running on a harvested energy only system, is more accurate than energy-constraint- optimized DNNs, running on a fully-powered system. Although the current work is limited to HAR, this can further be extended to many suitable tasks which need to leverage a distributed sensor system for DNN inference. We believe that the co- optimization of deep learning and energy harvesting techniques for edge devices will further invigorate research on the next generations of intelligent and sustainable IoT platforms. VI. ACKNOWLEDGMENTS This work was supported in part by Semiconductor Research Corporation (SRC), Center for Brain-inspired Computing (C- BRIC) and NSF Grant 1822923 (SPX: SOPHIA). REFERENCES [1] Taking an ecg with the ecg app on apple watch series 4 or later, 2020, [2] Use fall detection with apple watch, 2020, us HT208944. [3] T.-J. Yang, A. Howard, B. Chen, X. Zhang, A. Go, M. Sandler, V. Sze, and H. Adam, Netadapt: Platform-aware neural network adaptation for mobile applications, in ECCV, September 2018. [4] Google assistant for wearables, 2020, [5] K. Ma, X. Li, S. R. Srinivasa, Y. Liu, J. Sampson, Y. Xie, and V. Narayanan, Spendthrift: Machine learning based resource and fre- quency scaling for ambient energy harvesting nonvolatile processors, in 2017 (ASP-DAC), 2017, pp. 678 683. [6] K. Qiu, N. Jao, M. Zhao, C. S. Mishra, G. Gudukbay, S. Jose, J. Samp- son, M. T. Kandemir, and V. Narayanan, Resirca: A resilient energy harvesting reram crossbar-based accelerator for intelligent embedded processors, in 2020 HPCA, 2020, pp. 315 327. [7] G. Gobieski, B. Lucia, and N. Beckmann, Intelligence beyond the edge: Inference on intermittent embedded systems, in ASPLOS. ACM, 2019.",
    "source": "Origin.pdf",
    "length": 1755,
    "tokens": 484
  },
  {
    "text": "CONCLUSION AND FUTURE WORK The conventional paradigm of processor chip design is confronting three fundamental challenges: physical constrains in fabrication technology, requirements of design resources, and growing diversity of ecosystems. To achieve automated processor chip design based on LLMs and AI technologies, this work proposes QiMeng, an innovative paradigm for fully automated hardware and software design for processors and chips. QiMeng establishes a domain-specific LPCM and fur- ther develops Hardware Design Agent and Software Design Agent by leveraging the powerful knowledge representation and inferencing capabilities of LPCM. Then the two agents are applied to various application scenarios in processor hardware software design. Currently, several components of QiMeng have been applied in multiple applications, providing viable solutions for hardware software design. With the deeper convergence of AI technologies and EDA tools, automated processor chip design will evolve toward greater efficiency, generality, and intelligence. In future work, 18 we will follow the roadmap of QiMeng to accomplish the top-down phase, then proceed to the bottom-up and iter- ation phases. Simultaneously, we will continue exploring the integration of reinforcement learning, continual learning, and evolutionary algorithms to further enhance the capa- bilities of QiMeng. For the Hardware Design Agent, we will explore combining the framework of LPCM-based au- tomated HDL generation with verify-repair-driven feedback and performance-driven feedback, establishing the entire Hard- ware Design Agent which achieves both correctness and performance optimization. Additionally, we will investigate end-to-end design from functional specifications to transistor- level implementation, breaking through conventional Boolean logic and CMOS paradigms, while scaling up the design to achieve industrial-grade automated processor chip design. For the Software Design Agent, the current implementation primarily exploits the textual comprehension from LLMs. Future enhancements will integrate the graph-structured rep- resentation. Moreover, we will extend the agent s applicability to autonomous software migration, functional adaptation, and performance optimization for more foundational software. By realizing and continuously improving the capabilities of QiMeng, we aim to address increasingly diverse scenario demands, driving the entire processor chip domain toward intelligence and automation.",
    "source": "2506.05007v1_QiMeng_Fully_Automated_Hardware_and_Software_Desig.pdf",
    "length": 2503,
    "tokens": 441
  },
  {
    "text": "However, the domain-specialized data is extremely scarce, preventing the training of LPCM. In the middle-layer, the development of Hardware Software Design Agent depends on the domain knowledge provided by LPCM, while also needing to integrate specialized tools for verifying the correctness and evaluating performance. At the top-layer, implementation of the various applications relies on both LPCM and the two agents. Despite these challenges, the three layers exhibit strong interdependence and can provide mutual enhancement. The various applications at the top-layer can provide valuable domain-specialized data for LPCM, and also facilitate the use of specialized tools for functionality verifica- tion and performance assessment towards Hardware Software Design Agent. Furthermore, achieving constructing a complete interaction process between LPCM and specialized tools, the Hardware Software Design Agent can offer an automatic data generation mechanism for LPCM. Through the collaborative synergy of the three levels, the challenges each level faces can be effectively resolved. Although QiMeng is originally designed in a bottom-up manner, it is easier to start with a top-down manner during actual implementation. Driven by the aim of achieving var- ious hardware and software designs, the implementation of applications in top-layer can offer extensive synthetic domain- specialized data for LPCM, and also provide design expe- rience of collaborating with specialized tools for designing Hardware Software Design Agent. Based on the above analysis, we propose a three-phase roadmap to implement a complete QiMeng system. The first phase is to adopt a top-down construction approach, devel- oping various applications based on the LPCM, which is initialized with a general-purpose LLM. During the imple- mentation of applications, key components of and functions of the Hardware Software Design Agent are constructed, which are then combined to establish complete processes of the two agents. At the same time, extensive domain-specialized data of software and hardware design is synthesized for training LPCM, enabling LPCM to acquire domain knowledge superior to general-purpose LLMs. The second phase is to adopt a bottom-up construction approach, reconstructing the Hard- ware Software Design Agent based on the trained LPCM and re-developing the various applications. Due to LPCM being enhanced with domain knowledge and specialized capabilities, the applications redeveloped in the second phase will achieve better automated design results than those in the first phase.",
    "source": "2506.05007v1_QiMeng_Fully_Automated_Hardware_and_Software_Desig.pdf",
    "length": 2590,
    "tokens": 494
  },
  {
    "text": "The workflow is depicted in the left part of Fig. 3. Unlike conventional training, which only takes a dataset as input, our approach incorporates a user-specified tolerance for OOB accuracy loss. During training, each node records its majority class and the corresponding purity (i.e., proportion). Once the ensemble is constructed, rather than concluding with OOB estimation, we 6 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS TABLE II COMPARISON OF NAIVE UNIFIED MAPPING AND NAIVE INDEPENDENT MAPPING. Attribute Naive Unified Mapping Naive Independent Mapping Mapping Unit entire ensemble single tree Width per Unit l unique conditions S m TCAMs l unique conditions S m TCAMs Height per Unit l paths S m TCAMs l paths S m TCAMs Total TCAM Required l unique conditions S m l paths S m P l unique conditions S m l paths S m Key Strength lower input pre-processing cost higher space efficiency Category energy-efficient mapping space-efficient mapping iteratively determine the minimum purity threshold that main- tains accuracy within the specified tolerance. Nodes exceeding this threshold are converted into leaf nodes and assigned their respective majority class, as illustrated in the right part of Fig. 3. The core principle of purity threshold pruning is to reduce model complexity while preserving accuracy at the ensemble level. Since individual trees are trained on different subsets with distinct splits, a minority class overlooked in one tree can still be classified correctly in others. For highly imbalanced datasets, applying class weighting helps prevent the model from favoring majority classes. Additionally, by explicitly considering OOB accuracy during pruning, the method ensures that performance degradation remains controlled. Unlike stan- dard pruning techniques that may create leaf nodes with evenly distributed classes, potentially leading to misclassifications, purity threshold pruning ensures that each leaf node main- tains sufficient class purity to guarantee the desired level of accuracy. Moreover, when early splits already satisfy the target accuracy, further splits are omitted, significantly reducing both paths and unique conditions compared to conventional pruning methods. Different from most post-pruning algorithms that operate on individual decision trees in isolation, purity threshold pruning is specifically designed to account for the collective behavior of ensemble models during inference.",
    "source": "2506.05994v1_RETENTION_Resource-Efficient_Tree-Based_Ensemble_M.pdf",
    "length": 2471,
    "tokens": 474
  },
  {
    "text": "Moreover, we consider our approach orthogonal to these methods since they can naturally apply L3 for further enhancement. PIM-only architecture for LLM inference. The PIM computing paradigm addresses the data movement bottleneck between mem- ory and processors by placing computation near or inside memory devices, which provides promising methods for memory-intensive kernels [8, 10, 13, 14, 16, 21, 23, 36, 37, 39, 42, 45, 47, 49, 57, 64, 68, 69, 71, 73, 79, 87, 89]. In addition to the GPU-PIM collaborative inference work listed in Table 1, there are also several works, e.g., CENT [28], LoL-PIM [53], and CXL-PNM [74], that have imple- mented LLM inference entirely on PIM, addressing the scalability in the granularity of the entire model. They fall short on their ker- nel decoupling methods. Though they provide a collective path or 11 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen matrix unit for FC kernels, their efficiency is still restricted by low data reuse and computing resources. 8 Discussions Economic advantages. While both HBM and PIM provide high bandwidth, PIM has significant economic advantages [62]. To pro- vide external high bandwidth for general purpose, HBM relies on expensive manufacturing processes involving TSVs [59] and sili- con interposers [35, 52], substantially increasing production costs. Additionally, DIMM-PIM offers greater deployment flexibility, as it can be incrementally adopted within existing server infrastruc- tures, allowing for gradual investment rather than complete system replacement. Utilizing spare GPU memory capacity. Since the entire KV cache is offloaded to the host memory, GPU memory is now utilized for storing model weights and intermediate data for prefilling requests, e.g., the activation, the partial KV cache of unfinished chunked- prefill requests, etc. Besides, in cases where there is spare GPU memory, decoding requests with their KV cache can be added to saturate the GPU.",
    "source": "2504.17584v1_L3_DIMM-PIM_Integrated_Architecture_and_Coordinati.pdf",
    "length": 2065,
    "tokens": 481
  },
  {
    "text": "24 D N formats bfloat16 E4M3 E5M2 E4M3 E5M2 bfloat16 bfloat16 bfloat16 E4M3 E5M2 144.14 4.57e 18 0.794 0.001 0.006 0.006 0.011 93.81 0.784 0.001 0.001 0.001 0.011 62.41 0.780 0.0 0.005 0.005 0.01 42.37 0.774 0.001 0.001 0.001 0.006 29.30 0.772 -0.002 0.003 0.003 0.003 14.74 0.767 -0.002 0.003 0.003 0.003 10.70 0.766 -0.001 0.004 -0.001 0.004 7.87 0.766 -0.001 0.004 -0.001 0.004 4.42 0.769 0.001 0.001 0.001 0.006 3.37 0.772 -0.002 0.003 0.003 0.003 2.60 0.775 0.0 0.0 0.0 0.005 2.02 0.779 0.001 0.001 0.001 0.001 136.47458 1e 19 0.748 0.002 0.002 0.002 0.002 92.646 0.741 -0.001 0.004 0.004 0.009 64.075 0.736 -0.001 0.004 0.004 0.009 45.084 0.731 -0.001 0.004 0.004 0.009 32.233 0.728 0.002 0.002 0.002 0.007 23.391 0.725 0.0 0.005 0.0 0.005 17.210 0.724 0.001 0.001 0.001 0.006 12.826 0.724 0.001 0.001 0.001 0.311 9.674 0.723 0.002 0.002 0.002 0.002 7.38 0.723 0.002 0.002 0.002 0.077 4.43 0.727 -0.002 0.003 0.003 0.003 2.75 0.732 -0.002 0.023 0.003 0.003 Table 5: MXFP8 of the validation loss table, with separate rows for Weight and Activation precisions. For the last 2 columns, we quantize only the forward pass. The second column indicates the FLOP count used.",
    "source": "2506.20752v1_Characterization_and_Mitigation_of_Training_Instab.pdf",
    "length": 1172,
    "tokens": 553
  },
  {
    "text": "arXiv:2506.14606v1 [cs.CL] 17 Jun 2025 Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees Ahmed Heakl , Sarim Hashmi , Chaimaa Abi Celine Lee , Abdulrahman Mahmoud MBZUAI Cornell University {ahmed.heakl, sarim.hashmi, Abstract The hardware ecosystem is rapidly evolving, with increasing interest in translating low-level programs across different instruction set archi- tectures (ISAs) in a quick, flexible, and correct way to enhance the portability and longevity of existing code. A particularly challenging class of this transpilation 1 problem is translating between complex- (CISC) and reduced- (RISC) hardware architectures, due to fundamental differences in instruction complexity, memory models, and execution paradigms. In this work, we introduce GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the translation power of pre-trained large language models (LLMs) with the rigor of established software testing constructs. Our method generates candidate translations using an LLM from one ISA to another, and embeds such translations within a software-testing framework to build quantifiable confidence in the translation. We evaluate our GG approach over two diverse datasets, enforce high code coverage ( 98 ) across unit tests, and achieve functional semantic correctness of 99 on HumanEval programs and 49 on BringupBench programs, respectively. Further, we compare our approach to the state-of-the-art Rosetta 2 framework on Apple Silicon, showcasing 1.73 faster runtime performance, 1.47 better energy efficiency, and 2.41 better memory usage for our transpiled code, demonstrating the effectiveness of GG for real-world CISC-to-RISC translation tasks. We will open-source our codes, data, models, and benchmarks to establish a common foundation for ISA-level code translation research. 1 Introduction The modern hardware landscape is undergoing a fundamental transformation. As Moore s Law slows and Dennard scaling ends (Dennard et al., 1We use transpilation to describe the task of translating code between assembly languages.",
    "source": "2506.14606v1_Guaranteed_Guess_A_Language_Modeling_Approach_for_.pdf",
    "length": 2119,
    "tokens": 457
  },
  {
    "text": "Best results are highlighted in bold. Benchmark GPT-3.5 GPT-4 o1-preview DeepRTL-220m DeepRTL-16b syntax function syntax function syntax function syntax function syntax function Logic Johnson Counter 40 0 100 0 100 0 100 0 100 0 alu 0 0 0 0 0 0 0 0 0 0 edge detect 60 20 100 100 100 0 100 0 100 0 freq div 100 0 100 0 100 0 100 0 100 0 mux 100 100 100 40 100 100 100 100 100 100 parallel2serial 80 0 100 0 100 0 100 0 100 0 pulse detect 60 40 100 20 100 40 100 100 100 100 right shifter 60 60 100 100 100 100 100 100 100 100 serial2parallel 60 0 100 0 100 20 100 0 100 0 width 8to16 100 0 20 0 100 0 100 0 100 0 Arithmetic accu 100 0 40 0 100 0 100 0 100 0 adder 16bit 40 0 20 20 40 40 100 0 60 0 adder 16bit csa 80 80 0 0 100 100 100 100 100 100 adder 32bit 100 0 40 0 100 0 80 0 100 0 adder 64bit 100 0 100 0 100 0 100 0 100 0 adder 8bit 100 100 40 40 100 100 80 20 100 80 div 16bit 0 0 0 0 0 0 0 0 0 0 multi 16bit 80 0 100 20 100 100 100 0 100 0 multi booth 100 0 60 0 80 40 60 0 100 0 multi pipe 4bit 60 20 100 100 100 100 100 100 100 100 multi pipe 8bit 0 0 0 0 0 0 0 0 0 0 Advanced 1x2nocpe 40 40 80 80 100 100 100 80 100 100 1x4systolic 100 100 100 100 100 100 100 100 100 100 2x2systolic 0 0 0 0 0 0 0 0 0 0 4x4spatialacc 0 0 0 0 0 0 0 0 0 0 fsm 60 0 100 0 100 20 100 100 100 100 macpe 0 0 0 0 0 0 0 0 0 0 5state fsm 100 0 100 60 100 100 100 100 100 100 3state fsm 20 0 80 20 100 100 100 100 100 100 4state fsm 60 40 100 80 100 20 100 100 100 100 2state fsm 80 20 100 80 100 0 100 20 0 0 Success Rate 60.65 20.00 63.87 27.74 78.06 38.06 78.06 36.13 76.13 38.06 32.26 19.35 51.61 29.03 74.19 35.48 70.97 32.26 74.19 35.48 80.65 35.48 77.42 45.16 80.65 51.61 80.65 41.94 77.42 38.71 5.3 VERILOG GENERATION Given the inferior performance of models like CodeT5 and DeepRTL-direct in the Verilog under- standing task, our comparison focuses on the GPT series models. As shown in Table 3, OpenAI s o1-preview, the latest model designed to tackle complex tasks including programming, achieves the highest performance across all metrics.",
    "source": "2502.15832v1_DeepRTL_Bridging_Verilog_Understanding_and_Generat.pdf",
    "length": 2037,
    "tokens": 642
  },
  {
    "text": "a) Class-Level Feedback: Training requires both the target class and a randomly chosen class (referred to as the negated class) for each training datapoint. The target class is padded into the input data stream sent to the accelerator (presented 4For example if a paragraph has a bold label Class-level feedback - view this part of Fig 2 against the same algorithm name in this section. This should make it easier to follow the nested condition parts in the algorithms Algorithm 3: Class-level Feedback Generation Input: T (hyper parameter), yc (input class, 1 means target class, 0 means negated class), c rand (random number for negated class choice), Lw rand (random number length for clause-level feedback compute), Target Class (target class index), Update csum (Class sum for updated class), Class num (Class number) 1 NC_Gen(c rand, Target Class, Class num) RNC c rand (Class num 2); 2 if RNC Target Class then 3 Negated Class RNC; 4 else 5 Negated Class RNC 1; 6 return Negated Class; 7 Update_Probability_Compute(yc, c rand, Lw rand, Target Class, Class num, Update csum) 8 if yc 1 then 9 Update csum cum[Target Class]; 10 else 11 Negated index NC_Gen(c rand, Target Class, Class num); 12 Update csum cum[Negated index]; 13 CSum clip(Update csum, [ T, T]); 14 if yc 1 then 15 PCl Update (T CSum) 2LT 1; 16 else 17 PCl Update (T CSum) 2LT 1; 18 return PCl Update; later), while the randomly chosen negated class is determined within the hardware accelerator (see Function NC Gen in Algorithm 3 (NC_Gen)). The clause-level update probability is computed in Function UpdateProbabilityCompute with some changes to be more easily translated to hardware. b) Clause-Level Feedback: After selecting a class to be updated (yc), the clause-level feedback should be determined based on the comparison result between the product of a Lw rand-bit random number w rand with the hyperparameter T and the clause update probability (see Algorithm 3). The clause-level feedback will be stored in Clause Feedback Buffer as either registers or BRAM.",
    "source": "2504.19797v1_Dynamic_Tsetlin_Machine_Accelerators_for_On-Chip_T.pdf",
    "length": 2038,
    "tokens": 491
  },
  {
    "text": "However, to increase the security of a system this virtual address space is randomized on each execution. This is done with two methods: Address Space Layout Random- ization (ASLR) and Position Independent Execution (PIE). These are controlled at two different levels of the system and interact with the program at different points. PIE is implemented during compilation and must be changed using a flag when building the program. PIE will adjust all program addresses to be in a relative position to one another instead of an absolute position in the virtual address space. When disabled, the global variables, constants, and other program data will be constant across executions. However, these adjustments do not impact the stack or the heap. To keep the stack and heap relatively consistent through executions, ALSR must be disabled prior to the execution of the program, and is changed for a process or globally for the operating system. With both of these features disabled, the next challenge arises. Even with a constant stack and heap location, the stack and heap can still vary between ex- ecutions. Before the first variable is loaded into the stack, there can be a number of arguments or environmental variables passed to the program. This adjusts the address of every variable on 14 the stack, but their offsets remain the same. The heap is similarly affected, in that the allocator determines where an address on the heap will go. This behavior could be consistent, but is not guaranteed. There are a few ways to overcome this obstacle. The first is to utilize a global variable or other constant address to determine where the stack starts. If an address is assigned to a global variable pointer, the instruction corresponding to that operation can be found, and other variables may be found using that address and the relative offsets or source code. The next option is to use the instruction address directly to determine where a variable is located. The binary must be disassembled to find where the instruction address is in the source code, then the address can be found. Since the area of interest lies outside of any global variable declarations, the second option must be used. We used ChampSim to generate the data used for the memory analysis since this would provide an accurate representation of how the memory was flowing through the caches. ChampSim s caches are connected to each other and the CPU through data channels, which channels transmit requests for data packets.",
    "source": "2506.01827v1_Memory_Access_Characterization_of_Large_Language_M.pdf",
    "length": 2501,
    "tokens": 472
  },
  {
    "text": "DeepGate2 [24] improved upon this by incorporating truth-table supervision and a more expressive single-round GNN, which enhanced both modeling precision and training efficiency. DeepGate3 [25] further extended the framework by combining GNNs and transformers with circuit-specific pretraining strategies, greatly improving scalability and generalization, albeit with increased resource requirements. HOGA [9] proposed hop-aware aggregation and gated attention mechanisms, offering better scalability for large circuits, though it still struggled with modeling global contextual dependencies. J. ACM, Vol. 1, No. 2, Article 3. Publication date: May 2025. 3:4 Qiyun Zhao Fig. 1. The overall framework of FuncGNN 2.2 Message-Passing Methods Message-passing models emphasize local propagation and structural coherence through lightweight architectures. PolarGate [18] introduced polarity embeddings and differentiable logic operators to enrich the message flow. While effective in certain tasks, it has difficulty generalizing across diverse AIG topologies. GAMORA [29] proposed a multi-task framework that jointly models circuit structure and logic function, improving gate-level recognition. However, its capacity to represent global logic information remains limited. Despite their advances, current methods exhibit several limitations. Attention-based models often involve substantial computational overhead, limiting scalability to very large circuits. Message- passing approaches, though efficient, tend to lose global context and suffer from information degradation across deeper layers. Additionally, both categories face challenges in modeling the structural heterogeneity typical of AIG circuits. These issues reduce estimation accuracy and affect generalization across tasks. To address these challenges, FuncGNN introduces a novel architecture that integrates Hybrid Feature Aggregation, Global Context Normalization, and Multi-Layer Integration. These compo- nents are designed to maintain computational efficiency while improving functional accuracy and structural adaptability. Together, they enable robust and scalable AIG representation learning, offering support for downstream EDA tasks such as logic synthesis and equivalence checking. J. ACM, Vol. 1, No. 2, Article 3. Publication date: May 2025. FuncGNN: Learning Functional Semantics of Logic Circuits with Graph Neural Networks 3:5 3 FuncGNN Model 3.1 Overview The overall framework of FuncGNN is illustrated in Figure 1.",
    "source": "2506.06787v1_FuncGNN_Learning_Functional_Semantics_of_Logic_Cir.pdf",
    "length": 2492,
    "tokens": 478
  },
  {
    "text": "arXiv:2507.00937v1 [cs.RO] 1 Jul 2025 RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles David Hunt , Shaocheng Luo , Spencer Hallyburton, Shafii Nillongo, Yi Li, Tingjun Chen, and Miroslav Pajic Abstract Low-cost indoor mobile robots have gained pop- ularity with the increasing adoption of automation in homes and commercial spaces. However, existing lidar and camera- based solutions have limitations such as poor performance in visually obscured environments, high computational overhead for data processing, and high costs for lidars. In contrast, mmWave radar sensors offer a cost-effective and lightweight alternative, providing accurate ranging regardless of visibility. However, existing radar-based localization suffers from sparse point cloud generation, noise, and false detections. Thus, in this work, we introduce RaGNNarok, a real-time, lightweight, and generalizable graph neural network (GNN)-based framework to enhance radar point clouds, even in complex and dynamic environments. With an inference time of just 7.3 ms on the low- cost Raspberry Pi 5, RaGNNarok runs efficiently even on such resource-constrained devices, requiring no additional computa- tional resources. We evaluate its performance across key tasks, including localization, SLAM, and autonomous navigation, in three different environments. Our results demonstrate strong reliability and generalizability, making RaGNNarok a robust solution for low-cost indoor mobile robots. I. INTRODUCTION Indoor mobile robots, such as unmanned ground vehicles (UGVs), are increasingly used in homes and commercial spaces, requiring accurate sensing for GNSS-free mapping and navigation in potentially complex environments. Tradi- tionally, these robots rely on lidar and cameras, but both technologies have limitations that hinder their wide adoption on low-cost platforms. Lidar offers high precision but is expensive (e.g., 800 for the Livox MID-360), power-intensive, and struggles in visually occluded environments (e.g., smoke, dust). Cameras, while affordable, face challenges in low-light or visually uniform environments and lack depth perception unless combined with additional sensors.",
    "source": "2507.00937v1_RaGNNarok_A_Light-Weight_Graph_Neural_Network_for_.pdf",
    "length": 2225,
    "tokens": 472
  },
  {
    "text": "Table 2 highlights how transformers on an edge GPU are consistently slower and less efficient during generation, and scale unfavorably to longer sequence lengths. During prefill, transformers show low throughput and efficiency for shorter sequences due to under-utilization of the GPU, then reach optimal performance between 4000-8000 tokens, after which both throughput and efficiency decline. Table 2 also compares the MatMul-free model on Loihi against the same model running on an H100 GPU. Loihi 2 delivers 3 higher throughput during generation with at least 14 less energy per token. During prefill, the H100 delivers higher throughput than Loihi 2, but outperforms in energy efficiency only for large sequence lengths. Latency characteristics are particularly important for interactive applications at the edge. Our exper- iments reveal that for batch size 1, which is typical in edge deployment scenarios, the MatMul-free model on Loihi 2 demonstrates significant latency advantages, with a 6.6 lower time-to-first to- ken on a 500-token input sequence (99ms for the MatMul-free model on the Loihi 2 vs. 659ms for the Llama-style model on the Jetson). This advantage increases with sequence length due to the linear scaling properties of our approach versus the quadratic complexity of transformer models. For real-time applications like voice assistants or mobile chatbots, this latency reduction directly translates to more responsive user experiences while maintaining significantly lower energy con- sumption. While we benchmarked against the Jetson Orin Nano as a representative edge GPU platform, we expect our approach to show similar advantages compared to other edge computing solutions. Platforms such as mobile SoCs, FPGAs, and various edge TPUs all face similar challenges with transformer-based models: quadratic scaling with sequence length and significant memory move- ment costs. Since our neuromorphic approach addresses both fundamental limitations through linear scaling and closer compute-memory integration, we anticipate the relative throughput and energy ef- ficiency advantages to persist across other edge computing architectures. The benefits would be most pronounced for resource-constrained IoT or mobile platforms where energy efficiency is paramount, with potentially more modest gains against specialized NPUs that have already optimized for recur- rent operations. 5 CONCLUSION We demonstrated that neuromorphic principles and co-design can be leveraged to build efficient LLMs.",
    "source": "2503.18002v2_Neuromorphic_Principles_for_Efficient_Large_Langua.pdf",
    "length": 2518,
    "tokens": 491
  },
  {
    "text": "REFERENCES [1] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio, Binarized Neural Networks, in Advances in Neural Information Pro- cessing Systems (NeurIPS), 2016, pp. 4107 4115. [2] J. Bethge, C. Bartz, H. Yang, Y. Chen, and C. Meinel, MeliusNet: An Improved Network Architecture for Binary Neural Networks, in Proceedings of the IEEE CVF Winter Conference on Applications of Computer Vision (WACV), January 2021, pp. 1439 1448. [3] J. Choi, Review of low power image sensors for always-on imaging, in 2016 International SoC Design Conference (ISOCC), 2016, pp. 11 12. [4] M. Gazivoda and V. Bilas, Always-On Sparse Event Wake-Up Detec- tors: A Review, IEEE Sensors Journal, vol. 22, no. 9, pp. 8313 8326, 2022. [5] D. Garrett, Y. S. Park, S. Kim, J. Sharma, W. Huang, M. Shaghaghi, V. Parthasarathy, S. Gibellini, S. Bailey, M. Moturi, P. Vorenkamp, K. Busch, J. Holleman, B. Javid, A. Yousefi, M. Judy, and A. Gupta, A 1mW Always-on Computer Vision Deep Learning Neural Decision Processor, in 2023 IEEE International Solid-State Circuits Conference (ISSCC), 2023, pp. 8 10.",
    "source": "2505.13462v1_End-to-end_fully-binarized_network_design_from_Gen.pdf",
    "length": 1092,
    "tokens": 358
  },
  {
    "text": "129 134. [51] G. Huang, J. Hu, Y. He, J. Liu, M. Ma, Z. Shen, J. Wu, Y. Xu, H. Zhang, K. Zhong, X. Ning, Y. Ma, H. Yang, B. Yu, H. Yang, and Y. Wang, Machine learning for electronic design automation: A survey, ACM Trans. Des. Autom. Electron. Syst., vol. 26, 6 2021. [52] Q. Huang, H. Shojaei, F. Zyda, A. Nazi, S. Vasudevan, S. Chatterjee, and R. Ho, Test parameter tuning with blackbox optimization: A simple yet effective way to improve coverage, in Proceedings of the design and verification conference and exhibition US (DVCon), 2022. [53] S. Ikram and J. Ellis, Dynamic regression suite generation using coverage-based clustering, in Proceedings of the design and verification conference and exhibition US (DVCon), 2017. [54] C. Ioannides and K. I. Eder, Coverage-directed test generation automated by machine learning a review, ACM Trans. Des. Autom. Electron. Syst., vol. 17, 1 2012. [55] K. A. Ismail and M. A. A. E. Ghany, High performance machine learning models for functional verification of hardware designs, in 2021 3rd Novel Intelligent and Leading Emerging Sciences Conference (NILES), 2021, pp. 15 18. [56] K. A. Ismail and M. A. A. E. Ghany, Survey on machine learning algorithms enhancing the functional verification process, Electronics, vol. 10, 2021. [57] H. Jang, S. Yim, S. Choi, S. B. Choi, and A. Cheng, Machine learning based verification planning methodology using design and verification data, in Design and Verification Conf. (DVCON), 2022. [58] A. Jayasena and P. Mishra, Directed test generation for hardware validation: A survey, ACM Comput. Surv., vol. 56, 1 2024.",
    "source": "2503.11687v1_Review_of_Machine_Learning_for_Micro-Electronic_De.pdf",
    "length": 1600,
    "tokens": 458
  },
  {
    "text": "3 illustrates our three novel generative reasoning tasks on netlists, along with the benchmarks developed to support them. These tasks aim to reversely generate high-level circuit functionality, including natural language specifications and exact RTL code, from low-level bit-blasted netlists. Please note that models are only provided with netlists, their corresponding specifications and RTL code serve as ground-truth and and unknown to the model. Our proposed benchmarks evaluate the generative model s ability to truly understand the functionality of netlists, setting a new direction for generative EDA tasks. We detail the three tasks below. A. Task 1 2: specification reasoning from netlist. Task and benchmark description. Tasks 1 and 2 aim to reversely generate high-level natural language specifications from gate-level netlists, as shown in Fig. 3. Task 1 Function description generation focuses on generating circuit functional descriptions from low-level netlists, emphasizing the overall behavior of the circuit. Task 2 Im- plementation detail generation targets the reconstruction of step-by- step signal propagation and logic behavior from the netlist, reflecting the underlying design implementation. As these are novel tasks with no prior benchmarks, we construct new datasets and benchmarks to support model training and evaluation. Specifically, we collect 400 circuit netlists with various design scales and complexities, annotated with natural language specifications as ground-truth. For each design, our proposed benchmark provides the following information in three separate files: Netlist text. Gate-level netlist in Verilog text format synthesized from RTL code. Please note that in these reverse tasks, the RTL code is unknown to the model. Question prompt. For Task 1, the prompt inquires the model to describe the interface, purpose, functionality, and constraints of the netlist. For Task 2, the prompt asks the model to explain the combinational logic, sequential behavior, and control flow. Ground-truth specification text. Since real-world RTL speci- fications are rarely available, we generate reference answers for these two tasks using GPT-4o prompted with RTL code and manually verify their quality through expert review. Evaluation metrics. As there are no standard metrics for evalu- ating the functional similarity of specification texts, we follow prior work DeepRTL [22], which addresses the RTL-to-specification task, and adopt a combination of natural language metrics.",
    "source": "2504.09485v1_GenEDA_Unleashing_Generative_Reasoning_on_Netlist_.pdf",
    "length": 2515,
    "tokens": 489
  },
  {
    "text": "59, pp. 1243 1254, 2013. [10] P. Bernardi, K. Christou, M. Grosso, M. K. Michael, E. S anchez, and M. S. Reorda, Exploiting moea to automatically geneate test programs for path-delay faults in microprocessors, in Applications of Evolutionary Computing, M. Giacobini, A. Brabazon, S. Cagnoni, G. A. D. Caro, R. Drechsler, A. Ek art, A. I. Esparcia-Alc azar, M. Farooq, A. Fink, J. McCormack, M. O Neill, J. Romero, F. Rothlauf, G. Squillero, A. S ima Uyar, and S. Yang, Eds. Springer Berlin Heidelberg, 2008, pp. 224 234. [11] H. Bhargav, V. Vs, B. Kumar, and V. Singh, Enhancing testbench quality via genetic algorithm, in 2021 IEEE International Midwest Symposium on Circuits and Systems (MWSCAS), 2021, pp. 652 656. [12] M. Bose, J. Shin, E. M. Rudnick, T. Dukes, and M. Abadir, A genetic approach to automatic bias generation for biased random instruction generation, in Proceedings of the 2001 Congress on Evolutionary Computation (IEEE Cat. No.01TH8546), vol. 1, 2001, pp. 442 448 vol. 1. [13] M. Braun, W. Rosenstiel, and K.-D. Schubert, Comparison of bayesian networks and data mining for coverage directed verification category simulation-based verification, in Eighth IEEE International High-Level Design Validation and Test Workshop, 2003, pp. 91 95. [14] M. Braun, S. Fine, and A. Ziv, Enhancing the efficiency of bayesian network based coverage directed test generation, in Proceedings. Ninth IEEE International High-Level Design Validation and Test Workshop (IEEE Cat. No.04EX940), 2004, pp. 75 80.",
    "source": "2503.11687v1_Review_of_Machine_Learning_for_Micro-Electronic_De.pdf",
    "length": 1511,
    "tokens": 466
  },
  {
    "text": "And the label for the i-th head is first derived by left-shifting the base model s label by i, resulting in Li L0[i :]. [PAD] tokens are then appended to ensure that the head labels align in length with the base model s label. The labels for all heads are then concatenated with the base model s label, forming the combined label Labels with size (num heads 1) sequence length. As shown in Fig. 4, for each sequence position, we identify the position of the last [FRAG] token along the decoding heads. Labels beyond this position are replaced with [IGNORE] tokens, excluding them from loss computation. This ensures that, for any given sequence position s, the base model s label and the corresponding head labels, Labels [:, s], represent syntac- tically meaningful fragments by excluding incomplete information. The syntax-enriched labels align decoding stops with syntactically significant tokens, enabling the model to better capture the syntactic structure of Verilog code. Additionally, the progressive increase of the proportion of [IGNORE] tokens in the labels of later heads reduces their prediction difficulty, improving the model s training efficiency and allowing us to train more robust heads than the original MEDUSA method, which further accelerates inference. To optimize performance, we design a parallel algorithm that fully parallelizes the procedure, significantly speeding up the process and minimizing computational overhead during training. The pseudo-code for this algorithm is provided in Fig. 4. IV. EXPERIMENTS A. Experimental Setup We select CodeLlama-7b-Instruct-hf [3] (CodeLlama) and CodeT5p-220m-bimodal [2] (CodeT5p) as our base models.",
    "source": "2503.14153v1_Speculative_Decoding_for_Verilog_Speed_and_Quality.pdf",
    "length": 1669,
    "tokens": 368
  },
  {
    "text": "with language models, various datasets, approaches, and pre-trained models have been introduced over the past decade. Due to the lack of model capability, the early-stage methods [11, 16] of code generation mainly focus on a few programming lan- guages such as Python or Java. Subsequently, with the increasing computational power, larger datasets are introduced to train models for multiple general-purpose programming languages. CodeXGLUE [19] presents a comprehensive code dataset consisting of different code tasks such as clone detection and code repair. HumanEval [5] dataset together with the code model CodeX marks as a milestone by using pre-trained LLMs for code generation. The promising capability shown by CodeX sparks significant academic and industrial in- terests in developing LLM-assisted code generation. Larger code datasets, such as StarCoder [14] and CodeParrot [27], and LLMs, including Code-LLaMA [22] and CodeFuse [7], are open-sourced in this community. However, most of these recent efforts focus on software programming languages. 2.2 Automated Hardware Design Generation Building on the success of LLM-assisted software programming, recent studies have explored using language models for automated hardware generation. Since the data is the key to training LLMs for hardware code generation, multiple HDL datasets have been introduced recently. Thakur et al. present Verigen [25] dataset that contains 17 hardware designs with 0.3K lines of HDL code. To in- crease the diversity of hardware designs for training and evaluation, Lu et al. open-source a larger benchmark consisting of 30 designs with more than 2.5K lines. Sourced from HDLBits , Verilogeval [18] introduces larger datasets with 156 problems. These open-sourced datasets provide public benchmarks for text-to-HDL generation. The evaluation metrics of LLM-assisted hardware code genera- tion focus on three aspects: i) syntax, ii) functionality, and iii) qual- ity. In previous literature [5, 18], syntax correctness and functional- ity are measured using which represents whether any of ùëògenerated code samples can pass the syntax check of synthesis tools or functional unit tests. The quality usually is de- fined as power, performance, and area of the generated hardware, collectively reflect the capability of the code generation methods.",
    "source": "2502.13921v2_Exploring_Code_Language_Models_for_Automated_HLS-b.pdf",
    "length": 2334,
    "tokens": 492
  },
  {
    "text": "model s performance. These observations can be explained by the presence of additional noise in the analog model training process. Introducing additional noise during training is equivalent to performing some form of noise injection during the forward pass of the training process. Noise injection during the model training process can be beneficial as it is used for some form of model regularization, which can help improve the model inference performance [33]. This form of model regularization can be highly beneficial in a TL scenario where the amount of training data is limited. However, the inherent presence of noise in analog model training can be problematic if the training process is sensitive to these noise sources. For the 2-class and 5-class fine-tuning tasks, digital TL outperforms the analog TL by about 2 . The performance difference increases with respect to the task complexity. This difference is expected as the increase in task complexity increases the computation s noisy nature, leading to lower ana- log model inference performance. However, the performance difference is insignificant compared to the earlier algorithm [20], validating the effectiveness of the c-TTv2 algorithm. B. Analog TL and Weight Transfer Noise In Fig. 3, we determine the effect of the weight transfer noise on analog TL inference performance. For a 2-class task, the test error is mostly constant until the weight transfer noise exceeds 15 . For larger noise magnitudes, the degradation in model performance becomes more substantial. Similar behavior is also noticed for the 5-class task, except that the elbow point was 10 . These observations imply that the weight transfer noise can negatively affect model inference performance. At weight transfer noise below a certain point (or elbow), called the critical weight transfer noise, the effect of weight transfer noise on the analog TL model inference performance is negligible. The critical weight transfer noise point is affected by the complexity of the task and the model itself. C. Analog TL and Device parameters One major requirement of the proposed training algorithm to solve the problem of non-linear switching behavior in ReRAM devices is the need for a bi-directional switching device of unrealistically high symmetry and precision. Improvement in the algorithm further relaxes the requirement for high symmetry, but the need for it still exists. Hence, there is a need to investigate the robustness of the training algorithm in relation to the presence absence of symmetry for analog TL.",
    "source": "2505.11067v1_Assessing_the_Performance_of_Analog_Training_for_T.pdf",
    "length": 2557,
    "tokens": 468
  },
  {
    "text": "The multi- template strategy is particularly beneficial for classes with high intra-class variability. In our implementation, these multiple templates are generated using a clustering approach on the feature representations of each class. For instance, when using three templates per class, we apply k-means clustering with k 3 on the class-specific feature maps, and the centroids of these clusters serve as the templates. K-means was chosen for its computational efficiency and ability to partition data into spherical clusters, making it a well-suited for this task. During the matching process, an input query is compared against all templates for each class, and the best match among these templates determines the class similarity score. To further optimise the clustering process, we utilised silhouette scores to evaluate the quality of clustering for different configurations. Silhouette scores provided a quantita- tive measure of how well-separated and cohesive the clusters are, enabling the selection of the number of templates that best captured the intra-class variability. This ensures that the generated templates represent meaningful and distinct sub- IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL., NO., FEBRUARY 2025 5 clusters within each class. While the multi-template approach increases the memory footprint and the number of comparisons required, it offers a trade-off between model complexity and representational power, potentially leading to more robust and accurate classifications, especially for complex datasets with significant intra-class variations. 2) ACAM Pattern Matching Computing Concept: We pro- pose using pattern matching to serve as the inference mech- anism, replacing the traditional final classifcation function of conventional digital DNNs with a hardware-optimised similar- ity search operation. The main principle of pattern matching is to compare an input pattern against a set of stored templates. In this work this is performed by a RRAM-CMOS ACAM system which is discussed in Section III. To evaluate the performance of the hybrid software-hardware classifier, the modelling of the ACAM is required towards introducing a good approximation of its behaviour to the software flow. Additionally, due to the intrinsic properties of the RRAM devices, the employ- ment of program-once-read-many can provide better stability and endurance of the emerging RRAM devices.",
    "source": "2502.10089v1_A_Hybrid_Edge_Classifier_Combining_TinyML-Optimise.pdf",
    "length": 2426,
    "tokens": 470
  },
  {
    "text": "How- ever, traditional CPU cores have limited memory-level par- allelism, and can only track a few in-flight lookups (b), stalling the CPU pipeline. This results in low memory re- quest throughput (c), and low HBM per core utilization (d). 0.95x 1.00x 1.05x 1.10x 1.15x Performance 1R.1L.1M 1R.1L.2M 1R.2L.1M 1R.2L.2M 2R.1L.1M 2R.1L.2M 2R.2L.1M 2R.2L.2M 1R.1L.1M 1R.1L.2M 1R.2L.1M 1R.2L.2M 2R.1L.1M 2R.1L.2M 2R.2L.1M 2R.2L.2M 1R.1L.1M 1R.1L.2M 1R.2L.1M 1R.2L.2M 2R.1L.1M 2R.1L.2M 2R.2L.1M 2R.2L.2M 1R.1L.1M 1R.1L.2M 1R.2L.1M 1R.2L.2M 2R.1L.1M 2R.1L.2M 2R.2L.1M 2R.2L.2M 0.80x 0.85x 0.90x 0.95x 1.00x 1.05x Perf watt arxiv mag products proteins Figure 4: Implications of scaling up the memory-level par- allelism of a traditional CPU core (details in Figure 5b) for GNNs embedding operations (Section 2.2.3). Doubling reorder buffer, load-store queue, and L1D miss-status handling regis- ters (2R.2L.2M) provides limited performance improvements and worse perf watt than off-the-shelf cores (1R.1L.1M). ratio than DLRMs, with MP having the highest.",
    "source": "2504.09870v1_Ember_A_Compiler_for_Efficient_Embedding_Operation.pdf",
    "length": 1047,
    "tokens": 486
  },
  {
    "text": "24.1 JETSTREAM Cooling JET Surface Thermal Regulation via Evaporative Array Manifold (JETSTREAM) uses two-phase immersion cooling (2-PIC) with individual tuned submerged jets of liquid coolant directed to each logic chip stack on the ZettaLith WSSCB. 24.2 JETSTREAM manifold To achieve the required mass flow rate evenly to each SLD or Table 17: JETSTREAM cooling system Aspect Value Units 2-PIC coolant (Opteon 2P50) pressure 100 kPa 2-PIC coolant density (œÅ) 1,456 kg m3 2-PIC coolant specific heat capacity (cp) 1,090 J kg K 2-PIC coolant thermal conductivity (Œ∫) 0.07 W (m K) 2-PIC coolant viscosity (Œº) 0.00062 Pa s 2-PIC coolant surface tension (Œ≥) 0.011 N m Heat to be removed (Q) 84,305 Watts Incoming 2-PIC coolant temperature 30 C Outgoing 2-PIC coolant temperature 49 C Temperature difference (ŒîT) 19 C Mass flow rate (·πÅ Q (cp ŒîT)) 4.07 kg s Volume flow rate (V Q (œÅ cp ŒîT)) 0.0028 m3 s Volume flow rate in litres minute 168 litres min Nozzle width 11 mm Nozzle height 0.5 mm Nozzle area 5.5 mm2 Total area of all nozzles (A) 946 mm2 Nozzle 2-PIC coolant velocity 2.96 m s Discharge coefficient (Cd) 0.9 Pressure difference (ŒîP ·πÅ2 (2œÅ Cd2 A2)) 7.85 kPa 2-PIC coolant cycle time 10 seconds 2-PIC coolant required to circulate 41 kg 2-PIC coolant in chamber 93 kg Pump redundancy 3 pumps Pump motor power (each) 2 kW 39 CPU, ZettaLith employs a separate 2-PIC coolant jet interfacing with the silicon heatsink fins etched into the back side of each TRIMERA stack. This would enable effective heat removal at the required power densities while maintaining acceptable junction temperatures across the entire WSSCB and its attached chip stacks.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 1650,
    "tokens": 482
  },
  {
    "text": "DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xi- aosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia 7 Published at ICLR 2025 Workshop (SCOPE) Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, January 2025.",
    "source": "2503.18002v2_Neuromorphic_Principles_for_Efficient_Large_Langua.pdf",
    "length": 2552,
    "tokens": 965
  },
  {
    "text": "ISSN 2150-8097. doi: 10.14778 3229863.3236256. URL Brandon Haynes, Maureen Daum, Dong He, Amrita Mazumdar, Magdalena Balazinska, Alvin Cheung, and Luis Ceze. Vss: A storage system for video analytics. In Proceedings of the 2021 International Conference on Management of Data, pp. 685 696, 2021. Qunying Huang and Chen Xu. A data-driven framework for archiving and exploring social media data. Annals of GIS, 20(4):265 277, 2014. Zhiyu Huang, Haochen Liu, and Chen Lv. Gameformer: Game-theoretic modeling and learning of transformer-based interactive prediction and planning for autonomous driving, 2023. URL ITU-T. Advanced video coding for generic audiovisual services. International Telecommunication Union, June 2019a. URL ITU-T Recommendation H.264. ITU-T. High efficiency video coding. International Telecommunication Union, June 2019b. URL ITU-T Recommendation H.265. Myoungsoo Jung. Hello bytes, bye blocks: Pcie storage meets compute express link for memory expansion (cxl-ssd). In Proceedings of the 14th ACM Workshop on Hot Topics in Storage and File Systems, pp. 45 51, 2022. Hwajung Kim, Heon Y Yeom, and Hanul Sung. Understanding the performance characteristics of computational storage drives: A case study with smartssd. Electronics, 10(21):2617, 2021. Yoonsung Kim, Changhun Oh, Jinwoo Hwang, Wonung Kim, Seongryong Oh, Yubin Lee, Hardik Sharma, Amir Yazdanbakhsh, and Jongse Park. Dacapo: Accelerating continuous learning in autonomous systems for video analytics. arXiv preprint arXiv:2403.14353, 2024. 20 Terhi Korkiakangas. Challenges in archiving and sharing video data: Considering moral, pragmatic and substantial arguments. Journal of Research Practice, 10(1), 2014. Dur E Shahwar Kundi, Song Bian, Ayesha Khalid, Chenghua Wang, M√°ire O Neill, and Weiqiang Liu.",
    "source": "SaLT.pdf",
    "length": 1785,
    "tokens": 485
  },
  {
    "text": "IEEE Transactions on Geoscience and Remote Sensing 60 (2022), 1 14. https: doi.org 10.1109 TGRS.2021.3125567 [38] Carlos Gomes, Isabelle Wittmann, Damien Robert, Johannes Jakubik, Tim Reichelt, Michele Martone, Stefano Maurogiovanni, Rikard Vinge, Jonas Hurst, Erik Scheurer, Rocco Sedona, Thomas Brunschwiler, Stefan Kesselheim, Matej Batic, Philip Stier, Jan Dirk Wegner, Gabriele Cavallaro, Edzer Pebesma, Michael Marszalek, Miguel A. Belenguer-Plomer, Kennedy Adriko, Paolo Fraccaro, Romeo Kienzler, Rania Briq, Sabrina Benassou, Michele Lazzarini, and Conrad M. Albrecht. 2025. Lossy Neural Compression for Geospatial Analytics: A Review. arXiv:2503.01505 [eess] [39] Google. 2019. Coral TPU Dev Board. system-components. [40] P. Gordo, T. Frederico, R. Melicio, S. Duzellier, and A. Amorim. 2020. System for Space Materials Evaluation in LEO Environment. Advances in Space Research 66, 2 (July 2020), 307 320. [41] Naresh Grover and M. K.Soni. 2012. Reduction of Power Consumption in FPGAs - An Overview. International Journal of Information Engineering and Electronic Business 4, 5 (Oct. 2012), 50 69. [42] Dubacharla Gyaneshwar and Rama Rao Nidamanuri. 2022. A Real-Time SC SUP 2 SUP S-based Open-Set Recognition in Remote Sensing Imagery. JOURNAL OF REAL-TIME IMAGE PROCESSING 19, 5 (Oct. 2022), 867 880. [43] Bilal Hammoud, Charbel Bou Maroun, Jonas Ney, and Norbert Wehn. 2022. Artificial Neural Networks-Based Radar Remote Sensing to Estimate Geographical Information during Oil-Spills. In 2022 30th European Signal Processing Conference (EUSIPCO). 1801 1805.",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 1571,
    "tokens": 478
  },
  {
    "text": "For example, LLM4EDA [3] automates multiple steps in the EDA process by combining text and multimodal data such as circuit diagrams and code, significantly simplifying the design workflow. ChatEDA [4] further showcases how LLMs can generate processor design scripts and improve EDA efficiency by verifying accuracy This work is funded by, China Postdoctoral Science Foundation (No. 2022M723913) and Fundamental Research Funds for the Central Universities (No. 3208002309A2). (Hong Cai Chen and Yi Pin Xu are co-first author. Corresponding author is Yang Zhang) through automated tools. Additionally, \"The Dawn of AI- Native EDA\" [5] discusses the application of AI-driven circuit models in circuit analysis and design, especially in standard cells and analog circuits, highlighting their advantages. Lastly, the AutoMage model [6] validates its high precision and reliability in EDA script generation tasks by comparing it with LLMs like GPT-4. DocEDA [7] integrates object detection, OCR, and LLM inference to automate the extraction of circuit diagrams and electrical parameters from documents, significantly enhancing the efficiency of circuit design document processing. Overall, these models primarily focus on design optimization and code generation in QA systems, promoting intelligent design development in EDA. While, despite of design assistance, effective parameter extraction is also crucial for design processes and ensuring the accuracy of chip models [8], [9]. Engineers often consult extensive documentation to extract component parameters needed for constructing circuit simulation SPICE models. Manually searching through lengthy documents for parameters is time-consuming and labor-intensive [10], [11]. Automating parameter extraction and generating corresponding SPICE models would greatly enhance electronic design efficiency and increase EDA automation levels [12], [13]. LLMs possess a high capacity for document analysis and information extraction, offering potential to automate the parameter extraction process by accurately identifying and extracting model specifications and parameters from technical documents [14], [15]. Despite the advanced capabilities of LLMs, engineers face considerable challenges when extracting parameters from chip technical documentation. These documents are often lengthy and complex, making manual retrieval cumbersome. While methods like Retrieval-Augmented Generation (RAG) have shown promise in technical document analysis, directly applying LLMs does not efficiently facilitate parameter extraction due to the inherent complexity of chip documentation.",
    "source": "2502.16540v2_D2S-FLOW_Automated_Parameter_Extraction_from_Datas.pdf",
    "length": 2615,
    "tokens": 481
  },
  {
    "text": "Table 1: Accuracy ablation using different quantization strategies. (a) Different FP4 choices Type CosSim L1 RMSE MXFP4 98.37 0.294 0.994 NVFP4 99.52 0.077 0.201 (b) Different scale strategies for eP Method CosSim L1 RMSE Direct 93.32 0.193 1.103 Two-level 99.52 0.077 0.201 (c) Different data types for dOV Method CosSim L1 RMSE INT8 97.47 0.171 2.440 FP16 99.77 0.039 0.692 6 1K 2K 4K 8K 16K 32K Sequence Length 0 500 1000 1500 2000 Speed (TOPS) 15 16 16 OOM OOM OOM 84 89 93 94 94 95 173 198 208 212 215 214 442 467 473 479 480 479 477 530 537 548 556 559 964 1032 1022 1038 1022 1027 RTX5090, (Head dim 128, causal False) Torch xformers FlashAttn SageAttn1 SageAttn2 SageAttn3 1K 2K 4K 8K 16K 32K Sequence Length 0 500 1000 1500 2000 7 7 7 OOM OOM OOM 75 84 90 93 94 95 147 175 192 202 207 209 289 369 433 457 469 468 365 423 484 523 544 554 924 1005 1015 1020 1022 1027 RTX5090, (Head dim 128, causal True) Torch xformers FlashAttn SageAttn1 SageAttn2 SageAttn3 Figure 4: Speed comparison between SageAttention3 and Baselines (RTX5090, headim 128). 1K 2K 4K 8K 16K 32K Sequence Length 0 500 1000 1500 Speed (TOPS) 10 10 11 OOM OOM OOM 101 106 109 111 111 111 191 206 213 218 220 220 405 441 423 428 427 427 429 492 495 492 500 504 751 812 827 839 826 825 RTX5090, (Head dim 64, causal False) Torch xformers FlashAttn SageAttn1 SageAttn2 SageAttn3 1K 2K 4K 8K 16K 32K Sequence Length 0 500 1000 1500 4 5 4 OOM OOM OOM 81 92 101 105 107 109 149 174 195 207 213 217 239 312 379 404 416 421 321 400 432 463 481 491 679 750 802 803 804 806 RTX5090, (Head dim 64, causal True) Torch xformers FlashAttn SageAttn1 SageAttn2 SageAttn3 Figure 5: Speed comparison between SageAttention3 and Baselines (RTX5090, headim 64).",
    "source": "2505.11594v1_SageAttention3_Microscaling_FP4_Attention_for_Infe.pdf",
    "length": 1716,
    "tokens": 585
  },
  {
    "text": "28 Hyperparameter Value learning rate 1e-4 seed 11 dataset name cnn dailymail dataset config 3.0.0 pad to max length True max source length 512 num train epochs 15 per device train batch size 8 per device eval batch size 32 gradient accumulation steps 32 model name or path facebook bart-large evaluation strategy epoch predict with generate True Table 8: Hyperparameters for fine-tuning Bart-Large on CNN DailyMail. Hyperparameter Value learning rate 1e-4 seed 11 dataset name xsum dataset config 3.0.0 pad to max length True max source length 512 num train epochs 25 per device train batch size 4 per device eval batch size 32 gradient accumulation steps 32 model name or path facebook bart-large evaluation strategy epoch Table 9: Hyperparameters for fine-tuning Bart-Large on XSUM. 29 Hyperparameter Value model name or path meta-llama Llama-2-7b-hf data seed 42 evaluation strategy steps eval dataset size 1024 max eval samples 500 per device eval batch size 1 max new tokens 32 dataloader num workers 3 group by length True logging strategy steps remove unused columns False lora r 64 lora alpha 64 lora modules all bf16 True warmup ratio 0.03 lr scheduler type constant gradient checkpointing True dataset oasst1 source max len 16 target max len 512 per device train batch size 4 gradient accumulation steps 4 max steps 1875 eval steps 200 learning rate 0.0002 adam beta2 0.999 max grad norm 0.3 lora dropout 0.1 weight decay 0.0 seed 0 Table 10: Hyperparameters used for all Llama experiments on Open Assistant (oasst1). 30 J Github Issues Related To The Lack of A Practical Quantization Primitive The use of NF fake quantization in LoftQ Discrepancy between real model weights and expected model weights due to fake quantization in LoftQ The use of NF fake quantization in LoftQ Unrealized GPU memory saving due to fake quantization in LoftQ Unrealized LLM model size reduction due to fake quantization in PiSSA K Detailed LoftQ Results on Bart-Large In this appendix, Table 11 presents results for LoftQ-based fine-tuning under two interpretations: (1) ordering layers by index (Layer Index), and (2) treating encoder layers as preceding decoder layers (Encoder First).",
    "source": "2502.08141v1_LowRA_Accurate_and_Efficient_LoRA_Fine-Tuning_of_L.pdf",
    "length": 2179,
    "tokens": 496
  },
  {
    "text": "The best result among all models is bolded, and the second-best result is underscored. 5.3 Ablation Studies In this section, we conduct ablation studies to demonstrate the effectiveness of different dataset components and training strategies. In the first training stage, we adopt a curriculum learning strat- egy, where the model is progressively trained on line-level data, module-level data with specifica- tions, module-level data with high-level descrip- tions, and data with varying prompts. While the benefits of curriculum learning have been shown in DeepRTL (Liu et al., 2025), we extend this analysis with additional comparisons. Specifically, we compare our first-stage model (DeepRTL21st) with a variant trained without curriculum learning (DeepRTL21st-Direct), both focused on generation- based tasks. As shown in Table 1 and Table 3, the incorporation of curriculum learning significantly improves performance for both code generation and understanding tasks. When we further intro- duce the second-stage training, i.e., GRIT-based fine-tuning, the performance improves even more, demonstrating the effectiveness of both curriculum learning and GRIT-based fine-tuning strategies. In the second training stage, we combine con- trastive learning and curriculum learning to ensure that our model performs effectively on embedding- based tasks. Specifically, we start with data that excludes hard negatives and gradually introduce hard negative samples, which improves overall performance. To evaluate this strategy, we com- pare DeepRTL2 with and without hard negatives (DeepRTL2no-hard) in Tables 2, 4, and 5. Since hard negatives primarily influence contrastive learn- ing, these comparisons focus on embedding-based tasks, with negligible impact on generation-based performance. The results show a minor drop in natural language code search accuracy but substan- tial gains in functionality equivalence checking and performance prediction. Despite the small accuracy decrease in the natural language code search task, DeepRTL2 still outperforms powerful baseline em- bedding models. This improvement in functionality equivalence checking and performance prediction justifies our decision to integrate hard negatives into the training process.",
    "source": "2506.15697v1_DeepRTL2_A_Versatile_Model_for_RTL-Related_Tasks.pdf",
    "length": 2256,
    "tokens": 448
  },
  {
    "text": "Personalized Inference-Time Acceleration. Generating an optimized hardware design may require a large number of samples, resulting in high generation latency and energy costs. Since the performance of auto-regressive generation in LLM inference is typically memory-bound, several techniques have been introduced to leverage idle compute resources to accelerate LLM inference. Among these are speculative TABLE I: Test-Time Optimization Strategies Sampling Description Hyperparameters Nucleus Sampling Selects tokens with cumulative probability p p (cumulative probability) Temperature Sampling Scales token probabilities by temperature Temperature, Number of generated candidates Top-k Sampling Chooses from the top k most probable tokens k (number of sequences to consider) Beam Search Expands search using a fixed beam width Beam width w o Finetune Model Merging Hierarchical Merging 0 20 40 60 80 100 accuracy ( ) 52.15 0.38 42.31 40.38 91.18 90.11 Syntax Accuracy Semantic Accuracy (a) HLS MachineEval. w o Finetune Model Merging Hierarchical Merging 0 20 40 60 80 100 accuracy ( ) 12.0 2.0 58.0 50.0 92.0 88.0 Syntax Accuracy Semantic Accuracy (b) HLS HumanEval. w o Finetune Model Merging Hierarchical Merging 0 10 20 30 accuracy ( ) 18.0 20.0 22.0 (c) Qiskit Benchmark. Fig. 3: Effect of hierarchical approach on both classical and quantum hardware benchmarks. decoding [23], [32] and parallel decoding [3], [6], which generate multiple tokens in parallel to improve the processing speed. However, most existing approaches rely on a separate training process to learn the multi-token generation capability. In this work, we propose an inference-time learning ap- proach, where each client locally learns acceleration param- eters during the model s deployment phase while serving real user requests. Specifically, we observe that the learning process of multi-token generation involves tuning the accel- eration parameters to approximate the predictive distribution of the original model. Therefore, rather than depending on a training dataset, our approach utilizes the generation outputs produced during deployment, while serving user requests, for learning multi-token generation. This method offers two key benefits.",
    "source": "2506.00002v1_Advancing_AI-assisted_Hardware_Design_with_Hierarc.pdf",
    "length": 2227,
    "tokens": 474
  },
  {
    "text": "As such, AnalogGenie demonstrates superior correctness, achieving 73.5 valid circuits after pretraining and an impressive 93.2 after fine-tuning. Scalability: CktGNN and LaMAGIC are developed for specific types of circuits, such as Op-Amps or power converters, which limits their ability to generate beyond those particular types. Analog- Coder is able to design 7 circuit types. AnalogGenie demonstrates a zero-shot capability to gen- erate circuit types outside its training set, which includes 11 analog circuit types, as shown in Ap- pendix A.4.2. Beyond topology types, LaMAGIC is constrained by its graph representation, which contains only four device nodes, preventing it from generating larger circuits. CktGNN has a similar limitation with its fixed-node graph representation, which supports at most 22 devices. AnalogCoder also struggles with scalability, making it capable of generating circuit topologies with a maximum of 10 devices. This restriction arises from its prompt template, which requires users to specify small circuit examples with limited device counts, leading the GPT model to generate circuits of similar size. In contrast, AnalogGenie benefits from a comprehensive dataset, which includes topologies with over 50 devices. Its sequential representation efficiently captures these larger designs, enabling AnalogGenie to generate circuits with up to 64 devices after pretraining and 56 devices after fine- tuning, all within a limited sequence length. Novelty (unseen designs): AnalogCoder generates only 8.9 novel circuits, as it is designed pri- marily for task completion rather than exploring new topologies. LaMAGIC, limited to circuits with just four devices, produces 12.7 novel circuits. CktGNN, with its graph model supporting up to 22 device nodes, achieves 93.1 novel circuit discovery. Remarkably, AnalogGenie generates nearly 100 novel circuits, leveraging its ability to design large circuits from scratch. Visualizations of these novel circuits are provided in Appendix A.4.1 Performance: For Op-Amp design, AnalogCoder achieves a low FoM of just 1.7, constrained by the limited design options available through its prompt engineering. CktGNN performs significantly better with a FoM of 10.9, though it is still restricted by the lack of detailed low-level design con- trol.",
    "source": "2503.00205v1_AnalogGenie_A_Generative_Engine_for_Automatic_Disc.pdf",
    "length": 2318,
    "tokens": 481
  },
  {
    "text": "In contrast, 11 2 4 8 of H100s 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Throughput (MTps) 4.90 9.50 17.70 1.60 2.60 3.10 3.90 1.90 3.60 3.68 3.60 3.60 Forward Pass T 8K E 32 k 2 is better FlashDMoE FasterMoE Megatron-CUTLASS Megatron-TE Figure 13: Throughput as the amount of GPUs increases. We compute throughput as T NG latency, where NG is the number of GPUs. 8 16 32 64 128 Number of Experts 0 5 10 15 20 25 30 35 Runtime (s) Forward Latency T 16K k 2 4 H100s is better FlashDMoE Comet FasterMoE Megatron-CUTLASS Megatron-TE (a) 4 H100s 8 16 32 64 128 Number of Experts 0 10 20 30 40 50 60 Runtime (s) Forward Latency T 16K k 2 8 H100s is better FlashDMoE FasterMoE Megatron-CUTLASS Megatron-TE (b) 8 H100s Figure 14: Forward Latency as the Number of experts increases. baselines exhibit superlinear latency increases due to increased kernel launch overheads. FlashDMoE outperforms these baselines by up to 4X at 4 H100s and 6.6X at 8 H100s, both at 128 experts. FlashDMoE s payload-efficient communication and scheduler-driven in-kernel dispatching allow it to sustain expert parallelism without incurring the communication and orchestration penalties seen in other systems. These results reinforce FlashDMoE s scalability for ultra-sparse MoE configurations. 4.7 Memory Overhead We measure the GPU memory required for the symmetric tensor L and runtime bookkeeping state of FlashDMoE. Memory overhead depends primarily on the tile size, expert capacity (EC), and the number of experts (E). Table 3 summarizes memory overhead under various configurations, confirming that FlashDMoE maintains a modest and predictable memory footprint. 5 Limitations and Future Work Despite the performance gains and architectural innovations of FlashDMoE, there are several limita- tions worth acknowledging both practical and conceptual that open the door to future research. Programming complexity.",
    "source": "2506.04667v2_FlashDMoE_Fast_Distributed_MoE_in_a_Single_Kernel.pdf",
    "length": 1888,
    "tokens": 496
  },
  {
    "text": "The training set of both Monkey K and L are combined together. TABLE V: Comparison of the proposed neural signal compression scheme with existing works. Shoaran et al. [25] Li et al. [58] Liu et al. [59] Park et al. [60] Khazaei et al. [61] Valencia et al. [54] Turcotte et al. [62] Shrivastwa et al. [63] Our Work Platform ASIC 180-nm ASIC 130-nm ASIC 180-nm ASIC 180-nm ASIC 130-nm ASIC 180-nm Xilinx Spartan-6 FPGA Xilinx Virtex-7 FPGA Efinix Ti60 FPGA Signal Type EEG Spike LFP LFP LFP LFP Spike ECoG LFP Compression Algorithm CS CS CS DRR Hufmann Coding DRR AE DWT CS CAE Precision 10b 10b 10b 10b 10b I P:16b, O P:10b 16b 16b W: 8b, Act. : 8b Compression Ratio 16 10 8-16 4.3-5.8 2 19.2 4.17 4 150 , 37.5 SNDR (dB) 21.8 N A 9.78 N A N A 19 3 17 N A 22.61 2.21 [K], 27.43 2.41 [L] 24.37 2.19 [K], 28.49 2.28 [L] R2 Score N A N A N A N A N A 0.72 0.23[K] 0.93 0.09[L] N A N A 0.81 0.13 [K], 0.94 0.13 [L] 0.87 0.10 [K], 0.95 0.15 [L] [K]Monkey K recordings, [L]Monkey L recordings obtained from dataset [53]. DS-CAE1 model, MobileNetV1-CAE(0.25x) model plementation of the CS algorithm supporting a CR of up to 16, achieving an SNDR of 21.8 dB. The design was implemented using ASIC 180-nm process technology with an area of 0.008 mm2 per channel and power of 0.95 ¬µW per channel. Li et al.",
    "source": "2504.06996v1_Neural_Signal_Compression_using_RAMAN_tinyML_Accel.pdf",
    "length": 1294,
    "tokens": 484
  },
  {
    "text": "2964 2968. [17] J. Elseberg, D. Borrmann, and A. N uchter, One billion points in the cloud an octree for efÔ¨Åcient processing of 3d laser scans, ISPRS Journal of Photogrammetry and Remote Sensing, pp. 76 88, 2013. [18] Eugene d Eon, Bob Harrison, Taos Myers and Philip A. Chou, JPEG Pleno Database: 8i Voxelized Full Bodies (8iVFB v2) - A Dynamic Voxelized Point Cloud Dataset, 3cJQ61a , 2017. [19] Y. Feng, S. Liu, and Y. Zhu, Real-time spatio-temporal lidar point cloud compression, in 2020 IEEE RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 10 766 10 773. [20] Y. Feng, B. Tian, T. Xu, P. Whatmough, and Y. Zhu, Mesorasi: Architecture support for point cloud analytics via delayed- aggregation, in Proceedings of the International Symposium on Microarchitecture (MICRO), 2020, pp. 1037 1050. [21] Geo Week News Staff, Cloud Chamber: Low-cost smartphone app captures point clouds for AEC, , 2018. [22] S. Giancola, J. Zarzar, and B. Ghanem, Leveraging shape completion for 3d siamese tracking, in Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. [23] Google, Draco, , 2021. [24] B. Han, Y. Liu, and F. Qian, ViVo: Visibility-Aware Mobile Volumetric Video Streaming. Association for Computing Machinery, 2020. [25] X. He, H. L. Cao, and B. Zhu, Advectivenet: An eulerian- lagrangian Ô¨Çuidic reservoir for point cloud processing, arXiv preprint arXiv:2002.00118, 2020.",
    "source": "PCcompress.pdf",
    "length": 1453,
    "tokens": 443
  },
  {
    "text": "As depicted in Fig. 3, InstantFT first checks that the entry for the input x0 (with an index j) is present in the cache. If so, InstantFT reuses the precomputed results and only performs the adapter part to compute P i xi; otherwise, InstantFT also runs a forward pass of the pre-trained network to obtain xi , ÀÜxL and stores them in the cache for reuse. As such, InstantFT performs a forward pass of the pre-trained network only once per input, and repeatedly uses LoRA adapters in later epochs. For a fixed dataset, the intermediate results for all samples are computed and cached in the first epoch. This significantly reduces the overall computational cost and training time, as the pre-trained network involves more computation than LoRA modules (e.g., 33.7 40.7x higher forward FLOPs in our case, Table 1). Additionally, InstantFT has 3.8 5.9 less forward FLOPs than LoRA-All, due to a lowered output dimension of LoRA modules. 3.2 Cache Quantization Since the Forward Cache stores forward activations for every input, its size grows with the dataset size, model depth, and output dimensions, resulting in a prohibitive memory overhead. To mitigate this, InstantFT adopts the 4-bit NormalFloat (NF4) quantization [5], which is well-suited for normally distributed parameters and activations. This reduces the cache size by approximately 8x compared to FP32. Note that the cached data is dequantized before used in the forward pass. 3 InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models A PREPRINT Figure 4: InstantFT for LeNet-5-like model.",
    "source": "2506.06505v1_InstantFT_An_FPGA-Based_Runtime_Subsecond_Fine-tun.pdf",
    "length": 1563,
    "tokens": 368
  },
  {
    "text": "Machine learning methods introduce compute cost. To mitigate this cost, applications cite benefits including reducing training time, the need to retrain regularly, reuse of existing simulation data [50], a low training cost relative to simulation time [115], and scalable re-training as new training data is generated. Most research on applying machine learning to coverage closure highlights the benefit of reducing the time to achieve coverage closure. This can be accomplished not only by decreasing the number of simulations but also by shortening the time needed to generate inputs and training data. Reducing verification time was created to encompass applications that report faster coverage closure without specifically mentioning fewer simulations. Hitting coverage holes faster relates to techniques that propose to be good at covering hard-to-hit coverage points including methods that create a direct mapping from a coverage point to the input required to reach it. Reducing expert resources includes applications that reduce the need for human written directives, domain knowledge to set up the technique, and human intervention during coverage closure. This review finds the research lacks an emphasis on generality. However, a selection of research cites compatibility with standard UVM environments and different test generators as a benefit. Approaches that treat the DUV as a black box also cite generality to different DUV designs. Improving machine learning performance was rarely cited as a benefit, suggesting an emphasis from research on proposing new applications rather than improving existing methods. A small selection of the sampled material cites the benefits of a proposed technique to operate with constrained resources, such as maximising coverage subject to a time constraint or testing with constrained computing and licenses. Finally, research also cites the benefits of adding features not necessarily present in a verification workflow. For example, increasing the diversity of inputs to a DUV is one such feature. Another is decreasing the number of cases where a pseudo-random test generator fails to generate a sequence of outputs respecting its constraints. Additionally, increasing the frequency of a single event of interest in the DUV is also cited as a benefit. The overarching benefit of using ML for verification in the sampled literature is reducing the time spent on verification. This is motivated by the frequently cited figure of 70 of design time spent on verification. However, the time saved by an application may not be realisable in all scenarios. A device that is quick to simulate relative to the time to generate inputs would not necessarily see the time savings from methods that generate many inputs and simulate only a few.",
    "source": "2503.11687v1_Review_of_Machine_Learning_for_Micro-Electronic_De.pdf",
    "length": 2786,
    "tokens": 485
  },
  {
    "text": "PMLR 267, 2025. Copyright 2025 by the author(s). Finetune with 120k 3, 4, 5-comp circuits: PM FM Then finetune with 1k 6-comp circuits: PM FM Adjacency-matrix circuit formulation: Duty cycle: Connections: VIN no_edge no_edge VOUT GND Sa Sb C L sep Token lengths with complexity ùëÇ( ùëâ!) Input with float embeddings: Duty cycle options 0.1 0.3 0.5 0.7 0.9 voltage 0.9554 Embedding (E) Linear layer (L) E L Low success rates in strict tolerances Specifications Available devices: two switches, a capacitor, Voltage conversion ratio: 0.18887 Efficiency: 0.95544 Power converter Duty cycle: 0.5 Analog topology generation LaMAGIC s analysis LM VIN VOUT Figure 1. Analog topology generation and the analysis of a state- of-the-art work LaMAGIC (Chang et al., 2024). sion ratio and power efficiency, often requires varied topolo- gies. However, traditional topology design relies heavily on manual processes, which demand large engineering ef- forts and prolong the time-to-market period for new designs. Thus, automating analog topology design has become cru- cial to accelerate circuit development. Early efforts (Fan et al., 2021; Zhao Zhang, 2022; Lu et al., 2023) have focused on search-based approaches, which explore vast design spaces guided by simulation rewards. However, they are inefficient and are not suitable to gener- ate circuits for diverse specifications. The work (Fan et al., 2021) designs a upper-confidence-bound-tree-based rein- 1 LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog Topology Generation Table 1. Average token lengths of LM outputs from: SFCI in LaM- AGIC2 and FM in LaMAGIC (Chang et al., 2024). Average token length 3, 4, 5-comp circuits 6-comp circuits LaMAGIC2-SFCI 36.40 41.41 LaMAGIC-FM 84.79 105.00 forcement learning (RL) method for power converter design, which requires hundreds of simulation queries to generate a new topology.",
    "source": "2506.10235v1_LaMAGIC2_Advanced_Circuit_Formulations_for_Languag.pdf",
    "length": 1890,
    "tokens": 488
  },
  {
    "text": "arXiv:1812.00332 [cs.LG] [8] Ping Chi, Shuangchen Li, Cong Xu, Tao Zhang, Jishen Zhao, Yongpan Liu, Yu Wang, and Yuan Xie. 2016. PRIME: A Novel Processing-in-Memory Architecture for Neural Network Computation in ReRAM-Based Main Memory. In Proceedings of the 43rd International Symposium on Computer Architecture (Seoul, Republic of Korea) (ISCA 16). 27 39. [9] Mohammed Elbtity, Abhishek Singh, Brendan Reidy, Xiaochen Guo, and Ramtin Zand. 2021. An In-Memory Analog Computing Co-Processor for Energy-Efficient CNN Inference on Mobile Devices. In 2021 IEEE Computer Society Annual Sympo- sium on VLSI (ISVLSI). 188 193. doi:10.1109 ISVLSI51109.2021.00043 [10] Mohammed E. Elbtity, Brendan Reidy, Md Hasibul Amin, and Ramtin Zand. 2023. Heterogeneous Integration of In-Memory Analog Computing Architectures with Tensor Processing Units. In Proceedings of the Great Lakes Symposium on VLSI 2023 (Knoxville, TN, USA) (GLSVLSI 23). Association for Computing Machinery, New York, NY, USA, 607 612. doi:10.1145 3583781.3590256 [11] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. 2020. Single Path One-Shot Neural Architecture Search with Uniform Sampling. In Computer Vision ECCV 2020, Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (Eds.). Springer International Publishing, Cham, 544 560. [12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
    "source": "2505.22868v1_CrossNAS_A_Cross-Layer_Neural_Architecture_Search_.pdf",
    "length": 1531,
    "tokens": 449
  },
  {
    "text": "611 626. doi:10.1145 3600006.3613165 [33] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park. 2024. OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 13355 13364. [34] Janghwan Lee, Minsoo Kim, Seungcheol Baek, Seok Hwang, Wonyong Sung, and Jungwook Choi. 2023. Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. doi:10.18653 v1 2023.emnlp-main.910 [35] Jungi Lee, Wonbeom Lee, and Jaewoong Sim. 2024. Tender: Accelerating Large Language Models via Tensor Decomposition and Runtime Requantization. In Proceedings of the 51st Annual International Symposium on Computer Architecture. [36] Jiamin Li, Yimin Jiang, Yibo Zhu, Cong Wang, and Hong Xu. 2023. Accelerating Distributed MoE Training and Inference with Lina. In 2023 USENIX Annual Technical Conference (USENIX ATC 23). 945 959. [37] Liang Li, Qingyuan Li, Bo Zhang, and Xiangxiang Chu. 2024. Norm Tweaking: High-Performance Low-Bit Quantization of Large Language Models. Proceedings of the AAAI Conference on Artificial Intelligence (Mar. 2024), 18536 18544. [38] Wenjie Li, Aokun Hu, Ningyi Xu, and Guanghui He. 2024. Quantization and Hardware Architecture Co-Design for Matrix-Vector Multiplications of Large Language Models. IEEE Transactions on Circuits and Systems I: Regular Papers (2024). [39] Haokun Lin, Haobo Xu, Yichen Wu, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi Song, Zhenan Sun, and Ying Wei. 2024. DuQuant: Distributing Out- liers via Dual Transformation Makes Stronger Quantized LLMs.",
    "source": "2503.18599v2_Oaken_Fast_and_Efficient_LLM_Serving_with_Online-O.pdf",
    "length": 1751,
    "tokens": 491
  },
  {
    "text": "CDFG generated through this compilation process serves as the foundational input to the CoGNNs prediction model. Each design configuration is then synthesized using an HLS tool to extract latency and resource utilization metrics. Although HLS reports provide reasonably accurate estimates, we further refine the dataset by incorporating post- implementation metrics, resulting in a more realistic and comprehensive training set. This dual-source labeling strategy combining HLS and implementation reports ensures high-fidelity ground truth for model training. We train our prediction model on datasets from [15] and [32]. For the GNN-DSE dataset [32], the labels for each benchmark are extracted from the HLS reports, and the CDFGs are generated by the compilation front-end. For the HGBO-DSE dataset [15], the labels are derived from the implementation reports, and the CDFGs come from the HLS process. The two aforementioned datasets differ not only in their label sources but also in the benchmarks used for their construction. By conducting experiments on two distinct datasets, we can comprehensively validate the performance of CoGNNs and evaluate its efficiency in handling complex tasks. 4.2 Cooperative Graph Neural Networks The approach proposed in [1] introduces a dynamic message-passing mechanism by assigning each node one of four states Standard (S), Listen (L), Broadcast (B), or Isolate (I), which govern information flow: nodes in the Standard state both receive and propagate information, Listen nodes only receive, Broadcast nodes only propagate, and Isolate nodes remain inactive. This state-based approach transforms node feature updates into a two-step process: 1) each node selects its optimal state based on task-specific objectives, and 2) updates its features accordingly. As illustrated in Fig. 3, CoGNNs implements this mechanism through dual MPNNs: an action network that determines node states and an environment network that performs context-aware feature aggregation. A complete CoGNN (œÄ, Œ∑) model consists of environment networks œÄ and action networks Œ∑. These two MPNNs, working in tandem, can better adapt to multi-target prediction tasks. Action networks œÄ help each node determine the 6 Running Title for Header Figure 3: An abstract illustration of the CoGNNs architecture, where a dedicated MLP is constructed for each prediction target. way information flows in and out, while environment networks Œ∑ update the features of each node according to the state of the node.",
    "source": "2504.19649v2_Intelligent4DSE_Optimizing_High-Level_Synthesis_De.pdf",
    "length": 2509,
    "tokens": 501
  },
  {
    "text": "Oaken s quantization technique consists of three components: (1) Threshold-based online-offline hybrid quantization that separates and quantizes inlier and out- lier values, (2) Group-shift quantization that quantizes values with larger magnitude, and (3) Fused dense-and-sparse encoding that minimizes capacity overhead due to sparse matrices. The following sections introduce a detailed design of each component. 4.3 Threshold-based Online-Offline Hybrid Quantization Oaken minimizes quantization loss by isolating outlier values that are either exceptionally large or exceptionally small compared to typical inlier values. We propose a threshold-based online-offline hybrid quantization method for more fine-grained grouping. Oaken separates the per-token KV vector into three quantization groups: outer, middle, and inner (Figure 7(a)). The middle group consists of inliers where most KV values belong, while the outer and inner groups consist of outliers, with large and small magnitudes, respec- tively. Oaken prevents the quantization scale from being skewed due to large-magnitude outliers by isolating the outer group, and ensures the small-magnitude outliers do not vanish during quanti- zation by isolating the inner group [2, 13, 27, 34]. Offline outlier threshold profiling. To separate outliers from inliers, the topK operation is typically used to maintain a constant ratio of outliers [22].",
    "source": "2503.18599v2_Oaken_Fast_and_Efficient_LLM_Serving_with_Online-O.pdf",
    "length": 1406,
    "tokens": 313
  },
  {
    "text": "SGX acts on the hardware level to create a container that guarantees the \"confidentiality and integrity\" of the data within [10]. This could be an important consideration for those utilizing data centers for their models, as this blocks unauthorized software that could be running on a machine from interacting with the model in a malicious way, such as taking or modifying the model s data. Most processes run by data centers have little to no need for accelerators, and the environments 2 in a data center are tailored towards CPU usage. As LLMs become more popular, their usage in cloud services and data centers have increased, as well as the number of accelerators to handle these [6]. If smaller models could run inference without an accelerator and CPUs could supplement the processing power of GPUs by handling some of the model layers [6] [8], these services could achieve better resource utilization. Energy cost is a concern when a system has significant power constraints, such as relying on battery power or energy harvesting. This would be most prominent in edge computing and IoT, with TinyML focusing on the use of small machine learning models to consume milliwatts of power [11]. Currently, CPUs have less energy efficiency than a GPU, but have a lower average power consumption [12]. A GPU will draw more power than a CPU, but due to the inefficient execution of the model on a CPU, the run time is much larger, and by extension the total energy cost. This is much less an advantage and more a trade-off, but for an environment with reliable power flow, using a CPU instead of a GPU could cut the average power consumption by half [12]. For systems with no accelerator, their options for running LLM inference are limited. One option is to leverage resources in the cloud to perform this task, but this exposes the data to external servers, raising privacy concerns. Locally hosting an LLM offers increased privacy and the ability to adjust the model with confidential data [8]. Privacy is especially important for Internet of Things (IoT) devices that rely on machine learning for tasks like speech recognition [13], as transmitting sensitive user data over the internet increases the risk of interception or misuse. These resource constrained environments would benefit from being able to use a CPU to enable or supplement the ability to run an LLM locally. 1.1.2 Why an LLM on CPU is Uncommon With these advantages, why is running inference on an LLM with a CPU uncommon?",
    "source": "2506.01827v1_Memory_Access_Characterization_of_Large_Language_M.pdf",
    "length": 2493,
    "tokens": 494
  },
  {
    "text": "MME defines matrix shape configuration instructions, 2D memory moves, MMA instructions, but also vector-matrix register- register move, as well as many element-wise instructions on matrix register (e.g., data type conversions). Compared to MME, MTE reuses the vector registers, and leverages existing RISC-V V instructions for element-wise operations to reduce the implementation overhead. The Arm Scalable Matrix Extension (SME) [10] defines a new matrix storage state of size V LEN V LEN, where V LEN is the vector register length of the Scalable Vec- tor Extension (SVE) ISA. SME defines an outer product instruction that regards the matrix storage as a number of independent tile registers (e.g. 4 tiles for fp32) employed as accumulators during the outer product of two SVE vectors. Besides the outer product, SME includes 2d memory moves, mixed vector to multi-vector operations, and others. MTE differs from SME by i) using existing architectural state for accumulators; and ii) implementing MMAs as matrix dot products, which matches with long vector architectures and other contexts where the V LEN 2 architectural matrix register is prohibitively expensive. The Hopper architecture [38] is the fourth generation of NVIDIA GPUs augmented with tensor cores to accelerate deep learning workloads via 4 4 8 mixed-precision, and 4 4 4 single-precision, matrix multiplication instructions. Recent work showcases the benefits of explicit tensor core program- ming via the Warp Matrix Multiply Accumulate (WMMA) TABLE X MATRIX EXTENSIONS SUMMARY ISA Tile Size FP32 Tile 2D Memory Dedicated Name ( Bits) Shape ( Elements) Instructions Registers OpenPower [9] 512 4 4 No Yes MME [8] 512-16384 Agnostic Yes Yes SME [10] V LEN 2 Agnostic (Square) Yes Yes Tensor Cores [38] 512 4 4 Yes - AMX [5] 8192 16 16 Yes Yes SiFiveInt [7] 512-16384 4 (V LEN 128) No No MTE (Section III) Agnostic Agnostic Yes No API, which exposes tile shape configuration, matrix load s- tores, and multiplication, to accelerate cross-correlation [61] and convolutions [62].",
    "source": "2507.03522v1_A_Flexible_Instruction_Set_Architecture_for_Effici.pdf",
    "length": 2045,
    "tokens": 488
  },
  {
    "text": "Lastly, ASPO also introduces a cost-aware Expected Improvement (EI) acquisition function, enhancing the efficiency of configuration selection during the BO process. A. Adaptation for Categorical Parameters In ASPO, a one-hot encoding method is introduced to encode all categorical related design parameters into a vector, where each element corresponds to a specific category. During the BO process, every element in this vector is treated as an independent real-valued variable bounded within [0, 1]. This transformation enables categorical parameters to be processed within a structured, high-dimensional continuous space that supports gradient-based optimization. Additionally, it explic- itly captures the dependencies between categorical and real- valued parameters, allowing models to better learn and predict performance based on these interactions. Additionally, the covariance kernel K(x, x ) in BO is cus- tomized to accommodate these categorical parameters. This customized kernel is essential for calculating the covariance function œÉ2(x, x ), which measures the similarity or correla- tion between different sample points in the input space. The covariance function guides the acquisition function in selecting the next point to sample [19]. The kernel customization is to introduce a categorical trans- formation for the one-hot encoded vectors derived from cate- gorical parameters before the covariance kernel computation. This transformation sets the maximum value within each one- hot encoded vector to one and all remaining elements to zero. As a result, samples that map to the same one-hot vectors share the same customized kernel function value K (x, x ). Consequently, when a sample is evaluated, the covariance function œÉ2(x, x ) for any sample that can be transformed to the same one-hot vector becomes zero [10]. This causes the acquisition function to exclude these regions from further sam- pling, thereby effectively preventing the repetitive selection of identical soft processor designs and enhancing the efficiency of the BO process. B. Constraint Functions with Smooth Numeric Ranges There are three main types of parameter constraint, either from the specification [3] or from the empirical experiment.",
    "source": "2506.06817v1_ASPO_Constraint-Aware_Bayesian_Optimization_for_FP.pdf",
    "length": 2236,
    "tokens": 431
  },
  {
    "text": "We compare standard Pytorch initialization, typically taken to be a Kaiming uniform distribution between [ 1 fan in, 1 fan in], against a variant using lower gain (gain 0.5) under the Xavier normal distribution. Reducing the variance of initial weights appears to improve loss spikes. 64 96 128 256 384 512 768 896 1024 Width 1 2 3 4 5 6 7 8 Depth 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 2 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 FP32 64 96 128 256 384 512 768 896 1024 Width 1 2 3 4 5 6 7 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 2 1 1 2 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 2 3 2 0 0 0 0 0 2 0 1 1 MX-mix 64 96 128 256 384 512 768 896 1024 Width 1 2 3 4 5 6 7 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 2 1 0 0 0 0 0 1 1 1 2 0 0 0 0 3 0 1 2 1 MX-e4m3 64 96 128 256 384 512 768 896 1024 Width 1 2 3 4 5 6 7 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 2 0 0 0 0 0 1 1 2 0 0 0 0 0 1 1 1 1 1 MX-fp6 0.0 0.5 1.0 1.5 2.0 2.5 3.0 spikes Figure 9: Instability spikes measured in training, for different model depths and widths.",
    "source": "2506.20752v1_Characterization_and_Mitigation_of_Training_Instab.pdf",
    "length": 1246,
    "tokens": 479
  },
  {
    "text": "Such platforms represent the future of the Internet of Things (IoT) and energy harvesting wireless sensor networks (EH-WSNs). Equipped with modern machine learning (ML) techniques, these devices can revolutionize computing, monitoring, and analytics in remote, risky, and critical environments such as oil wells, mines, deep forests, oceans, remote industries, and smart cities. However, the intermittent and limited energy income of these deployments demands optimizations for ML applications at the algorithm (Yang et al., 2017; Shen et al., 2022; Mendis et al., 2021), orchestration (Maeng Lucia, 2018; Mishra et al., 2021), compilation (Gobieski et al., 2018), and hardware development (Qiu et al., 2020; Islam et al., 2022; Mishra et al., 2024) layers. Despite these advancements, achieving consistent and accurate inference thereby meeting service level objectives (SLOs) in such intermittent environments remains a significant challenge, exacerbated by unpredictable resources, form-factor limitations, and variable computational availability, particularly when employing task-optimized deep neural networks (DNNs). There are two major problems with performing DNN inference under intermittent power. (I) Energy Variability: Even though DNNs can be tailored to match the average energy income of the energy harvesting (EH) source through pruning, quantization, distillation, or network architecture search Preprint. Under review. arXiv:2408.13696v2 [cs.LG] 26 Jan 2025 (NAS) (Yang et al., 2018, 2017; Mendis et al., 2021), there is no guarantee that the energy income con- sistently meets or exceeds this average. When the income falls below the threshold, the system halts the inference and checkpoints the intermediate states (via software or persistent hardware) (Maeng Lucia, 2018; Qiu et al., 2020), resuming upon energy recovery. Depending on the EH profile, this might lead to significant delays and SLO violations. (II) Computational Approximation: To address (I) and maintain continuous operation, EH-WSNs may skip some compute during energy shortfalls by dropping neurons (zero padding) or by approximating computations (quantization).",
    "source": "NexUME.pdf",
    "length": 2152,
    "tokens": 460
  },
  {
    "text": "Our approach utilizes algorithmic optimizations along with specialized hardware modules to enable the IOM method on resource-constrained edge devices by efficiently handling the previous three key challenges, i.e., the overlapping sum problem, ineffectual computations due to cropped outputs, and tiling TCONV computations. We develop our design using the SECDA-TFLite [14] toolkit and evaluate its performance on an edge platform that includes an FPGA across various TCONV problems, including end-to-end execution of GAN models. Furthermore, we compare the performance of MM2IM against similar TCONV accelerators for resource-constrained edge FPGAs, demonstrating superior throughput per DSP. The contributions of this paper can be summarized as follows: MM2IM: a new accelerator architecture that solves the three main TCONV problems and efficiently processes TCONV operations on resource-constrained edge devices using our custom IOM-based tiling strategy. Two specialized hardware modules: the TCONV map- ping engine that generates compute and output maps on- the-fly for any TCONV problem configuration, and the processing module that efficiently computes and stores the required outputs while skipping ineffectual computa- tions using the compute and output maps. Integration and evaluation of MM2IM: we integrate MM2IM within TFLite and run a range of experiments that compare MM2IM against our ARM Neon opti- mized CPU baseline and other FPGA-based resource- constrained accelerators for TCONV. We obtain an aver- age speedup of 1.9 across 261 TFLite TCONV problem configurations and, similarly, achieve up to 3 speedup and 2.4 energy reduction across two GAN models while improving GOPs DSP efficiency by at least 2 over other accelerators. II. BACKGROUND A. Transposed Convolution Transposed Convolution (TCONV) is the key operation used within generative AI models to enable upscaling of input data.",
    "source": "2507.07683v1_Accelerating_Transposed_Convolutions_on_FPGA-based.pdf",
    "length": 1910,
    "tokens": 416
  },
  {
    "text": "2011, pp. 187 197. [30] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer, A survey of quantization methods for efficient neural network inference, 2021. [31] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han, On the variance of the adaptive learning rate and beyond, in 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020. [32] A. Graves, Sequence Transduction with Recurrent Neural Net- works, in Proc. ICML Workshop on Representation Learning, Edinburgh, Scotland, Jun. 2012.",
    "source": "2505.24721v1_Running_Conventional_Automatic_Speech_Recognition_.pdf",
    "length": 570,
    "tokens": 168
  },
  {
    "text": "5 TABLES AND COMPARISON The following contains the summary of components and software used: Component Specification Master Node 10.80.0.100, Intel i5 CPU, 16 GB RAM Compute Nodes c1, c2, c3 (10.80.0.101-103), Intel i5 CPU, 16 GB RAM GPUs NVIDIA GTX 1650 (on all nodes) Storage 1TB (shared across all nodes) Network Ethernet (1 Gbps) Table 1: Cluster Configuration Summary Software Version Purpose Rocky Linux 9.4 Operating System SLURM 22.05 Job Scheduling Lmod Installed Environment Management Anaconda 2023.09 Python-based Workflows Table 2: Software Stack 6 CONCLUSION Setting up a local GPU cluster within a small department or university presents a highly cost-effective alternative to renting cloud GPUs. Our cost analysis demonstrates that operating a locally installed cluster with four NVIDIA GTX 1650 GPUs (4 x 4GB 16GB total) incurs a monthly electricity cost of only NPR 5,760 (based on an 800W power consumption per node) in Nepal. In contrast, renting an NVIDIA T4 GPU (16GB) in the cloud costs approximately NPR 123.50 per hour, amounting to NPR 29,640 per month for similar usage [35, 36]. This stark difference underscores the long-term financial benefits of local GPU clusters, particularly in regions like Nepal, where electricity costs are relatively low. Beyond cost savings, a locally managed cluster offers greater control over data privacy, independence from internet connectivity, and uninterrupted access to computational resources, making it an ideal solution for research and education. While the initial hardware investment may seem substantial, many institutions can leverage existing computing labs to set up these clusters efficiently. Ultimately, the autonomy, sustainability, and reduced recurring costs of an in-house GPU cluster make it a far more viable and strategic investment for academic and research institutions compared to reliance on cloud-based alternatives. 7 ACKNOWLEDGEMENTS We express our sincere gratitude to Herald College Kathmandu for providing the research space, hardware, and infrastructure essential for this project.",
    "source": "2503.11246v1_Cost-effective_Deep_Learning_Infrastructure_with_N.pdf",
    "length": 2075,
    "tokens": 430
  },
  {
    "text": "2. Block floating point and microscaling (MX) formats. While our design uses a group size of 16, the illustration depicts a group size of 4 for clarity. 0000-0000 2025 IEEE. Personal use is permitted, but republication redistribution requires IEEE permission. See for more information. arXiv:2504.08398v1 [cs.AR] 11 Apr 2025 IEEE COMPUTER ARCHITECTURE LETTERS, VOL. 24, NO. 1, JANUARY-JUNE 2025 2 TABLE I IMAGE QUALITY AFTER APPLYING MX TO STABLE DIFFUSION 3 Method Precision (W A) FID in COCO-1k ( ) FP 16 16 74.07 MX 9 9 72.98 6 9 71.88 9 6 203.37 6 6 199.78 (a) Weight (b) Activation Large Small Fig. 3. Magnitude distributions of weights and activations in DiT-XL-256. Microscaling (MX) format. Block floating point (BFP) is one of the low-precision formats that has gained attention for balancing accuracy and computational efficiency. Figure 2(a) shows how BFP multiplication operates. Unlike conventional floating point, BFP simplifies multiplication by grouping val- ues that share an exponent. Microscaling (MX) format [9], [11] is a variant of BFP, as shown in Figure 2(b). MX has demonstrated potential in LLMs but has not yet been explored for DiTs. This work first characterizes the challenges of applying MX to DiT-based models and then leverages these insights to develop mixed-precision MX quantization for DiTs. C. Challenges of Applying MX formats to DiT Activation sensitivity to low precision. Table I presents the image quality (FID; lower is better) after applying the MX format to Stable Diffusion 3. Applying MX6 to the weights while using MX9 for the activations preserves image quality comparable to FP16, whereas applying MX6 to both weights and activations causes a noticable degradation in image quality. This is due to the value distribution of DiTs. Figure 3 illustrates the magnitude distribution of values in the weight and activation matrices, revealing more pronounced outliers in the activation matrix. These outliers in activations significantly contribute to degradation. Source of quality degradation.",
    "source": "2504.08398v1_MixDiT_Accelerating_Image_Diffusion_Transformer_In.pdf",
    "length": 2040,
    "tokens": 489
  },
  {
    "text": "Therefore, By the end of training, the supernet acts as an approximate model, representing the weights of fully-trained subnet architectures. Once the supernet is trained, we sample subnets from the trained supernet architecture and evaluate them based on the desired FF. During the subnet search, each of the sampled architecture inherits weights from the supernetùëäùëÜùëàùëÉasùëäùëÜùëàùëÉ(ùëé). The best architecture is selected as, ùëé argmax ùëéùúñùê¥ ùêπùêπùë°ùëíùë†ùë°N (ùëé,ùëäùëÜùëàùëÉ(ùëé)) (3) Since each fitness function (ùêπùêπùë°ùëíùë†ùë°) utilized in this work includes accuracy as a key metric, we assess accuracy through inference using weights inherited from the supernet. With access to these pre-trained supernet weights, our search process becomes highly ef- ficient, eliminating the need for retraining. To search for optimized subnets based on specific FFs, we apply an evolutionary algorithm as described in the following. 3.2 Neural Network (NN) Architecture Search As shown in Figure 4, we build an overparameterized supernet containing the weights for all possible architecture choices in the search space. As building blocks for our network configuration, we design three different convolutional choice blocks based on the well-known VGG [25] and ResNet [12] architectures, as shown in Fig. 4 (a). The three building blocks are: (1) the VGG block with two 3 3 convolution layers with a stride of 1 followed by a 2 2 max pooling layer, (2) the modified VGG block (MVGG) which is similar to VGG without the sub-sampling layer, and (3) the RES Table 1: Network Configuration Settings Architecture Search Space No. of blocks in the network (ùëõ) 1-8 The block type (ùêµùëôùëúùëêùëò) VGG, MVGG, RES No. of out.",
    "source": "2505.22868v1_CrossNAS_A_Cross-Layer_Neural_Architecture_Search_.pdf",
    "length": 1659,
    "tokens": 477
  },
  {
    "text": "We provide detailed statistics of our netlist dataset in TABLE II, including the number of circuits, the average number of text tokens, and the average number of gates. Model and training. For the encoder model in GenEDA, we employ the state-of-the-art netlist encoder NetTAG [19] as the backbone. For the decoder LLMs in GenEDA, we fine-tune the DeepSeek-Coder [45] 1.3B and 6.7B models in the trainable embedding-based alignment (Paradigm 1), and directly leverage the commercial APIs of OpenAI GPT-4o and DeepSeek-V3 [46] as the frozen LLMs in the prediction-based alignment (Paradigm 2). The training process utilizes DeepSpeed ZeRO and LoRA techniques. In alignment paradigm 1, we adopt a three-layer MLP with dimensions 768, 2048, and 4096 to transform the netlist embedding (768 dimensions) into the LLM word embeddings (4096 You are a professional Verilog designer that needs to evaluate the similarity between two textual functional summaries describing VLSI designs. The first summary is the ground truth description of the circuit, and the second summary is the generated description of the circuit. Please read the following summaries and provide a similarity score between 0 and 1 based on how similar the two summaries are in terms of describing the functionality of the designs, where 0 means completely dissimilar and 1 means identical. Note that you should strictly only output the score without any additional information. SPEC Similarity Evaluation by GPT You are a professional Verilog designer that needs to evaluate the functional similarity between two VLSI designs in Verilog code format. The first Verilog is the ground truth, and the second one is the generated Verilog of the circuit. Please first analyze their implemented arithmetic functionality and provide a similarity score between 0 and 1 based on how similar the two Verilog code are in terms implemented arithmetic, where 0 means completely dissimilar and 1 means their implemented functionalities are identical. Note that you should strictly only output the score without any additional information. RTL Code Similarity Evaluation by GPT Fig. 7: Prompt used in GPT-assisted evaluation. dimensions).",
    "source": "2504.09485v1_GenEDA_Unleashing_Generative_Reasoning_on_Netlist_.pdf",
    "length": 2185,
    "tokens": 472
  },
  {
    "text": "There is a broad fitness peak for these three values, so they can be optimized together with relatively little consequence. 13.7 ZettaLith Aggregation of TRIMERA Stacks While a single TRIMERA stack is optimized for 8,192 columns, there are 156 TRIMERA stacks in a ZettaLith, allowing for up to 1,277,952 columns to be calculated simultaneously, without requiring transfer of partial sums. The entire ZettaLith enables batches of 32,768 arrays of 24,576 activations 8,192 columns x 156 TRIMERAs (1,029,142,883,598,340 FLOPs) to be calculated in 33,260 clock cycles (HILT to HILT) at 98.52 efficiency. This is the limit of ZettaLith calculation without transfer of activations, weights, or sums between TRIMERA stacks. Larger matrix calculations require transfer of data between TRIMERA stacks using the 7,800 TB s data fabric bandwidth of the WSSCB. Table 7: Activations HILT and broadcast latch tree Clock Phase Activations Spare Bits Fanout Clock gen. 1-3 Read MUX 32,768 1 131,072 0.0625 1 4-7 Read MUX 2,048 1 8,192 0.0625 1 8-11 Read MUX 128 1 512 0.0625 1 12-15 Read MUX 8 1 32 0.1250 1 16 Read MUX 1 1 4 1 1 17 HILT- SLD 1 1 4 2 1 18 Broadcast 2 1 8 2 2 19 Broadcast 4 1 16 2.25 4 20 Broadcast 8 1 36 4 9 21 Broadcast 32 1 132 4 33 22 Broadcast 128 1 520 4 130 23 Broadcast 512 4 2,064 4 516 24 Broadcast 2,048 8 8,224 4 2,056 25 PE 8,192 16 32,832 Within PEs 8,208 26 14 Transformer Inference Model sizes for OpenAI and Anthropic LLMs are not publicly available, so this calculation is based on Meta s Llama 3.1 405B, the largest open source LLM currently available. Table 8 shows the basic model size characteristics of Llama 3.1 405B. Table 9 shows the order of calculation size, and the number of calculations required to calculate the model at the batch size and context length of Table 8.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 1800,
    "tokens": 493
  },
  {
    "text": "The computation involves calculating the normalized Hamming distance between truth tables and the distance between embedding vectors, followed by zero-normalization to align scales, as specified in Equations 16 to 19: ùê∑ùëá (ùëñ,ùëó) ùêªùëéùëöùëöùëñùëõùëîùê∑ùëñùë†ùë°ùëéùëõùëêùëí(ùëáùëñ,ùëáùëó) ùëôùëíùëõùëîùë°‚Ñé(ùëáùëñ) , (ùëñ, ùëó) N (16) ùê∑ùëç (ùëñ,ùëó) 1 ùê∂ùëúùë†ùëñùëõùëíùëÜùëñùëöùëñùëôùëéùëüùëñùë°ùë¶(ùëßùëñ,ùëßùëó) (17) ùê∑ùëá (ùëñ,ùëó) ùëçùëíùëüùëúùëÅùëúùëüùëö(ùê∑ùëá (ùëñ,ùëó)), ùê∑ùëç (ùëñ,ùëó) ùëçùëíùëüùëúùëÅùëúùëüùëö(ùê∑ùëç (ùëñ,ùëó)) (18) where ùëÅis the set of sampled node pairs, ùëáùëñand ùëáùëóare the truth-table vectors for nodes ùëñ and ùëó, ùêªùëéùëöùëöùëñùëõùëîùê∑ùëñùë†ùë°ùëéùëõùëêùëí(ùëáùëñ,ùëáùëó) computes the number of differing bits between ùëáùëñand ùëáùëó, and ùëôùëíùëõùëîùë°‚Ñé(ùëáùëñ) is the length of the truth table. ùëßùëñand ùëßùëódenote the embedding vectors of nodes ùëñand ùëó, ùê∂ùëúùë†ùëñùëõùëíùëÜùëñùëöùëñùëôùëéùëüùëñùë°ùë¶(ùëßùëñ,ùëßùëó) measures their similarity, and ùê∑ùëç (ùëñ,ùëó) represents the embedding distance. ùëçùëíùëüùëúùëÅùëúùëüùëö( ) normalizes the distances to a zero-mean distribution, and ùê∑ùëá (ùëñ,ùëó) and ùê∑ùëç (ùëñ,ùëó) are the normalized truth-table and embedding distances, respectively.",
    "source": "2506.06787v1_FuncGNN_Learning_Functional_Semantics_of_Logic_Cir.pdf",
    "length": 917,
    "tokens": 735
  },
  {
    "text": "This major difference in runtime arises from communication because splitwise-sim employs a dummy link-based communication model with specified lower-bound bandwidth number. In contrast, we use Astra-sim to model client communication, which introduces slight differences in overall runtime. Conv 20 Code 20 Conv 40 Code 40 0 500 1000 1500 End-to-End Runtime (ms) 0.0 0.1 -0.8 -6.5 Bloom-176B Splitwise HERMES Conv 20 Code 20 Conv 40 Code 40 0.0 0.0 -0.0 -0.5 Llama-2-70B Request_Type RPS Fig. 5: End-to-end validation results comparing Splitwise and HERMES on an 80-GPU system configured with 8TP. IV. Understanding impact of stages on inference In this section we use HERMES to study i) optimal batching strategies for serving reasoning based requests, and ii) hardware affinity requirements for requests requiring retrieval augmented generation(RAG). 7 0 20 -14.65 0.70 -7.33 -1.95 -0.49 -0.11 -2.75 -0.86 TP8 512 vLLM Time HERMES Time 0 20 -4.39 -0.40 -3.98 -3.07 -1.19 -0.79 -3.50 -2.08 TP8 1024 0 20 -3.31 0.13 -1.06 -0.94 -0.45 0.51 -2.36 -0.72 TP4 512 10 1k 10 2k 20 1k 20 2k 50 1k 50 2k 100 1k 100 2k Num Requests Input Size 0 20 -1.06 0.42 -1.30 -1.11 -1.07 -0.22 -2.41 -1.53 TP4 1024 Runtime (s) Chunk: Fig. 6: End-to-end runtime comparation of vLLM and HER- MES on different parallelization with HGX:H100x8 running Llama3.1-70B. For each hardware configuration, we vary the context length, number of requests, and chunk size. Prefill Prefill (a) Single Path (b) Multiple parallel thoughts 1 2 n Prefill 3 Fig. 7: Scaling test time compute through single-path and multiple parallel reasoning thoughts. A.",
    "source": "2504.09775v3_Understanding_and_Optimizing_Multi-Stage_AI_Infere.pdf",
    "length": 1613,
    "tokens": 496
  },
  {
    "text": "Graphene is currently more promising for analog RF applications or as an advanced interconnect material (replacing copper) rather than as a direct replacement for silicon in ZettaLith's logic PEs. Foundational work: (Schwierz, 2010); recent perspective: (Mak et al., 2024). 25.11 Accommodating High Defect Densities in Early Post-CMOS Technologies A major challenge for any nascent semiconductor technology is achieving high manufacturing yields. Early post-CMOS processes will likely exhibit significantly higher defect densities than even bleeding-edge CMOS. ZettaLith's Resilience: The SHAPE architecture and CREST fault tolerance mechanism are inherently suited to managing this. While the baseline design assumes bleeding- edge CMOS-level defect rates (allocating 16 spare columns per 8,192-column CASCADE array), this allocation can be readily increased. Example Scenario: If, for example, 10 of a post-CMOS SLD's area were dedicated to spare columns, each CASCADE array could have approximately 900 spare columns. This would provide tolerance of potentially thousands of defects per square centimeter. Specifically, up to 120,705 uncorrelated defects cm¬≤ could theoretically be tolerated across the 172,608 spare CASCADE columns on an SLD chip, though correlated defects would reduce this substantially in practice. Implication: This adaptability significantly lowers the barrier for adopting promising but initially high-defect-rate post-CMOS technologies within the ZettaLith framework, potentially accelerating their deployment for high- performance computing. 26 Comparison to Current GPU Rack ZettaLith is not a general purpose GPU, so comparison to GPUs is only meaningful if the application is exclusively inference of transformers in FP4 format. ZettaLith is a hardwired array of CASCADE arrays dedicated to transformer inference with FP4 weights. ZettaLith is not intended for non-inference workloads. At the system level, the key performance metrics in Table 20 demonstrate ZettaLith s capabilities. Table 20 shows a balanced system which could provide the memory capacity, memory bandwidth, CPU capacity, CPU memory, chip-to-chip fabric bandwidth and the fabric topology required for the system to keep up with the TRIMERA arrays.",
    "source": "2507.02871v1_ZettaLith_An_Architectural_Exploration_of_Extreme-.pdf",
    "length": 2248,
    "tokens": 467
  },
  {
    "text": "inst.2 Figure 2: Example of converting a circuit to a graph. 4.2 Acquisition of Training Data In the post-layout netlist (SPF files) generated by RC extraction, the net parasitics form a complex ùúã-type RC network. As the net capacitance is of concern, we generate the lumped sum of capacitance (Ceff) for each net from the post-layout netlist and use it as the ground-truth label for training the 2-stage model. We extract three types of feature vectors from the schematic netlist. The definitions of different feature elements are listed in Table 1. Device nodes need to extract different features according to their device types. Compared to existing works, we collect more features for different nodes.",
    "source": "2507.06549v1_Deep-Learning-Based_Pre-Layout_Parasitic_Capacitan.pdf",
    "length": 705,
    "tokens": 156
  },
  {
    "text": "Moreover, we adopt the 1T1R technique to build high resistance and low 2000 2500 3000 3500 1 30 59 88 117 146 175 204 233 262 291 320 349 378 407 436 465 494 523 552 581 610 639 668 697 726 755 784 813 842 871 900 929 958 987 1016 1045 1074 Power (uW) Thermal Signals Pmax 3378ÁÖíW Pmean 2943ÁÖíW Pmedian Êæ¶Êæ≠Êæ®Êæ¶ÁÖíÊøã Ratiomax mean 1.1 Ratiomax median 1.1 Data volume: 1098, Sampling time: 0.2 second, Time duration: 3.66 minutes 0 5000 10000 15000 20000 25000 30000 1 48 95 142 189 236 283 330 377 424 471 518 565 612 659 706 753 800 847 894 941 988 1035 1082 1129 1176 1223 1270 1317 1364 1411 1458 1505 1552 1599 1646 1693 1740 1787 Power (ÁÖíÊøã) TV-RF Signals Pmax 25828ÁÖíW Pmean 5571ÁÖíW Pmedian Êæ±Êæ¶Êæ©Êæ•Êæ´ÁÖíÊøã Ratiomax mean 4.6 Ratiomax median 10.3 Data volume: 1800, Sampling time: 0.1 second, Time duration: 3 minutes 100 600 1100 1600 2100 2600 3100 3600 4100 1 77 153 229 305 381 457 533 609 685 761 837 913 989 1065 1141 1217 1293 1369 1445 1521 1597 1673 1749 1825 1901 1977 2053 2129 2205 2281 2357 2433 2509 2585 2661 2737 2813 2889 2965 Power (uW) WiFi-office Signals Pmax 3981ÁÖíW Pmean 419ÁÖíW Pmedian ÊæßÊæ©Êæ©ÁÖíÊøã Ratiomax mean 9.5 Ratiomax median 11.2 Data volume: 3000, Sampling time: 0.2 second, Time duration: 10 minutes 300 400 500 600 700 800 900 1 52 103 154 205 256 307 358 409 460 511 562 613 664 715 766 817 868 919 970 1021 1072 1123 1174 1225 1276 1327 1378 1429 1480 1531 1582 1633 1684 1735 1786 1837 1888 1939 1990 ÊøÑÊø£Êø´ÊøôÊø¶ÊæîÊæúÁÖíÊøãÊæù WiFi-home Signals Pmax 891ÁÖíW Pmean 500ÁÖíW Pmedian Êæ±Êæ©Êæ§Êæ•ÁÖíÊøã Ratiomax mean 1.8 Ratiomax median 1.8 Data volume: 2000, Sampling time: 0.2 second, Time duration: 6.7 minutes 0 200 400 600 800 1000 1 165 329 493 657 821 985 1149 1313 1477 1641 1805 1969 2133 2297 2461 2625 2789 2953 3117 3281 3445 3609 3773 3937 4101 4265 4429 4593 4757 4921 5085 5249 5413 5577 5741 5905 6069 6233 6397 6561 6725 6889 7053 7217 7381 7545 7709 7873 8037 8201 8365 8529 8693 8857 9021 9185 9349 9513 9677 9841 Power (uW) Piezo Signals Pmax 998ÁÖíW Ratiomax mean 42.3 Ratiomax median 249.0 Pmean 24ÁÖíW Pmedian Êæ±Êæ®ÁÖíÊøã Data volume: 10000, Sample time: 0.1 millisecond, Time duration: 1 second Fig. 4. Variance feature of different power sources III-ADC BL WL SL Column 1 CSA - SAR Ref Shift Add Driver Row 1 Row 2 Row m Driver Driver I-DAC II-Comp 4-bit inputs 1-bit resolution 1-bit weights 4-bit outputs bit- serial Input Reg.",
    "source": "ResiRCA.pdf",
    "length": 2321,
    "tokens": 936
  },
  {
    "text": "(2) An evaluation of the potential of DAE architectures for embedding operations (Section 3). (3) The DLC IR, a low-level IR designed to abstract and optimize decoupled lookup and compute code (Section 4). arXiv:2504.09870v1 [cs.AR] 14 Apr 2025 Siracusa et al. dlrm_uni dlrm_gau llm_bigbird kg_biokg kg_wikikg2 gnn_arxiv gnn_mag gnn_products gnn_proteins 0 20 40 60 80 100 Time breakdown Other Scalar elem Vector elem GEMM Embedding 10 100 Operational Intensity (FLOPs Byte) 10 100 Performance (TFLOPs s) 1 BW util 3 BW util 6 BW util 12 BW util 25 BW util 50 BW util HBM bandwidth dlrm_uni dlrm_gau kg_biokg kg_wikikg2 gnn_arxiv gnn_mag gnn_products gnn_proteins Figure 1: Deep-learning recommendation models (dlrm) (Sec- tion 2.2.1), large language models (llm) with sparse attention (Section 2.2.2), knowledge graphs (kg) (Section 2.2.3), and graph neural networks (gnn) (Section 2.2.3) heavily rely on embedding operations that do not perform efficiently even on modern Nvidia H100 GPUs [45]. All experiments use highly- optimized models from the literature (Section 2.2). ptrs vec: 0 2 3 idxs vec: 2 Embedding table: 4 0 0.4 -0.3 0.7 0.1 0.8 -0.1 0.2 0.0 -0.7 0.6 0.3 -0.5 0.0 -0.2 -0.6 0.4 -0.1 0.3 0.4 -0.3 Hi there! Hello! Sentence 1 [0 2] Word Pos: 0 1 2 Emb. idx: 2 4 0 Sentence 2 [2 3] Figure 2: Feature embedding requires scattered memory lookups to fetch embedding vectors from embedding tables.",
    "source": "2504.09870v1_Ember_A_Compiler_for_Efficient_Embedding_Operation.pdf",
    "length": 1408,
    "tokens": 475
  },
  {
    "text": "For instance, MobileNets architectures [51, 52, 106] replace standard convolution layers by depth-wise separable convolutions [111]. This form of factorized convolutions separate the standard convolution operation in two stages: filtering, with single 2D filters for each input channel, and combination, by linearly combining the output of the filtering with a 1x1 convolution. When considering an input tensor ‚Ñéùëñ ùë§ùëñ ùëëùëñ, a kernel ùëò ùëò ùëëùëñ ùëëùëó, and an output tensor ‚Ñéùëñ ùë§ùëñ ùëëùëó, a standard convolution costs Manuscript submitted to ACM 16 C√©dric L√©onard, Dirk Stober, and Martin Schulz ‚Ñéùëñ ùë§ùëñ ùëëùëñ ùëëùëó ùëò ùëò, while depth-wise separable convolution costs ‚Ñéùëñ ùë§ùëñ ùëëùëñ(ùëò2 ùëëùëó), a computational reduction of a factor ùëò2. For example, Howard et al. [52] use depth-wise separable convolution to reduce computation by 9 for a loss of 1 on ImageNet. As an alternative path to convolution approximation, All Adder Networks [17] replace the multiplications in convolution kernels by additions. While it does not reduce computational complexity, additions are generally less expensive than multiplications. In particular, the All Adder NN of Zhang et al. [139] achieves 3 TOP s, the highest computational throughput of the survey. Pruning Techniques for Compact Networks. Pruning reduces computational complexity and memory costs by removing non-essential weights or entire channels from an NN. While the usual overparameterization of NNs aids training, it results in many redundant weights [114]. After training, pruning nullifies the least significant weights (in contribution to the output) to reduce this redundancy, but often requires fine-tuning to recover lost accuracy. The resulting trade-off between parameter reduction and task accuracy can be adjusted with a pruning factor, which determines the percentage of weights to be nullified.",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 1818,
    "tokens": 499
  },
  {
    "text": "Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint arXiv:2501.12948 (Jan 2025).",
    "source": "2506.12708v3_Serving_Large_Language_Models_on_Huawei_CloudMatri.pdf",
    "length": 393,
    "tokens": 148
  },
  {
    "text": "We also evaluate the performance of OPT3, OPT4C, and OPT4E (32 32PEgs) in comparison with other bit-slice architectures. Others TPU Ascend Trapezoid FlexFlow Laconic Bitlet Sibia Bitwave Frequency(MHz) 1000 1000 1000 1000 1000 1000 250 250 Area(um2) 370631 320783 283704 332848 213248 415800 1069000 861681 Power(W) 0.25 0.24 0.22 0.28 1.21 0.23 0.10 0.01 Peak Performance(TOPS) 2.05 2.05 2.05 2.05 0.81 0.74 0.77 0.22 Energy Efficiency(TOPS W) 8.05( 1.00) 8.21( 1.00) 9.31 1.00) 7.29( 1.00) 0.67( 1.00) 3.29( 4.91) 7.65( 11.42) 14.77( 22.04) Area Efficiency(TOPS mm2) 5.53( 1.00) 7.22( 1.00) 7.22( 1.00) 6.15( 1.00) 3.77( 1.00) 1.79( 0.47) 0.72( 0.19) 0.25( 0.07) Ours OPT1 (TPU) OPT1 (Ascend) OPT1 (Trapezoid) OPT1 (FlexFlow) OPT2 (FlexFlow) OPT3 OPT4C OPT4E Frequency(MHz) 1500 1500 1500 1500 1500 2000 2500 2000 Area(um2) 436646 332185 271989 373898 347216 460349 259298 672419 Power(W) 0.37 0.24 0.22 0.38 0.35 0.70 0.51 0.89 Peak Performance(TOPS) 3.07 3.07 3.07 3.07 3.07 1.80 2.25 7.22 Energy Efficiency(TOPS W) 8.41( 1.04) 12.82( 1.56) 13.89( 1.49) 8.08( 1.11) 8.77( 1.20) 2.57( 3.83) 4.41( 6.58) 8.11( 12.10) Area Efficiency(TOPS mm2) 7.04( 1.27) 9.25( 1.28) 11.29( 1.56) 8.22( 1.34) 8.85( 1.44) 3.91( 1.04) 8.68( 2.30) 10.73( 2.85) Reports on timing, power, and area after logic synthesis. Reports on timing, power, and area after placing and routing by chip layout.",
    "source": "2503.06342v1_Exploring_the_Performance_Improvement_of_Tensor_Pr.pdf",
    "length": 1377,
    "tokens": 597
  },
  {
    "text": "96 25 65 - 0.4 - 5.7e-6 - 7A35T SVM i32 - -? 49 6 72 - 0.2 - 5.1e-6 - VGT CNN S [8] 5CSXC6 C-FCN i16 4.70K 0.0 - - 100 - - - 1.5e-1 7 Unclassified - CNN S [18] 7K325T LeNet-5 f32 - - - 33 100 - - - 2.3e-3 - F [113] ZU3EG SSD 0.25x i8 4.04G - 73 47 200 138 2.4 57.5 - 34.2 ZU3EG SSD 0.5x i8 15.6G - 73 55 200 150 2.6 57.7 - 9.6 ZU3EG SSD 1.0x i8 61.6G - - - 200 158 2.0 79.0 - 2.6 - [125] 7Z020 Lightweight CNN i8 4.60G - 70 25 100 - 3.3 - - 60 ML S [43] 7Z020 MLP f32 - - 26 1 - - 0.1 - - - - [117] 7Z020 WNS f32 - - - - - - - - - - FPGA: AMD XILINX FPGA names are stripped of leading XC to increase readability Mem: Uses On-Chip Memory, ? Memory Location unknown Latency: Latency of a single Pixel Model Name: RDBC Roller Dung Beetle Clustering, WNS Weightless Neural System Manuscript submitted to ACM 20 C√©dric L√©onard, Dirk Stober, and Martin Schulz with on-chip memory, pre-loading memory to the weight buffer, and amortizing the access over multiple classifications. The support for different layers comes at a small cost in computational performance, but allows implementing larger models that would otherwise not fit on the FPGA using a Specific approach. The average footprint of the 20 Flexible designs is 25ùëÄùêµcompared to 15ùëÄùêµfor the 10 Specific designs. Unlike Flexible designs, Specific designs only support a single network architecture. Deploying a different model requires re-implementation of the bitstream or is not supported.",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 1443,
    "tokens": 491
  },
  {
    "text": "Standard CL methods suffer from catastrophic forgetting, where previously learned knowledge deteriorates as new tasks are intro- duced. To address this, LightCL [116, 117] freezes the lower and middle layers of a pre-trained model while updating only higher layers, reducing memory consumption while preserving past knowledge. Additionally, task-aware dynamic masking (TDM) selectively retains critical parameters, balancing adaptation and retention. Decentralized Continual Learning. Continual learning in decentralized settings introduces additional challenges due to device heterogeneity and intermittent con- nectivity. Decoupled replay mechanisms, such as Chameleon [118], maintain separate short-term and long-term memory buffers, optimizing data retention while minimiz- ing resource usage. Further improvements are achieved through hardware-accelerated optimizations, as demonstrated in Chameleon-Accel [119, 120], which achieves up to 2.1 speedup over SLDA and 3.5 over Latent Replay, making CL feasible for onboard deployment. While CL enhances adaptability, it still operates under the assumption of rela- tively stable resource availability. However, in highly dynamic environments, onboard AI must continuously adjust its learning strategy based on fluctuating constraints, requiring adaptive learning techniques. 4.3 Dynamic Adaptation Under Resource Constraints: Adaptive Learning Adaptive learning extends continual learning by dynamically adjusting model behavior based on real-time changes in computational resources, task complexity, and environ- mental variations. In edge AI scenarios, where resources fluctuate, learning strategies must be both efficient and responsive. Real-Time Model Adjustment. IMPALA [121] and SEED RL [122] employ asyn- chronous updates and parallel experience collection, enabling models to efficiently learn from decentralized environments while reducing synchronization bottlenecks. Importance-weighted corrections ensure stability by mitigating inconsistencies in decentralized updates [123]. Resource-Aware Learning Strategies. Jin et al. [124] proposed a resource-aware optimization framework that formulates a non-linear mixed-integer program to dynam- ically allocate computational resources for learning tasks. Their method reduces communication overhead and optimizes onboard switching between edge devices, ensuring efficient adaptation in dynamic environments. Scalability and Fault Tolerance in Decentralized Systems. By prioritizing localized learning and reducing reliance on central servers, decentralized approaches enhance scalability, improve latency efficiency, and strengthen fault tolerance.",
    "source": "2505.08793v1_Onboard_Optimization_and_Learning_A_Survey.pdf",
    "length": 2658,
    "tokens": 485
  },
  {
    "text": "These results highlight the importance of eval- uating both functional correctness and resource optimization in complex design scenarios. HEART 25, May 26 28, 2025, Kumamoto, Japan C. Guo and T. Zhao Table 4: Design correctness of LLMs in generating Verilog code across different categories Cell format: pass synthesis OK but incorrect design synthesis error GPT-3.5 GPT-4 GPT-4o GPT-o1 Llama3.1 Qwen-max Qwen-plus Qwen2.5-coder Codestral turbo mini 405B 32B Combinational Logic 112 5 3 117 3 0 120 0 0 118 1 1 115 2 3 117 2 1 109 1 10 112 2 6 120 0 0 Finite State Machines 23 15 22 32 22 6 31 24 5 39 18 3 31 24 5 34 26 0 27 23 10 39 10 11 36 6 18 Mathematical Functions 13 19 43 6 39 30 36 10 29 46 24 5 7 6 62 26 27 22 20 26 29 5 8 62 0 3 72 Basic Arithmetic Ops 37 2 36 63 8 4 66 9 0 68 4 3 43 2 30 38 22 15 27 13 35 54 6 15 62 13 0 Bitwise Logic Ops 35 0 25 55 0 5 58 2 0 59 0 1 52 0 8 47 0 13 33 11 16 36 0 24 55 0 5 Pipelining 0 59 16 11 54 10 26 49 0 15 38 22 7 38 30 15 32 28 16 26 33 21 31 23 6 56 13 Polynomial Evaluation 19 3 53 69 0 6 74 1 0 68 5 2 58 6 11 55 2 18 28 5 42 65 7 3 69 6 0 Machine Learning 31 3 41 60 8 7 60 13 2 73 1 1 45 28 2 63 12 0 61 12 2 57 2 16 64 8 3 Financial Computing 9 23 28 21 22 17 29 13 18 20 20 20 11 21 28 28 15 17 15 12 33 16 7 37 17 23 20 Encryption 30 0 15 30 2 13 25 20 0 30 0 15 26 0 19 25 9 11 30 1 14 30 0 15 30 0 15 Physics 45 3 12 57 0 3 53 4 3 54 5 1 41 11 8 49 7 4 40 17 3 38 15 7 55 2 3 Climate 8 15 37 21 30 9 41 11 8 41 15 4 24 23 13 38 19 3 19 31 10 32 14 14 28 19 13 Number of wins 0 2 4 5 0 0 0 1 1 5.3 Resource Usage Table 5 presents the LUT usage results for the benchmark problems. In this table, problems where all LLMs yield identical LUT usage are excluded for brevity.",
    "source": "2503.08823v2_ResBench_Benchmarking_LLM-Generated_FPGA_Designs_w.pdf",
    "length": 1736,
    "tokens": 529
  },
  {
    "text": "Dataflow type defines the internal structure of PEs and the movement of data through the array. Dataflow types include weight- stationary (WS), input-stationary (IS), and output-stationary (OS) [36]. In the weight-stationary dataflow, the weight matrix is first pre-loaded into a systolic array by loading one row per cycle, then the input matrix propagates through the array from left to right. The outputs (results of MAC operation) propagate from top to bottom. In this scenario, weights are stationary and are kept in the systolic array during matrix multiplication execution. This dataflow maximizes the reuse of the weights. In the input-stationary dataflow, activations are pre-loaded in the systolic array in a similar manner as WS and kept in the systolic array during the matrix multiplication process. The data movement and structure of the systolic array are similar to WS dataflow. In the output-stationary dataflow, outputs are kept in the PEs of the systolic array, while weights and activations are streaming through the systolic array (Fig. 1). Weights are streaming from top to bottom, and activations (inputs) are streaming from left to right. Outputs are accumulated inside PEs (Fig. 2). This dataflow type does not need a pre-loading stage. An output-stationary systolic array is considered for this work. It is commonly used and allows high inputs and outputs reuse [37], [38] and on average has higher performance [33]. Previous works have shown that it is more fault tolerant [39]. The convolution operation is mapped to the systolic array by transforming it to matrix multiplication using im2col algorithm [40]. An input tensor is transformed into a (Hout Wout) (Hk Wk Cout) matrix and weights are transformed into a (Hk Wk Cin) Cout matrix, where Hout and Wout are the height and width of the output tensor, Hk and Wk are the UNDER REVIEW IN AN IEEE JOURNAL 3 Fig. 1: Dataflow in the output-stationary systolic array Fig. 2: Architecture of the individual PE in the output- stationary systolic array height and width of a kernel, Cin and Cout are the number of channels in the input and output tensor correspondingly.",
    "source": "2503.04426v1_FORTALESA_Fault-Tolerant_Reconfigurable_Systolic_A.pdf",
    "length": 2143,
    "tokens": 499
  },
  {
    "text": "This drawback can be the reason of the bad generalizability of CF. (2) Numeric tokenization: When numeric values (e.g., 0.95544) are fed into the tokenizer, they are split into multiple subword tokens (e.g., 0 , . , 9 , 5 , ...), di- luting their semantic meaning and making it harder for the model to learn numeric relationships. 3.2. Pure-Text Adjacency-Matrix Formulation (PM) PM represents circuit connections as an adjacency matrix for hypergraph, where rows and columns are indexed based on the vertex order given in the input.",
    "source": "2506.10235v1_LaMAGIC2_Advanced_Circuit_Formulations_for_Languag.pdf",
    "length": 533,
    "tokens": 133
  },
  {
    "text": "1. However, we can highlight one key difference. The exponent function hardware operators and floating-point multipliers are replaced by a single ExpMul unit which performs those two operations in a fused manner using integer only arithmetic and preparing the result in floating-point as needed by subsequent operations. V. EVALUATION Experimental evaluation aims to examine the impact of the new fused ExpMul hardware operators on real machine- learning applications and also highlight the achieved hardware savings. TABLE I PERFORMANCE OF GOOGLE S FLAN-T5 LLM MODEL FOR 10 BENCHMARKS OF THE GLUE DATASET [16]. Benchmarks STS-2 MNLI-m MNLI-mm QQP QNLI CoLA MRPC RTE WNLI STS-B Accuracy ( ) FP32 92.1 87.5 84.2 93.1 93.3 72.0 86.0 74.3 62.0 92.0 FP32-ExpMul 92.1 87.5 84.2 93.1 93.3 72.0 86.0 74.3 62.8 92.0 BF16 91.1 87.3 83.3 93.1 93.3 72.0 84.0 73.8 62.0 91.0 BF16-ExpMul 91.2 87.3 82.1 93.1 93.3 69.0 88.0 73.8 62.0 90.0 F1-score FP32 0.921 0.794 0.845 0.930 0.933 0.830 0.900 0.726 0.690 - FP32-ExpMul 0.931 0.800 0.833 0.930 0.920 0.840 0.900 0.738 0.710 - BF16 0.910 0.780 0.845 0.930 0.900 0.800 0.890 0.725 0.690 - BF16-ExpMul 0.910 0.780 0.833 0.930 0.900 0.780 0.920 0.733 0.690 - Fig. 2. The FlashAttention-2 kernel optimized with the proposed ExpMul operators. According to Alg. 3 each ExpMul operator involves clipping, integer shift-and-add operations and an exponent increment producing directly floating-point results without further dequantization.",
    "source": "2505.14314v2_Low-Cost_FlashAttention_with_Fused_Exponential_and.pdf",
    "length": 1466,
    "tokens": 502
  },
  {
    "text": "We further measure the per-message energy consumption of the IDS by monitoring the power rails on the ZCU104 board during IDS execution. Table III compares SecCAN s energy consumption per infer- ence against other works in the literature which have reported energy consumption. Our model consumes only 73.7¬µJ per message on average, a 3.1 improvement over our 2-bit quantised coupled accelerator IDS [20], achieved primarily through lower operating speed and the Q-IDS IP optimisa- tions. Compared to software IDS, SecCAN achieves a 34.1 reduction in energy consumption over an 8-bit variant of our model on a Jetson Xavier. Similarly, SecCAN consumes 24.1 and 28.4 lower energy per inference than a GRU- based IDS [12] and a convolutional autoencoder-model [21] on Jetson Xavier and Zynq Ultracale platforms respectively. While other competing schemes such as MTH-IDS [11] and iForest [9] do not explicitly report their energy consumption, they are re-implemented on Raspberry Pi-3 and Pi-4 devices for comparison (as in the original article). In comparison to these, SecCAN s energy consumption was found to be 17.6 and 5.3 lower per inference. It should be noted that energy measurement for SecCAN is for the extended CAN controller (SecCAN) as opposed to the IDS-only energy consumption reported and measured for competing schemes. IV. CONCLUSION In this letter, we explore a smart CAN controller archi- tecture that integrates a light-weight machine learning model as an IDS accelerator within the CAN controller (SecCAN controller) to detect the onset of intrusions from CAN traffic flow. By integrating the IDS within the controller, the proposed SecCAN controller can classify benign attack messages before the reception window is completed an approach that has not been explored before to the best of our knowledge. This flow allows safety mechanisms and intrusion prevention systems to be triggered as soon as the ECU receives a message, unlike conventional integration approaches where the IDS processing can begin only after the ECU has fully received the message from the CAN controller.",
    "source": "2505.14924v1_SecCAN_An_Extended_CAN_Controller_with_Embedded_In.pdf",
    "length": 2100,
    "tokens": 452
  },
  {
    "text": "Energy (J) [Our scheme] Energy (J) [SOTA] Video SOTA Energy Our Energy SOTAs (b) Energy consumption 0 15 30 45 60 0.00 5.00 10.00 15.00 20.00 Raw TMC13 CWIPC Intra-Only Intra-Inter-V1 Intra-Inter-V2 Raw TMC13 CWIPC Intra-Only Intra-Inter-V1 Intra-Inter-V2 Raw TMC13 CWIPC Intra-Only Intra-Inter-V1 Intra-Inter-V2 Raw TMC13 CWIPC Intra-Only Intra-Inter-V1 Intra-Inter-V2 Raw TMC13 CWIPC Intra-Only Intra-Inter-V1 Intra-Inter-V2 Raw TMC13 CWIPC Intra-Only Intra-Inter-V1 Intra-Inter-V2 Raw TMC13 CWIPC Intra-Only Intra-Inter-V1 Intra-Inter-V2 Redandblack Longdress Loot Soldier Andrew10 Phil10 Avg. PSNR (dB) Size(MB) Video Size(Geometry) Size(Attribute) PSNR (dB) 40dB (c) Compressed size PSNR Figure 8: Results: (a) Latency breakdown. (b) Energy consumption. (c): Compression efÔ¨Åciency. in parallel with the help of Morton codes, which speeds up the geometry compression by 37 ; 2). by utilizing the spatial locality with Morton code for attribute compression, instead of performing transforms through octree layers, in our proposal, only simple subtractions are needed for computing deltas. Further, all the points are processed in parallel; 3). we discard the entropy encoding for further speedup. Our Intra-Inter-V1 (Quality-oriented): This design favors the quality over compression efÔ¨Åciency, and it only takes 124ms (41ms for geometry compression, and 83ms for attribute compression), contributing to around 34 speedup w.r.t. CWIPC. This speedup comes from: 1). instead of searching the matched macro block from the entire space (the I-MB-Tree traversal as described in Sec.",
    "source": "PCcompress.pdf",
    "length": 1580,
    "tokens": 479
  },
  {
    "text": "In 2021 ACM IEEE 48th Annual International Symposium on Computer Architecture (ISCA). IEEE, Valencia, Spain, 1 14. [60] Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon. 2017. In-Datacenter Performance Analysis of a Tensor Processing Unit. In Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA 17). Association for Computing Machinery, New York, NY, USA, 1 12. [61] Shreeyam Kacker, Alex Meredith, Kerri Cahoy, and Georges Labr√®che. 2022. Machine Learning Image Processing Algorithms Onboard OPS-SAT. Small Satellite Conference (Aug. 2022). [62] Xudong Kang. 2017.",
    "source": "2506.03938v1_FPGA-Enabled_Machine_Learning_Applications_in_Eart.pdf",
    "length": 1645,
    "tokens": 493
  },
  {
    "text": "1.2.3 Cache Prefetchers Prefetching is a concept in which caches attempt to predict the next required address using some sort of algorithm or heuristic and request it prior to the CPU needing the address, reduc- ing the amount of stalling and speeding up the process of accessing memory. The most simple prefetching algorithm grabs the next line of addresses using the current address as a point of ref- erence. There are three different types of prefetchers: stride prefetching, spatial prefetching, and temporal prefetching. Temporal prefetching attempts to prefetch data by identifying temporal cor- relations in cache misses and replaying these misses when they are found again [16]. However, these prefetchers require large amounts of metadata to function and introduce an overhead [16]. In the context of an LLMs execution, a temporal prefetcher is unlikely to provide a significant 6 advantage to stride or spatial prefetching since an LLM lacks high temporal locality. Stride prefetching attempts to calculate the stride, or delta, of addresses and use this to predict what the next address will be. A simple implementation of this uses a stride prediction table to calculate the stride of memory accesses in arrays during looping [17]. Prefetchers in this category may find success in the LLMs repeated weight and KV cache accesses with each token, but the prefetcher must be sophisticated enough to determine these addresses and have enough confidence to start prefetching those addresses. If we notice a decrease in performance, the prefetcher is predicting the wrong deltas, as a misprediction is more expensive than making no guess. Berti, the winning prefetcher of the 3rd Data Prefetching Championship makes use of the idea the best timeliness will be found with the best delta, and that this varies from page to page. Berti has two sets of data that it captures: data used to make predictions and data used to evaluate confidence with a prediction [18]. Because confidence is determined by a series of matches dependent on the instruction pointer and accessed pages [18], Berti can avoid irregular memory patterns which prevents mispredictions. For pages that have not been accessed in some time, Berti uses a burst mode which prefetches every memory block from the first access in that page to the calculated stride [18].",
    "source": "2506.01827v1_Memory_Access_Characterization_of_Large_Language_M.pdf",
    "length": 2338,
    "tokens": 484
  },
  {
    "text": "0 : 0x40000000; for (int_fast32_t y_in_o 0; y_in_o 16; y_in_o ) { Increase number of Y dimension inner loop iterations to increase tile size uint32_t preload_index (((zo 4 z) 4) ((16 4) 16) y_in_o (16 16)) 16; Find correct scratchpad index to load B from preload(b_offset preload_index , res (y_in_o (16 16)) 16 preload_flag , 16, 16, 16, 16); compute_preloaded(a_index (z (16 16)) 16, (( uint32_t)0), 16, 16, 16, 16); }}} Store C matrix slice for (int_fast32_t y_in_o 0; y_in_o 16; y_in_o ) { Move out a larger tile in the Y dimension mvout( C[a_base_x ][ b_base_y 16 y_in_o], res (y_in_o 16 16) 16, 16, 16); Divide number of elements by 16 since accumulator is row -indexed }}} Figure 15: In-context learning example of tiling, provided during the code generation phase in Sec. 3.2. Inserted in the prompt only when the string \"tiling\" is detected in the plan generated in Phase 1. 19 Rules: 1. The rewritten program should be semantically equivalent to the original program 2. Limit the scope of the plan to the selected optimization 3. All code must be inside the test() function 4. Do not use C preprocessing directives ( ifdef , define , etc.) 5. If modifying loops , modify other related loop bounds and adjust address and index calculations to ensure the code is still correct 6. If increasing loaded tile size , ensure that data is spread throughout the scratchpad across all relevant dimensions 7. If loading across new dimensions , add the loop indices of those dimensions to scratchpad address calculations 8. If increasing loaded tile size , update preload and compute instructions to match the new data layout 9. If increasing loaded tile size , update base scratchpad addresses to fit new tile size Figure 16: The list of rules provided during both the planning and code implementation phases, as described in Sec. 3.2.",
    "source": "2505.18574v2_Autocomp_LLM-Driven_Code_Optimization_for_Tensor_A.pdf",
    "length": 1834,
    "tokens": 492
  },
  {
    "text": "This allows the MM2IM Mapper to support partitioning of the data in a tiled manner, since the rowid can be initialized to the starting row of the output tile instead of the starting row of the output matrix. Overall, the MM2IM Mapper generates the compute and output mappings only once per row, and each map is broadcast to all PMs, thus saving hardware Algorithm 2: MM2IM Mapper 1 rowid load(rowid), rowwidth load(rowwidth) 2 foreach r in MMrows do 3 hpad paddingtop (S (rowid rowwidth)) 4 wpad paddingleft (S (rowid rowwidth)) 5 imdex hpad Ow wpad 6 col 0 7 foreach ih in Ks do 8 foreach iw in Ks do 9 if (ih hpad 0 ih hpad Oh 10 iw wpad 0 iw wpad Ow) then 11 PMs cmap.broadcast write(col) 12 PMs omap.broadcast write(imdex) 13 col , imdex 14 imdex Ow Ks 15 rowid resources and additional computational overhead. Note that the MM2IM Mapper is configured dynamically through the 0x01 opcode, and can support any shape of TCONV layers. F. Computational Flow Finally, we discuss in more detail the flow of computation within each PM and how each PM is able to calculate an output channel across multiple steps. Figure 5 provides an example of an input feature map being processed through a single filter to produce a complete output channel over N steps (N IH); this happens within each PM. The inputs on the left of each step represent the input row (presented as a column in Figure 5), which will be processed by all PMs during that step. Each input row contains Iw Ic data elements; this data is fetched from the Row Buffer after each step, as seen in Figure 3. Before starting the computations, each PM is preloaded with a single filter (shown in the middle); the filter contains Ks Ks Ic data elements, which are stored in the PM s local buffer. Note that the color of filter columns represents the intermediate outputs they produce within that step, and the weight data remains the same as the filter does not change between steps.",
    "source": "2507.07683v1_Accelerating_Transposed_Convolutions_on_FPGA-based.pdf",
    "length": 1936,
    "tokens": 480
  },
  {
    "text": "2018. Fine-Grained Energy and Performance Profiling framework for Deep Convolutional Neural Networks. arXiv:1803.11151 [cs.PF] [118] Bita Darvish Rouhani, Azalia Mirhoseini, and Farinaz Koushanfar. 2016. DeLight: Adding Energy Dimension To Deep Neural Networks. In Proceedings of the 2016 International Symposium on Low Power Electronics and Design (San Francisco Airport, CA, USA) (ISLPED 16). Association for Computing Machinery, New York, NY, USA, 112 117. doi:10.1145 2934583.2934599 [119] Amelie Royer and Christoph H. Lampert. 2020. A Flexible Selection Scheme for Minimum-Effort Transfer Learning. CoRR abs 2008.11995 (2020). arXiv:2008.11995 [120] Sara Sabour, Nicholas Frosst, and Geoffrey E. Hinton. 2017. Dynamic Routing Between Capsules. CoRR abs 1710.09829 (2017). arXiv:1710.09829 Energy-Aware Deep Learning on Resource-Constrained Hardware 21 [121] Adnan Sabovic, Michiel Aernouts, Dragan Subotic, Jaron Fontaine, Eli De Poorter, and Jeroen Famaey. 2023. Towards energy-aware tinyML on battery-less IoT devices. Internet of Things 22 (2023), 100736. doi:10.1016 j.iot.2023.100736 [122] Adnan Sabovic, Ashish Kumar Sultania, Carmen Delgado, Lander De Roeck, and Jeroen Famaey. 2022. An Energy-Aware Task Scheduler for Energy-Harvesting Batteryless IoT Devices. IEEE Internet of Things Journal 9, 22 (2022), 23097 23114. doi:10.1109 JIOT.2022.3185321 [123] Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. 2020. Do Adversarially Robust ImageNet Models Transfer Better? CoRR abs 2007.08489 (2020). arXiv:2007.08489 [124] Eric Samikwa, Antonio Di Maio, and Torsten Braun. 2022.",
    "source": "2505.12523v1_Energy-Aware_Deep_Learning_on_Resource-Constrained.pdf",
    "length": 1619,
    "tokens": 490
  },
  {
    "text": "However, GNNs for clock meshes are still challenged by the over-smoothing problem with deep layers. We model the SPICE netlist of clock meshes as an undi- rected graph as shown in Figure 2. The SPICE netlist clock mesh is a resistor-capacitor (RC) network driven by mesh buffers. We model the interconnect as Wire nodes (W) in the graph. The clock sink pins are modeled as load capacitance to ground which we represent as Sink nodes (S). The mesh buffers are Buffer nodes (B) that connect directly to mesh wires. Each clock sink is connected to a mesh wire through a stub wire represented by an additional W node and a Tap node (T). The exemplary graph representation of a single mesh wire with two sinks and two buffers is shown in Figure 2. One challenge with the above model, and many GNN mod- els in general, is that the number of hops in the graph between buffers and sinks depend on the mesh implementation and the resulting graph topology. If there are many sinks connected to a mesh wire, the mesh wire gets divided into smaller segments with additional T and W nodes. In Figure 2, for example, the buffer B0 drives both the sinks S0 and S1 with S0 being four hops away, whereas S1 is six hops away because it must pass buf1_in B1 T1 S1 T0 buf0_in S0 B0 B0 W1 T0 W2 S0 W3 T1 W4 S1 B1 X0 X1 X2 X3 X4 B0 W1 T0 W2 S0 W3 T1 W4 S1 B1 auxiliary connections W5 W5 Fig. 2. Left: A single mesh wire, its buffers and its graph representation. Right: Graph representation after adding buffer contention (X0) and sink driving (X1 X4) auxiliary connections. through the intermediate T0 and T1 nodes and the divided W1 and W3 node resulting from the split mesh wire. Any GNN model would require an increasingly deeper number of layers to allow message passing through deep graphs, yet so many deep layers would also result in well-known over-smoothing of features in connected regions [12]. While it is possible to reduce the number of nodes by using edge features, most GNN models are primarily node centric. We tried edge feature representations and saw inferior results.",
    "source": "2507.05681v1_GATMesh_Clock_Mesh_Timing_Analysis_using_Graph_Neu.pdf",
    "length": 2067,
    "tokens": 501
  },
  {
    "text": "This approach improves generalization and enhances both the visual and contextual capabilities of the network. Later, Sun et al. [55] proposed a method where the temperature (scaling factor) of both the teacher and student models, as well as the sample logits, is varied. They introduced a Z-score preprocessing method that standardizes logits, en- abling the student model to better learn the relationships from the teacher model. 3) Layer Fusion: In CNNs, layers such as convolution, batch normalization, activation, and pooling are typically ex- ecuted in sequence. However, researchers have shown that re- ordering or merging certain layers can significantly reduce data transfer times and computational complexity when executing the CNN models in hardware. Alwani et al. [56] introduced a pyramid-shaped multi-layer sliding window that processes input feature maps, allowing multiple layer results to be computed in advance. Abtahi et al. [57] proposed integrating Batch Normalization (BN) layer parameters directly into the preceding convolutional layer during inference, as BN is a lin- ear operation with fixed parameters. This integration reduces the number of operations. Syafeeza et al. [58] demonstrated that performing max pooling before ReLU activation helps reduce the computational workload on the ReLU layer. Liu et al. [59] applied layer fusion to the Fast Fourier Transform (FFT) approach, combining pooling and convolutional layers to eliminate unnecessary operations and improve efficiency. C. Computation Reduction Computation reduction involves simplifying complex op- erations in neural networks, typically leveraging mathemat- ical theorems. Techniques like the Fast Fourier Transform 8 (FFT) and the Winograd algorithm can significantly reduce the computational complexity of convolutional layers. Further advancements, such as fully spectral CNNs, boost performance by transforming each layer into the frequency domain. 1) FFT Approach: The FFT approach accelerates convo- lution operations by using the convolution theorem, which states that spatial domain convolutions are equivalent to point- wise multiplications in the frequency domain. By transforming both the input data and weights into the frequency domain with FFT, performing element-wise multiplication, and then converting the result back to the spatial domain through Inverse FFT (IFFT), convolution operations can be executed more efficiently. Mathieu et al.",
    "source": "2505.13461v1_FPGA-based_Acceleration_for_Convolutional_Neural_N.pdf",
    "length": 2450,
    "tokens": 488
  },
  {
    "text": "While the FPGA-implemented InstantFT consumes 0.24W more power (3.79W 4.03W) compared to only using ARM Cortex-A53, it achieves 16.32x higher energy-efficiency thanks to the attained speedup. 5.5 Forward Cache Size The size of Forward Cache depends on both the number of samples in a given dataset and model architecture. On Rot{MNIST, FMNIST} and SVHN, the full-precision cache requires 7.33 524.52MB of memory, which is brought down to 1.02 72.89MB by NF4 quantization (7.20x reduction) without compromising accuracy (Fig. 7a). 5.6 FPGA Resource Utilization Fig. 7c shows the FPGA resource utilization of InstantFT on Kria KV260. The design effectively uses on-chip memory resources to store (frozen LoRA) parameters, activations, and gradients, thereby minimizing external memory accesses and improving overall performance. The on-chip memory can be further saved by applying low-bit quantization to parameters and gradients as well. In addition to LeNet-5, larger-scale networks can be fine-tuned on KV260, thanks to the low utilization of logic resources (LUTs DSPs). 6 Summary This paper presents InstantFT, a new ultra-fast fine-tuning method that enables edge applications to quickly adapt to data distribution shifts within subseconds. InstantFT leverages trainable adapters directly connected to the output layer, significantly saving the computational cost of backpropagation. In addition, it introduces 4-bit quantized Forward Cache to eliminate redundant forward passes of the frozen base network. The proposed FPGA-based design fully 7 InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models A PREPRINT exploits the high parallelism and simplified computation flow of the adapters. Experimental results demonstrate that InstantFT achieves accuracy comparable to the original LoRA in just 0.36s on Xilinx Kria KV260, while being 17.4x faster and 16.3x more energy-efficient than ARM Cortex-A53. While InstantFT is currently evaluated on small-scale networks, we aim to extend its application to large-scale models including LLMs.",
    "source": "2506.06505v1_InstantFT_An_FPGA-Based_Runtime_Subsecond_Fine-tun.pdf",
    "length": 2055,
    "tokens": 485
  },
  {
    "text": "- Offline: all training data is available during model creation. The model is not retrained regularly. - Hybrid: a small set of training data is used to initialise the model, and new information is regularly integrated during the model s use. Figure 7. The number of papers by learning type. Figure 7 shows the distribution of work by learning type. Online learning is synonymous with reinforcement learning and genetic algorithms that require feedback to guide their learning. These approaches trade weaker initial performance for the continuous integration of new information. Conversely, offline learning favours techniques where large amounts of information is available, the cost of errors is high, or training times are long relative to the time to collect new information. Hybrid learning is a trade-off between online and offline learning. One example compared online and offline learning, finding online learning had lower overall accuracy, but a lower retraining time made it more scalable compared to offline learning. In the literature, many offline learning methods used training data obtained through random based test generation [41]. Since random-based methods are common in microelectronic device verification, there is likely to be an abundance of this type of data. However, as with other fields of ML, learning requires a balanced, unbiased, dataset. Randomly generated data sets for a DUV may not achieve this if, for example, some coverage points are hit substantially more regularly than others. Balancing datasets is discussed, but in general the sampled literature does not examine how information collection may affect the machine learning performance. Online or hybrid methods retrained regularly in small batches were commonly used when selecting constraints or DUV inputs based on novelty. Novelty is measured against past examples [115]. A novel example may not be novel over time after more examples have been seen, necessitating regular retraining to keep the machine learning assessment relevant. Termed concept drift in [45], the choice of when to deploy a model and how to retrain can be important. Once deployed, the learner influences the future examples it will be retrained on, potentially preventing sufficient exploration of the DUV s states to be verified, leading to performance that decreases over time. Overall, online is the most common learning approach. In an industrial design and verification process, design changes and continuous production of simulation data mean that all machine learning applications would benefit from integrating new information. The question is how and when to retrain and any associated trade-off between accuracy and training time. This question is not commonly addressed in the literature.",
    "source": "2503.11687v1_Review_of_Machine_Learning_for_Micro-Electronic_De.pdf",
    "length": 2767,
    "tokens": 499
  },
  {
    "text": "In Design Automation Conference (DAC), 2022. [104] Zhengyuan Shi, Hongyang Pan, Sadaf Khan, Min Li, Yi Liu, Junhua Huang, Hui-Ling Zhen, Mingxuan Yuan, Zhufei Chu, and Qiang Xu. DeepGate2: Functionality-aware circuit representation learning. In International Conference on Computer-Aided Design (ICCAD), 2023. [105] Ziyang Zheng, Shan Huang, Jianyuan Zhong, Zhengyuan Shi, Guohao Dai, Ningyi Xu, and Qiang Xu. Deepgate4: Efficient and effective representation learning for circuit design at scale. In International Conference on Learning Representations (ICLR), 2025. [106] Chenhui Deng, Zichao Yue, Cunxi Yu, Gokce Sarar, Ryan Carey, Rajeev Jain, and Zhiru Zhang. Less is more: Hop-wise graph attention for scalable and generalizable learning on circuits. In Design Automation Conference (DAC), 2024. [107] Jiawei Liu, Jianwang Zhai, Mingyu Zhao, Zhe Lin, Bei Yu, and Chuan Shi. Polargate: Breaking the functionality representation bottleneck of and-inverter graph neural network. In International Conference on Computer-Aided Design (ICCAD), 2024. [108] Sadaf Khan, Zhengyuan Shi, Min Li, and Qiang Xu. Deepseq: Deep sequential circuit learning. In Design, Automation and Test in Europe Conference and Exhibition (DATE), 2024. [109] Sadaf Khan, Zhengyuan Shi, Ziyang Zheng, Min Li, and Qiang Xu. Deepseq2: Enhanced sequential circuit learning with disentangled representations. In Asia and South Pacific Design Automation Conference (ASP-DAC), 2025. [110] Ziyi Wang, Chen Bai, Zhuolun He, Guangliang Zhang, Qiang Xu, Tsung-Yi Ho, Bei Yu, and Yu Huang. Functionality matters in netlist representation learning. In Design Automation Conference (DAC), 2022. [111] Ziyi Wang, Chen Bai, Zhuolun He, Guangliang Zhang, Qiang Xu, Tsung-Yi Ho, Yu Huang, and Bei Yu.",
    "source": "2504.03711v1_A_Survey_of_Circuit_Foundation_Model_Foundation_AI.pdf",
    "length": 1758,
    "tokens": 486
  },
  {
    "text": "Background and Related Work Very basic, just the outline, compact and make robust 2.1. Energy Harvesting Wireless Sensor Networks Energy harvesting wireless sensor networks (EH-WSNs) have emerged as a sustainable solution for long-term envi- ronmental monitoring, infrastructure surveillance, and IoT applications (?). By harnessing ambient energy sources such as solar, thermal, or kinetic energy, EH sensors can operate indefinitely without the need for battery replace- ment or external power supplies. However, the intermittent and unpredictable nature of harvested energy introduces significant challenges in maintaining reliable and consistent network performance (?). The unreliability of individual EH sensors, due to fluctua- tions in energy availability, necessitates the deployment of a large number of inexpensive and potentially unreliable de- vices to ensure network robustness. This redundancy allows for continuous operation despite individual sensor failures or downtime. However, it also introduces complexities in coordinating sensor activities, managing energy resources, and ensuring efficient data collection and processing (?). 2.2. Participation Strategies in EH-WSNs Efficient participation strategies are critical in EH-WSNs to optimize network performance while conserving limited energy resources. Traditional approaches often assume con- tinuous participation of all sensors, which is impractical in energy-constrained environments (?). Some methods pro- pose selecting a subset of sensors based on energy levels or predefined schedules (? ), but these can lead to suboptimal performance by not considering the sensors data quality or potential future contributions. Several works have explored adaptive participation strate- gies that consider energy harvesting rates, energy consump- tion patterns, and application-specific requirements (?). These strategies aim to balance energy expenditure with the need for timely and accurate data, often using heuristic or optimization-based approaches. However, they may not fully exploit the potential for collaboration among sensors or account for the strategic interactions inherent in decen- tralized networks. 2 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 2.3. Game-Theoretic Models in Sensor Networks Game theory provides a powerful framework for modeling and analyzing strategic interactions in distributed systems, including sensor networks (?).",
    "source": "ParticipationGamesICML25.pdf",
    "length": 2600,
    "tokens": 480
  },
  {
    "text": "In such a scenario, Kraken batches these requests ( _ùëëùëíùëôùëéùë¶ùëíùëë_ùëüùëíùëûùë¢ùëíùë†ùë°ùë†in Algorithm 2) onto a newly-spawned container(s) (Algorithm 2 c ). This is because requests that have to wait longer than the cold start would be served faster at a newly created container than by waiting at an overloaded container. Similarly, for stages where container overprovisioning has occurred, the RS 7 gradually scales down its allocated containers to the appropriate number, if its Function Idler module 7b detects excess containers for serving the current load (Algorithm 2 a). Thus, the RS 7 , in combination with the PWS 2 and re- quest batching 5 , helps Kraken remain SLO compliant while using minimum resources. 5 Implementation and Evaluation We have implemented a prototype of Kraken using open- source tools for evaluation with synthetic and real-world traces. The details are described below. 5.1 Prototype Implementation Kraken is implemented primarily using Python and Go on top of OpenFaaS [11], an open-source serverless platform.",
    "source": "kraken.pdf",
    "length": 1024,
    "tokens": 252
  },
  {
    "text": "2113 2122 (2022). IEEE [173] Zhou, M., Gao, X., Wu, J., Liu, K., Sun, H., Li, L.: Investigating white- box attacks for on-device models. In: Proceedings of the IEEE ACM 46th International Conference on Software Engineering, pp. 1 12 (2024) [174] Lattner, C., Amini, M., Bondhugula, U., Cohen, A., Davis, A., Pienaar, J., Riddle, R., Shpeisman, T., Vasilache, N., Zinenko, O.: Mlir: Scaling compiler infrastructure for domain specific computation. In: 2021 IEEE ACM Interna- tional Symposium on Code Generation and Optimization (CGO), pp. 2 14 (2021). IEEE [175] Majumder, K., Bondhugula, U.: Hir: An mlir-based intermediate representation for hardware accelerator description. In: Proceedings of the 28th ACM Inter- national Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4, pp. 189 201 (2023) [176] Akkisetty, P.K. : An overview of ai platforms, frameworks, libraries, and proces- sors. Model Optimization Methods for Efficient and Edge AI: Federated Learning Architectures, Frameworks and Applications, 43 55 (2025) [177] Tabanelli, E., Tagliavini, G., Benini, L.: Dnn is not all you need: Parallelizing non-neural ml algorithms on ultra-low-power iot processors. ACM Transactions on Embedded Computing Systems 22(3), 1 33 (2023) [178] Lee, J., Kang, S., Lee, J., Shin, D., Han, D., Yoo, H.-J. : The hardware and algorithm co-design for energy-efficient dnn processor on edge mobile devices. IEEE Transactions on Circuits and Systems I: Regular Papers 67(10), 3458 3470 (2020) [179] Zhang, X., Li, Y., Pan, J., Chen, D.: Algorithm accelerator co-design and co- search for edge ai. IEEE Transactions on Circuits and Systems II: Express Briefs 69(7), 3064 3070 (2022) 36",
    "source": "2505.08793v1_Onboard_Optimization_and_Learning_A_Survey.pdf",
    "length": 1715,
    "tokens": 497
  },
  {
    "text": "Comparisons on loop code and ReRAM activation with tile- size activation over full-size activation. (a) The original MAC codes; (b) The kernel loops are mapped to a full-size ReRAM activation; (c) The tiled MAC codes; (d) The tiled kernel loops are mapped to a tiled-size ReRAM activation If we tentatively use loop tiling to decompose the MAC operations at the kernel level as shown in Figure 2 and perform the MAC operations on the ReRAM tile one by one sequentially, the system can achieve continuous progress under lower power supply. This is because the starting power requirement of the RCA is reduced, and the system can thus get through power failures and translate even the low input energy into forward progress. If only one tile is activated to perform the MAC operations at one time, the system can still make progress during time windows of power cycles PC2, PC4, PC5 and PC8 under limited power budget, as depicted in Table II and Figure 1. The annotations indicate the consumed power (¬µW), tile size and duplication count (e.g., 25x2x1) and power efÔ¨Åciency. With the resilient activation approach supported by loop tiling and ReRAM duplication, it can be seen that the power exploitation is increased from an average of 180¬µW to 330¬µW, and the throughput is increased by 85.7 . Note that the partial activation of computation cells can be realized by partially activating the peripheral circuits of the corresponding rows and columns in the ReRAM crossbar. This resilient activation approach can effectively combat Nonideal scenario 1. The underlying reason of encountering so many power failures in the case of conventional working mode is that the power threshold of the system to remain alive is set too high. With the loop tiling technique, the power failure threshold can be dropped to the requirements of the minimum activation tile of a ReRAM. With this, the RCA can be active in a very large power range and Ô¨Ånd more opportunities to make execution progress. Further, if the power supply is larger than the starting power threshold of one entire ReRAM, we can even arrange multiple ReRAMs to work in a parallel fashion, as seen in power cycles PC6, PC7 and PC8. Parallel computations across multiple ReRAMs and loop tiling-based computation for each ReRAM are orthogonal optimizations.",
    "source": "ResiRCA.pdf",
    "length": 2308,
    "tokens": 497
  }
]