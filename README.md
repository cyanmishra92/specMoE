# Enhanced Pre-gated MoE for RTX 3090

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)](https://pytorch.org/)
[![CUDA](https://img.shields.io/badge/CUDA-11.8+-green.svg)](https://developer.nvidia.com/cuda-toolkit)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

An enhanced implementation of speculative gating for Mixture of Experts (MoE) models, specifically optimized for RTX 3090 and similar GPUs. This project extends the [ISCA'24 Pre-gated MoE paper](https://arxiv.org/pdf/2308.12066) with novel speculation strategies, learnable neural models, and memory optimizations.

## ğŸš€ Quick Start

### Prerequisites
- NVIDIA RTX 3090 (or similar GPU with 16GB+ VRAM)
- Python 3.8+
- CUDA 11.8+
- PyTorch 2.0+

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd specMoE

# Install dependencies
pip install torch transformers accelerate datasets
pip install numpy matplotlib seaborn psutil tqdm
```

### Quick Demo

Run the complete pipeline with 128-expert model:

```bash
# Complete pipeline (recommended - uses 128-expert Switch Transformer)
python scripts/pipelines/run_working_pipeline.py --use-128-experts

# Quick test with smaller model
python scripts/pipelines/run_working_pipeline.py
```

Or run individual components:

```bash
# Step 1: Collect routing traces from Switch Transformers
python scripts/collection/collect_robust_traces.py --traces 3000

# Step 2: Analyze expert usage patterns
python scripts/analysis/comprehensive_expert_analysis.py

# Step 3: Visualize expert traces
python scripts/analysis/visualize_expert_traces.py

# Step 4: Train speculation models
python scripts/training/improved_speculation_training.py

# Step 3: Test individual approaches
python scripts/evaluation/test_individual_approaches.py

# Step 4: Compare all approaches
python scripts/evaluation/compare_all_approaches.py
```

## ğŸ“ Project Structure

```
specMoE/
â”œâ”€â”€ ğŸ“š Core Framework
â”‚   â”œâ”€â”€ models/                     # MoE model implementations
â”‚   â”‚   â”œâ”€â”€ small_switch_transformer.py
â”‚   â”‚   â””â”€â”€ pretrained_switch_model.py
â”‚   â”œâ”€â”€ gating/                     # Speculation engines
â”‚   â”‚   â””â”€â”€ speculation_engine.py   # Heuristic + learnable speculation
â”‚   â”œâ”€â”€ training/                   # Neural model training
â”‚   â”‚   â”œâ”€â”€ learnable_gating_models.py
â”‚   â”‚   â”œâ”€â”€ gating_trainer.py
â”‚   â”‚   â””â”€â”€ gating_data_collector.py
â”‚   â”œâ”€â”€ memory/                     # Memory management
â”‚   â”‚   â””â”€â”€ adaptive_memory_manager.py
â”‚   â””â”€â”€ utils/                      # Utilities
â”‚       â””â”€â”€ device_profiler.py
â”œâ”€â”€ ğŸ› ï¸ Scripts (Working Pipeline)
â”‚   â”œâ”€â”€ collection/                 # Data collection
â”‚   â”‚   â””â”€â”€ collect_robust_traces.py      # 128-expert traces from Switch Transformers
â”‚   â”œâ”€â”€ analysis/                   # Expert analysis & visualization
â”‚   â”‚   â”œâ”€â”€ comprehensive_expert_analysis.py  # Statistical analysis
â”‚   â”‚   â”œâ”€â”€ visualize_expert_traces.py         # Trace visualization
â”‚   â”‚   â””â”€â”€ create_small_dataset.py            # Small experimental dataset
â”‚   â”œâ”€â”€ training/                   # Model training
â”‚   â”‚   â””â”€â”€ improved_speculation_training.py   # Main training script
â”‚   â”œâ”€â”€ benchmarks/                 # Performance benchmarks
â”‚   â”‚   â”œâ”€â”€ memory_transfer_benchmark.py       # Memory transfer analysis
â”‚   â”‚   â””â”€â”€ run_memory_benchmarks.py           # Comprehensive benchmarks
â”‚   â””â”€â”€ visualization/              # Plot generation
â”‚       â””â”€â”€ latency_analysis_plots.py          # Publication-quality plots
â”œâ”€â”€ ğŸ¯ Applications
â”‚   â”œâ”€â”€ main.py                     # Custom model demo
â”‚   â””â”€â”€ main_pretrained.py          # Pre-trained model demo
â”œâ”€â”€ ğŸ“Š Data & Results
â”‚   â”œâ”€â”€ routing_data/               # Collected routing traces & statistics
â”‚   â”‚   â”œâ”€â”€ comprehensive_expert_statistics.json  # Layer-wise statistics
â”‚   â”‚   â”œâ”€â”€ expert_statistics_3d.json             # 3D structure (layerâ†’expertâ†’freq)
â”‚   â”‚   â””â”€â”€ sample_trace_paths.json               # Sample expert sequences
â”‚   â”œâ”€â”€ benchmark_results/          # Performance benchmarks
â”‚   â”œâ”€â”€ plots/                      # Generated visualizations
â”‚   â””â”€â”€ simulation_results/         # Memory simulation results
â”œâ”€â”€ ğŸ“– Documentation
â”‚   â”œâ”€â”€ README.md                   # This file
â”‚   â”œâ”€â”€ docs/
â”‚   â”‚   â”œâ”€â”€ COLLECTION_GUIDE.md     # Trace collection guide
â”‚   â”‚   â”œâ”€â”€ EXPERT_ANALYSIS_GUIDE.md # Expert analysis guide
â”‚   â”‚   â”œâ”€â”€ FINAL_PERFORMANCE_REPORT.md # Performance analysis
â”‚   â”‚   â””â”€â”€ MEMORY_MANAGEMENT_GUIDE.md  # Memory optimization guide
â””â”€â”€ ğŸ—ƒï¸ Archive
    â””â”€â”€ archive_unused/            # Unused/broken scripts
```

## ğŸ§  Speculation Modes

| Mode | Description | Performance | Use Case |
|------|-------------|-------------|----------|
| `none` | No speculation (baseline) | Baseline | Comparison |
| `layer_minus_1` | Previous layer prediction | ~13% accuracy | Simple |
| `multi_layer` | Multi-layer history | ~13% accuracy | Standard |
| `adaptive` | Confidence-based adaptation | ~10% accuracy | Dynamic |
| `learnable` | **Trained neural models** | **TBD** | **Best** |

## ğŸ”¬ Key Features

### Enhanced Speculation Engine
- **Multi-layer Lookahead**: Uses L-3, L-2, L-1 to predict expert needs for L+1
- **Learnable Models**: Neural networks trained on real MoE routing patterns
- **Confidence-based Adaptation**: Dynamically adjusts speculation aggressiveness
- **Pattern Learning**: Learns expert transition matrices over time

### Memory Management
- **Dynamic Compression**: INT8 (4x) and INT4 (8x) quantization
- **Hierarchical Caching**: GPU â†’ Unified â†’ Compressed storage tiers
- **Smart Prefetching**: Loads experts based on speculation confidence
- **RTX 3090 Optimization**: Tuned for 24GB VRAM constraints

### Data Collection
- **Real MoE Traces**: Extracts routing from Switch Transformers
- **128-Expert Support**: Works with `google/switch-base-128`
- **Diverse Datasets**: WikiText, SQuAD, GLUE for training data
- **Robust Data Splits**: Prevents overfitting with proper train/test separation

## ğŸ“Š Performance Results

### Current Status (6,000 traces collected)
- **Data**: 6,000 routing samples from Switch Transformer
- **Training**: Learnable models with 30% loss reduction
- **Models**: Multiple architectures (contextual, transformer, hierarchical)
- **Hardware**: Optimized for RTX 3090 (24GB VRAM)

### Speculation Accuracy (Heuristic Methods)
```
Method              Top-1 Acc   Top-2 Acc   Confidence
Layer-Minus-1       12.0%       26.6%       Stable
Multi-Layer         13.2%       25.2%       Best
Adaptive            9.6%        25.4%       Variable
Learnable           TBD         TBD         High
```

## ğŸ¯ Usage Examples

### Complete Pipeline
```bash
# Run everything with 128-expert model (recommended)
python scripts/pipelines/run_working_pipeline.py --use-128-experts

# Quick test pipeline
python scripts/pipelines/run_working_pipeline.py
```

### Individual Components
```bash
# Collect traces from 128-expert Switch Transformer
python scripts/collection/collect_robust_traces.py

# Train with proper data splits (no overfitting)
python scripts/training/proper_train_test.py

# Test all speculation approaches
python scripts/evaluation/test_individual_approaches.py

# Comprehensive comparison
python scripts/evaluation/compare_all_approaches.py
```

### Demo Applications
```bash
# Custom model with speculation
python main.py --mode demo --speculation-mode multi_layer

# Pre-trained Switch Transformer
python main_pretrained.py --mode demo --pretrained-model google/switch-base-8

# Compare all modes
python main.py --mode compare
```

## ğŸ”§ Configuration

### Speculation Parameters
- **Confidence Threshold**: 0.7 (optimal for RTX 3090)
- **History Length**: 4 layers
- **Top-K Experts**: 2 concurrent experts
- **Memory Strategy**: Adaptive based on available VRAM

### Training Configuration
- **Batch Size**: 16 (RTX 3090 optimized)
- **Learning Rate**: 3e-4
- **Epochs**: 25
- **Validation Split**: 20%
- **Mixed Precision**: Enabled

## ğŸ“ˆ Benchmarking

Check current status:
```bash
python scripts/check_current_status.py
```

Expected output:
```
âœ… routing_data/proper_traces.pkl (188 MB, 6,000 samples)
âœ… trained_models/simple_speculation_model.pt
âœ… GPU Memory: 204 MB / 24576 MB (0.8% - idle)
```

## ğŸ› ï¸ Development

### Adding New Features

1. **New Speculation Strategy**:
   - Add to `gating/speculation_engine.py`
   - Test with `scripts/evaluation/test_individual_approaches.py`

2. **New Model Architecture**:
   - Add to `training/learnable_gating_models.py`
   - Train with `scripts/training/proper_train_test.py`

3. **New Collection Method**:
   - Add to `scripts/collection/`
   - Follow patterns in `collect_robust_traces.py`

### Testing
```bash
# Test individual components
python scripts/evaluation/test_individual_approaches.py

# Full pipeline test
python scripts/pipelines/run_working_pipeline.py

# Status check
python scripts/check_current_status.py
```

## ğŸ” Troubleshooting

### Common Issues

**CUDA Out of Memory**
```bash
# Use smaller batch size or enable compression
python scripts/training/proper_train_test.py --batch-size 8
```

**No traces found**
```bash
# Collect traces first (will take time for 128-expert model)
python scripts/collection/collect_robust_traces.py
```

**Low speculation accuracy**
```bash
# Check if learnable models are trained
python scripts/training/proper_train_test.py
```

### Performance Tips
1. **Use 128-expert model** for best diversity in traces
2. **Allow time** for trace collection (large model)
3. **Monitor GPU memory** during training
4. **Use proper train/test splits** to avoid overfitting

## ğŸ“š Research Applications

This codebase supports research in:
- **MoE Efficiency**: Memory-constrained inference
- **Speculation Algorithms**: Neural vs heuristic prediction
- **Hardware Optimization**: GPU-specific tuning
- **Model Analysis**: Expert usage patterns

## ğŸ“– Citation

```bibtex
@misc{enhanced_pregated_moe_2025,
  title={Enhanced Pre-gated MoE for Small GPUs: Advanced Speculation and Memory Optimization},
  author={Cyan Subhra Mishra},
  year={2025},
  note={Extension of ISCA'24 Pre-gated MoE with learnable speculation for RTX 3090},
  url={https://github.com/your-repo/enhanced-pregated-moe}
}
```

## ğŸ¤ Contributing

We welcome contributions! See `docs/CONTRIBUTING.md` for guidelines.

Areas of interest:
- New speculation strategies
- Hardware optimizations
- Model architectures
- Evaluation methods

## ğŸ“„ License

MIT License - see `docs/LICENSE` for details.

---

**Ready to use!** Start with:
```bash
python scripts/pipelines/run_working_pipeline.py --use-128-experts
```